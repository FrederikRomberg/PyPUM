{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Demand for Cars with the IPDL model\n",
    "\n",
    "In this notebook, we will explore the dataset used in\n",
    "Goldberg & Verboven (2005). We will estimate the IPDL Model\n",
    "model given the available data using the functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "from numba import jit\n",
    "import sparse as sp\n",
    "\n",
    "# Files\n",
    "import Logit_file as logit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "The dataset consists of approximately 110 vehicle makes per year in the period 1970-1999 in five european markets (Belgium, France, Germany, Italy, and the United Kingdom). Furthermore, the data contains information on various characteristics of the makes such as sales, prices, horse power, weight and other physical car characteristics. Also these characteristics may vary across markets. \n",
    "\n",
    "A observation in our analysis will be a market in a given year such that e.g. the French car market in 1995 counts as a single observation. If $Y = 30$ is the number of years, and $M = 5$ is the number of country-level markets, we thus have $T=Y\\cdot M = 150$ markets and observations. In addition, since the available vehicle makes vary across time and place, let $\\mathcal{J}_t$ denote the set of available makes in each market $t=1,\\ldots,T$, and let $\\mathcal{J} := \\bigcup_{t=1}^T \\mathcal{J}_t$ be the set of all makes which were available in some market. Then $J:=\\#\\mathcal{J}$ is the number of makes which were available at some point of time in the period in at least one country-level market. In our dataset there are $J = 356$ unique vehicle makes. Note also however that characteristics of vehicle makes vary across markets.\n",
    "\n",
    "Our dataset includes 47 variables in total. The first three columns are market and product codes for the year, country, and make. Another variable is quantity sold (No. of new registrations) which will be used in computing observed market shares. The remaining 43 variables are potential explanatory variables. We will only consider the subset of these which describes car characteristics such as brand, after-tax price, horse power, etc. which adds up to $K=20$ characteristics. The remaining 23 variables are mainly macroeconomic variables such as e.g. GDP per capita which have been used to construct estimates of e.g. the average wage income and purchasing power. Since we are only interested in utility-shifting variables, we will not consider the latter columns. \n",
    "\n",
    "Reading in the dataset `eurocars.csv` we thus have a dataframe of $\\sum_{t=1}^T \\#\\mathcal{J}_t = 11459$ rows and $47$ columns. The `ye` column runs through $y=70,\\ldots,99$, the `ma` column runs through $m=1,\\ldots,M$, and the ``co`` column takes values $j\\in \\mathcal{J}$. \n",
    "\n",
    "Because we consider a country-year pair as the level of observation, we construct a `market` column taking values $t=1,\\ldots,T$. We also construct a `market_share` variable giving us the market share of any product $j$ in any market $t$; this will obviously take values in $[0,1]$. To deal with the fact that choice sets $\\mathcal{J}_t$ vary across markets, we expand the dataframe so that every car $j\\in \\mathcal{J}$ which was observed in some market $t$ is in the choice set of all other markets as well, i.e. we impose $\\mathcal{J}_t = \\mathcal{J}$ for all markets $t$. We then impute a market share of $q_{jt}=0$ for any car $j$ which in reality was not available in market $t$. To this end we first construct an outside option $j=0$ in each market $t$  of not buying a car by letting the 'sales' of $j=0$ being determined as \n",
    "\n",
    "$$\\mathrm{sales}_{0t} = \\mathrm{pop}_t - \\sum_{j=1}^J \\mathrm{sales}_{jt}$$\n",
    "\n",
    "where $\\mathrm{pop}_t$ is the total population in market $t$.\n",
    "\n",
    "We also read in the variable description of the dataset contained in `eurocars.dta`. We will use the list `x_vars` throughout to work with our explanatory variables.\n",
    "\n",
    "Lastly, we access the underlying 3-dimensional numpy array of the explonatory variables `x` by sorting on `market` and then `co`, and subsequently resizing the explanatory variables as\n",
    "\n",
    "> `x = dat[x_vars].values.resize((T,J,K))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and variable names\n",
    "os.chdir('../GREENCAR_notebooks/')\n",
    "input_path = os.getcwd() # Assigns input path as current working directory (cwd)\n",
    "descr = (pd.read_stata('eurocars.dta', iterator = True)).variable_labels()\n",
    "dat = pd.read_csv(os.path.join(input_path, 'eurocars.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ye</td>\n",
       "      <td>year (=first dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ma</td>\n",
       "      <td>market (=second dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>co</td>\n",
       "      <td>model code (=third dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zcode</td>\n",
       "      <td>alternative model code (predecessors and succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brd</td>\n",
       "      <td>brand code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type</td>\n",
       "      <td>name of brand and model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>name of model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>org</td>\n",
       "      <td>origin code (demand side, country with which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>loc</td>\n",
       "      <td>location code (production side, country where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frm</td>\n",
       "      <td>firm code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qu</td>\n",
       "      <td>sales (number of new car registrations)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pl</td>\n",
       "      <td>places (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>doors (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>li1</td>\n",
       "      <td>measure 1 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>li2</td>\n",
       "      <td>measure 2 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>li3</td>\n",
       "      <td>measure 3 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>princ</td>\n",
       "      <td>=pr/(ngdp/pop): price relative to per capita i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eurpr</td>\n",
       "      <td>=pr/avdexr: price in common currency (in SDR t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>exppr</td>\n",
       "      <td>=pr/avexr: price in exporter currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>avexr</td>\n",
       "      <td>av. exchange rate of exporter country (exporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>avdexr</td>\n",
       "      <td>av. exchange rate of destination country (dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>avcpr</td>\n",
       "      <td>av. consumer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>avppr</td>\n",
       "      <td>av. producer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>avdcpr</td>\n",
       "      <td>av. consumer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>avdppr</td>\n",
       "      <td>av. producer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xexr</td>\n",
       "      <td>avdexr/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tax</td>\n",
       "      <td>percentage VAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pop</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ngdp</td>\n",
       "      <td>nominal gross domestic product of destination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rgdp</td>\n",
       "      <td>real gross domestic product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>engdp</td>\n",
       "      <td>=ngdp/avdexr: nominal gdp in common currency (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ergdp</td>\n",
       "      <td>=rgdp/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>engdpc</td>\n",
       "      <td>=engdp/pop: nominal gdp per capita in common c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ergdpc</td>\n",
       "      <td>=ergdp/pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              ye                   year (=first dimension of panel)\n",
       "1              ma                market (=second dimension of panel)\n",
       "2              co             model code (=third dimension of panel)\n",
       "3           zcode  alternative model code (predecessors and succe...\n",
       "4             brd                                         brand code\n",
       "5            type                            name of brand and model\n",
       "6           brand                                      name of brand\n",
       "7           model                                      name of model\n",
       "8             org  origin code (demand side, country with which c...\n",
       "9             loc  location code (production side, country where ...\n",
       "10            cla                              class or segment code\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            frm                                          firm code\n",
       "13             qu            sales (number of new car registrations)\n",
       "14             cy            cylinder volume or displacement (in cc)\n",
       "15             hp                                 horsepower (in kW)\n",
       "16             we                                     weight (in kg)\n",
       "17             pl             places (number, not reliable variable)\n",
       "18             do              doors (number, not reliable variable)\n",
       "19             le                                     length (in cm)\n",
       "20             wi                                      width (in cm)\n",
       "21             he                                     height (in cm)\n",
       "22            li1  measure 1 for fuel efficiency (liter per km, a...\n",
       "23            li2  measure 2 for fuel efficiency (liter per km, a...\n",
       "24            li3  measure 3 for fuel efficiency (liter per km, a...\n",
       "25             li          average of li1, li2, li3 (used in papers)\n",
       "26             sp                            maximum speed (km/hour)\n",
       "27             ac  time to acceleration (in seconds from 0 to 100...\n",
       "28             pr   price (in destination currency including V.A.T.)\n",
       "29          princ  =pr/(ngdp/pop): price relative to per capita i...\n",
       "30          eurpr  =pr/avdexr: price in common currency (in SDR t...\n",
       "31          exppr              =pr/avexr: price in exporter currency\n",
       "32          avexr  av. exchange rate of exporter country (exporte...\n",
       "33         avdexr  av. exchange rate of destination country (dest...\n",
       "34          avcpr       av. consumer price index of exporter country\n",
       "35          avppr       av. producer price index of exporter country\n",
       "36         avdcpr    av. consumer price index of destination country\n",
       "37         avdppr    av. producer price index of destination country\n",
       "38           xexr                                       avdexr/avexr\n",
       "39            tax                                     percentage VAT\n",
       "40            pop                                         population\n",
       "41           ngdp  nominal gross domestic product of destination ...\n",
       "42           rgdp                        real gross domestic product\n",
       "43          engdp  =ngdp/avdexr: nominal gdp in common currency (...\n",
       "44          ergdp                                        =rgdp/avexr\n",
       "45         engdpc  =engdp/pop: nominal gdp per capita in common c...\n",
       "46         ergdpc                                         =ergdp/pop"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(descr, index=['description']).transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the data to fit our setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we create the 'market' column \n",
    "\n",
    "dat = dat.sort_values(by = ['ye', 'ma'], ascending = True)\n",
    "market_vals = [*iter.product(dat['ye'].unique(), dat['ma'].unique())]\n",
    "market_vals = pd.DataFrame({'year' : [val[0] for val in market_vals], 'country' : [val[1] for val in market_vals]})\n",
    "market_vals = market_vals.reset_index().rename(columns={'index' : 'market'})\n",
    "dat = dat.merge(market_vals, left_on=['ye', 'ma'], right_on=['year', 'country'], how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Second we expand the dataset such that all cars are at least vacuously available in all markets\n",
    "\n",
    "dat = dat.sort_values(['market', 'co'])\n",
    "product_vals = [*iter.product(dat['market'].unique(), dat['co'].unique())]\n",
    "product_vals = pd.DataFrame({'market' : [val[0] for val in product_vals], 'co' : [val[1] for val in product_vals]})\n",
    "dat = product_vals.merge(dat, on=['market','co'], how='outer')\n",
    "dat['active'] = np.where(dat['qu'].notna(), 1, 0) # Create a column of whether cars was actually active or not.\n",
    "dat['qu'] = np.where(dat['qu'].isna(), 0, dat['qu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Third we construct an outside option for each market t\n",
    "\n",
    "outside_shares = dat.groupby('market', as_index=False)['qu'].sum()\n",
    "outside_shares = outside_shares.merge(dat[['market', 'pop']], on = 'market', how='left').dropna().drop_duplicates(subset = 'market', keep = 'first')\n",
    "outside_shares['qu'] = outside_shares['pop'] - outside_shares['qu']\n",
    "outside_shares['co'] = 0\n",
    "outside_shares['active'] = 1\n",
    "dat = pd.concat([dat, outside_shares])\n",
    "\n",
    "# Potentially set characteristics equal to 0 for outside option. However consider different data types!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We also create an indicator on whether a car was active in given market as a numpy array\n",
    "A = pd.pivot(dat[['market', 'co', 'active']], index='market', columns='co', values='active').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fourth we compute market shares for each product j in each market t \n",
    "\n",
    "dat['ms'] = dat.groupby('market')['qu'].transform(lambda x: x/x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine explanatory variables and find variable description as 'x_lab'\n",
    "x_vars =  [dat.keys()[k] for k in [*range(6,14), *range(15,23), *range(26,30)]]\n",
    "nest_vars = [var for var in x_vars if (var != 'type')&(var != 'model')&(var != 'pr')] # we will nest on variables which are not price, brand, model.\n",
    "nest_cont_vars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac']\n",
    "x_lab = (pd.DataFrame(descr, index=['description'])[x_vars].transpose().reset_index().rename(columns={'index' : 'variable names'}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>type</td>\n",
       "      <td>name of brand and model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>model</td>\n",
       "      <td>name of model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>org</td>\n",
       "      <td>origin code (demand side, country with which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>loc</td>\n",
       "      <td>location code (production side, country where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>frm</td>\n",
       "      <td>firm code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pl</td>\n",
       "      <td>places (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>do</td>\n",
       "      <td>doors (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0            type                            name of brand and model\n",
       "1           brand                                      name of brand\n",
       "2           model                                      name of model\n",
       "3             org  origin code (demand side, country with which c...\n",
       "4             loc  location code (production side, country where ...\n",
       "5             cla                              class or segment code\n",
       "6            home  domestic car dummy (appropriate interaction of...\n",
       "7             frm                                          firm code\n",
       "8              cy            cylinder volume or displacement (in cc)\n",
       "9              hp                                 horsepower (in kW)\n",
       "10             we                                     weight (in kg)\n",
       "11             pl             places (number, not reliable variable)\n",
       "12             do              doors (number, not reliable variable)\n",
       "13             le                                     length (in cm)\n",
       "14             wi                                      width (in cm)\n",
       "15             he                                     height (in cm)\n",
       "16             li          average of li1, li2, li3 (used in papers)\n",
       "17             sp                            maximum speed (km/hour)\n",
       "18             ac  time to acceleration (in seconds from 0 to 100...\n",
       "19             pr   price (in destination currency including V.A.T.)"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the dimensions of Data\n",
    "T = dat['market'].nunique()\n",
    "J = dat['co'].nunique()\n",
    "K = len(x_vars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also convert our discrete explanatory variables to numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_columns = dat.select_dtypes(['object'])\n",
    "for col in obj_columns:\n",
    "    dat[col] = dat[col].astype('category').cat.codes.astype('float64') # Possibly a problem with Nan's being mapped to -1 ?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally we fill Nan values with '$-1$' in remaining columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat.fillna(-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also scale values such that they lie in the interval $[-1,1]$. This has various numerical benefits. Also, this will not affect elasticities or diversion ratios, but semielasticities will be affected by the scaling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat[x_vars] = dat[x_vars] / dat[x_vars].abs().max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will primarily use numpy data types and numpy functions in this notebook. Hence we store our response variable 'y' and our explanatory variables 'x' as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy arrays of response and explanatory variables\n",
    "dat = dat.sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "x = dat[x_vars].values.reshape((T,J,K))\n",
    "y = dat['ms'].to_numpy().reshape((T,J))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logit - for comparison\n",
    "Estimating a Logit model via maximum likelihood with an initial guess of parameters $\\hat \\beta^0 = 0$ yields estimated parameters $\\hat \\beta^{\\text{logit}}$ given as..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "beta_0 = np.zeros((K,))\n",
    "\n",
    "# Estimate the model\n",
    "res_logit = logit.estimate_logit(logit.q_logit, beta_0, a, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_beta = res_logit['beta']\n",
    "pd.DataFrame(logit_beta.reshape(1,len(logit_beta))) # Our estimates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the corresponding Logit choice probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_q = logit.logit_ccp(logit_beta, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also find the elasticities and diversion ratios implied by the logit model as follows..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epsilon_logit = logit.logit_elasticity(logit_q, logit_beta, 0) # Elasticities wrt. the price-to-log-income characteristic\n",
    "DR_logit_hat = logit.logit_diversion_ratio(logit_q, logit_beta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The IPDL model - Nesting structure\n",
    "\n",
    "The IPDL model is a generalization of the nested logit model where each alternative may belong to more than one nest. Before fully introducing the model, we construct the nesting structure.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing nests\n",
    "\n",
    "Let $\\Delta=\\left\\{q\\in \\mathbb{R}^J_+: \\sum_{j=1}^J q_j=1\\right\\}$ denote the probability simplex. For each group of nests $g=1,\\ldots, G$, nest membership is denoted by the matrix $\\Psi^g\\in \\mathbb R^{C_g\\times J}$: $\\Psi^g_{cj}=1$ if product $j$ belongs to nest $c$ and zero otherwise, and each product can only belong to one nest within each group, meaning that $\\sum_{c=1}^{C_g}\\Psi^g_{cj}=1$ for all $j$ and all $g$. The matrix-vector product $\\Psi^gq$ is then\n",
    "$$\n",
    "\\Psi^g q=\\sum_j \\Psi^{g}_{cj}q_j=\\left(\\begin{array}{c}\n",
    "\\sum_{j:\\Psi^g_{1j}=1} q_j \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{j: \\Psi^g_{C_gj}=1}q_j\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "and the vector $\\Psi^gq$ is a vector of nest-specific choice probabilities, i.e. the sum of the probabilities within each nest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perturbation function $\\Omega$\n",
    "\n",
    "In the following, a vector $z\\in \\mathbb R^d$ is always a column vector. We now construct the IPDL perturbation function which has the form (where for a vector $z$, the logarithm is applied elementwise and $z'$ denote the transpose)\n",
    "$$\n",
    "\\Omega(q|\\lambda)= (1-\\sum_{g=1}^G \\lambda_g) q'\\ln q +\\sum_{g=1}^{G} \\lambda_g \\left(\\Psi^g q \\right)'\\ln \\left(\\Psi^g q\\right).\n",
    "$$\n",
    "Note that since $\\Psi^g q$ denotes a probability distribution over the nests, the term $(\\Psi^gq)'\\ln (\\Psi^gq)$ is the (negative) entropy of the probability distribution $\\Psi^g q$. Similarly, $q'\\ln q$ is the negative entropy of q. Note also that as each nest has at least one member, and $q$ is strictly positive, $\\Psi^gq$ is also strictly positive. When the parameters $\\lambda_g$ satisfy $\\lambda_g>0$ and\n",
    "$$\n",
    "\\sum_g \\lambda_g<1,\n",
    "$$\n",
    "the function $\\Omega(\\cdot|\\lambda)$ is a strictly convex function of $q$, and the utility maximization problem has a unique interior (meaning strictly positive choice probabilities) solution. If $\\lambda_g = 0$ for all groupings $g$, we immediately see that the  IPDL becomes the standard multinomial Logit model for the choice probabilities $q$. When there is only one group of nests, $G=1$, then $\\Omega$ induces the nested logit choice probabilities (note though that the nested logit model is often parameterized in terms of the nesting parameter $\\mu=1-\\lambda$ instead!). \n",
    "\n",
    "It will be convenient to define a choice probability function for a given vector of payoffs $u$ as\n",
    "$$\n",
    "P(u|\\lambda)=\\arg \\max_{q\\in \\Delta}\\left\\{q'u-\\Omega(q|\\lambda)\\right\\}\n",
    "$$\n",
    "Letting $\\theta$ denote the full vector of parameters, $\\theta=(\\beta',\\lambda')'$, the individual choice probabilities is a function of the matrix $\\mathbf{X}_i$ and the parameters $\\theta$, as\n",
    "$$\n",
    "p(\\mathbf{X}_i,\\theta)=\\arg\\max_{q\\in \\Delta}\\left\\{q'\\mathbf{X}_i \\beta-(1-\\sum_{g=1}^G\\lambda_g)q'\\ln q-\\sum_{g=1}^G\\lambda_g \\left(\\Psi^g q \\right)'\\ln \\left(\\Psi^g q\\right)\\right\\}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-rescaling for numerical stability\n",
    "\n",
    "Let $\\alpha$ be a scalar, and let $\\iota$ be the all-ones vector in $\\mathbb R^J$. Note that $q'(u+\\alpha\\iota)=q'u+(q'\\iota)\\alpha=q'u+\\alpha$, since $q$ sums to one. For this reason, $\\alpha$ does not enter into the utility maximization when calculating $P(u+\\alpha\\iota|\\lambda)$, and we have $P(u+\\alpha\\iota|\\lambda)=P(u|\\lambda)$.\n",
    "\n",
    "This allows us to re-scale the utilities just as in the logit model, since $P(u-(\\max_{j}u_j)\\iota|\\lambda)=P(u|\\lambda)$. The numerical benefits of this approach carry over to the IPDL model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient and Hessian\n",
    "\n",
    "For purposes of computing the gradient and Hessian of $\\Omega$, it is convenient to define\n",
    "$$\n",
    "\\Gamma=\\left(\\begin{array}{c}\n",
    "(1-\\sum_g \\lambda_g)I_J\\\\\n",
    "\\lambda_1 \\Psi^1\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_G \\Psi^G\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "where $I_J$ is the identity matrix in $\\mathbb R^J$. The matrix $\\Gamma$ is a block matrix with $J+\\sum_g C_g$ rows and $J$ columns. Note that \n",
    "\n",
    "$$\n",
    "\\Gamma q=\\left(\\begin{array}{c}\n",
    "(1-\\sum_g\\lambda_g)q \\\\\n",
    "\\lambda_1\\Psi^g q\\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_G \\Psi^Gq\n",
    "\\end{array}\\right)>0\n",
    "$$\n",
    "if $q>0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\Gamma$, we can show that\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(\\Gamma q)'\\ln (\\Gamma q)+c\\\\\n",
    "\\nabla_q \\Omega(q|\\lambda)=\\Gamma'\\ln (\\Gamma q)+\\iota\\\\\n",
    "\\nabla^2_{qq}\\Omega(q|\\lambda)=\\Gamma'\\mathrm{diag}(\\Gamma q)^{-1}\\Gamma,\n",
    "$$\n",
    "where $c$ is a scalar that depends on $\\lambda$ but not on $q$ and therefore does not affect the utility maximization problem, $\\iota=(1,\\ldots,1)'\\in \\mathbb R^J$ is the all-ones vector and $\\mathrm{diag}(z)$ is a diagonal matrix with the elements of the vector $z$ on the diagonal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we impose on all nests on all markets. We deal with this by setting $\\psi_{tcj} = 0$ for all products $j$ if the nest $c$ was not in fact observed in market $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_nests(data, markets_id, products_id, columns, cont_var = None, cont_var_bins = None):\n",
    "    '''\n",
    "    This function creates the nest matrices \\Psi^g from any specified columns in data\n",
    "\n",
    "    Args.\n",
    "        data: a pandas DataFrame\n",
    "        markets_id: a string denoting the column of 'data' containing an enumeration t=0,1,...,T-1 of markets\n",
    "        products_id: a string denoting the column of 'data' containing product code which uniquely identifies products\n",
    "        columns: a list containing the column names of columns in 'data' from which nest groupings g=0,1,...,G-1 for each market t are to be generated\n",
    "        cont_var: a list of the continuous variables in 'columns'\n",
    "        caont_var_bins: a list containing the number of bins to make for each continuous variable in 'columns'\n",
    "\n",
    "    Returns\n",
    "        Psi_dict: a dictionary of dictionaries of the Psi_g matrices for each market t and each gropuing g\n",
    "        nest_dict: a dictionary of dictionaries of pandas dataframes describing the structure of each nest for each market t and each grouping g \n",
    "    '''\n",
    "\n",
    "    J = data[products_id].nunique()\n",
    "    T = data[markets_id].nunique()\n",
    "    G = len(columns)\n",
    "\n",
    "    dat = data.sort_values(by = [markets_id, products_id]) # This is good :)\n",
    "    \n",
    "    Psi_dict = {}\n",
    "    nest_dict = {}\n",
    "\n",
    "    ### Bin continuous variables\n",
    "\n",
    "    if cont_var == None:\n",
    "        None\n",
    "    else:\n",
    "        for var,n_bins in zip(cont_var,cont_var_bins):\n",
    "            dat[var] = pd.cut(dat[var], bins=n_bins, labels=[str(i) for i in range(1,n_bins +1)], include_lowest=True)\n",
    "        \n",
    "\n",
    "    nest_counts = dat[columns].nunique().values\n",
    "\n",
    "    ### New - find unique nests over all markets t and impose all nests into all markets t \n",
    "    for g in range(G):\n",
    "        \n",
    "        col = columns[g]\n",
    "        vals = pd.DataFrame({'nests' : dat[col].sort_values().unique()}).reset_index().rename(columns={'index' :'nest_index'})\n",
    "        descr = vals.rename_axis(col, axis='columns')\n",
    "        nest_dict[g] = descr\n",
    "\n",
    "        product_enumeration = pd.DataFrame({products_id : dat[products_id].sort_values().unique(), 'product_enumeration' : np.arange(dat[products_id].nunique())})\n",
    "        C_g = dat[col].nunique()\n",
    "        Psi_dict_t = {}\n",
    "\n",
    "        for t in range(T):\n",
    "            frame = dat[dat[markets_id] == t][[products_id, col]].merge(vals, left_on = col, right_on = 'nests')\n",
    "            allocation = frame[[products_id, 'nest_index']].merge(product_enumeration, on=products_id, how='left')\n",
    "\n",
    "            mat = np.zeros((int(C_g), J))\n",
    "\n",
    "            for c,j in zip(allocation['nest_index'], allocation['product_enumeration']):\n",
    "                mat[c, j] = 1\n",
    "\n",
    "            Psi_dict_t[t] = mat\n",
    "        \n",
    "        Psi_dict[g] = Psi_dict_t\n",
    "\n",
    "    C = np.concatenate([np.eye(J) if g==0 else Psi_dict[g-1][0] for g in range(G+1)]).shape[0]\n",
    "    Gamma_tilde = np.empty((T,C,J))\n",
    "\n",
    "    for t in range(T):\n",
    "        Gamma_tilde[t,:,:] = np.concatenate([np.eye(J) if g==0 else Psi_dict[g-1][t] for g in range(G+1)])\n",
    "\n",
    "    return Gamma_tilde, Psi_dict, nest_dict, nest_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bin all the continuous explanatory variables different from `pr` (i.e. the price) in 10 bins, and the grouping of `pr` includes 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi_stack, Psi_dict, Nest_descr, Nest_count = Create_nests(dat, 'market', 'co', nest_vars, nest_cont_vars, [*[np.int64(10) for i in range(len(nest_cont_vars))]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Gamma( Lambda, Psi_stack, nest_count):\n",
    "    '''\n",
    "    This function \n",
    "    '''\n",
    "\n",
    "    T,C,J = Psi_stack.shape\n",
    "\n",
    "    lambda0 = 1 - sum(Lambda)\n",
    "    Lambda_long = np.empty((C))\n",
    "    Lambda_full = [lambda0, *Lambda]\n",
    "    indices = np.array([J,*nest_count]).cumsum()\n",
    "\n",
    "    for i in range(len(indices)):\n",
    "        if i == 0:\n",
    "            Lambda_long[0:indices[i]] = Lambda_full[i]\n",
    "        else:\n",
    "            Lambda_long[indices[i-1]:indices[i]] = Lambda_full[i]\n",
    "    \n",
    "    Gamma =  Lambda_long[None,:,None] * Psi_stack # np.einsum('c,tcj->tcj', Lambda_long, Psi_stack, optimize=True)\n",
    "\n",
    "    return Gamma"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def Create_Gamma2(x,Lambda, Psi_dict):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    G = len(Psi_dict.keys())\n",
    "    T,J,K = x.shape\n",
    "\n",
    "    lambda0 = 1 - sum(Lambda)\n",
    "    Lambda_full = [lambda0, *Lambda]\n",
    "\n",
    "    C = np.concatenate([np.eye(J) if g==0 else Psi_dict[g-1][0] for g in range(G+1)]).shape[0]\n",
    "    Gamma = np.empty((T,C,J))\n",
    "\n",
    "    for t in range(T):\n",
    "        Gamma[t,:,:] = np.concatenate([Lambda_full[g]*np.eye(J) if g==0 else Lambda_full[g]*Psi_dict[g-1][t] for g in range(G+1)])\n",
    "\n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = K\n",
    "lambda0 = np.ones((G,))/(G+1)\n",
    "Gamma0 = Create_Gamma(lambda0, Psi_stack, Nest_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "While it is possible to solve for the choice probabilities explicitly by maximizing utility, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Suppose we are evaluating the likelihood at some guess of the parameters $\\theta=(\\beta',\\lambda')$. Let $u_i=\\mathbf{X}_i\\beta$, and let $q_i^0$ denote some initial vector of choice probabilities e.g. $q^0_i=\\frac{e^{u_i}}{\\sum_{j'=1}^Je^{u_{ij'}}}$, we update the choice probabilities according to the formula\n",
    "$$\n",
    "v_i^{k} =u_i+\\ln q_i^{k-1}-\\Gamma'\\ln (\\Gamma q_i^{k-1})\\\\\n",
    "q_i^{k} = \\frac{e^{v_i^{k}}}{\\sum_{j=1}^J e^{v_{ij}^{k}}},\n",
    "$$\n",
    "they show that $\\lim_{k\\rightarrow \\infty}q_i^k=p(\\mathbf{X}_i,\\theta)$ for any starting value $q^0_i$ in the interior of $\\Delta$. For numerical stability, it can be a good idea to also do max-rescaling of $v^k_i$ at every iteration.\n",
    "\n",
    "Let $p$ denote the solution to the utility maximization problem. Formally, the Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p||q^{k+1})\\leq \\left(1- \\sum_g \\lambda_g \\right)D_{KL}(p||q^k),\n",
    "$$\n",
    "Noting that $(1-\\sum_g \\lambda_g)\\in [0,1)$ by assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_ccp(Beta, x, Gamma, active_mat, tol = 1.0e-15, maximum_iterations = 1000, MAXRESCALE:bool = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    u = logit.util(Beta, x)\n",
    "    q = np.exp(u) / np.exp(u).sum(axis = 1, keepdims=True) # Find logit choice probabilities\n",
    "    q0 = active_mat*q\n",
    "\n",
    "    assert u.ndim == 2\n",
    "    assert q.ndim == 2\n",
    "\n",
    "    T,J,K = x.shape\n",
    "    \n",
    "    Epsilon = 1.0e-8\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        # Calculate v\n",
    "        gamma_q = np.einsum('tcj,tj->tc', Gamma, q0, optimize=True)\n",
    "        gamma_log_prod = np.einsum('tcj,tc->tj', Gamma, np.log(gamma_q + Epsilon), optimize=True)\n",
    "        v = u - gamma_log_prod\n",
    "\n",
    "        # Calculate iterated ccp q^k\n",
    "        denom = np.sum(q0 * np.exp(v), axis=1, keepdims=True)\n",
    "        numerator = q0*np.exp(v)\n",
    "        q1 = active_mat * numerator / denom\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.sum((q1-q0)**2/q , axis=1)) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "\n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "        \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Products</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Markets</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021786</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019555</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.020951</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.023076</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.022384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>0.021566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026945</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.012376</td>\n",
       "      <td>0.013339</td>\n",
       "      <td>0.018787</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.035550</td>\n",
       "      <td>0.028714</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.000349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>0.023629</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037783</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.010011</td>\n",
       "      <td>0.009253</td>\n",
       "      <td>0.027931</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.037549</td>\n",
       "      <td>0.028656</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.000437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0.022068</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035059</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045526</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.007867</td>\n",
       "      <td>0.009455</td>\n",
       "      <td>0.027532</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.025389</td>\n",
       "      <td>0.015806</td>\n",
       "      <td>0.013101</td>\n",
       "      <td>0.001698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>0.022802</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.036639</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033189</td>\n",
       "      <td>0.001251</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.011026</td>\n",
       "      <td>0.031485</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>0.041191</td>\n",
       "      <td>0.028609</td>\n",
       "      <td>0.003931</td>\n",
       "      <td>0.000802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>0.022999</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032431</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.008532</td>\n",
       "      <td>0.023083</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044706</td>\n",
       "      <td>0.032581</td>\n",
       "      <td>0.005941</td>\n",
       "      <td>0.001806</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows  357 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Products       0    1    2    3    4         5    6    7    8    9    ...  \\\n",
       "Markets                                                               ...   \n",
       "0         0.021786  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...   \n",
       "1         0.019555  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...   \n",
       "2         0.020951  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...   \n",
       "3         0.023076  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...   \n",
       "4         0.022384  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...   \n",
       "...            ...  ...  ...  ...  ...       ...  ...  ...  ...  ...  ...   \n",
       "145       0.021566  0.0  0.0  0.0  0.0  0.026945  0.0  0.0  0.0  0.0  ...   \n",
       "146       0.023629  0.0  0.0  0.0  0.0  0.000000  0.0  0.0  0.0  0.0  ...   \n",
       "147       0.022068  0.0  0.0  0.0  0.0  0.035059  0.0  0.0  0.0  0.0  ...   \n",
       "148       0.022802  0.0  0.0  0.0  0.0  0.036639  0.0  0.0  0.0  0.0  ...   \n",
       "149       0.022999  0.0  0.0  0.0  0.0  0.032431  0.0  0.0  0.0  0.0  ...   \n",
       "\n",
       "Products       347       348       349       350       351       352  \\\n",
       "Markets                                                                \n",
       "0         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "3         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "4         0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "145       0.000000  0.000989  0.012376  0.013339  0.018787  0.001146   \n",
       "146       0.037783  0.003200  0.010011  0.009253  0.027931  0.000595   \n",
       "147       0.045526  0.002995  0.007867  0.009455  0.027532  0.001046   \n",
       "148       0.033189  0.001251  0.007858  0.011026  0.031485  0.000999   \n",
       "149       0.000000  0.004301  0.004619  0.008532  0.023083  0.000000   \n",
       "\n",
       "Products       353       354       355       356  \n",
       "Markets                                           \n",
       "0         0.000000  0.000000  0.000000  0.000000  \n",
       "1         0.000000  0.000000  0.000000  0.000000  \n",
       "2         0.000000  0.000000  0.000000  0.000000  \n",
       "3         0.000000  0.000000  0.000000  0.000000  \n",
       "4         0.000000  0.000000  0.000000  0.000000  \n",
       "...            ...       ...       ...       ...  \n",
       "145       0.035550  0.028714  0.004241  0.000349  \n",
       "146       0.037549  0.028656  0.008825  0.000437  \n",
       "147       0.025389  0.015806  0.013101  0.001698  \n",
       "148       0.041191  0.028609  0.003931  0.000802  \n",
       "149       0.044706  0.032581  0.005941  0.001806  \n",
       "\n",
       "[150 rows x 357 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta0 = 0.1*np.ones((K,))\n",
    "theta0 = np.append(beta0, lambda0)\n",
    "\n",
    "q0_hat = IPDL_ccp(beta0, x, Gamma0, A)\n",
    "pd.DataFrame(q0_hat).rename_axis(index='Markets', columns='Products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert q0_hat.sum(axis=1).all() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gammaq0 = np.einsum('tcj,tj->tc', Gamma0, q0_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00103744, 0.        , 0.        , ..., 0.00373226, 0.00165312,\n",
       "        0.        ],\n",
       "       [0.0009312 , 0.        , 0.        , ..., 0.00611345, 0.00146074,\n",
       "        0.        ],\n",
       "       [0.00099767, 0.        , 0.        , ..., 0.00463209, 0.00126549,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.00105085, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00946753],\n",
       "       [0.00108583, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.01027736],\n",
       "       [0.0010952 , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.00860977]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Gammaq0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 963.90816481,    0.        ,    0.        , ...,  267.93450181,\n",
       "         604.91577253,    0.        ],\n",
       "       [1073.88142216,    0.        ,    0.        , ...,  163.57388385,\n",
       "         684.58404991,    0.        ],\n",
       "       [1002.33229683,    0.        ,    0.        , ...,  215.88544514,\n",
       "         790.2092947 ,    0.        ],\n",
       "       ...,\n",
       "       [ 951.606512  ,    0.        ,    0.        , ...,    0.        ,\n",
       "           0.        ,  105.62420253],\n",
       "       [ 920.95825552,    0.        ,    0.        , ...,    0.        ,\n",
       "           0.        ,   97.30127433],\n",
       "       [ 913.07657408,    0.        ,    0.        , ...,    0.        ,\n",
       "           0.        ,  116.14710251]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.divide(1,Gammaq0, out=np.zeros(Gammaq0.shape), where= Gammaq0!=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand derivatives and price Elasticity\n",
    "\n",
    "While the demand derivatives in the IPDL model are not quite as simple as in the logit model, they are still easy to compute. \n",
    "Let $q=P(u|\\lambda)$, then\n",
    "$$\n",
    "\\nabla_u P(u|\\lambda)=\\left(\\nabla^2_{qq}\\Omega(q|\\lambda)\\right)^{-1}-qq'\n",
    "$$\n",
    "where the $()^{-1}$ denotes the matrix inverse. The derivatives with respect to any $x_{ij\\ell}$ can now easily be computed by the chain rule,\n",
    "$$\n",
    "    \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{\\partial u_{ik}}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell,\n",
    "$$\n",
    "\n",
    "Finally, moving to price elasticity is the same as in the logit model, if $x_{ik\\ell}$ is the log price of product $k$ for individual $i$, then\n",
    "$$\n",
    "    \\mathcal{E}_{jk}= \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}\\frac{1}{P_j(u_i|\\lambda)}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{1}{P_j(u_i|\\lambda)}\\beta_\\ell=\\frac{\\partial \\ln P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell$$\n",
    "we can also write this compactly as\n",
    "$$\n",
    "\\nabla_u \\ln P(u|\\lambda)=\\mathrm{diag}(P(u|\\lambda))^{-1}\\nabla_u P(u|\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pertubation_hessian(q, x, Theta, Psi, active_mat, nest_count):\n",
    "    '''\n",
    "    This function calucates the hessian of the pertubation function \\Omega\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        Lambda: a (G,) numpy array of nesting parameters\n",
    "        Psi: a dictionary of the \\Psi^g matrices as columns as outputted 'Create_incidence_matrix'\n",
    "    \n",
    "    Returns\n",
    "        Hess: a (N,J,J) numpy array of second partial derivatives of the pertubation function \\Omega\n",
    "    '''\n",
    "    assert q.ndim == 2\n",
    "    assert Theta.ndim == 1\n",
    "    \n",
    "    T,J,K = x.shape\n",
    "\n",
    "    Gamma = Create_Gamma(Theta[K:], Psi, nest_count)\n",
    "    Active_indicator = active_mat[:,:,None]*active_mat[:,None,:]\n",
    "\n",
    "    gamma_q = np.einsum('tcj,tj->tc', Gamma, q)\n",
    "    inv_gamma_q = np.divide(1, gamma_q, out=np.zeros(gamma_q.shape), where= gamma_q!=0) # Might have numerical implications... We handle division by zeros coming from imputation of non-active products by imputing a zero whenever the divisor is zero. \n",
    "    Hess = Active_indicator * np.einsum('tcj,tc,tck->tjk', Gamma, inv_gamma_q, Gamma) # Works since einsum merely divides through by c'th element in gamma_q (E.g. diag(\\Gamma q)^-1) \n",
    "\n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_gradient(q, x, Theta, Psi, active_mat, nest_count):\n",
    "    '''\n",
    "    This function calucates the gradient of the choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        x: a (N,J,K) numpy array of covariates\n",
    "        Lambda: a (G,) numpy array of nesting parameters\n",
    "        Psi: a dictionary of the \\Psi^g matrices as columns as outputted 'Create_incidence_matrix'\n",
    "    \n",
    "    Returns\n",
    "        Grad: a (N,J,K) numpy array of partial derivatives of the choice proabilities wrt. characteristics\n",
    "    '''\n",
    "\n",
    "    assert q.ndim == 2\n",
    "\n",
    "    T,J,K = x.shape\n",
    "\n",
    "    inv_omega_hess = la.pinv(compute_pertubation_hessian(q, x, Theta, Psi, active_mat, nest_count)) # (N,J,J) # For each i=1,...,N , computes the inverse of the J*J Hessian\n",
    "    qqT = np.einsum('tj,tk->tjk', q, q) # (N,J,J) outerproduct\n",
    "    Grad = inv_omega_hess - qqT\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradccp0 = ccp_gradient(q0_hat, x, theta0, Psi_stack, A, Nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.781943e-02</td>\n",
       "      <td>3.007997e-16</td>\n",
       "      <td>-1.812873e-16</td>\n",
       "      <td>5.220196e-17</td>\n",
       "      <td>-1.362745e-17</td>\n",
       "      <td>1.549284e-18</td>\n",
       "      <td>-3.236327e-17</td>\n",
       "      <td>1.082968e-17</td>\n",
       "      <td>4.965247e-17</td>\n",
       "      <td>-2.551926e-17</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.014843e-16</td>\n",
       "      <td>-7.818688e-29</td>\n",
       "      <td>4.835970e-28</td>\n",
       "      <td>1.370093e-28</td>\n",
       "      <td>2.401517e-29</td>\n",
       "      <td>3.813243e-29</td>\n",
       "      <td>-5.631777e-29</td>\n",
       "      <td>-1.115197e-29</td>\n",
       "      <td>3.793552e-29</td>\n",
       "      <td>-9.408704e-30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.947801e-18</td>\n",
       "      <td>1.813704e-30</td>\n",
       "      <td>-1.217105e-29</td>\n",
       "      <td>-3.440989e-30</td>\n",
       "      <td>-6.140219e-31</td>\n",
       "      <td>-9.289169e-31</td>\n",
       "      <td>1.446439e-30</td>\n",
       "      <td>2.690396e-31</td>\n",
       "      <td>-9.738666e-31</td>\n",
       "      <td>2.723197e-31</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.546293e-17</td>\n",
       "      <td>-2.020212e-29</td>\n",
       "      <td>1.199973e-28</td>\n",
       "      <td>3.328397e-29</td>\n",
       "      <td>5.883409e-30</td>\n",
       "      <td>9.368972e-30</td>\n",
       "      <td>-1.374349e-29</td>\n",
       "      <td>-2.731887e-30</td>\n",
       "      <td>9.092608e-30</td>\n",
       "      <td>-2.127954e-30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.203531e-17</td>\n",
       "      <td>8.322390e-30</td>\n",
       "      <td>-5.602999e-29</td>\n",
       "      <td>-1.581445e-29</td>\n",
       "      <td>-2.616778e-30</td>\n",
       "      <td>-4.459988e-30</td>\n",
       "      <td>6.698170e-30</td>\n",
       "      <td>1.210625e-30</td>\n",
       "      <td>-4.544609e-30</td>\n",
       "      <td>1.087358e-30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows  357 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0             1             2             3             4    \\\n",
       "0    3.781943e-02  3.007997e-16 -1.812873e-16  5.220196e-17 -1.362745e-17   \n",
       "1    2.014843e-16 -7.818688e-29  4.835970e-28  1.370093e-28  2.401517e-29   \n",
       "2   -3.947801e-18  1.813704e-30 -1.217105e-29 -3.440989e-30 -6.140219e-31   \n",
       "3    3.546293e-17 -2.020212e-29  1.199973e-28  3.328397e-29  5.883409e-30   \n",
       "4   -3.203531e-17  8.322390e-30 -5.602999e-29 -1.581445e-29 -2.616778e-30   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "352  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "353  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "354  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "355  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "356  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "              5             6             7             8             9    \\\n",
       "0    1.549284e-18 -3.236327e-17  1.082968e-17  4.965247e-17 -2.551926e-17   \n",
       "1    3.813243e-29 -5.631777e-29 -1.115197e-29  3.793552e-29 -9.408704e-30   \n",
       "2   -9.289169e-31  1.446439e-30  2.690396e-31 -9.738666e-31  2.723197e-31   \n",
       "3    9.368972e-30 -1.374349e-29 -2.731887e-30  9.092608e-30 -2.127954e-30   \n",
       "4   -4.459988e-30  6.698170e-30  1.210625e-30 -4.544609e-30  1.087358e-30   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "352  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "353  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "354  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "355  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "356  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "     ...  347  348  349  350  351  352  353  354  355  356  \n",
       "0    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "352  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "354  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "355  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "356  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[357 rows x 357 columns]"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gradccp0[0,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_u_grad_Log_ccp(q, x, Theta, Psi, active_mat, nest_count):\n",
    "    '''\n",
    "    This function calucates the gradient of the log choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        x: a (N,J,J) numpy array of covariates\n",
    "        Theta: a (K+G,) numpy array of IPDL parameters\n",
    "        Psi: a dictionary of the \\Psi^g matrices as columns as outputted 'Create_incidence_matrix'\n",
    "    \n",
    "    Returns\n",
    "        Epsilon: a (N,J,K) numpy array of partial derivatives of the log choice proabilities wrt. characteristics\n",
    "    '''\n",
    "\n",
    "    assert q.ndim == 2\n",
    "    assert x.ndim == 3\n",
    "    assert Theta.ndim == 1\n",
    "\n",
    "    N,J,K = x.shape\n",
    "    Active_indicator = active_mat[:,:,None]*active_mat[:,None,:]\n",
    "    ccp_grad = ccp_gradient(q, x, Theta, Psi, active_mat, nest_count)\n",
    "    inv_q = np.divide(1, q, out=np.zeros(q.shape), where= q!=0)\n",
    "    #inv_Q = np.einsum('tj,jk->tjk', inv_q, np.eye(J))\n",
    "    Epsilon = Active_indicator*inv_q[:,:,None]*ccp_grad # Is equivalent to (1./q)[:,:,None]*ccp_grad an elementwise product. Einsum merely divides through by the nj'th elemnt of q in k'th row of ccp_grad.\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_elasticity(q, x, Theta, Psi, active_mat, nest_count, char_number = 0):\n",
    "    ''' \n",
    "    This function calculates the elasticity of choice probabilities wrt. any characteristic or nest grouping of products\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        x: a (N,J,K) numpy array of covariates\n",
    "        Theta: a (K+G,) numpy array of IPDL parameters\n",
    "        Psi: a dictionary of the \\Psi^g matrices as columns as outputted 'Create_incidence_matrix'\n",
    "        char_number: an integer which is an index of the parameter in theta wrt. which we wish calculate the elasticity \n",
    "\n",
    "    Returns\n",
    "        an (N,J,J) array of choice probability elasticities\n",
    "    '''\n",
    "    return IPDL_u_grad_Log_ccp(q, x, Theta, Psi, active_mat, nest_count)*Theta[char_number]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "       0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "       0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "       0.1       , 0.1       , 0.1       , 0.1       , 0.1       ,\n",
       "       0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905,\n",
       "       0.04761905, 0.04761905, 0.04761905, 0.04761905, 0.04761905])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using guess parameters $\\hat \\theta^0$ we calculate price-to-log-income elasticities for individual $i=0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>347</th>\n",
       "      <th>348</th>\n",
       "      <th>349</th>\n",
       "      <th>350</th>\n",
       "      <th>351</th>\n",
       "      <th>352</th>\n",
       "      <th>353</th>\n",
       "      <th>354</th>\n",
       "      <th>355</th>\n",
       "      <th>356</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.173593</td>\n",
       "      <td>1.380682e-15</td>\n",
       "      <td>-8.321156e-16</td>\n",
       "      <td>2.396090e-16</td>\n",
       "      <td>-6.255052e-17</td>\n",
       "      <td>7.111274e-18</td>\n",
       "      <td>-1.485487e-16</td>\n",
       "      <td>4.970867e-17</td>\n",
       "      <td>2.279068e-16</td>\n",
       "      <td>-1.171344e-16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>-0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>357 rows  357 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0             1             2             3             4    \\\n",
       "0    0.173593  1.380682e-15 -8.321156e-16  2.396090e-16 -6.255052e-17   \n",
       "1    0.000000 -0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "2   -0.000000  0.000000e+00 -0.000000e+00 -0.000000e+00 -0.000000e+00   \n",
       "3    0.000000 -0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "4   -0.000000  0.000000e+00 -0.000000e+00 -0.000000e+00 -0.000000e+00   \n",
       "..        ...           ...           ...           ...           ...   \n",
       "352  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "353  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "354  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "355  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "356  0.000000  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "              5             6             7             8             9    \\\n",
       "0    7.111274e-18 -1.485487e-16  4.970867e-17  2.279068e-16 -1.171344e-16   \n",
       "1    0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00 -0.000000e+00   \n",
       "2   -0.000000e+00  0.000000e+00  0.000000e+00 -0.000000e+00  0.000000e+00   \n",
       "3    0.000000e+00 -0.000000e+00 -0.000000e+00  0.000000e+00 -0.000000e+00   \n",
       "4   -0.000000e+00  0.000000e+00  0.000000e+00 -0.000000e+00  0.000000e+00   \n",
       "..            ...           ...           ...           ...           ...   \n",
       "352  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "353  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "354  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "355  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "356  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00  0.000000e+00   \n",
       "\n",
       "     ...  347  348  349  350  351  352  353  354  355  356  \n",
       "0    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4    ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "..   ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  \n",
       "352  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "353  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "354  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "355  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "356  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[357 rows x 357 columns]"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon0 = IPDL_elasticity(q0_hat, x, theta0, Psi_stack, A, Nest_count)\n",
    "pd.DataFrame(epsilon0[0,:,:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of IPDL\n",
    "\n",
    "The log-likelihood contribution is\n",
    "$$\n",
    "\\ell_i(\\theta)=y_i'\\ln p(\\mathbf{X}_i,\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_i$ and $\\theta$ - calculates $u_i=\\mathbf{X}_i\\beta$ and constructs $\\Gamma$, and then calls the fixed point routine described above. That routine will return $p(\\mathbf{X}_i,\\theta)$, and we can then evaluate $\\ell_i(\\theta)$. Using our above defined functions we now construct precisely such an estimation procedure.\n",
    "\n",
    "For maximizing the likelihood, we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_i=p(\\mathbf{X}_i,\\theta)$, then we have\n",
    "$$\n",
    "\\nabla_\\theta \\ln p(\\mathbf{X}_i,\\theta)=\\mathrm{diag}(q_i)^{-1}\\left(\\nabla_{qq}^2\\Omega(q_i|\\lambda)^{-1}-q_iq_i' \\right)\\left[\\mathbf{X}_i,-\\nabla_{q,\\lambda}^2 \\Omega(q_i|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P(u|\\lambda)$ and the last term is a block matrix of size $J\\times dim(\\theta)$. The derivative of the log-likelihood function can be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_i(\\theta)=\\nabla_\\theta \\ln p(\\mathbf{X}_i,\\theta)' y_i \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_loglikelihood(Theta, y, x, psi_stack, active_mat, nest_count):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a numpy array (N,J) of observed choices in onehot encoding,\n",
    "        x: a numpy matrix (N,J,K) of covariates,\n",
    "        Psi: a dictionary of the matrices \\psi^g as columns as outputted by 'Create_incidence_matrix'\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (N,) of IPDL loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    N,J,K = x.shape\n",
    "\n",
    "    gamma = Create_Gamma(Theta[K:], psi_stack, nest_count) # The last G parameters of theta are the nesting parameters \\lambda_g\n",
    "    ccp_hat = IPDL_ccp(Theta[:K], x, gamma, active_mat) # The first K parameters of theta are those of \\beta\n",
    "\n",
    "    ll = np.log(np.einsum('tj,tj->j',y,ccp_hat)) # DOESNT WORK! For each individual find (the log of) the choice probability of the chosen alternative. Is an (N,) array\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.60409150e+02, -8.41949808e-02, -3.75812167e-02, -1.78383974e-02,\n",
       "       -5.05103270e-03, -3.98470478e-02, -3.14959823e-02, -8.59679824e-02,\n",
       "       -5.49939710e-03, -5.23021929e-04, -2.12700410e-02, -4.48311829e-01,\n",
       "       -2.16972435e-01, -4.29845087e-01, -2.22580082e-01, -1.26231318e-01,\n",
       "       -2.98305685e-01, -2.90984197e-01, -3.34671172e-02, -1.05236275e-01,\n",
       "       -7.31674613e-02, -3.24824508e-02, -2.02865985e-01, -1.00429418e-01,\n",
       "       -7.72329669e-02, -1.39688023e-01, -7.82234211e-02, -8.20218088e-03,\n",
       "       -1.76633288e-02, -1.42700492e-03, -1.97417888e-03, -1.65183164e-01,\n",
       "       -3.36900845e-01, -5.56133108e-01, -1.27764231e-01, -4.47449320e-02,\n",
       "       -7.59956295e-02, -3.38279936e-02, -7.95835521e-03, -1.80427857e-01,\n",
       "       -3.63448663e-01, -4.88764008e-02, -1.27080558e-01, -3.86846778e-01,\n",
       "       -4.30332034e-04, -4.21568045e-02, -3.50370423e-01, -7.11447280e-01,\n",
       "       -9.70295142e-01, -9.83250723e-02, -2.13760336e-01, -7.22458475e-02,\n",
       "       -9.45383752e-02, -2.58509218e-01, -1.18193771e-01, -1.85568332e-01,\n",
       "       -1.21521176e-02, -8.85123575e-02, -1.14823073e-03, -3.76540445e-03,\n",
       "       -4.22194442e-03, -3.48721604e-03, -3.18253502e-02, -3.24287865e-03,\n",
       "       -1.43442914e-03, -1.30763565e-02, -4.39507860e-03, -2.42319826e-04,\n",
       "       -1.59128813e-03, -1.31381366e-03, -8.36190265e-04, -3.00271088e-02,\n",
       "       -2.21796333e-03, -4.81276267e-03, -2.13557348e-01, -4.76903832e-02,\n",
       "       -3.61972388e-02, -2.03528133e-02, -2.59402611e-02, -1.30439839e-01,\n",
       "       -2.22441235e-02, -8.89008034e-04, -2.11746989e-02, -2.10749121e-01,\n",
       "       -1.20335451e-01, -1.42882268e-02, -3.40798212e-03, -1.24823198e-01,\n",
       "       -3.12870108e-01, -9.03964723e-02, -1.07466563e-01, -9.86367179e-02,\n",
       "       -3.37484603e-02, -3.52768403e-03, -4.80570834e-02, -4.20821990e-03,\n",
       "       -2.75064423e-03, -4.88241471e-04, -1.64583309e-01, -1.82234402e-01,\n",
       "       -6.60468949e-02, -6.42264733e-02, -3.83374868e-03, -8.45873300e-03,\n",
       "       -1.25749079e-01, -8.91357540e-04, -8.18222637e-03, -1.18748640e-02,\n",
       "       -4.35258067e-01, -7.53312660e-01, -3.38124296e-01, -9.28033024e-02,\n",
       "       -9.44354253e-03, -3.43391705e-01, -1.94194250e-01, -2.48958635e-03,\n",
       "       -1.60878966e-02, -2.99591694e-02, -3.37370236e-01, -5.98639614e-01,\n",
       "       -1.62054635e-01, -2.30819466e-01, -5.37475509e-02, -2.03723014e-02,\n",
       "       -1.71521015e-01, -1.45283319e-01, -1.32731199e-02, -1.24826099e-01,\n",
       "       -1.23678651e-01, -1.88605586e-01, -1.76110455e-01, -2.28680212e-01,\n",
       "       -8.60519901e-01, -2.56735325e-01, -1.59783604e-01, -6.17823510e-02,\n",
       "       -4.14117060e-01, -1.14281551e-01, -1.44875912e-01, -1.85654483e-01,\n",
       "       -5.87900219e-02, -1.14874030e-01, -7.44534569e-03, -1.19748492e-01,\n",
       "       -9.97234033e-02, -1.78379239e-01, -1.84455858e-02, -1.28965572e-01,\n",
       "       -1.30801529e-01, -1.51293404e-01, -3.73384911e-02, -3.74075670e-02,\n",
       "       -4.81562452e-02, -1.27435665e-01, -2.16542548e-02, -9.71983509e-05,\n",
       "       -1.56219758e-02, -2.26655965e-03, -1.05594108e-02, -4.63208868e-02,\n",
       "       -8.53685334e-03, -1.22523262e-02, -3.71314668e-02, -6.69014877e-02,\n",
       "       -1.17184574e-02, -5.15858731e-02, -1.67543659e-02, -2.39596502e-02,\n",
       "       -1.00135430e-02, -4.54273082e-03, -2.05548207e-02, -1.05955451e-01,\n",
       "       -3.52650240e-03, -3.74160288e-03, -3.67871211e-03, -1.77231423e-02,\n",
       "       -4.55543978e-03, -5.59516258e-04, -3.63725728e-03, -2.50330463e-03,\n",
       "       -8.08459555e-04, -1.12487448e-02, -2.61969560e-02, -1.12457036e-03,\n",
       "       -8.21425939e-02, -2.85391932e-01, -7.03418843e-02, -1.34772609e-02,\n",
       "       -4.24300281e-02, -1.94939736e-02, -9.52529582e-03, -2.05848120e-03,\n",
       "       -2.15159469e-03, -3.93802308e-01, -1.28472747e+00, -1.12490831e-01,\n",
       "       -3.55492853e-01, -7.24435796e-03, -1.55730390e-01, -2.62463033e-02,\n",
       "       -2.81032337e-02, -4.62770589e-02, -3.16807174e-02, -3.56102428e-02,\n",
       "       -2.56130583e-02, -1.15398671e-02, -1.26147123e-02, -4.44093328e-03,\n",
       "       -6.99341952e-02, -2.18374593e-02, -2.04639219e-02, -1.01980437e-02,\n",
       "       -7.61989593e-03, -1.12715383e-03, -1.48655214e-03, -9.45347982e-02,\n",
       "       -2.29483356e-02, -4.96917955e-03, -2.77165881e-02, -4.18949473e-03,\n",
       "       -9.04660574e-03, -2.92406290e-02, -1.25138263e-02, -8.71856862e-03,\n",
       "       -1.67908714e-02, -1.35550750e-02, -1.12295775e-02, -4.63770266e-03,\n",
       "       -4.17754177e-02, -1.04508206e-02, -3.21477585e-02, -7.24043944e-03,\n",
       "       -8.10195008e-04, -7.77452129e-02, -3.53048403e-02, -1.13501576e-02,\n",
       "       -8.17485013e-03, -1.83486693e-02, -4.36573430e-02, -3.63705233e-02,\n",
       "       -1.54503050e-02, -1.64514202e-03, -1.16081077e-01, -3.70368647e-02,\n",
       "       -5.90929942e-03, -8.03877748e-03, -1.50718605e-02, -9.05729084e-04,\n",
       "       -9.13897218e-04, -1.70453382e-03, -1.54328621e-03, -4.09804054e-03,\n",
       "       -1.04165962e-02, -2.24602618e-03, -1.05808532e-03, -9.91212943e-03,\n",
       "       -2.81640470e-02, -1.46271515e-03, -2.65095599e-04, -1.26283227e-03,\n",
       "       -7.27914524e-03, -1.70938339e-03, -6.34217063e-04, -7.29948015e-04,\n",
       "       -8.90353400e-03, -3.01649511e-04, -2.25314801e-04, -1.54317432e-02,\n",
       "       -6.16325064e-02, -1.45566206e-01, -2.00032035e-02, -4.30085077e-02,\n",
       "       -1.40357896e-04, -1.25625003e-02, -4.87122433e-02, -4.45423821e-04,\n",
       "       -3.37034342e-03, -3.62228713e-04, -9.79943251e-03, -7.85649397e-04,\n",
       "       -8.27419271e-03, -1.97821257e-02, -1.31077800e-03, -4.45875442e-05,\n",
       "       -5.85106244e-05, -5.82977471e-02, -1.98441717e-04, -2.87564097e-04,\n",
       "       -5.89841235e-02, -4.04799223e-02, -3.04551327e-03, -5.73037015e-03,\n",
       "       -8.77817294e-03, -1.17217756e-03, -1.29834735e-04, -1.04924219e-02,\n",
       "       -2.99461484e-02, -1.48954523e-02, -6.85468833e-03, -2.04569448e-02,\n",
       "       -1.52615975e-02, -5.08131373e-02, -2.36938438e-03, -4.67284579e-03,\n",
       "       -1.38714019e-01, -4.75795943e-03, -1.41029248e-02, -3.21648029e-03,\n",
       "       -4.44391646e-03, -7.31006459e-02, -1.54325899e-02, -4.21775475e-03,\n",
       "       -9.41575845e-02, -7.43641158e-02, -1.84392711e-02, -1.11794584e-02,\n",
       "       -7.73468394e-03, -4.66297234e-03, -3.40584538e-02, -1.02288183e-01,\n",
       "       -4.57214225e-02, -6.96759198e-02, -1.14510670e-03, -2.03219435e-02,\n",
       "       -1.72843861e-02, -4.01579680e-02, -9.59688815e-03, -5.77000108e-03,\n",
       "       -2.72934670e-02, -7.03231120e-03, -4.60442435e-02, -1.60944263e-02,\n",
       "       -1.83434836e-05, -2.45297298e-03, -5.40399107e-03, -5.91964257e-03,\n",
       "       -1.10999238e-02, -2.82865968e-04, -2.43543681e-03, -5.92552730e-03,\n",
       "       -1.06552731e-03, -7.05988562e-04, -4.49400288e-03, -3.27626626e-02,\n",
       "       -4.92911482e-03, -4.85522228e-04, -2.01144232e-03, -2.72991529e-03,\n",
       "       -3.91703407e-02, -4.29934276e-03, -1.30028685e-02, -1.71962823e-03,\n",
       "       -1.32173786e-03, -1.01213322e-03, -5.92287660e-03, -7.67721386e-03,\n",
       "       -9.93915254e-03])"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilon0 = 1.0e-10\n",
    "np.einsum('tj,tj->j', y, np.log(q0_hat + epsilon0)) - epsilon0*np.einsum('tj,t->j', y, np.ones((T,)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
