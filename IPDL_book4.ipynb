{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Demand for Cars with the IPDL model\n",
    "\n",
    "In this notebook, we will introduce and estimate the Inverse Product Differentiation Logit (IPDL) model of Fosgerau et al. (2023) using publically available data on the European car market from Frank Verboven's website at https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market. We begin by introducing the data set. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "The dataset consists of approximately 110 vehicle makes per year in the period 1970-1999 in five European markets (Belgium, France, Germany, Italy, and the United Kingdom). The data set includes 47 variables in total. The first four columns are market and product codes for the year, country, and make as well as quantity sold (No. of new registrations) which will be used in computing observed market shares. The remaining variables consist of car characteristics such as prices, horse power, weight and other physical car characteristics as well as macroeconomic variables such as GDP per capita which have been used to construct estimates of the average wage income and purchasing power.\n",
    "\n",
    "We have in total 30 years and 5 countries, totalling $T=150$ year-country combinations, indexed by $t$, and we refer to each simply as market $t$. In market $t$, the choice set is $\\mathcal{J}_t$ which includes the set of available makes as well as an outside option. Let $\\mathcal{J} := \\bigcup_{t=1}^T \\mathcal{J}_t$ be the full choice set and \n",
    " $J:=\\#\\mathcal{J}$ the number of choices which were available in at least one market, for this data set there are $J=357$ choices.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the dataset `eurocars.csv` we thus have a dataframe of $\\sum_{t=1}^T \\#\\mathcal{J}_t = 11459$ rows and $47$ columns. The `ye` column runs through $y=70,\\ldots,99$, the `ma` column runs through $m=1,\\ldots,M$, and the ``co`` column takes values $j\\in \\mathcal{J}$. \n",
    "\n",
    "Because we consider a country-year pair as the level of observation, we construct a `market` column taking values $t=1,\\ldots,T$. In Python, this variable will take values $t=0,\\ldots,T-1$. We construct an outside option $j=0$ in each market $t$ by letting the 'sales' of $j=0$ be determined as \n",
    "\n",
    "$$\\mathrm{sales}_{0t} = \\mathrm{pop}_t - \\sum_{j=1}^J \\mathrm{sales}_{jt}$$\n",
    "\n",
    "where $\\mathrm{pop}_t$ is the total population in market $t$, and the car characteristics of the outside option is set to zero. The market shares of each product in market $t$ can then be found as\n",
    "$$\n",
    "\\textrm{market share}_{jt}=\\frac{\\mathrm{sales_{jt}}}{\\mathrm{pop}_t}.\n",
    "$$\n",
    "We also read in the variable description of the dataset contained in `eurocars.dta`. We will use the list `x_vars` throughout to work with our explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 834,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "\n",
    "# Files\n",
    "import Logit_file as logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 835,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and variable names\n",
    "# os.chdir('../GREENCAR_notebooks/') # Assigns work directory\n",
    "\n",
    "input_path = os.getcwd() # Assigns input path as current working directory (cwd)\n",
    "descr = (pd.read_stata('eurocars.dta', iterator = True)).variable_labels() # Obtain variable descriptions\n",
    "dat = pd.read_csv(os.path.join(input_path, 'eurocars.csv')) # reads in the data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 836,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ye</td>\n",
       "      <td>year (=first dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ma</td>\n",
       "      <td>market (=second dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>co</td>\n",
       "      <td>model code (=third dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zcode</td>\n",
       "      <td>alternative model code (predecessors and succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brd</td>\n",
       "      <td>brand code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type</td>\n",
       "      <td>name of brand and model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>name of model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>org</td>\n",
       "      <td>origin code (demand side, country with which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>loc</td>\n",
       "      <td>location code (production side, country where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frm</td>\n",
       "      <td>firm code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qu</td>\n",
       "      <td>sales (number of new car registrations)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pl</td>\n",
       "      <td>places (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>doors (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>li1</td>\n",
       "      <td>measure 1 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>li2</td>\n",
       "      <td>measure 2 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>li3</td>\n",
       "      <td>measure 3 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>princ</td>\n",
       "      <td>=pr/(ngdp/pop): price relative to per capita i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eurpr</td>\n",
       "      <td>=pr/avdexr: price in common currency (in SDR t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>exppr</td>\n",
       "      <td>=pr/avexr: price in exporter currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>avexr</td>\n",
       "      <td>av. exchange rate of exporter country (exporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>avdexr</td>\n",
       "      <td>av. exchange rate of destination country (dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>avcpr</td>\n",
       "      <td>av. consumer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>avppr</td>\n",
       "      <td>av. producer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>avdcpr</td>\n",
       "      <td>av. consumer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>avdppr</td>\n",
       "      <td>av. producer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xexr</td>\n",
       "      <td>avdexr/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tax</td>\n",
       "      <td>percentage VAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pop</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ngdp</td>\n",
       "      <td>nominal gross domestic product of destination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rgdp</td>\n",
       "      <td>real gross domestic product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>engdp</td>\n",
       "      <td>=ngdp/avdexr: nominal gdp in common currency (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ergdp</td>\n",
       "      <td>=rgdp/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>engdpc</td>\n",
       "      <td>=engdp/pop: nominal gdp per capita in common c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ergdpc</td>\n",
       "      <td>=ergdp/pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              ye                   year (=first dimension of panel)\n",
       "1              ma                market (=second dimension of panel)\n",
       "2              co             model code (=third dimension of panel)\n",
       "3           zcode  alternative model code (predecessors and succe...\n",
       "4             brd                                         brand code\n",
       "5            type                            name of brand and model\n",
       "6           brand                                      name of brand\n",
       "7           model                                      name of model\n",
       "8             org  origin code (demand side, country with which c...\n",
       "9             loc  location code (production side, country where ...\n",
       "10            cla                              class or segment code\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            frm                                          firm code\n",
       "13             qu            sales (number of new car registrations)\n",
       "14             cy            cylinder volume or displacement (in cc)\n",
       "15             hp                                 horsepower (in kW)\n",
       "16             we                                     weight (in kg)\n",
       "17             pl             places (number, not reliable variable)\n",
       "18             do              doors (number, not reliable variable)\n",
       "19             le                                     length (in cm)\n",
       "20             wi                                      width (in cm)\n",
       "21             he                                     height (in cm)\n",
       "22            li1  measure 1 for fuel efficiency (liter per km, a...\n",
       "23            li2  measure 2 for fuel efficiency (liter per km, a...\n",
       "24            li3  measure 3 for fuel efficiency (liter per km, a...\n",
       "25             li          average of li1, li2, li3 (used in papers)\n",
       "26             sp                            maximum speed (km/hour)\n",
       "27             ac  time to acceleration (in seconds from 0 to 100...\n",
       "28             pr   price (in destination currency including V.A.T.)\n",
       "29          princ  =pr/(ngdp/pop): price relative to per capita i...\n",
       "30          eurpr  =pr/avdexr: price in common currency (in SDR t...\n",
       "31          exppr              =pr/avexr: price in exporter currency\n",
       "32          avexr  av. exchange rate of exporter country (exporte...\n",
       "33         avdexr  av. exchange rate of destination country (dest...\n",
       "34          avcpr       av. consumer price index of exporter country\n",
       "35          avppr       av. producer price index of exporter country\n",
       "36         avdcpr    av. consumer price index of destination country\n",
       "37         avdppr    av. producer price index of destination country\n",
       "38           xexr                                       avdexr/avexr\n",
       "39            tax                                     percentage VAT\n",
       "40            pop                                         population\n",
       "41           ngdp  nominal gross domestic product of destination ...\n",
       "42           rgdp                        real gross domestic product\n",
       "43          engdp  =ngdp/avdexr: nominal gdp in common currency (...\n",
       "44          ergdp                                        =rgdp/avexr\n",
       "45         engdpc  =engdp/pop: nominal gdp per capita in common c...\n",
       "46         ergdpc                                         =ergdp/pop"
      ]
     },
     "execution_count": 836,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(descr, index=['description']).transpose().reset_index().rename(columns={'index' : 'variable names'}) # Prints data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variable names                                        description\n",
      "0              cy            cylinder volume or displacement (in cc)\n",
      "1              hp                                 horsepower (in kW)\n",
      "2              we                                     weight (in kg)\n",
      "3              le                                     length (in cm)\n",
      "4              wi                                      width (in cm)\n",
      "5              he                                     height (in cm)\n",
      "6              li          average of li1, li2, li3 (used in papers)\n",
      "7              sp                            maximum speed (km/hour)\n",
      "8              ac  time to acceleration (in seconds from 0 to 100...\n",
      "9              pr   price (in destination currency including V.A.T.)\n",
      "10          brand                                      name of brand\n",
      "11           home  domestic car dummy (appropriate interaction of...\n"
     ]
    }
   ],
   "source": [
    "# Choose which variables to include in the analysis, and assign them either as discrete variables or continuous.\n",
    "\n",
    "x_discretevars = [ 'brand', 'home']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "# x_ivvars = ...\n",
    "x_allvars =  [*x_contvars, *x_discretevars]\n",
    "\n",
    "# Outside option is included if OO == True, otherwise analysis is done on the inside options only.\n",
    "OO = False\n",
    "\n",
    "# Print list of chosen variables as a dataframe\n",
    "print(pd.DataFrame(descr, index=['description'])[x_allvars].transpose().reset_index().rename(columns={'index' : 'variable names'}))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the data to fit our setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 838,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].str.replace(',', '.').astype('float64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].str.replace(',', '.').astype('float64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].str.replace(',', '.').astype('float64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:30: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3455450421.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].str.replace(',', '.').astype('float64')\n"
     ]
    }
   ],
   "source": [
    "# Create the 'market' column of market index t\n",
    "\n",
    "dat = dat.sort_values(by = ['ye', 'ma'], ascending = True) # Sorts data set by year and market\n",
    "Used_cols = [*dat.keys()[:28], 'pr', 'princ', 'pop', 'xexr']  \n",
    "dat = dat[Used_cols] # Leaves out unused macro variables\n",
    "market_vals = [*iter.product(dat['ye'].unique(), dat['ma'].unique())] # creates a list of ma-ye combinations\n",
    "market_vals = pd.DataFrame({'ye' : [val[0] for val in market_vals], 'ma' : [val[1] for val in market_vals]}) \n",
    "market_vals = market_vals.reset_index().rename(columns={'index' : 'market'}) # Creates market index\n",
    "dat = dat.merge(market_vals, left_on=['ye', 'ma'], right_on=['ye', 'ma'], how='left') # Merges market index variable onto dat\n",
    "dat_org = dat # Save the original data with the 'market'-column added as 'dat_org'.\n",
    "\n",
    "# Create an inside/outside-option column if the outside option is included\n",
    "\n",
    "if OO:\n",
    "    dat['in_out'] = 1\n",
    "else:\n",
    "    None\n",
    "\n",
    "# Drop rows which contain NaN values in any explanatory variable or in the response variable.\n",
    "\n",
    "dat = dat.dropna()\n",
    "\n",
    "# Convert discrete explanatory variables to integer valued variables and make sure continuous variables are floats.\n",
    "\n",
    "obj_columns = dat.select_dtypes(['object'])\n",
    "for col in obj_columns:\n",
    "    if col in [*x_contvars, 'xexr']:\n",
    "        dat[col] = dat[col].str.replace(',', '.').astype('float64')\n",
    "    else:\n",
    "        dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 839,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3556445044.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64') # re-assigns category zero as category 1, and moves other categories up by one\n"
     ]
    }
   ],
   "source": [
    "# Re-encode discrete variables such that only the outside option takes the value 0\n",
    "\n",
    "x_0vars = [var for var in x_discretevars if len(dat[(dat['co'] != 0)&(dat[var].isin([0]))]) > 0] # Picks out discrete variables where at least one car has category 0\n",
    "\n",
    "for col in x_0vars:\n",
    "    dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64') # re-assigns category zero as category 1, and moves other categories up by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct outside option for each market t\n",
    "if OO:\n",
    "    outside_shares = dat.groupby('market', as_index=False)['qu'].sum() # sum of sales in each market\n",
    "    outside_shares = outside_shares.merge(dat[['market', 'pop']], on = 'market', how='left').dropna().drop_duplicates(subset = 'market', keep = 'first')  # Adds population to dataframe\n",
    "    outside_shares['qu'] = outside_shares['pop'] - outside_shares['qu'] # Assigns quantity for outside option as pop minus sum of sales\n",
    "    keys_add = [key for key in dat.keys() if (key!='market')&(key!='qu')&(key!='pop')] \n",
    "    for key in keys_add:\n",
    "        outside_shares[key] = 0 # Sets all variables other than market, qu and pop to zero for the outside option\n",
    "\n",
    "    dat = pd.concat([dat, outside_shares]) # Add outside option to data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\514239637.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat['ms'] = dat.groupby('market')['qu'].transform(lambda x: x/x.sum())\n"
     ]
    }
   ],
   "source": [
    "# Compute market shares for each product j in each market t \n",
    "\n",
    "dat['ms'] = dat.groupby('market')['qu'].transform(lambda x: x/x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 842,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = dat['market'].nunique() # Assigns the total number of markets T\n",
    "J = np.array([dat[dat['market'] == t]['co'].nunique() for t in np.arange(T)]) # Array of number of choices in market t\n",
    "\n",
    "\n",
    "# Number of observations \n",
    "if OO:\n",
    "    N = np.array([dat[dat['market'] == t]['pop'].unique().sum() for t in np.arange(T)]).sum() # If outside option is included, number of observations in market t is the total population\n",
    "else:\n",
    "    N = np.array([dat[dat['market'] == t]['qu'].sum() for t in np.arange(T)]).sum() # If outside option is not included, number of observations in market t is the total number of sales\n",
    "\n",
    "\n",
    "# Get each market's share of total population N\n",
    "pop_share = np.empty((T,))\n",
    "for t in np.arange(T):\n",
    "    pop_share[t] = dat[dat['market'] == t]['qu'].sum() / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000000000002"
      ]
     },
     "execution_count": 843,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pop_share.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3839401880.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[x_contvars] = dat[x_contvars] / dat[x_contvars].abs().max() # Rescale continuous variables so that they lie in the interval [-1,1]. This is done for numerical stability.\n"
     ]
    }
   ],
   "source": [
    "dat[x_contvars] = dat[x_contvars] / dat[x_contvars].abs().max() # Rescale continuous variables so that they lie in the interval [-1,1]. This is done for numerical stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct dummies of discrete variables. For each variable, one of the columns is left out due to colinearity\n",
    "\n",
    "dat_disc = pd.get_dummies(dat[x_discretevars], prefix = x_discretevars, columns=x_discretevars, drop_first=True)  \n",
    "\n",
    "# If outside option is included, then each variable results in a column which is 1 for the outside option, and zero for all other options. These columns are identical to the 'in_out' variable column,\n",
    "# so a second column must be dropped for each variable.\n",
    "if OO:\n",
    "    dat_disc = dat_disc[[var for var in dat_disc.keys() if not var.endswith('1')]] # Drops a second column from discrete columns if outside option is included\n",
    "\n",
    "dat = pd.concat([dat, dat_disc], axis = 1)\n",
    "\n",
    "if OO:\n",
    "    x_vars = ['in_out', *x_contvars, *dat_disc.keys() ]\n",
    "else:\n",
    "    x_vars = [*x_contvars, *dat_disc.keys() ]\n",
    "\n",
    "K = len(x_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of numpy arrays for each market. This allows the size of the data set to vary over markets.\n",
    "\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)} # Dict of explanatory variables\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)} # Dict of market shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 847,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has full rank\n"
     ]
    }
   ],
   "source": [
    "# This function tests whether the utility parameters are identified, by looking at the rank of the stacked matrix of explanatory variables.\n",
    "\n",
    "def rank_test(x):\n",
    "    x_stacked = np.concatenate([x[t] for t in np.arange(T)], axis = 0)\n",
    "    eigs=la.eig(x_stacked.T@x_stacked)[0]\n",
    "\n",
    "    if np.min(eigs)<1.0e-8:\n",
    "        print('x does not have full rank')\n",
    "    else:\n",
    "        print('x has full rank')\n",
    "\n",
    "rank_test(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed utility, logit and nested logit\n",
    "\n",
    "In the following, a vector $z\\in \\mathbb R^d$ is always a column vector. The IPDL model is a discrete choice model, where the probability vector over the alternatives is given by the solution to a utility maximization problem of the form\n",
    "$$\n",
    "p=\\arg\\max_{q\\in \\Delta} q'u-\\Omega(q)\n",
    "$$\n",
    "where $\\Delta$ is the probability simplex over the set of discrete choices, $u$ is a vector of payoffs for each option, $\\Omega$ is a convex function and $q'$ denotes the transpose of $q$. All additive random utility models can be represented in this way (Fosgerau and Sørensen (2021)). For example, the logit choice probabilities result from the perturbation function $\\Omega(q)=q'\\ln q$ where $\\ln q$ is the elementwise logarithm.\n",
    "\n",
    "In the nested logit model, the choice set is divided into a partition $\\mathcal C=\\left\\{C_1,\\ldots,C_L\\right\\}$, and the perturbation function is given by\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\sum_{\\ell =1}^L \\left( \\sum_{j\\in C_\\ell}q_j\\right)\\ln \\left( \\sum_{j\\in C}q_j\\right),\n",
    "$$\n",
    "where $\\lambda\\in [0,1)$ is a parameter. This function can be written equivalently as\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\left(\\psi q\\right)'\\ln \\left( \\psi q\\right),\n",
    "$$\n",
    "where $\\psi$ is a $J \\times L$ matrix, where $\\psi_{j\\ell}=1$ if option $j$ belongs to nest $C_\\ell$ and zero otherwise.\n",
    " This specification generates nested logit choice probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The IPDL model\n",
    "\n",
    "In the IPDL model, we allow for multiple nesting structures. For each $g=1,\\ldots, G$, let $\\mathcal C_g$ and $\\psi^g$ be constructed as described for the nested logit, and let $L_g$ be the number of nests in group $g$. The IPDL perturbation function is then\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\sum_g \\lambda_g) q'\\ln q +\\sum_g \\lambda_g \\left(\\psi^g q \\right)'\\ln \\left( \\psi^g q\\right),\n",
    "$$\n",
    "where $\\lambda=(\\lambda_1,\\ldots,\\lambda_G)$ is a parameter vector satisfying $\\lambda_g \\geq 0$ and $\\sum_g \\lambda_g<1$. In this model, each option belongs to $G\\geq 1$ nests. When $G=1$, it simplifies to the nested logit model, and when $\\sum_g \\lambda_g=0$, it simplifies to the logit model. The IPDL model therefore allows more flexibility than a single nested logit model in the types of substitution patterns it can represent, without having to specify a hierarchical structure over the nests.\n",
    "\n",
    "In this note, the nesting is done according to a subset of the explanatory variables. For categorical variables, each category is a nest. For continuous variables, the data set is partitioned according to the deciles of the variable, resulting in `at most` 10 nests of roughly equal size, as well as a nest for the outside option. This construction implies that $\\Omega$ is a function of the data.\n",
    "\n",
    "## Similarity and negative values of $\\lambda$\n",
    "\n",
    "Bla bla bla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 848,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OO:\n",
    "    nest_vars = [var for var in ['in_out', *x_allvars] if (var != 'pr')] # We nest over all variables other than price, but an alternative list can be specified here if desired.\n",
    "else:\n",
    "    nest_vars = [var for var in x_allvars if (var != 'pr')] # See above\n",
    "\n",
    "nest_cont_vars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac'] # The list of continuous variables, from which nests will be created according to the deciles of the distribution.\n",
    "\n",
    "G = len(nest_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The utility function\n",
    "\n",
    "Let $x_{tj}$ be the vector of product characteristics for option $j$ in market $t$, and let $X_t$ denote the $J_t\\times K $ matrix with elements $x_{tjk}$. The payoff of option $j$ is a linear function of the characteristics $x_{tj}$ of product $j$, which means that the vector of utilities may be written\n",
    "$$\n",
    "u(X_t,\\beta)=X_t\\beta.\n",
    "$$\n",
    "\n",
    "Letting $\\theta=(\\beta',\\lambda')'$ denote the full parameter vector of length $D=K+G$, the choice probabilities in market $t$ may be written as\n",
    "$$\n",
    "p_t(\\theta)=\\arg \\max_{q\\in \\Delta_{J_t}} \\left\\{q'X_t \\beta-(1-\\sum_g \\lambda_g)q'\\ln q +\\sum_{g=1}^G\\lambda_g \\left(\\psi^{gt} q \\right)'\\ln \\left(\\psi^{gt} q\\right)\\right\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-rescaling for numerical stability\n",
    "\n",
    "Let $\\alpha$ be a scalar, and let $\\iota$ be the all-ones vector in $\\mathbb R^J$. Note that $q'(u+\\alpha\\iota)=q'u+(q'\\iota)\\alpha=q'u+\\alpha$, since $q$ sums to one. For this reason, $\\alpha$ does not enter into the utility maximization when calculating $P(u+\\alpha\\iota|\\lambda)$, and we have $P(u+\\alpha\\iota|\\lambda)=P(u|\\lambda)$.\n",
    "\n",
    "This allows us to re-scale the utilities just as in the logit model, since $P(u-(\\max_{j}u_j)\\iota|\\lambda)=P(u|\\lambda)$. The numerical benefits of this approach carry over to the IPDL model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient and Hessian\n",
    "\n",
    "The gradient of $\\Omega$ with respect to the choice probabilities is\n",
    "\n",
    "$$\n",
    "\\nabla_q \\Omega_t(q|\\lambda)=(1-\\sum_g \\lambda_g)\\ln q+ \\sum_g \\lambda_g(\\psi^{gt})'\\ln \\left( \\psi^{gt}q\\right)+\\iota=\\ln q-Z_t(q)\\lambda+\\iota\n",
    "$$\n",
    "where $\\iota$ is the all-ones vector and\n",
    "$$Z_{tg}(q)=\\ln q - (\\psi^{tg})' \\ln (\\psi^{tg}q)$$\n",
    "\n",
    "The Hessian of $\\Omega$ is\n",
    "$$\n",
    "\\nabla_{qq}^2 \\Omega_t(q|\\lambda)=(1-\\sum_g \\lambda_g) \\mathrm{diag}(q)^{-1}+\\sum_g\\lambda_g (\\psi^{gt})'\\mathrm{diag}(\\psi^{gt}q)^{-1}\\psi^{gt}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\Gamma$, we can show that\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(\\Gamma q)'\\ln (\\Gamma q)+c\\\\\n",
    "\\nabla_q \\Omega(q|\\lambda)=\\Gamma'\\ln (\\Gamma q)+\\iota\\\\\n",
    "\\nabla^2_{qq}\\Omega(q|\\lambda)=\\Gamma'\\mathrm{diag}(\\Gamma q)^{-1}\\Gamma,\n",
    "$$\n",
    "where $c$ is a scalar that depends on $\\lambda$ but not on $q$ and therefore does not affect the utility maximization problem, $\\iota=(1,\\ldots,1)'\\in \\mathbb R^J$ is the all-ones vector and $\\mathrm{diag}(z)$ is a diagonal matrix with the elements of the vector $z$ on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For purposes of computing the gradient and Hessian of $\\Omega$, it is convenient to define\n",
    "$$\n",
    "\\Gamma=\\left(\\begin{array}{c}\n",
    "(1-\\sum_g \\lambda_g)I_J\\\\\n",
    "\\lambda_1 \\Psi^1\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_G \\Psi^G\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "where $I_J$ is the identity matrix in $\\mathbb R^J$. The matrix $\\Gamma$ is a block matrix with $J+\\sum_g C_g$ rows and $J$ columns. Note that \n",
    "\n",
    "$$\n",
    "\\Gamma q=\\left(\\begin{array}{c}\n",
    "(1-\\sum_g\\lambda_g)q \\\\\n",
    "\\lambda_1\\Psi^g q\\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_G \\Psi^Gq\n",
    "\\end{array}\\right)>0\n",
    "$$\n",
    "if $q>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 849,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_nests(data, markets_id, products_id, columns, cont_var = None, cont_var_bins = None, outside_option = True):\n",
    "    '''\n",
    "    This function creates the nest matrices \\Psi^{gt}, and stack them over g for each t.\n",
    "\n",
    "    Args.\n",
    "        data: a pandas DataFrame\n",
    "        markets_id: a string denoting the column of 'data' containing an enumeration t=0,1,...,T-1 of markets\n",
    "        products_id: a string denoting the column of 'data' containing product codes which uniquely identifies products\n",
    "        columns: a list containing the column names of columns in 'data' from which nest groupings g=0,1,...,G-1 for each market t are to be generated\n",
    "        cont_var: a list of the continuous variables in 'columns'\n",
    "        cont_var_bins: a list containing the number of bins to make for each continuous variable in 'columns'\n",
    "        outside_option: a boolean indicating whether the model is estimated with or without an outside option. Default is set to 'True' i.e. with an outside option.\n",
    "\n",
    "    Returns\n",
    "        Psi: a dictionary of length T of the J[t] by J[t] identity stacked on top of the Psi_g matrices for each market t and each gropuing g\n",
    "        nest_dict: a dictionary of length T of pandas series describing the structure of each nest for each market t and each grouping g\n",
    "        nest_count: a dictionary of length T of (G,) numpy arrays containing the amount of nests in each category g\n",
    "    '''\n",
    "\n",
    "    T = data[markets_id].nunique()\n",
    "    J = np.array([data[data[markets_id] == t][products_id].nunique() for t in np.arange(T)])\n",
    "    \n",
    "    # We include nest on outside vs. inside options. The amount of categories varies if the outside option is included in the analysis.\n",
    "    dat = data.sort_values(by = [markets_id, products_id]) # We sort the data in ascending, first according to market and then according to the product id\n",
    "    \n",
    "    Psi = {}\n",
    "    nest_dict = {}\n",
    "    nest_counts = {}\n",
    "\n",
    "    # Assign nests for products in each market t\n",
    "    for t in np.arange(T):\n",
    "        data_t = dat[dat[markets_id] == t] # Subset data on market t\n",
    "\n",
    "\n",
    "        ### Bin continuous variables according to quantiles of the variable\n",
    "\n",
    "        if cont_var == None:\n",
    "            None\n",
    "        else:\n",
    "            for var,n_bins in zip(cont_var,cont_var_bins):\n",
    "                if outside_option:\n",
    "                    q_dat = np.unique(np.quantile(data_t[var].rank(method = 'min'), q = np.arange(1,n_bins + 1) / n_bins)) # Get the unique 'n_bins' equally spaced quantiles of each continuous variable given in the cont_var list\n",
    "                    data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = [0.99,1, *q_dat], labels=False) # Quantiles are equally spaced with 'n_bins' quantiles for the variable. The outside option gets its own bin (0.99,1].\n",
    "                else:\n",
    "                    q_dat = np.unique(np.quantile(data_t[var].rank(method = 'min'), q = np.arange(1,n_bins + 1) / n_bins)) # Get the unique 'n_bins' equally spaced quantiles of each continuous variable given in the cont_var list\n",
    "                    data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
    "\n",
    "        nest_dict[t] = data_t[columns].apply(lambda col: list(np.unique(col))) # Get the unique values of each 'col' in columns\n",
    "        nest_counts[t] = data_t[columns].nunique().values # Find the number of unique values in each column in columns and output as a numpy array\n",
    "\n",
    "        nest_count_total = data_t[columns].nunique().sum() # Find the sum of nest counts L_g\n",
    "        nests = pd.get_dummies(data_t[columns], columns = columns).values.reshape((J[t], nest_count_total)).transpose() # Finds dummies for each category in columns, and converts these to numpy arrays of the appropiate size. Note that the data has been sorted according to market and then product.\n",
    "        Psi_t = np.concatenate([np.eye(J[t]), nests], axis = 0) # Stack a J[t] by J[t] identity on top of the stacked \\Psi^g matrices for each market t\n",
    "\n",
    "        Psi[t] = Psi_t\n",
    "\n",
    "    return Psi, nest_dict, nest_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 850,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\3709419965.py:46: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n"
     ]
    }
   ],
   "source": [
    "cont_bins=[np.int64(10) for i in range(len(nest_cont_vars))] # Sets the number of bins to 10 for each continuous variable.\n",
    "Psi, Nest_descr, Nest_count = Create_nests(dat, 'market', 'co', nest_vars, nest_cont_vars,cont_bins , outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 851,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Gamma(Lambda, Psi, nest_count):\n",
    "    '''\n",
    "    This function \n",
    "    '''\n",
    "\n",
    "    T = len(Psi)\n",
    "    \n",
    "    Gamma = {}\n",
    "    lambda0 = np.array([1 - sum(Lambda)])\n",
    "    Lambda_full = np.concatenate((lambda0, Lambda)) # create vector (1- sum(lambda), lambda_1, ..., lambda_G)\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        C,J = Psi[t].shape # The amount of alternatives in market t\n",
    "        Lambda_long = np.empty((C,)) # Initialize a row vector with as many rows as psi_stack\n",
    "        indices = np.concatenate((np.array([J]) , nest_count[t])).cumsum().astype('int64') # Get the indices of where the identity and the nests in psi_stack are located along the rows of psi_stack.\n",
    "\n",
    "        for i in np.arange(len(indices)):\n",
    "            if i == 0:\n",
    "                Lambda_long[0:(indices[i])] = Lambda_full[i] # Assign 1-sum(lambda) to the first J coordinates of Lambda_long\n",
    "            else:\n",
    "                Lambda_long[indices[i-1]:indices[i]] = Lambda_full[i] # Assign lambda_g to the coordinates of Lambda_long corresponding to the rows of psi_stack equal to the block matrix \\psi^g \n",
    "    \n",
    "        Gamma[t] =  np.einsum('c,cj->cj', Lambda_long, Psi[t]) # Compute hadamard product of lambda parameters and psi_stack\n",
    "\n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 852,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda0 = np.ones((G,))/(2*(G+1))\n",
    "Gamma0 = Create_Gamma(lambda0, Psi, Nest_count)\n",
    "\n",
    "theta0=np.ones((K+G,))/(K+G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "Suppose we are evaluating the choice probability function $p_t(\\theta)$ at some parameter vector $\\theta$. While it is possible to solve for the choice probabilities explicitly by numerical maximization, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Let $u_t=X_t\\beta$ and let $q_t^0$ be an initial guess of the choice probabilities, e.g. $q_t^0\\propto \\exp(X_t\\beta)$. Define further\n",
    "$$\n",
    "a=\\sum_{g:\\lambda_g\\geq 0} \\lambda_g   \\qquad b=\\sum_{g:\\lambda_g<0} |\\lambda_g|.\n",
    "$$\n",
    "\n",
    "The choice probabilities are then updated iteratively as\n",
    "$$\n",
    "q_t^{r} = \\frac{e^{v_t^{r}}}{\\sum_{j\\in \\mathcal J_t} e^{v_{tj}^{r}}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "v_t^{r} =\\ln q_t^{r-1}+\\left(u_t-\\nabla_q \\Omega_t(q^{r-1}_t|\\lambda)\\right)/(1+b).\n",
    "$$\n",
    "Using the definition of $Z_{gt}$ above, this becomes\n",
    "$$\n",
    "v^r_t=\\ln q_t^{r-1}+\\left(u_t+Z_{t}(q^{r-1})\\lambda-\\ln q_t^{r-1}  \\right)/(1+b) =  \\left( u_t+ b\\ln q^{r-1}_t+Z_{t}(q^{r-1})\\lambda\\right)/(1+b)\n",
    "$$\n",
    "\n",
    "\n",
    "For numerical stability, it can be a good idea to also do max-rescaling of $v^r_t$ at every iteration. The Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p_t(\\theta)||q_t^{r})\\leq \\frac{a+b}{1+b}D_{KL}(p_t(\\theta)||q^{r-1}_t).\n",
    "$$\n",
    "This is implemeneted in the function \"IPDL_ccp\" below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_ccp(Theta, x, psi, nest_count, tol = 1.0e-15, maximum_iterations = 1000):\n",
    "    '''\n",
    "    This function finds approximations to the true conditional choice probabilities given parameters.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        tol: tolerated approximation error\n",
    "        maximum_iterations: a no. of maximum iterations which if reached will stop the algorithm\n",
    "\n",
    "    Output\n",
    "        q_1: a dictionary of T numpy arrays (J[t],) of IPDL choice probabilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x) # Number of markets\n",
    "    K = x[0].shape[1] # Number of car characteristics\n",
    "\n",
    "    # Parameters\n",
    "    Beta = Theta[:K]\n",
    "    Lambda = Theta[K:]\n",
    "    G = len(Lambda)  # Number of groups\n",
    "\n",
    "    print(Lambda)\n",
    "    # Calculate small beta\n",
    "    C_minus = np.array([True if Lambda[g] < 0 else False for g in np.arange(G)])\n",
    "    print(C_minus) # Find the categories g with negative a negative parameter lambda_g\n",
    "    if C_minus.all() == False:\n",
    "        b = 0\n",
    "    else:    \n",
    "        b = np.abs(Lambda[C_minus]).sum() # sum of absolute value of negative lambda parameters.\n",
    "\n",
    "    Gamma = Create_Gamma(Lambda, psi, nest_count) # Find the Gamma matrix\n",
    "\n",
    "    u = {t: np.einsum('jk,k->j', x[t], Beta) for t in np.arange(T)} # Calculate linear utilities\n",
    "    q = {t: np.exp(u[t] - u[t].max()) / np.exp(u[t] - u[t].max()).sum() for t in np.arange(T)} # Find logit choice probabilities\n",
    "    q0 = q\n",
    "    \n",
    "    Epsilon = 1.0e-14\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        q1 = {}\n",
    "        for t in np.arange(T):\n",
    "            # Calculate v\n",
    "            psi_q = np.einsum('cj,j->c', psi[t], q0[t]) # Compute matrix product\n",
    "            log_psiq =  np.log(psi_q) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "            gamma_log_prod = np.einsum('cj,c->j', Gamma[t], log_psiq) # Compute matrix product\n",
    "            v = np.log(q0[t], out = -np.inf*np.ones_like(q0[t]), where = (q0[t] > 0)) + (u[t] - gamma_log_prod)/(1 + b) # Calculate v = log(q) + (u - Gamma^T %o% log(Gamma %o% q) %o% Gamma)/(1 + b)\n",
    "            v -= v.max(keepdims = True) # Do max rescaling wrt. alternatives\n",
    "\n",
    "            # Calculate iterated ccp q^k\n",
    "            numerator = np.exp(v)\n",
    "            denom = numerator.sum()\n",
    "            q1[t] = numerator/denom\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.array([np.sum((q1[t]-q0[t])**2/q[t]) for t in np.arange(T)])) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "\n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assert np.array([np.sum(q1[t]) for t in np.arange(T)]).all() == 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand derivatives and price Elasticity\n",
    "\n",
    "While the demand derivatives in the IPDL model are not quite as simple as in the logit model, they are still easy to compute. \n",
    "Let $q=P(u|\\lambda)$, then\n",
    "$$\n",
    "\\nabla_u P(u|\\lambda)=\\left(\\nabla^2_{qq}\\Omega(q|\\lambda)\\right)^{-1}-qq'\n",
    "$$\n",
    "where the $()^{-1}$ denotes the matrix inverse. The derivatives with respect to any $x_{ij\\ell}$ can now easily be computed by the chain rule,\n",
    "$$\n",
    "    \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{\\partial u_{ik}}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell,\n",
    "$$\n",
    "\n",
    "Finally, moving to price elasticity is the same as in the logit model, if $x_{ik\\ell}$ is the log price of product $k$ for individual $i$, then\n",
    "$$\n",
    "    \\mathcal{E}_{jk}= \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}\\frac{1}{P_j(u_i|\\lambda)}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{1}{P_j(u_i|\\lambda)}\\beta_\\ell=\\frac{\\partial \\ln P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell$$\n",
    "we can also write this compactly as\n",
    "$$\n",
    "\\nabla_u \\ln P(u|\\lambda)=\\mathrm{diag}(P(u|\\lambda))^{-1}\\nabla_u P(u|\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 854,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pertubation_hessian(q, x, Theta, psi, nest_count):\n",
    "    '''\n",
    "    This function calucates the hessian of the pertubation function \\Omega\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Hess: a dictionary of T numpy arrays (J[t],J[t]) of second partial derivatives of the pertubation function \\Omega for each market t\n",
    "    '''\n",
    "    \n",
    "    T = len(q.keys())\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    Gamma = Create_Gamma(Theta[K:], psi, nest_count) # Find the \\Gamma matrices \n",
    "    #Hess = {}\n",
    "    Hess={}\n",
    "    for t in np.arange(T):\n",
    "        psi_q = np.einsum('cj,j->c', psi[t], q[t]) # Compute a matrix product\n",
    "        Hess[t] = np.einsum('cj,c,cl->jl', Gamma[t], 1/psi_q, psi[t]) # Computes the product \\Gamma' diag(\\psi q)^{-1} \\psi (but faster)\n",
    "\n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_gradient(q, x, Theta, psi_stack, nest_count):\n",
    "    \n",
    "    '''\n",
    "    This function calucates the gradient of the choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K) of partial derivatives of the choice proabilities wrt. utilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Grad = {}\n",
    "    Hess = compute_pertubation_hessian(q, x, Theta, psi_stack, nest_count) # Compute the hessian of the pertubation function\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        inv_omega_hess = la.inv(Hess[t]) # (J,J) for each t=1,...,T , computes the inverse of the Hessian\n",
    "        qqT = q[t][:,None]*q[t][None,:] # (J,J) outerproduct of ccp's for each market t\n",
    "        Grad[t] = inv_omega_hess - qqT  # Compute IPDL gradient of ccp's wrt. utilities\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calucates the gradient of the log choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Epsilon: a dictionary of T numpy arrays (J[t],J[t]) of partial derivatives of the log choice proabilities of products j wrt. utilites of products k for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack, nest_count) # Find the gradient of ccp's wrt. utilities\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        #ccp_grad = Grad[t]\n",
    "        #inv_diagq = np.divide(1, q[t], out = np.inf*np.ones_like(q[t]), where = (q[t] > 0)) # Find the inverse of the ccp's and assign infinity to any entry if that entry has q = 0\n",
    "        Epsilon[t] = Grad[t]/q[t][:,None] # Computes diag(q)^{-1}Grad[t]\n",
    "        #np.einsum('j,jk->jk', inv_diagq, ccp_grad) # Computes a Hadamard product. Is equivalent to:   diag(q)^-1 %o% ccp_grad\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 857,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_elasticity(q, x, Theta, psi_stack, nest_count, char_number = K-1):\n",
    "    ''' \n",
    "    This function calculates the elasticity of choice probabilities wrt. any characteristic or nest grouping of products\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        char_number: an integer which is an index of the parameter in theta wrt. which we wish calculate the elasticity. Default is the index for the parameter of 'pr'.\n",
    "\n",
    "    Returns\n",
    "        a dictionary of T numpy arrays (J[t],J[t]) of choice probability semi-elasticities for each market t\n",
    "    '''\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count) # Find the gradient of log ccp's wrt. utilities\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = Grad[t]*Theta[char_number] # Calculate semi-elasticities\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using guess parameters $\\hat \\theta^0$ we calculate price-to-log-income elasticities for individual $i=0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(IPDL_elasticity(q0_hat, x, theta0, Psi, Nest_count)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of IPDL\n",
    "\n",
    "The log-likelihood contribution is\n",
    "$$\n",
    "\\ell_t(\\theta)=y_t'\\ln p(\\mathbf{X}_t,\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_t$ and $\\theta$ - calculates $u_t=\\mathbf{X}_t\\beta$ and constructs $\\Gamma$, and then calls the fixed point routine described above. That routine will return $p(\\mathbf{X}_t,\\theta)$, and we can then evaluate $\\ell_t(\\theta)$. Using our above defined functions we now construct precisely such an estimation procedure.\n",
    "\n",
    "For maximizing the likelihood, we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_t=p(\\mathbf{X}_t,\\theta)$, then we have\n",
    "$$\n",
    "\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)=\\mathrm{diag}(q_t)^{-1}\\left(\\nabla_{qq}^2\\Omega(q_t|\\lambda)^{-1}-q_tq_t' \\right)\\left[\\mathbf{X}_t,-\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P(u|\\lambda)$ and the last term is a block matrix of size $J\\times dim(\\theta)$. Note that the latter cross derivative $\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)$ is given by $\\nabla_{q,\\lambda} \\Omega(q_t|\\lambda)_g = \\ln(q) - (\\Psi^g)' \\ln(\\Psi^g q)$ for each row $g=1,\\ldots,G$. The derivative of the log-likelihood function can be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_t(\\theta)=\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)' y_t \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 858,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (T,) of IPDL loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "    ccp_hat = IPDL_ccp(Theta, x, psi_stack, nest_count)\n",
    "    sum_lambdaplus = np.array([theta for theta in Theta[K:] if theta >0]).sum()\n",
    "\n",
    "    if sum_lambdaplus >= 1:\n",
    "        ll = np.NINF*np.ones((T,))\n",
    "\n",
    "    else:\n",
    "        ll=np.empty((T,))\n",
    "        for t in np.arange(T):\n",
    "            ll[t] = sample_share[t]*(y[t].T@np.log(ccp_hat[t]))#np.einsum('j,j', y[t], np.log(ccp_hat[t], out = -np.inf*np.ones_like(ccp_hat[t]), where = (ccp_hat[t] > 0)))\n",
    "\n",
    "    print([sum_lambdaplus, -ll.mean()])\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' The negative loglikelihood criterion to minimize\n",
    "    '''\n",
    "    Q = -IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the derivative of the loglikehood wrt. parameters $\\nabla_\\theta \\ell_t(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation(q, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)} # Determine log(q), and set entries equal minus inifinity if entry <= 0\n",
    "    Z = {}\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        G = len(nest_count[t])\n",
    "        indices = np.int64(np.cumsum(nest_count[t])) # Find the indices of the categories g used in the psi_stack matrices\n",
    "        J = np.int64(psi_stack[t].shape[0] - np.sum(nest_count[t])) # Find the number of alternatives\n",
    "        Z_t = np.empty((J,G)) # Initialize a J[t] by G numpy matrix for market t\n",
    "\n",
    "        for g in np.arange(G):\n",
    "\n",
    "            # Find the \\psi^g matrix for category g\n",
    "            if g == 0:\n",
    "                Psi = psi_stack[t][J:J+indices[g],:] \n",
    "            else:\n",
    "                Psi = psi_stack[t][J+indices[g-1]:J+indices[g],:]\n",
    "\n",
    "            Psi_q = np.einsum('cj,j->c', Psi, q[t]) # Compute a matrix product\n",
    "            log_Psiq = np.log(Psi_q, out = -np.inf*np.ones_like(Psi_q), where = (Psi_q > 0)) # Determine log of Psi_q, and set entries equal to minus infinity if entry <= 0.\n",
    "            Psi_logPsiq = np.einsum('cj,c->j', Psi, log_Psiq) # Compute matrix product\n",
    "\n",
    "            Z_t[:,g] = log_q[t] - Psi_logPsiq # Compute cross differential\n",
    "        \n",
    "        Z[t] = Z_t\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_theta_grad_log_ccp(Theta, x, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the derivative of the IPDL log ccp's wrt. parameters theta\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K+G) of derivatives of the IPDL log ccp's wrt. parameters theta for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "\n",
    "    q = IPDL_ccp(Theta, x, psi_stack, nest_count) # Find choice probabilities\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack, nest_count) # Find cross differentials of the pertubation function\n",
    "    u_grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count)  # Find the gradient of log ccp's wrt. utilities\n",
    "    Grad={}\n",
    "\n",
    "    for t in range(T):\n",
    "        G=np.concatenate((x[t], Z[t]), axis=1)\n",
    "        Grad[t]=u_grad[t]@G\n",
    "   \n",
    "   # G = [np.concatenate((x[t], Z[t]), axis=1) for t in np.arange(T)] # Construct the block matrix of the covariates and the cross differentials as block matrices\n",
    "    #Grad = {t: np.einsum('jk,kd->jd', u_grad[t], G[t]) for t in np.arange(T)} # Compute the derivative by matrix multiplication.\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the score of the IPDL loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of IPDL scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = IPDL_theta_grad_log_ccp(Theta, x, psi_stack, nest_count) # Find derivatives of the IPDL log ccp's wrt. parameters theta\n",
    "    D = log_ccp_grad[0].shape[1] # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] =sample_share[t]*(log_ccp_grad[t].T@y[t]) #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' The derivative of the negative loglikelihood criterion\n",
    "    '''\n",
    "    return -IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_angrad(y,x,delta,theta,pop_share,Psi,Nest_count):\n",
    "\n",
    "    numgrad = np.empty((T, K+G))\n",
    "\n",
    "    for i in np.arange(K+G):\n",
    "        vec = np.zeros((K+G,))\n",
    "        vec[i] = 1\n",
    "        numgrad[:,i] = (IPDL_loglikelihood(theta + delta*vec, y, x, pop_share, Psi, Nest_count) - IPDL_loglikelihood(theta0, y, x, pop_share, Psi, Nest_count)) / delta\n",
    "    \n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "angrad = IPDL_score(theta0, y, x, pop_share, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numgrad.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(numgrad[0,:]).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(angrad[0,:]).transpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors in Maximum Likelihood estimation\n",
    "\n",
    "As usual we may consistently estimate the Covariance Matrix  of the IPDL maximum likelihood estimator for some estimate $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'\\in \\mathbb{R}^{K+G}$ as:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\left( \\sum_{i=1}^N \\nabla_\\theta \\ell_i (\\hat \\theta) \\nabla_\\theta \\ell_i (\\hat \\theta)' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Thereby we may find the estimated standard error of parameter $d$ as the squareroot of the d'th diagonal entry of $\\hat \\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_d = \\sqrt{\\hat \\Sigma_{dd}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_se(score, N):\n",
    "    '''\n",
    "    This function computes the asymptotic standard errors of the MLE.\n",
    "\n",
    "    Args.\n",
    "        score: a numpy array (T,K+G) of IPDL scores\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns\n",
    "        SE: a numpy array (K+G,) of asymptotic IPDL MLE standard errors\n",
    "    '''\n",
    "\n",
    "    SE = np.sqrt(np.diag(la.inv(np.einsum('td,tm->dm', score, score))) / N)\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        SE: a numpy array (K+G,) of asymptotic IPDL MLE standard errors\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        N: an integer giving the number of observations\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated asymptotic p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE\n",
    "    p = 2*scstat.t.sf(T, df = N-1)\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 867,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_IPDL(f, Theta0, y, x, sample_share, psi_stack, nest_count, N, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given starting values and variables necessary in the IPDL model specification.\n",
    "\n",
    "    Args:\n",
    "        f: a function to minimize,\n",
    "        Theta0 : a numpy array (K+G,) of initial guess parameters (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests', \n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t,\n",
    "        N: an integer giving the number of observations,\n",
    "        Analytic_jac: a boolean. Default value is 'True'. If 'True' the analytic jacobian of the IPDL loglikelihood function is used in estimation. Else the numerical jacobian is used.\n",
    "        options: dictionary with options for the optimizer (e.g. disp=True which tells it to display information at termination.)\n",
    "    \n",
    "    Returns:\n",
    "        res: a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(f(Theta, y, x, sample_share, psi_stack, nest_count))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Theta0.tolist(), options=options, jac=Grad, **kwargs) # optimize.minimize takes a list of parameters Theta0 (not a numpy array) as initial guess.\n",
    "    se = IPDL_se(IPDL_score(result.x, y, x, sample_share, psi_stack, nest_count), N)\n",
    "    T,p = IPDL_t_p(se, result.x, N)\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'theta': result.x,\n",
    "        'se': se,\n",
    "        't': T,\n",
    "        'p': p,\n",
    "        'success':  result.success, # bool, whether convergence was succesful 1\n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p_theta=IPDL_ccp(theta0,x,Psi,Nest_count)\n",
    "\n",
    "H=compute_pertubation_hessian(p_theta,x,theta0,Psi,Nest_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.023989\n",
      "         Iterations: 259\n",
      "         Function evaluations: 265\n",
      "         Gradient evaluations: 265\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.ones((K,))\n",
    "\n",
    "# Estimate the model\n",
    "res_logit = logit.estimate_logit(logit.q_logit, beta_0, y, x, sample_share=pop_share, Analytic_jac=True)\n",
    "\n",
    "theta0=np.append(res_logit['beta'],lambda0*0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 869,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.023988953476331697]\n",
      "[0.0014520970299060063, 0.023988679233698205]\n",
      "[0.007260485149530031, 0.02398764175216794]\n",
      "[0.023498462360929836, 0.023985187757716842]\n",
      "[0.04552659533041656, 0.023982888092721543]\n",
      "[0.04762646011621456, 0.023982511271870476]\n",
      "[0.05602591925940656, 0.023981076992827588]\n",
      "[0.07645212576664609, 0.023976800299988846]\n",
      "[0.12439698682291579, 0.023967877238319665]\n",
      "[0.198616903516793, 0.023958678900521477]\n",
      "[0.2030907066992202, 0.023958232501100723]\n",
      "[0.19601512816309413, 0.02395899415411581]\n",
      "[0.20228751799043476, 0.023958293126525258]\n",
      "[0.20299766340704511, 0.023958239180478926]\n",
      "[0.20307990291267886, 0.023958233272053396]\n",
      "[0.2030894518666148, 0.02395823259058251]\n",
      "[0.20309056094893824, 0.023958232511493295]\n",
      "[0.2030906897700909, 0.023958232502307827]\n",
      "[0.20309070473287472, 0.023958232501240934]\n",
      "[0.20309070647082877, 0.02395823250111701]\n",
      "[0.20309070667269255, 0.02395823250110262]\n",
      "[0.20309070669614349, 0.023958232501100945]\n",
      "[0.20309070669886475, 0.02395823250110075]\n",
      "[0.20309070669918058, 0.023958232501100726]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.20309070669921592, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.20309070669921597, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.203090706699216, 0.023958232501100723]\n",
      "[0.20309070669921603, 0.02395823250110072]\n",
      "[0.19601512816309413, 0.02395899415411581]\n",
      "[0.2012682785407662, 0.023958379685803253]\n",
      "[0.20262696631553084, 0.023958266686632677]\n",
      "[0.2029682915648289, 0.0239582413077446]\n",
      "[0.20305827542552707, 0.0239582348190337]\n",
      "[0.20308209885461537, 0.023958233115253632]\n",
      "[0.20308842121182893, 0.02395823266409051]\n",
      "[0.20309009980501588, 0.023958232544376148]\n",
      "[0.20309054553857486, 0.02395823251259213]\n",
      "[0.20309066390271185, 0.023958232504152268]\n",
      "[0.2030906953345164, 0.023958232501911068]\n",
      "[0.20309070368130072, 0.023958232501315912]\n"
     ]
    }
   ],
   "source": [
    "resbla2 = estimate_IPDL(q_IPDL, theta0, y, x, pop_share, Psi, Nest_count, N, Analytic_jac=True,options={'gtol':1e-15})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 870,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.023988953476331697]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.023988953476331697"
      ]
     },
     "execution_count": 870,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-IPDL_loglikelihood(theta0, y, x, pop_share, Psi, Nest_count).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 871,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theta': array([-3.80927815e+00, -5.75097444e+00,  1.33818097e+00, -9.04156933e-01,\n",
       "         7.64624073e+00, -1.16108741e+00, -1.72686230e+00,  3.43906981e+00,\n",
       "        -5.16298383e-01, -3.49536422e+00, -7.74885730e-01, -2.83158082e-01,\n",
       "        -1.09683011e+00, -7.16899838e-01, -9.54208768e-01, -1.20226421e+00,\n",
       "        -1.45351252e+00, -2.95857841e+00, -5.62602995e-01, -5.23159613e-01,\n",
       "        -6.89613015e-01, -1.44855097e+00, -2.37939708e+00, -2.24907708e+00,\n",
       "        -1.42211322e+00, -1.19076816e+00,  5.83399544e-01, -1.42625142e+00,\n",
       "        -5.98569335e-01, -5.15086922e-01, -5.05526624e-01, -3.35002467e-01,\n",
       "        -1.03112469e+00, -1.05752280e+00, -1.40302647e+00,  7.18015543e-01,\n",
       "        -1.40377790e+00, -7.58230378e-01, -1.92717295e+00, -8.58192359e-01,\n",
       "        -1.14590240e+00, -2.91430780e-01, -1.32982873e+00, -7.74522990e-01,\n",
       "        -4.16498095e-01,  1.59350044e+00,  5.85285409e-03,  8.85794878e-03,\n",
       "         3.68236186e-02,  2.25929955e-02, -7.58062701e-03,  4.05622306e-02,\n",
       "        -2.17107407e-02, -1.44432563e-02, -5.64560318e-02,  8.84010591e-02,\n",
       "        -1.25266460e-02]),\n",
       " 'se': array([0.39451268, 0.56515382, 0.49492066, 0.44454403, 0.85901881,\n",
       "        0.54992276, 0.41198793, 0.69662076, 0.18304549, 0.79108528,\n",
       "        0.49060091, 0.15932139, 0.22228848, 0.18815767, 0.15456661,\n",
       "        0.70654695, 0.87255293, 1.97236728, 0.14872698, 0.14130927,\n",
       "        0.32970733, 0.87947471, 0.27099621, 1.77425288, 0.19287308,\n",
       "        0.265231  , 0.16252238, 0.47678659, 0.29066288, 0.14510673,\n",
       "        0.13506359, 0.14920741, 0.14657983, 1.31723612, 0.29782352,\n",
       "        0.36913856, 0.83388953, 0.95756907, 1.71860937, 1.20467935,\n",
       "        0.21433111, 0.64664949, 0.19009228, 0.29434944, 0.23968811,\n",
       "        0.07103918, 0.01690356, 0.0174366 , 0.01548775, 0.02176853,\n",
       "        0.01841654, 0.01627638, 0.01601348, 0.01630911, 0.01965111,\n",
       "        0.03514439, 0.04893703]),\n",
       " 't': array([ 9.65565453, 10.17594535,  2.70382927,  2.03389739,  8.90113301,\n",
       "         2.11136452,  4.1915361 ,  4.93678917,  2.82060153,  4.41844172,\n",
       "         1.57946248,  1.77727601,  4.93426439,  3.81010164,  6.17344687,\n",
       "         1.70160555,  1.66581588,  1.50001393,  3.78279032,  3.70223145,\n",
       "         2.09159142,  1.64706381,  8.78018574,  1.26761923,  7.37331104,\n",
       "         4.48955118,  3.58965662,  2.99138324,  2.05932501,  3.54971073,\n",
       "         3.74287875,  2.2452133 ,  7.03456054,  0.80283465,  4.71093249,\n",
       "         1.9451112 ,  1.68340992,  0.79182839,  1.12135601,  0.71238239,\n",
       "         5.34641198,  0.45067813,  6.99570099,  2.63130448,  1.73766693,\n",
       "        22.4312899 ,  0.34624973,  0.50800886,  2.37759674,  1.03787418,\n",
       "         0.4116206 ,  2.49209154,  1.35577898,  0.88559422,  2.8729181 ,\n",
       "         2.51536778,  0.25597477]),\n",
       " 'p': array([4.65184143e-022, 2.53925652e-024, 6.85455034e-003, 4.19619439e-002,\n",
       "        5.52799049e-019, 3.47409936e-002, 2.77072201e-005, 7.94193078e-007,\n",
       "        4.79337068e-003, 9.94151326e-006, 1.14230017e-001, 7.55228409e-002,\n",
       "        8.04534631e-007, 1.38909710e-004, 6.68171364e-010, 8.88293385e-002,\n",
       "        9.57500933e-002, 1.33610795e-001, 1.55080092e-004, 2.13711580e-004,\n",
       "        3.64750820e-002, 9.95449319e-002, 1.63206129e-018, 2.04933962e-001,\n",
       "        1.66442173e-013, 7.13734379e-006, 3.31113913e-004, 2.77716753e-003,\n",
       "        3.94631151e-002, 3.85654755e-004, 1.81924077e-004, 2.47544437e-002,\n",
       "        1.99890631e-012, 4.22070314e-001, 2.46586081e-006, 5.17615981e-002,\n",
       "        9.22957667e-002, 4.28460746e-001, 2.62136354e-001, 4.76228017e-001,\n",
       "        8.97149722e-008, 6.52221548e-001, 2.63936712e-012, 8.50577996e-003,\n",
       "        8.22695200e-002, 1.94964785e-111, 7.29155045e-001, 6.11447127e-001,\n",
       "        1.74258723e-002, 2.99328635e-001, 6.80617529e-001, 1.26993310e-002,\n",
       "        1.75169504e-001, 3.75836226e-001, 4.06699572e-003, 1.18908278e-002,\n",
       "        7.97970318e-001]),\n",
       " 'success': False,\n",
       " 'nit': 8,\n",
       " 'nfev': 62,\n",
       " 'fun': 0.023958232501100723}"
      ]
     },
     "execution_count": 871,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resbla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 872,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_table(theta,se,N,x_vars,nest_vars):\n",
    "    IPDL_t, IPDL_p = IPDL_t_p(se, theta, N)\n",
    "\n",
    "    if OO:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "    else:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "\n",
    "    table  = pd.DataFrame({'theta': [ str(np.round(theta[i], decimals = 4)) + '***' if IPDL_p[i] <0.01 else str(np.round(theta[i], decimals = 3)) + '**' if IPDL_p[i] <0.05 else str(np.round(theta[i], decimals = 3)) + '*' if IPDL_p[i] <0.1 else np.round(theta[i], decimals = 3) for i in range(len(theta))], \n",
    "                'se' : np.round(se, decimals = 5),\n",
    "                't (theta == 0)': np.round(IPDL_t, decimals = 3),\n",
    "                'p': np.round(IPDL_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 873,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-3.8093***</td>\n",
       "      <td>0.39451</td>\n",
       "      <td>9.656</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-5.751***</td>\n",
       "      <td>0.56515</td>\n",
       "      <td>10.176</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>1.3382***</td>\n",
       "      <td>0.49492</td>\n",
       "      <td>2.704</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-0.904**</td>\n",
       "      <td>0.44454</td>\n",
       "      <td>2.034</td>\n",
       "      <td>0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>7.6462***</td>\n",
       "      <td>0.85902</td>\n",
       "      <td>8.901</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-1.161**</td>\n",
       "      <td>0.54992</td>\n",
       "      <td>2.111</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-1.7269***</td>\n",
       "      <td>0.41199</td>\n",
       "      <td>4.192</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>3.4391***</td>\n",
       "      <td>0.69662</td>\n",
       "      <td>4.937</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>-0.5163***</td>\n",
       "      <td>0.18305</td>\n",
       "      <td>2.821</td>\n",
       "      <td>0.005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-3.4954***</td>\n",
       "      <td>0.79109</td>\n",
       "      <td>4.418</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.775</td>\n",
       "      <td>0.49060</td>\n",
       "      <td>1.579</td>\n",
       "      <td>0.114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>-0.283*</td>\n",
       "      <td>0.15932</td>\n",
       "      <td>1.777</td>\n",
       "      <td>0.076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-1.0968***</td>\n",
       "      <td>0.22229</td>\n",
       "      <td>4.934</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.7169***</td>\n",
       "      <td>0.18816</td>\n",
       "      <td>3.810</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.9542***</td>\n",
       "      <td>0.15457</td>\n",
       "      <td>6.173</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-1.202*</td>\n",
       "      <td>0.70655</td>\n",
       "      <td>1.702</td>\n",
       "      <td>0.089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-1.454*</td>\n",
       "      <td>0.87255</td>\n",
       "      <td>1.666</td>\n",
       "      <td>0.096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-2.959</td>\n",
       "      <td>1.97237</td>\n",
       "      <td>1.500</td>\n",
       "      <td>0.134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>-0.5626***</td>\n",
       "      <td>0.14873</td>\n",
       "      <td>3.783</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.5232***</td>\n",
       "      <td>0.14131</td>\n",
       "      <td>3.702</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.69**</td>\n",
       "      <td>0.32971</td>\n",
       "      <td>2.092</td>\n",
       "      <td>0.036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-1.449*</td>\n",
       "      <td>0.87947</td>\n",
       "      <td>1.647</td>\n",
       "      <td>0.100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-2.3794***</td>\n",
       "      <td>0.27100</td>\n",
       "      <td>8.780</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-2.249</td>\n",
       "      <td>1.77425</td>\n",
       "      <td>1.268</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-1.4221***</td>\n",
       "      <td>0.19287</td>\n",
       "      <td>7.373</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-1.1908***</td>\n",
       "      <td>0.26523</td>\n",
       "      <td>4.490</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.5834***</td>\n",
       "      <td>0.16252</td>\n",
       "      <td>3.590</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-1.4263***</td>\n",
       "      <td>0.47679</td>\n",
       "      <td>2.991</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.599**</td>\n",
       "      <td>0.29066</td>\n",
       "      <td>2.059</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.5151***</td>\n",
       "      <td>0.14511</td>\n",
       "      <td>3.550</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>-0.5055***</td>\n",
       "      <td>0.13506</td>\n",
       "      <td>3.743</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>-0.335**</td>\n",
       "      <td>0.14921</td>\n",
       "      <td>2.245</td>\n",
       "      <td>0.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-1.0311***</td>\n",
       "      <td>0.14658</td>\n",
       "      <td>7.035</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-1.058</td>\n",
       "      <td>1.31724</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-1.403***</td>\n",
       "      <td>0.29782</td>\n",
       "      <td>4.711</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>0.718*</td>\n",
       "      <td>0.36914</td>\n",
       "      <td>1.945</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-1.404*</td>\n",
       "      <td>0.83389</td>\n",
       "      <td>1.683</td>\n",
       "      <td>0.092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-0.758</td>\n",
       "      <td>0.95757</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.927</td>\n",
       "      <td>1.71861</td>\n",
       "      <td>1.121</td>\n",
       "      <td>0.262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-0.858</td>\n",
       "      <td>1.20468</td>\n",
       "      <td>0.712</td>\n",
       "      <td>0.476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-1.1459***</td>\n",
       "      <td>0.21433</td>\n",
       "      <td>5.346</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-0.291</td>\n",
       "      <td>0.64665</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-1.3298***</td>\n",
       "      <td>0.19009</td>\n",
       "      <td>6.996</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.7745***</td>\n",
       "      <td>0.29435</td>\n",
       "      <td>2.631</td>\n",
       "      <td>0.009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.416*</td>\n",
       "      <td>0.23969</td>\n",
       "      <td>1.738</td>\n",
       "      <td>0.082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.5935***</td>\n",
       "      <td>0.07104</td>\n",
       "      <td>22.431</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>0.006</td>\n",
       "      <td>0.01690</td>\n",
       "      <td>0.346</td>\n",
       "      <td>0.729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.009</td>\n",
       "      <td>0.01744</td>\n",
       "      <td>0.508</td>\n",
       "      <td>0.611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.037**</td>\n",
       "      <td>0.01549</td>\n",
       "      <td>2.378</td>\n",
       "      <td>0.017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>0.023</td>\n",
       "      <td>0.02177</td>\n",
       "      <td>1.038</td>\n",
       "      <td>0.299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.008</td>\n",
       "      <td>0.01842</td>\n",
       "      <td>0.412</td>\n",
       "      <td>0.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>0.041**</td>\n",
       "      <td>0.01628</td>\n",
       "      <td>2.492</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.022</td>\n",
       "      <td>0.01601</td>\n",
       "      <td>1.356</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.01631</td>\n",
       "      <td>0.886</td>\n",
       "      <td>0.376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0565***</td>\n",
       "      <td>0.01965</td>\n",
       "      <td>2.873</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.088**</td>\n",
       "      <td>0.03514</td>\n",
       "      <td>2.515</td>\n",
       "      <td>0.012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.013</td>\n",
       "      <td>0.04894</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.798</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables         theta       se  t (theta == 0)      p\n",
       "cy           -3.8093***  0.39451           9.656  0.000\n",
       "hp            -5.751***  0.56515          10.176  0.000\n",
       "we            1.3382***  0.49492           2.704  0.007\n",
       "le             -0.904**  0.44454           2.034  0.042\n",
       "wi            7.6462***  0.85902           8.901  0.000\n",
       "he             -1.161**  0.54992           2.111  0.035\n",
       "li           -1.7269***  0.41199           4.192  0.000\n",
       "sp            3.4391***  0.69662           4.937  0.000\n",
       "ac           -0.5163***  0.18305           2.821  0.005\n",
       "pr           -3.4954***  0.79109           4.418  0.000\n",
       "brand_2          -0.775  0.49060           1.579  0.114\n",
       "brand_3         -0.283*  0.15932           1.777  0.076\n",
       "brand_4      -1.0968***  0.22229           4.934  0.000\n",
       "brand_5      -0.7169***  0.18816           3.810  0.000\n",
       "brand_6      -0.9542***  0.15457           6.173  0.000\n",
       "brand_7         -1.202*  0.70655           1.702  0.089\n",
       "brand_8         -1.454*  0.87255           1.666  0.096\n",
       "brand_9          -2.959  1.97237           1.500  0.134\n",
       "brand_10     -0.5626***  0.14873           3.783  0.000\n",
       "brand_11     -0.5232***  0.14131           3.702  0.000\n",
       "brand_12        -0.69**  0.32971           2.092  0.036\n",
       "brand_13        -1.449*  0.87947           1.647  0.100\n",
       "brand_14     -2.3794***  0.27100           8.780  0.000\n",
       "brand_15         -2.249  1.77425           1.268  0.205\n",
       "brand_16     -1.4221***  0.19287           7.373  0.000\n",
       "brand_17     -1.1908***  0.26523           4.490  0.000\n",
       "brand_18      0.5834***  0.16252           3.590  0.000\n",
       "brand_19     -1.4263***  0.47679           2.991  0.003\n",
       "brand_20       -0.599**  0.29066           2.059  0.039\n",
       "brand_21     -0.5151***  0.14511           3.550  0.000\n",
       "brand_22     -0.5055***  0.13506           3.743  0.000\n",
       "brand_23       -0.335**  0.14921           2.245  0.025\n",
       "brand_24     -1.0311***  0.14658           7.035  0.000\n",
       "brand_25         -1.058  1.31724           0.803  0.422\n",
       "brand_26      -1.403***  0.29782           4.711  0.000\n",
       "brand_27         0.718*  0.36914           1.945  0.052\n",
       "brand_28        -1.404*  0.83389           1.683  0.092\n",
       "brand_29         -0.758  0.95757           0.792  0.428\n",
       "brand_30         -1.927  1.71861           1.121  0.262\n",
       "brand_31         -0.858  1.20468           0.712  0.476\n",
       "brand_32     -1.1459***  0.21433           5.346  0.000\n",
       "brand_33         -0.291  0.64665           0.451  0.652\n",
       "brand_34     -1.3298***  0.19009           6.996  0.000\n",
       "brand_35     -0.7745***  0.29435           2.631  0.009\n",
       "brand_36        -0.416*  0.23969           1.738  0.082\n",
       "home_2        1.5935***  0.07104          22.431  0.000\n",
       "group_cy          0.006  0.01690           0.346  0.729\n",
       "group_hp          0.009  0.01744           0.508  0.611\n",
       "group_we        0.037**  0.01549           2.378  0.017\n",
       "group_le          0.023  0.02177           1.038  0.299\n",
       "group_wi         -0.008  0.01842           0.412  0.681\n",
       "group_he        0.041**  0.01628           2.492  0.013\n",
       "group_li         -0.022  0.01601           1.356  0.175\n",
       "group_sp         -0.014  0.01631           0.886  0.376\n",
       "group_ac     -0.0565***  0.01965           2.873  0.004\n",
       "group_brand     0.088**  0.03514           2.515  0.012\n",
       "group_home       -0.013  0.04894           0.256  0.798"
      ]
     },
     "execution_count": 873,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDL_theta = resbla2['theta']\n",
    "reg_table(resbla2['theta'],resbla2['se'],N,x_vars,nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 874,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2030907066992202"
      ]
     },
     "execution_count": 874,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in IPDL_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative approach\n",
    "\n",
    "The log-likelihood function is not globally concave, and finding the global optimum can be difficult. Using the estimation procedure of Fosgerau et. al. (2023 working paper), we can instead fit the parameters using the first-order conditions for optimality. The estimator takes the form\n",
    "\n",
    "$$\n",
    "\\hat \\theta^0=\\arg \\min_{\\theta} \\sum_t s_t \\hat \\varepsilon^0_t(\\theta)'\\hat W^0_t\\hat \\varepsilon^0 _t(\\theta),\n",
    "$$\n",
    "where $\\hat W^0_t$ is a positive semidefinite weight matrix, $s_t$ is market $t$'s share of the total population and \n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t(u(X_t,\\beta)- \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)) ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat D^0_t=\\textrm{diag}(\\hat q^0_t)-\\hat q^0_t (\\hat q^0_t)'.\n",
    "$$\n",
    "Using equation (...) above, we have that $\\hat \\epsilon_t$ is a linear function of $\\theta$,\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t \\left(\\hat G^0_t\\theta- \\ln \\hat q^0_t\\right)\\equiv \\hat A^0_t\\theta-\\hat r^0_t.\n",
    "$$\n",
    "Using linearity, the weighted least squares criterion has a unique closed form solution,\n",
    "$$\n",
    "\\hat \\theta^0 =\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat A^0_t \\right)^{-1}\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat r_t^0 \\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 875,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_array(q, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function calculates the G block matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        G: a dictionary  of T numpy arrays (J[t],K+G): a G matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack, nest_count) # Find the cross derivative of the pertubation function \\Omega wrt. lambda and ccp's q\n",
    "    G = {t: np.concatenate((x[t],Z[t]), axis=1) for t in np.arange(T)} # Join block matrices along 2nd dimensions  s.t. last dimension is K+G (same dimension as theta)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 876,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_array(q):\n",
    "    '''\n",
    "    This function calculates the D matrix - the logit derivative of ccp's wrt. utilities\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "\n",
    "    Returns\n",
    "        D: a dictionary of T numpy arrays (J[t],J[t]) of logit derivatives of ccp's wrt. utilities for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = {t: np.diag(q[t]) - np.einsum('j,k->jk', q[t], q[t]) for t in np.arange(T)}\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_array(q, x, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the A matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        A: a dictionary  of T numpy arrays (J[t],K+G): an A matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    D = D_array(q)\n",
    "    G = G_array(q, x, psi_stack, nest_count)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 878,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_array(q):\n",
    "    '''\n",
    "    This function calculates 'r'; the logarithm of observed or nonparametrically estimated market shares\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "    \n",
    "    Returns\n",
    "        r: a dictionary of T numpy arrays (J[t],) of the log of ccp's for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = D_array(q) \n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) for t in np.arange(T)}\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 879,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS_init(q, x, sample_share, psi_stack, nest_count, N):\n",
    "    ''' \n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the initial FKN parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "\n",
    "    #W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "    A = A_array(q, x, psi_stack, nest_count)\n",
    "    r = r_array(q)\n",
    "\n",
    "    d = A[0].shape[1]\n",
    "    \n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1/q[t], A[t], optimize = True) # Fast product using that the weights are diagonal.\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1/q[t], r[t], optimize = True)\n",
    "    \n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "    \n",
    "    return theta_hat,se_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the observed market shares we may thus find initial parameter estimates $\\hat \\theta^0$ as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 880,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFKN0,seFKN0 = WLS_init(y, x, pop_share, Psi, Nest_count, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0525992325879783"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in thetaFKN0[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00311397, 0.00337322, 0.00290639, 0.00323798, 0.00533055,\n",
       "       0.004687  , 0.00249208, 0.00468713, 0.00142938, 0.00328441,\n",
       "       0.00434429, 0.00058246, 0.00080285, 0.0005752 , 0.00060109,\n",
       "       0.0018064 , 0.00216946, 0.00269626, 0.000616  , 0.00056872,\n",
       "       0.00098117, 0.00169144, 0.00174439, 0.0032486 , 0.00069674,\n",
       "       0.00089782, 0.0005501 , 0.00136375, 0.00080514, 0.00054775,\n",
       "       0.00056954, 0.00059981, 0.00061002, 0.00149998, 0.00089356,\n",
       "       0.03999154, 0.00158447, 0.01662605, 0.0030851 , 0.00314238,\n",
       "       0.00111957, 0.01508594, 0.00083499, 0.00086465, 0.00081095,\n",
       "       0.00027577, 0.00013191, 0.00012982, 0.00013633, 0.00013971,\n",
       "       0.00012638, 0.00010282, 0.00011564, 0.00012739, 0.00011258,\n",
       "       0.0002028 , 0.00023091])"
      ]
     },
     "execution_count": 882,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seFKN0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for parameter bounds\n",
    "\n",
    "As we see above, the least squares estimator is not guaranteed to respect the parameter bounds $\\sum_g \\hat \\lambda_g<1$. We can use that if we replace $\\hat q^0_t$ with the choice probabilities from the maximum likelihood estimator of the logit model, $\\hat q^{logit}_t\\propto \\exp\\{X_t\\hat \\beta^{logit}\\}$, and plug these choice probabilities into the WLS estimator described above, it will return $\\hat \\theta=(\\hat \\beta^{logit},0,\\ldots,0)$ as the parameter estimate. Let $\\hat q_t(\\alpha)$ denote the weighted average of the logit probabilites and the market shares,\n",
    "$$\n",
    "\\hat q_t(\\alpha) =(1-\\alpha) \\hat q^{logit}_t+\\alpha \\hat q^0_t.\n",
    "$$\n",
    " Let $\\hat \\theta^0(\\alpha)$ denote the resulting parameter vector. We perform a line search for values of $\\alpha$, $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\ldots)$ until $\\hat \\theta^0(\\alpha)$ yields a feasible parameter vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogL(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' A function giving the mean IPDL loglikehood evaluated at data and an array of parameters 'Theta'\n",
    "    '''\n",
    "    return np.mean(IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, nest_count, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta0.shape[0]\n",
    "    K = x[0].shape[1]\n",
    "    G = d-K\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit.logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    #alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    alpha0=0.5\n",
    "    #LogL_alpha = np.empty((num_alpha,))\n",
    "    #theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in range(1,100):\n",
    "\n",
    "        alpha = alpha0**k\n",
    "\n",
    "      \n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha = WLS_init(q_alpha, x, sample_share, psi_stack, nest_count, N)[0]\n",
    "\n",
    "        lambda_alpha = theta_alpha[K:]\n",
    "        \n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() <1:\n",
    "            break\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, nest_count, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta0.shape[0]\n",
    "    K = x[0].shape[1]\n",
    "    G = d-K\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit.logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    LogL_alpha = np.empty((num_alpha,))\n",
    "    theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in np.arange(len(alpha_line)):\n",
    "\n",
    "        alpha = alpha_line[k]\n",
    "\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha[k,:] = WLS_init(q_alpha, x, sample_share, psi_stack, nest_count, N)[0]\n",
    "\n",
    "        lambda_alpha = theta_alpha[K:]\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() >= 1:\n",
    "            LogL_alpha[k] = np.NINF\n",
    "        else:\n",
    "            LogL_alpha[k] = LogL(theta_alpha, y, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    alpha_star = np.argmax(LogL_alpha)\n",
    "    theta_hat_star = theta_alpha[alpha_star,:]\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the grid search method we find corressponding parameters $\\hat \\theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_alpha = LineSearch(thetaFKN0, beta_0, y, x, pop_share, Psi, Nest_count, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9321185227610683"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in theta_alpha[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9321185227610683, 0.026656102742521504]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.026656102742521504"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_IPDL(theta_alpha, y, x, pop_share, Psi, Nest_count).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated FKN estimator\n",
    "\n",
    "The iterated estimator is as the initial one, except there is an additional term on $\\hat \\varepsilon$. First, we update the choice probabilities,\n",
    "$$\n",
    "\\hat q^k_i=p(\\mathbf X_i,\\hat \\theta^{k-1})\\\\\n",
    "$$\n",
    "Then we assign\n",
    "$$\n",
    "\\hat D^k_i=\\nabla^2_{qq}\\Omega(\\hat q_i^k|\\hat \\lambda^{k-1})^{-1}-(\\hat q^k_i \\hat q^k_i)'\n",
    "$$\n",
    "and then construct the residual\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)=\\hat D^k_i\\left( u(x_i,\\beta)-\\nabla_q \\Omega(\\hat q_i^k|\\lambda)\\right) -y_i+\\hat q_i^k,\n",
    "$$\n",
    "Which can once again be simplified as\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)= \\hat A_i^k \\theta-\\hat r^k_i,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat A^k_i=\\hat D_i^k\\hat G^k_i, \\hat r_i^k =\\hat D^k_i\\ln \\hat q_i^k-y_i\n",
    "$$\n",
    "and where $\\hat G^k_i$ is constructed as in the initial estimator. Using the weighted least squares estimator with weights $\\hat W_i^k=\\textrm{diag}(\\hat q^k_i)^{-1}$, we get the estimator\n",
    "$$\n",
    "\\hat \\theta^k = \\arg \\min_{\\theta}\\frac{1}{n}\\sum_i \\hat \\varepsilon^k_i(\\theta)'\\hat W_i^k \\hat \\varepsilon^k_i(\\theta).\n",
    "$$\n",
    "We can once again solve it in closed form as\n",
    "$$\n",
    "\\hat \\theta^k =\\left( \\frac{1}{n}\\sum_i \\hat (A^k_i)'\\hat W_i^k \\hat A^k_i)\\right)^{-1}\\left( \\frac{1}{n}\\sum_i (\\hat A_i^k)'\\hat W_i^k \\hat r_i^k\\right)\n",
    "$$\n",
    "Now we implement this procedure and iterate starting from our initial guess $\\hat \\theta^{*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS(Theta, y, x, sample_share, psi_stack, nest_count, N):\n",
    "    '''\n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the iterated parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta.shape[0]\n",
    "    \n",
    "    # Get ccp's\n",
    "    q = IPDL_ccp(Theta, x, psi_stack, nest_count)\n",
    "\n",
    "    # Construct A\n",
    "    D = ccp_gradient(q, x, Theta, psi_stack, nest_count) # A is here constructed using the IPDL derivative of ccp's wrt. utilities instead of teh Logit derivative\n",
    "    G = G_array(q, x, psi_stack, nest_count)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "    W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "\n",
    "    # Construct r\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where=(q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) + y[t] for t in np.arange(T)}\n",
    "\n",
    "    # Estimate parameters\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,jk,kp->dp', A[t], W[t], A[t], optimize = True)\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,jk,k->d', A[t], W[t], r[t], optimize = True)\n",
    "\n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "\n",
    "    return theta_hat,se_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKN_estimator(logit_beta, q_obs, x, sample_share, psi_stack, nest_count, N, tol = 1.0e-15, max_iters = 1000):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    theta_init = WLS_init(q_obs, x, sample_share, psi_stack, nest_count,  N)[0]\n",
    "    \n",
    "    if np.array([p for p in theta_init[K:] if p>0]).sum() >1:\n",
    "        theta_hat_star = GridSearch(theta_init, logit_beta, q_obs, x, sample_share, psi_stack, nest_count, N)\n",
    "        theta0 = theta_hat_star\n",
    "    else:\n",
    "        theta0 = theta_init\n",
    "\n",
    "    #logl0 = LogL(theta0, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    for k in np.arange(max_iters):\n",
    "        theta1, se1 = WLS(theta0, q_obs, x, sample_share, psi_stack, nest_count, N)\n",
    "\n",
    "        '''logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "        \n",
    "        for m in range(10):\n",
    "            if logl1<logl0:\n",
    "                theta1=0.5*theta0+0.5*theta1\n",
    "                logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "            else:\n",
    "                break'''\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.abs(theta1 - theta0))\n",
    "\n",
    "        if dist<tol:\n",
    "            succes = True\n",
    "            iter = k\n",
    "            break\n",
    "        elif k==max_iters:\n",
    "            succes = False\n",
    "            iter = max_iters\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        theta0 = theta1\n",
    "        logl0 = logl1\n",
    "\n",
    "    res = {'theta': theta1,\n",
    "           'se': se1,\n",
    "           'fun': -LogL(theta1, y, x, sample_share, psi_stack, nest_count),\n",
    "           'iter': iter,\n",
    "           'succes': succes}\n",
    "    \n",
    "    return res \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\IPDL_book4.ipynb Cell 88\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m res \u001b[39m=\u001b[39m FKN_estimator(beta_0, y, x, pop_share, Psi, Nest_count, N, tol\u001b[39m=\u001b[39;49m\u001b[39m1.0e-8\u001b[39;49m, max_iters\u001b[39m=\u001b[39;49m\u001b[39m1000\u001b[39;49m)\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\IPDL_book4.ipynb Cell 88\u001b[0m in \u001b[0;36mFKN_estimator\u001b[1;34m(logit_beta, q_obs, x, sample_share, psi_stack, nest_count, N, tol, max_iters)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m theta_init \u001b[39m=\u001b[39m WLS_init(q_obs, x, sample_share, psi_stack, nest_count,  N)[\u001b[39m0\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39marray([p \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m theta_init[K:] \u001b[39mif\u001b[39;00m p\u001b[39m>\u001b[39m\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39msum() \u001b[39m>\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     theta_hat_star \u001b[39m=\u001b[39m GridSearch(theta_init, logit_beta, q_obs, x, sample_share, psi_stack, nest_count, N)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     theta0 \u001b[39m=\u001b[39m theta_hat_star\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\IPDL_book4.ipynb Cell 88\u001b[0m in \u001b[0;36mGridSearch\u001b[1;34m(Theta0, Logit_Beta, y, x, sample_share, psi_stack, nest_count, N, num_alpha)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         LogL_alpha[k] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mNINF\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m         LogL_alpha[k] \u001b[39m=\u001b[39m LogL(theta_alpha, y, x, sample_share, psi_stack, nest_count)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m alpha_star \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(LogL_alpha)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m theta_hat_star \u001b[39m=\u001b[39m theta_alpha[alpha_star,:]\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\IPDL_book4.ipynb Cell 88\u001b[0m in \u001b[0;36mLogL\u001b[1;34m(Theta, y, x, sample_share, psi_stack, nest_count)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mLogL\u001b[39m(Theta, y, x, sample_share, psi_stack, nest_count):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m''' A function giving the mean IPDL loglikehood evaluated at data and an array of parameters 'Theta'\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count))\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\IPDL_book4.ipynb Cell 88\u001b[0m in \u001b[0;36mIPDL_loglikelihood\u001b[1;34m(Theta, y, x, sample_share, psi_stack, nest_count)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m T \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(x\u001b[39m.\u001b[39mkeys())\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m K \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m ccp_hat \u001b[39m=\u001b[39m IPDL_ccp(Theta, x, psi_stack, nest_count)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m sum_lambdaplus \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([theta \u001b[39mfor\u001b[39;00m theta \u001b[39min\u001b[39;00m Theta[K:] \u001b[39mif\u001b[39;00m theta \u001b[39m>\u001b[39m\u001b[39m0\u001b[39m])\u001b[39m.\u001b[39msum()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mif\u001b[39;00m sum_lambdaplus \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\IPDL_book4.ipynb Cell 88\u001b[0m in \u001b[0;36mIPDL_ccp\u001b[1;34m(Theta, x, psi, nest_count, tol, maximum_iterations)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     b \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39melse\u001b[39;00m:    \n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     b \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mabs(Lambda[C_minus])\u001b[39m.\u001b[39msum() \u001b[39m# sum of absolute value of negative lambda parameters.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m Gamma \u001b[39m=\u001b[39m Create_Gamma(Lambda, psi, nest_count) \u001b[39m# Find the Gamma matrix\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/IPDL_book4.ipynb#Y234sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m u \u001b[39m=\u001b[39m {t: np\u001b[39m.\u001b[39meinsum(\u001b[39m'\u001b[39m\u001b[39mjk,k->j\u001b[39m\u001b[39m'\u001b[39m, x[t], Beta) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(T)} \u001b[39m# Calculate linear utilities\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "res = FKN_estimator(beta_0, y, x, pop_share, Psi, Nest_count, N, tol=1.0e-8, max_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theta': array([-3.80927815e+00, -5.75097444e+00,  1.33818097e+00, -9.04156933e-01,\n",
       "         7.64624073e+00, -1.16108741e+00, -1.72686230e+00,  3.43906981e+00,\n",
       "        -5.16298383e-01, -3.49536422e+00, -7.74885730e-01, -2.83158082e-01,\n",
       "        -1.09683011e+00, -7.16899838e-01, -9.54208768e-01, -1.20226421e+00,\n",
       "        -1.45351252e+00, -2.95857841e+00, -5.62602995e-01, -5.23159613e-01,\n",
       "        -6.89613015e-01, -1.44855097e+00, -2.37939708e+00, -2.24907708e+00,\n",
       "        -1.42211322e+00, -1.19076816e+00,  5.83399544e-01, -1.42625142e+00,\n",
       "        -5.98569335e-01, -5.15086922e-01, -5.05526624e-01, -3.35002467e-01,\n",
       "        -1.03112469e+00, -1.05752280e+00, -1.40302647e+00,  7.18015543e-01,\n",
       "        -1.40377790e+00, -7.58230378e-01, -1.92717295e+00, -8.58192359e-01,\n",
       "        -1.14590240e+00, -2.91430780e-01, -1.32982873e+00, -7.74522990e-01,\n",
       "        -4.16498095e-01,  1.59350044e+00,  5.85285413e-03,  8.85794884e-03,\n",
       "         3.68236188e-02,  2.25929957e-02, -7.58062702e-03,  4.05622309e-02,\n",
       "        -2.17107408e-02, -1.44432563e-02, -5.64560321e-02,  8.84010595e-02,\n",
       "        -1.25266461e-02]),\n",
       " 'se': array([0.39451268, 0.56515382, 0.49492066, 0.44454403, 0.85901881,\n",
       "        0.54992276, 0.41198793, 0.69662076, 0.18304549, 0.79108528,\n",
       "        0.49060091, 0.15932139, 0.22228848, 0.18815767, 0.15456661,\n",
       "        0.70654695, 0.87255293, 1.97236728, 0.14872698, 0.14130927,\n",
       "        0.32970733, 0.87947471, 0.27099621, 1.77425288, 0.19287308,\n",
       "        0.265231  , 0.16252238, 0.47678659, 0.29066288, 0.14510673,\n",
       "        0.13506359, 0.14920741, 0.14657983, 1.31723612, 0.29782352,\n",
       "        0.36913856, 0.83388953, 0.95756907, 1.71860937, 1.20467935,\n",
       "        0.21433111, 0.64664948, 0.19009228, 0.29434944, 0.23968811,\n",
       "        0.07103918, 0.01690356, 0.0174366 , 0.01548775, 0.02176853,\n",
       "        0.01841654, 0.01627638, 0.01601348, 0.01630911, 0.01965111,\n",
       "        0.03514439, 0.04893703]),\n",
       " 't': array([ 9.65565454, 10.17594535,  2.70382927,  2.03389739,  8.90113301,\n",
       "         2.11136452,  4.19153611,  4.93678917,  2.82060153,  4.41844172,\n",
       "         1.57946248,  1.77727601,  4.93426439,  3.81010165,  6.17344687,\n",
       "         1.70160555,  1.66581588,  1.50001393,  3.78279032,  3.70223145,\n",
       "         2.09159142,  1.64706381,  8.78018575,  1.26761923,  7.37331105,\n",
       "         4.48955118,  3.58965663,  2.99138324,  2.05932501,  3.54971073,\n",
       "         3.74287875,  2.2452133 ,  7.03456055,  0.80283465,  4.71093249,\n",
       "         1.9451112 ,  1.68340992,  0.79182839,  1.12135601,  0.71238239,\n",
       "         5.34641199,  0.45067813,  6.995701  ,  2.63130448,  1.73766693,\n",
       "        22.4312899 ,  0.34624973,  0.50800887,  2.37759675,  1.03787419,\n",
       "         0.4116206 ,  2.49209156,  1.35577898,  0.88559423,  2.87291812,\n",
       "         2.51536779,  0.25597477]),\n",
       " 'p': array([4.65184121e-022, 2.53925649e-024, 6.85455034e-003, 4.19619439e-002,\n",
       "        5.52799030e-019, 3.47409935e-002, 2.77072200e-005, 7.94193068e-007,\n",
       "        4.79337065e-003, 9.94151310e-006, 1.14230017e-001, 7.55228406e-002,\n",
       "        8.04534621e-007, 1.38909708e-004, 6.68171338e-010, 8.88293385e-002,\n",
       "        9.57500931e-002, 1.33610795e-001, 1.55080090e-004, 2.13711576e-004,\n",
       "        3.64750819e-002, 9.95449318e-002, 1.63206118e-018, 2.04933962e-001,\n",
       "        1.66442162e-013, 7.13734364e-006, 3.31113911e-004, 2.77716753e-003,\n",
       "        3.94631150e-002, 3.85654747e-004, 1.81924074e-004, 2.47544436e-002,\n",
       "        1.99890616e-012, 4.22070314e-001, 2.46586079e-006, 5.17615979e-002,\n",
       "        9.22957666e-002, 4.28460747e-001, 2.62136354e-001, 4.76228016e-001,\n",
       "        8.97149710e-008, 6.52221547e-001, 2.63936698e-012, 8.50577995e-003,\n",
       "        8.22695200e-002, 1.94964755e-111, 7.29155043e-001, 6.11447125e-001,\n",
       "        1.74258716e-002, 2.99328632e-001, 6.80617528e-001, 1.26993304e-002,\n",
       "        1.75169502e-001, 3.75836224e-001, 4.06699551e-003, 1.18908274e-002,\n",
       "        7.97970317e-001]),\n",
       " 'success': False,\n",
       " 'nit': 8,\n",
       " 'nfev': 59,\n",
       " 'fun': 0.023958232501024277}"
      ]
     },
     "execution_count": 825,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resbla2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FKN_theta = res['theta']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8993978866426582, 0.001493980851035042]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.001493980851035042"
      ]
     },
     "execution_count": 519,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_IPDL(FKN_theta, y, x, pop_share, Psi, Nest_count).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-10.4067***</td>\n",
       "      <td>0.00309</td>\n",
       "      <td>3371.931</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-1.0728***</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>708.887</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-3.2092***</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>1644.597</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.2245***</td>\n",
       "      <td>0.00144</td>\n",
       "      <td>156.123</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.2365***</td>\n",
       "      <td>0.00126</td>\n",
       "      <td>981.018</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.4818***</td>\n",
       "      <td>0.00289</td>\n",
       "      <td>1896.496</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-0.1078***</td>\n",
       "      <td>0.00178</td>\n",
       "      <td>60.555</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.7848***</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>751.443</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>3.2451***</td>\n",
       "      <td>0.00204</td>\n",
       "      <td>1587.552</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.3005***</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>463.385</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.3012***</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>289.572</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-1.5212***</td>\n",
       "      <td>0.00366</td>\n",
       "      <td>415.663</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>0.1682***</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>841.254</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.6163***</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>996.355</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.316***</td>\n",
       "      <td>0.00034</td>\n",
       "      <td>928.361</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.2557***</td>\n",
       "      <td>0.00027</td>\n",
       "      <td>941.216</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.6987***</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>496.978</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.521***</td>\n",
       "      <td>0.00161</td>\n",
       "      <td>323.786</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7784***</td>\n",
       "      <td>0.00228</td>\n",
       "      <td>779.880</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.2869***</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>1092.526</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.4478***</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>605.214</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-0.9737***</td>\n",
       "      <td>0.00136</td>\n",
       "      <td>716.502</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-1.6539***</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>1188.029</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.7639***</td>\n",
       "      <td>0.00269</td>\n",
       "      <td>654.954</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-0.6619***</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>1462.724</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.5863***</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>868.915</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.3317***</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>933.389</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-0.9735***</td>\n",
       "      <td>0.00111</td>\n",
       "      <td>876.753</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.0925***</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>172.961</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.072***</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>324.341</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>0.1573***</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>717.591</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.2919***</td>\n",
       "      <td>0.00028</td>\n",
       "      <td>1028.992</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.8057***</td>\n",
       "      <td>0.00126</td>\n",
       "      <td>639.564</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.6385***</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>967.786</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.9966***</td>\n",
       "      <td>0.03669</td>\n",
       "      <td>81.677</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.6716***</td>\n",
       "      <td>0.00122</td>\n",
       "      <td>549.546</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.9601***</td>\n",
       "      <td>0.01417</td>\n",
       "      <td>208.963</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.445***</td>\n",
       "      <td>0.00255</td>\n",
       "      <td>565.682</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.3275***</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>437.946</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-2.8875***</td>\n",
       "      <td>0.01248</td>\n",
       "      <td>231.312</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.6226***</td>\n",
       "      <td>0.00055</td>\n",
       "      <td>1124.817</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.3051***</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>495.940</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1336***</td>\n",
       "      <td>0.00058</td>\n",
       "      <td>229.351</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.1092***</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>2568.826</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.6103***</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>2483.036</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.0415***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>443.823</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>-0.0**</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>2.323</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>-0.001***</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>9.316</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0397***</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>409.829</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0274***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>321.157</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.004***</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>52.212</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.0285***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>312.057</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0276***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>308.980</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0232***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>253.209</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.2891***</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>1464.184</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.2357***</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>1092.028</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)     p\n",
       "in_out        -10.4067***  0.00309        3371.931  0.00\n",
       "cy             -1.0728***  0.00151         708.887  0.00\n",
       "hp             -3.2092***  0.00195        1644.597  0.00\n",
       "we             -0.2245***  0.00144         156.123  0.00\n",
       "le             -1.2365***  0.00126         981.018  0.00\n",
       "wi              5.4818***  0.00289        1896.496  0.00\n",
       "he             -0.1078***  0.00178          60.555  0.00\n",
       "li             -0.7848***  0.00104         751.443  0.00\n",
       "sp              3.2451***  0.00204        1587.552  0.00\n",
       "ac              0.3005***  0.00065         463.385  0.00\n",
       "pr             -0.3012***  0.00104         289.572  0.00\n",
       "brand_2        -1.5212***  0.00366         415.663  0.00\n",
       "brand_3         0.1682***  0.00020         841.254  0.00\n",
       "brand_4        -0.6163***  0.00062         996.355  0.00\n",
       "brand_5         -0.316***  0.00034         928.361  0.00\n",
       "brand_6        -0.2557***  0.00027         941.216  0.00\n",
       "brand_7        -0.6987***  0.00141         496.978  0.00\n",
       "brand_8         -0.521***  0.00161         323.786  0.00\n",
       "brand_9        -1.7784***  0.00228         779.880  0.00\n",
       "brand_10        0.2869***  0.00026        1092.526  0.00\n",
       "brand_12       -0.4478***  0.00074         605.214  0.00\n",
       "brand_13       -0.9737***  0.00136         716.502  0.00\n",
       "brand_14       -1.6539***  0.00139        1188.029  0.00\n",
       "brand_15       -1.7639***  0.00269         654.954  0.00\n",
       "brand_16       -0.6619***  0.00045        1462.724  0.00\n",
       "brand_17       -0.5863***  0.00067         868.915  0.00\n",
       "brand_18        0.3317***  0.00036         933.389  0.00\n",
       "brand_19       -0.9735***  0.00111         876.753  0.00\n",
       "brand_20       -0.0925***  0.00053         172.961  0.00\n",
       "brand_22         0.072***  0.00022         324.341  0.00\n",
       "brand_23        0.1573***  0.00022         717.591  0.00\n",
       "brand_24       -0.2919***  0.00028        1028.992  0.00\n",
       "brand_25       -0.8057***  0.00126         639.564  0.00\n",
       "brand_26       -0.6385***  0.00066         967.786  0.00\n",
       "brand_27       -2.9966***  0.03669          81.677  0.00\n",
       "brand_28       -0.6716***  0.00122         549.546  0.00\n",
       "brand_29       -2.9601***  0.01417         208.963  0.00\n",
       "brand_30        -1.445***  0.00255         565.682  0.00\n",
       "brand_32       -0.3275***  0.00075         437.946  0.00\n",
       "brand_33       -2.8875***  0.01248         231.312  0.00\n",
       "brand_34       -0.6226***  0.00055        1124.817  0.00\n",
       "brand_35       -0.3051***  0.00062         495.940  0.00\n",
       "brand_36       -0.1336***  0.00058         229.351  0.00\n",
       "home_2          1.1092***  0.00043        2568.826  0.00\n",
       "group_in_out    0.6103***  0.00025        2483.036  0.00\n",
       "group_cy       -0.0415***  0.00009         443.823  0.00\n",
       "group_hp           -0.0**  0.00009           2.323  0.02\n",
       "group_we        -0.001***  0.00010           9.316  0.00\n",
       "group_le       -0.0397***  0.00010         409.829  0.00\n",
       "group_wi       -0.0274***  0.00009         321.157  0.00\n",
       "group_he        -0.004***  0.00008          52.212  0.00\n",
       "group_li       -0.0285***  0.00009         312.057  0.00\n",
       "group_sp       -0.0276***  0.00009         308.980  0.00\n",
       "group_ac       -0.0232***  0.00009         253.209  0.00\n",
       "group_brand     0.2891***  0.00020        1464.184  0.00\n",
       "group_home     -0.2357***  0.00022        1092.028  0.00"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_table(res['theta'],res['se'],N,x_vars,nest_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP Estimation and instruments\n",
    "\n",
    "The setting is now a bit different. Instead of the noise coming from random sampling of individuals, we now have an additional source of uncertainty, stemming frm the random sampling of the fixed effects $\\xi_{tj}$ for each market and each product. The number of ”observations” is therefore\n",
    "\n",
    "$$\n",
    "S = T \\cdot \\sum_t J_t\n",
    "$$\n",
    "\n",
    "Note that while random sampling of individuals choices (number of observations\n",
    "in the hundreds of millions) still has an effect on the estimated parameters in\n",
    "principle, this effect is completely drowned out by the sampling variance of the\n",
    "fixed effects (number of observations T ≈ 15000?), so we choose to ignore it\n",
    "here. When estimating random coefficients models, there is also a third source\n",
    "of uncertainty stemming from approximation of numerical integrals. This is not\n",
    "an issue in IPDL, as we have the inverse demand in closed form.\n",
    "\n",
    "The principles are pretty similar to what we have been doing already. When\n",
    "applicable, I will use the same notation as in the FKN section. Define the\n",
    "residual,\n",
    "\n",
    "$$\\xi_m(\\theta) = u(X_m, \\beta) − \\nabla_q \\Omega(q^0|\\lambda)$$\n",
    "\n",
    "In the IPDL model, this residual is a linear function of $\\theta$ which has the form\n",
    "\n",
    "$$\\xi_m(\\theta) =  G^0_m \\theta − r_m^0$$\n",
    "\n",
    "where $ G^0_m=[X_m, Z_m^0]$, where $Z_m^0 = \\nabla_{q,\\lambda}\\Omega(q_m^0|\\lambda)$ and $r^0_m = \\ln q^0_m$ as in the FKN section with $q^0_m$ being e.g. the observed market shares in market $m$. For the BLP estimator, we set this residual orthogonal to a matrix of instruments $\\hat Z_m$ of size $J_m \\times d$, and find the estimator $ \\hat \\theta^{IV}$ which solves the moment conditions\n",
    "\n",
    "$$\\frac{1}{T} \\sum_m \\hat Z_m' \\xi(\\hat \\theta^{IV}) = 0$$\n",
    "\n",
    "Since $\\hat \\xi$ is linear, the moment equations have a unique solution,\n",
    "\n",
    "$$\\hat \\theta^{IV} = \\left(\\frac{1}{T}\\sum_m \\hat Z_m' G^0_m \\right)^{-1}\\left(\\frac{1}{T}\\sum_m \\hat Z_m' r^0_m \\right)$$\n",
    "\n",
    "We require an instrument for the price of the goods. This is something which is correlated with the price, but uncorrelated with the error term $\\xi_m$ (in the BLP model, $\\xi_{mj}$ represents unobserved components of car quality). A standard instrument in this case would be a measure of marginal cost (or something which is correlated with marginal cost, like a production price index). For everything other than price, we can simply use the regressor itself as the instrument i.e. $ \\hat Z^{mjd} = G^0_{mjd}$, for all other dimensions than price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct our instruments $\\hat Z$. We'll use the average exchange rate of the destination country relative to average exchange rate of the origin country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "metadata": {},
   "outputs": [],
   "source": [
    "xexr = {t: dat[dat['market'] == t]['xexr'].values for t in np.arange(T)}\n",
    "G0 = G_array(y, x, Psi, Nest_count)\n",
    "pr_index = len(x_contvars)\n",
    "for t in np.arange(T):\n",
    "    G0[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z = G0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the moment estimator $\\hat \\theta^{IV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 784,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_estimator(y, z, x, sample_share, psi_stack, nest_count):\n",
    "    '''\n",
    "    Args.\n",
    "        y: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        z: a dictionary of T numpy arrays (J[t],K+G) of instruments for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a numpy array (K+G,) of BLP parameter estimates\n",
    "    '''\n",
    "    T = len(z)\n",
    "\n",
    "    G = G_array(y, x, psi_stack, nest_count)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t], out = np.NINF*np.ones_like((y[t])), where = (y[t] > 0)) for t in np.arange(T)}\n",
    "    \n",
    "    sZG = np.empty((T,d,d))\n",
    "    sZr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sZG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', z[t], G[t])\n",
    "        sZr[t,:] = sample_share[t]*np.einsum('jd,j->d', z[t], r[t])\n",
    "\n",
    "    theta_hat = la.solve(sZG.sum(axis=0), sZr.sum(axis=0))\n",
    "    \n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 785,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLP_theta = BLP_estimator(y, z, x, np.ones((T,)), Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 786,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0177604791351837"
      ]
     },
     "execution_count": 786,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in BLP_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Logit model we get the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 787,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_logit = x\n",
    "for t in np.arange(T):\n",
    "    G_logit[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z_logit = G_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogitBLP_beta = logit.LogitBLP_estimator(y, z_logit, x, np.ones((T,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 789,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.62007445, -5.2231269 ,  1.63482891, -0.29272541,  4.80219233,\n",
       "       -7.07244195,  0.19039548,  0.35357351, -1.19227403, -0.74513781,\n",
       "       -0.19538289, -0.53030469, -1.50554842, -0.80083257, -1.04325918,\n",
       "       -1.6444787 , -1.07661701, -2.89393384, -0.94133483, -0.53954202,\n",
       "       -1.08147988, -1.98547801, -2.9546429 , -2.76501592, -1.95331535,\n",
       "       -1.68678532,  0.49380633, -1.87788536, -0.70656193, -0.67725803,\n",
       "       -0.76844748, -0.54523664, -1.43066425, -1.11054745, -1.78965802,\n",
       "       -2.80699212, -1.42887733, -3.82869218, -2.3875114 , -1.55711233,\n",
       "       -0.61245025, -1.70484285, -1.35546094, -1.08931072, -0.65932002,\n",
       "        1.54918688])"
      ]
     },
     "execution_count": 789,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLP approximation to optimal instruments\n",
    "\n",
    "BLP propose an algorithm for constructing an approximation to the optimal instruments. It is described in simple terms in Reynaert & Verboven (2014), and it has the following steps.\n",
    "It requires an initial parameter estimator $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'$, here we can just usethe MLE we have already computed. Let $W_m$ denote the matrix of instruments (this is the matrix $X_m$ with the price replaced by the exchange rate). The steps are then as follows:\n",
    "\n",
    "First we form the regression equation of the covariates on the instruments:\n",
    "$$\n",
    "X_m = W_m \\Pi + E_m\n",
    "$$\n",
    "\n",
    "The OLS estimate is then given as:\n",
    "$$\n",
    "\\hat \\Pi = \\left( \\frac{1}{T}\\sum_m W_m' W_m \\right)^{-1}\\left( \\frac{1}{T}\\sum_m W_m' X_m\\right)\n",
    "$$\n",
    "\n",
    "Thus the predicted covariates given the instruments $W$ are:\n",
    "$$\n",
    "\\hat X_m = W_m \\hat \\Pi\n",
    "$$\n",
    "\n",
    "Having constructed $\\hat X_m$ (which consists of the exogenous regressors, and the predicted price given $W_m$), we compute the predicted mean utility:\n",
    "\n",
    "$$\n",
    "\\hat u_m = \\hat X_m \\hat \\beta\n",
    "$$\n",
    "\n",
    "and then the predicted market shares at the mean utility:\n",
    "\n",
    "$$\n",
    "\\hat q_m^{*} = P(\\hat u_m | \\hat \\lambda)\n",
    "$$\n",
    "\n",
    "Computationally, here we just use $\\hat X_m$ in place of $X_m$ in the CCP function.\n",
    "Given the predicted market shares, we compute\n",
    "\n",
    "$$\n",
    "\\hat G_m^{*} = \\left[\\hat X_m, \\nabla_{q,\\lambda} \\Omega (\\hat q_m^{*} | \\hat \\lambda)\\right]\n",
    "$$\n",
    "\n",
    "which is the same as the function $\\hat G_m^0$ we already have constructed, except we evaluate it at the\n",
    "predictions $\\hat X_m$ and $\\hat q_m^{*}$ instead of at $X_m$ and $\\hat q_m^0$.\n",
    "\n",
    "The procedure above gives an approximation to the optimal instruments. We also require a weight matrix. The optimal weight matrix is the (generalized) inverse of the conditional (on the instruments) covariance of the fixed effects. Assuming $\\xi_{jm}$ is independetly and identically distributed over j and m, the conditional covariance simplifies to a scalar $\\sigma^2$ times an identity matrix (of size $J_m$).\n",
    "This means that all fixed effects are weighted equally, and the weights therefore drop out of the IV regression. The optimal IV estimator is therefore\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} = \\left(\\frac{1}{T}\\sum_m (\\hat G_m^*)'\\hat G_m^0\\right)^{-1}\\left( \\frac{1}{T}\\sum_m (\\hat G_m^*)'\\hat r_m^0 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat \\xi^*$ denote the estimated residual evaluated at the new parameter estimates,\n",
    "\n",
    "$$\n",
    "\\hat \\xi_{mj}^* = \\hat \\xi_{mj}(\\hat \\theta^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "We may estimate the constant $\\sigma^2$ by\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{T}\\sum_{m}\\sum_{j = 1}^{J_m} \\left(\\hat \\xi_{mj}^*\\right)^2 \n",
    "$$\n",
    "\n",
    "The distribution of the estimator $\\hat \\theta^{\\text{IV}}$ is then\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} \\sim \\mathcal{N}(\\theta_0, \\Sigma^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "which can be consistently estimated by\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma^{\\text{IV}} = \\hat \\sigma^2 \\left( \\sum_m (\\hat G_m^*)'\\hat G_m^0 \\right)^{-1}\n",
    "$$\n",
    "\n",
    "and the standard errors are then the square root of the diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_x(x, w, sample_share):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(w)\n",
    "    K = w[0].shape[1]\n",
    "\n",
    "    sWW = np.empty((T,K,K))\n",
    "    sWX = np.empty((T,K,K))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sWW[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], w[t])\n",
    "        sWX[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], x[t])\n",
    "\n",
    "    Pi_hat = la.solve(sWW.sum(axis=0), sWX.sum(axis=0))\n",
    "    X_hat = {t: np.einsum('jl,lk->jk', w[t], Pi_hat) for t in np.arange(T)}\n",
    "\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_se(Theta, y, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    S = T * np.array([x[t].shape[0] for t in np.arange(T)]).sum()\n",
    "\n",
    "    G = G_array(y, x, psi_stack, nest_count)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t]) for t in np.arange(T)}\n",
    "    \n",
    "    # We calculate \\sigma^2\n",
    "    xi = {t: np.einsum('jd,d->j', G[t], Theta) - r[t] for t in np.arange(T)}\n",
    "    sum_xij2 = np.empty((T,))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sum_xij2[t] = (xi[t]**2).sum()\n",
    "    \n",
    "    sigma2 = np.sum(sum_xij2) / S\n",
    "\n",
    "    # We calculate GG for each market t\n",
    "    GG = np.empty((T,d,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        GG[t,:,:] = np.einsum('jd,jp->dp', G[t], G[t])\n",
    "\n",
    "    # Finally we compute \\Sigma and the standard errors\n",
    "    Sigma = sigma2*la.inv(GG.sum(axis=0))\n",
    "    SE = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalBLP_estimator(Theta0, y, w, x, sample_share, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    beta0 = Theta0[:K]\n",
    "    lambda0 = Theta0[K:]\n",
    "    \n",
    "    X_hat = predict_x(x, w, sample_share)\n",
    "    q0 = IPDL_ccp(Theta0, X_hat, psi_stack, nest_count)\n",
    "    G_star =G_array(q0, X_hat, psi_stack, nest_count)\n",
    "    #G_star =G_array(y, w, psi_stack, nest_count)\n",
    "\n",
    "\n",
    "    G0 = G_array(y, x, psi_stack, nest_count)\n",
    "    \n",
    "    #G_star=G0\n",
    "    \n",
    "    r = {t: np.log(y[t]) for t in np.arange(T)}\n",
    "\n",
    "    d = G0[0].shape[1]\n",
    "\n",
    "    sGG = np.empty((T,d,d))\n",
    "    sGr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sGG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', G_star[t], G0[t])\n",
    "        sGr[t,:] = sample_share[t]*np.einsum('jd,j->d', G_star[t], r[t])\n",
    "\n",
    "    Theta_IV = la.solve(sGG.sum(axis=0), sGr.sum(axis=0))\n",
    "    SE_IV = BLP_se(Theta_IV, y, x, psi_stack, nest_count)\n",
    "\n",
    "    return Theta_IV, SE_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThetaOptBLP, SEOptBLP = OptimalBLP_estimator(IPDL_theta, y, z_logit, x, np.ones((T,)), Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "metadata": {},
   "outputs": [],
   "source": [
    "G0 = G_array(y, x, Psi, Nest_count)\n",
    "d = G0[0].shape[1]\n",
    "r = {t: np.log(y[t]) for t in np.arange(T)}\n",
    "\n",
    "# We calculate \\sigma^2\n",
    "xi = {t: np.einsum('jd,d->j', G0[t], ThetaOptBLP) - r[t] for t in np.arange(T)}\n",
    "xi_np = np.empty((np.int64(J.sum()),))\n",
    "index = J.cumsum()\n",
    "for t in np.arange(T):\n",
    "    if t == 0:\n",
    "        xi_np[:index[t]] = xi[t]\n",
    "    else:\n",
    "        xi_np[index[t-1]:index[t]] = xi[t]\n",
    "\n",
    "xi_np -= xi_np.mean() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.0896520741824117e-17"
      ]
     },
     "execution_count": 797,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xi_np.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9199"
      ]
     },
     "execution_count": 798,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "J.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7979664676419304"
      ]
     },
     "execution_count": 799,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in ThetaOptBLP[K:]  if p > 0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 800,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.20309070785106975, 0.024145447739504586]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.024145447739504586"
      ]
     },
     "execution_count": 800,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-LogL(IPDL_theta, y, x, pop_share, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 801,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02404461,  0.04591335,  0.08334267,  0.15053443,  0.05053455,\n",
       "        0.09686762,  0.04067728,  0.06649408,  0.01909169,  0.22046619,\n",
       "       -0.23280802])"
      ]
     },
     "execution_count": 801,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOptBLP[K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 802,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'brand', 'home']"
      ]
     },
     "execution_count": 802,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nest_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00079199, 0.00077638, 0.00082446, 0.00082639, 0.00074314,\n",
       "       0.0005593 , 0.00068753, 0.0007575 , 0.00053441, 0.00091092,\n",
       "       0.00108079])"
      ]
     },
     "execution_count": 803,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SEOptBLP[K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 804,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1379850"
      ]
     },
     "execution_count": 804,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = T*np.array([x[t].shape[0] for t in np.arange(T)]).sum()\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.19538288638001783"
      ]
     },
     "execution_count": 805,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 806,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.05041510631794155"
      ]
     },
     "execution_count": 806,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOptBLP[pr_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logit - for comparison\n",
    "\n",
    "Estimating a Logit model via maximum likelihood with an initial guess of parameters $\\hat \\beta^0 = 0$ yields estimated parameters $\\hat \\beta^{\\text{logit}}$ given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001529\n",
      "         Iterations: 46\n",
      "         Function evaluations: 57\n",
      "         Gradient evaluations: 57\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.ones((K,))\n",
    "\n",
    "# Estimate the model\n",
    "res_logit = logit.estimate_logit(logit.q_logit, beta_0, y, x, sample_share=pop_share, Analytic_jac=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>se</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.824034</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>9.621010e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.002441</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>1.152751e+02</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.136166</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>6.325787e+03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.448615</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>2.068207e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.554939</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>7.506463e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-1.880407</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>6.215515e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.264675</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>9.039518e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.663435</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>5.886485e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.116575</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>4.302137e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>-0.652739</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>6.512255e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.292592</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>1.464038e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>0.962816</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>6.884404e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>1.007616</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>4.644013e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>0.622118</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.022116e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>0.671208</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.247544e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>0.310640</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>5.625435e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>0.924506</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>5.164515e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>0.915711</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>3.981255e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>0.745935</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>7.612129e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.714215</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.066520e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>0.808597</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>6.187411e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>0.867970</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>3.036605e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>0.494373</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.124315e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>0.907405</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4.471019e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>0.125223</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>2.287115e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>0.753806</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>5.693568e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>1.340068</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>2.512477e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>0.816371</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>5.217005e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>0.877694</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>5.443267e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.907569</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>2.386189e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>1.325475</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>4.531246e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.781480</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>2.391350e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>0.817750</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>2.839358e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>0.622594</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>4.035900e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>0.997831</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>8.464688e+03</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>0.936781</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>1.907966e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>0.967615</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>4.435074e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>0.845540</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>7.752257e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>0.873573</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>1.474135e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>0.975646</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>1.492581e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>0.567161</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>1.533327e+05</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>0.818422</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>5.331833e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>0.815737</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>7.711718e+04</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.899885</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>1.756395e+06</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          parameters        se             t    p\n",
       "in_out     -2.824034  0.000029  9.621010e+04  0.0\n",
       "cy         -0.002441  0.000021  1.152751e+02  0.0\n",
       "hp         -0.136166  0.000022  6.325787e+03  0.0\n",
       "we         -0.448615  0.000022  2.068207e+04  0.0\n",
       "le         -1.554939  0.000021  7.506463e+04  0.0\n",
       "wi         -1.880407  0.000030  6.215515e+04  0.0\n",
       "he         -2.264675  0.000025  9.039518e+04  0.0\n",
       "li         -0.663435  0.000011  5.886485e+04  0.0\n",
       "sp         -1.116575  0.000026  4.302137e+04  0.0\n",
       "ac         -0.652739  0.000010  6.512255e+04  0.0\n",
       "pr          0.292592  0.000002  1.464038e+05  0.0\n",
       "brand_2     0.962816  0.000014  6.884404e+04  0.0\n",
       "brand_3     1.007616  0.000002  4.644013e+05  0.0\n",
       "brand_4     0.622118  0.000006  1.022116e+05  0.0\n",
       "brand_5     0.671208  0.000003  2.247544e+05  0.0\n",
       "brand_6     0.310640  0.000006  5.625435e+04  0.0\n",
       "brand_7     0.924506  0.000018  5.164515e+04  0.0\n",
       "brand_8     0.915711  0.000023  3.981255e+04  0.0\n",
       "brand_9     0.745935  0.000010  7.612129e+04  0.0\n",
       "brand_10    0.714215  0.000003  2.066520e+05  0.0\n",
       "brand_12    0.808597  0.000013  6.187411e+04  0.0\n",
       "brand_13    0.867970  0.000029  3.036605e+04  0.0\n",
       "brand_14    0.494373  0.000004  1.124315e+05  0.0\n",
       "brand_15    0.907405  0.000020  4.471019e+04  0.0\n",
       "brand_16    0.125223  0.000005  2.287115e+04  0.0\n",
       "brand_17    0.753806  0.000013  5.693568e+04  0.0\n",
       "brand_18    1.340068  0.000005  2.512477e+05  0.0\n",
       "brand_19    0.816371  0.000016  5.217005e+04  0.0\n",
       "brand_20    0.877694  0.000016  5.443267e+04  0.0\n",
       "brand_22    0.907569  0.000004  2.386189e+05  0.0\n",
       "brand_23    1.325475  0.000003  4.531246e+05  0.0\n",
       "brand_24   -0.781480  0.000003  2.391350e+05  0.0\n",
       "brand_25    0.817750  0.000029  2.839358e+04  0.0\n",
       "brand_26    0.622594  0.000015  4.035900e+04  0.0\n",
       "brand_27    0.997831  0.000118  8.464688e+03  0.0\n",
       "brand_28    0.936781  0.000049  1.907966e+04  0.0\n",
       "brand_29    0.967615  0.000022  4.435074e+04  0.0\n",
       "brand_30    0.845540  0.000011  7.752257e+04  0.0\n",
       "brand_32    0.873573  0.000006  1.474135e+05  0.0\n",
       "brand_33    0.975646  0.000007  1.492581e+05  0.0\n",
       "brand_34    0.567161  0.000004  1.533327e+05  0.0\n",
       "brand_35    0.818422  0.000015  5.331833e+04  0.0\n",
       "brand_36    0.815737  0.000011  7.711718e+04  0.0\n",
       "home_2      1.899885  0.000001  1.756395e+06  0.0"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_beta = res_logit['beta']\n",
    "logit_score = logit.logit_score(logit_beta, y, x, pop_share) # maybe use 'logit.' functions from Logit_file instead of including e.g. standard errors in logit.estimate_logit function\n",
    "logit_se = logit.logit_se(logit_score, N)\n",
    "logit_t, logit_p = logit.logit_t_p(logit_beta, logit_score, N)\n",
    "pd.DataFrame({'parameters': logit_beta, 'se' : logit_se, 't': logit_t, 'p': logit_p}, index = x_vars) # Our estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the corresponding Logit choice probabilities. STILL FIX THIS PART IN LOGIT BOOK!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_q = logit.logit_ccp(logit_beta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also find the elasticities and diversion ratios implied by the logit model as follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_logit = logit.logit_elasticity(logit_q, logit_beta, K-1) # Elasticities wrt. the price characteristic\n",
    "DR_logit_hat = logit.logit_diversion_ratio(logit_q, logit_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_hat = IPDL_elasticity(q_hat, x, IPDL_theta, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market $t=1$ the price elasticities are:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(E_hat[0]).rename_axis(columns = 'wrt. product', index = 'elasticity of product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios for the IPDL model\n",
    "\n",
    "The diversion ratio to product j from product k is the fraction of consumers leaving product k and switching to product j following a one percent increase in the price of product k. Hence we have:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{jk}^i = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial x_{ik\\ell}}{\\partial P_k(u_i|\\lambda) / \\partial x_{ik\\ell}} = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial u_{ik}}{\\partial P_k(u_i|\\lambda) / \\partial u_{ik}}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{D}^i = \\left( \\mathcal{D}_{jk}^i \\right)_{j,k \\in \\{0,1,\\ldots ,5\\}}$ is the matrix of diversion ratios for individual i. This can be written more compactly as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}^i = -100 \\cdot  (\\nabla_u P(u|\\lambda) \\circ I_J)^{-1}\\nabla_u P(u|\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_diversion_ratio(q, x, Theta, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates diversion ratios from the IPDL model\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Diversion_ratio: a dictionary of T numpy arrays (J,J) of diversion ratios from product j to product k for each individual i\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack, nest_count) # Find the derivatives of ccp's wrt. utilities\n",
    "    inv_diaggrad = {t: np.divide(1, np.diag(Grad[t]), out = np.zeros_like(np.diag(Grad[t])), where = (np.diag(Grad[t]) != 0)) for t in np.arange(T)}  # Compute the inverse of the 'own'-derivatives of ccp's\n",
    "    DR = {t: np.multiply(-100, np.einsum('j,jk->jk', inv_diaggrad[t], Grad[t])) for t in np.arange(T)} # Compute diversion ratios as a hadamard product.\n",
    "    \n",
    "    return DR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the implied diversion ratios $\\mathcal{ D}^i$ from our estimates $\\hat \\theta^{\\text{IPDL}}$, we find for market $t=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_hat = IPDL_diversion_ratio(q_hat, x, IPDL_theta, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(DR_hat[0]).rename_axis(index = 'DR of products', columns = 'DR wrt. products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-102.21420624,   11.66516355,   85.92937549,  -13.21267058,\n",
       "        -15.01285777,  -15.18617088,  -16.3434682 ,  -12.34547186,\n",
       "        -36.1653491 ,  -20.31962142,    6.98589365, -162.86761639,\n",
       "        -40.49628454,   15.60402234,  -30.33618596, -264.72963404,\n",
       "        118.11701982,  -10.23483481,   28.2294045 ,  -39.19937787,\n",
       "        -11.7471248 ,   14.45551088,  -47.95254862,  -13.84307058,\n",
       "        -24.77048885, -907.77820789,  -37.59854995,   20.93792549,\n",
       "         38.44868774,  -37.7905102 ,  -18.0676801 ,  -32.19858363,\n",
       "        -12.90814536,  -32.11284049,  -45.70039748,   49.37834998,\n",
       "          4.20532629, -133.27006306, -103.88631936,    1.66629638,\n",
       "        -21.98431801,   -8.8069312 ,  -14.50843962,    9.20688017,\n",
       "         11.05250475])"
      ]
     },
     "execution_count": 550,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DR_hat[0].sum(axis = 1).round(decimals = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of elasticities and diversion ratios\n",
    "\n",
    "We now compare the elasticities and the diversion ratios of the Logit and IPDL model. To clarify the interpretation of our results we will aggregate these according to the categorical variable `cla` describing the class or segment code of each vehicle. This variable takes values 'subcompact', 'compact', 'intermediate', 'standard', and 'luxury' encoded as the integers $1,\\ldots, 5$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all classes/segments $c,\\ell \\in \\{1,\\ldots, 5\\}$ we calculate the change in the probability of class $c$, given as $q_c = \\sum_j 1_{\\{j\\in c\\}} q_j$, for a one unit increase in each of the utilities $u_j$ for products $j\\in\\ell$ i.e. we calculate the directional derivatives $\\frac{\\partial q_c}{\\partial u_{\\ell}}$. Then the price-to-income semi-elasticity of class $c$ wrt. class $\\ell$ is given as $\\bar E_{c\\ell} = \\frac{\\partial q_c}{\\partial u_\\ell} \\frac{1}{q_c} \\beta_{\\text{princ}}$. We use the fact that the directional derivative is calculated as $\\frac{\\partial q_c}{\\partial u_{\\ell}} = \\sum_{j\\in c} \\sum_{k\\in \\ell} \\frac{\\partial q_j}{\\partial u_k}$. In matrix notation this may be calulated as $\\bar E = \\psi^{\\text{class}} \\mathcal{E} {\\psi^{\\text{class}}}'$, where $\\bar E = (\\bar E_{c\\ell})_{c,\\ell = 1,\\ldots,5}$ is the matrix of semi-elasticities between vehicle classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Psi_clafull, cla_descr, cla_count = Create_nests(dat[['cla', 'market', 'co']], 'market', 'co', ['cla'], outside_option=OO)\n",
    "\n",
    "if OO:\n",
    "    Psi_cla = {t: Psi_clafull[t][J[t]:, :] for t in np.arange(T)}\n",
    "else:\n",
    "    Psi_cla = {t: Psi_clafull[t][J[t]:, :] for t in np.arange(T)}\n",
    "    \n",
    "T_agg = Psi_cla[0].shape[0]\n",
    "\n",
    "q_Logit_agg = {t: np.einsum('cj,j->c', Psi_cla[t], logit_q[t]) for t in np.arange(T)}\n",
    "q_IPDL_agg = {t: np.einsum('cj,j->c', Psi_cla[t], q_hat[t]) for t in np.arange(T)}\n",
    "\n",
    "Grad_Logit = {t: (np.diag(logit_q[t]) - np.einsum('j,k->jk', logit_q[t], logit_q[t])) for t in np.arange(T)}\n",
    "Grad_IPDL = ccp_gradient(q_hat, x, IPDL_theta, Psi, Nest_count)\n",
    "\n",
    "dq_dp_Logit_agg = {t: np.einsum('cj,jk,lk->cl', Psi_cla[t], Grad_Logit[t], Psi_cla[t])*logit_beta[K-1] for t in np.arange(T)}\n",
    "dq_dp_IPDL_agg = {t: np.einsum('cj,jk,lk->cl', Psi_cla[t], Grad_IPDL[t], Psi_cla[t])*IPDL_theta[K-1] for t in np.arange(T)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit_E_agg = {t:  np.einsum('c,cl->cl', 1./ q_Logit_agg[t], dq_dp_Logit_agg[t]) for t in np.arange(T)}\n",
    "IPDL_E_agg = {t: np.einsum('c,cl->cl', 1./q_IPDL_agg[t], dq_dp_IPDL_agg[t]) for t in np.arange(T)}\n",
    "\n",
    "E0, E1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    E0[t,:,:] = Logit_E_agg[t]\n",
    "    E1[t,:,:] = IPDL_E_agg[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot histograms of our results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E0p = {j : (E0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E0p[j], num_bins, range = (np.quantile(E0p[j], 0.10), np.quantile(E0p[j], 0.90)), color = 'r', alpha = 1) # Logit is blue\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of weigthed sums of Logit (red) and IPDL (blue) price elasticities by class')\n",
    "fig.supxlabel('Weigthed sum of elasticities wrt. classes')\n",
    "fig.supylabel('Weigthed sum of elasticities of classes')\n",
    "fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E1p = {j : (E1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig1, axes1 = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes1[p].hist(E1p[j], num_bins, range = (np.quantile(E1p[j], 0.10), np.quantile(E1p[j], 0.90)), color = 'b', alpha = 1) # IPDL is blue\n",
    "    axes1[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes1[p].get_xaxis().set_visible(False)\n",
    "    axes1[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig1.suptitle('Histograms of weigthed sums of Logit (red) and IPDL (blue) price elasticities by class')\n",
    "fig1.supxlabel('Weigthed sum of elasticities wrt. classes')\n",
    "fig1.supylabel('Weigthed sum of elasticities of classes')\n",
    "fig1.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "fig1.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "fig1.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "fig1.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "fig1.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "fig1.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "fig1.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "fig1.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "fig1.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "fig1.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mean elasticities for the logit model are given as..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(E0.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. product', index = 'Mean elasticity of product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For IPDL the mean elasticities are..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(E1.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. product', index = 'Mean elasticity of product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios\n",
    "\n",
    "We now visualize the implied diversion ratios $\\mathcal{D}$. If $\\bar D_{c\\ell}$ denotes the sum of choice probability weigthed diversion ratios, then we have as above that $\\bar D_{c\\ell} = \\sum_{j}\\sum_{k} \\mathrm{1}_{\\{j\\in c\\}} \\mathrm{1}_{\\{k\\in \\ell\\}} q_j q_k \\mathcal{D}_{jk}$ i.e. more generally $\\bar D = (\\psi^{\\text{class}} \\circ q) \\mathcal{D} (\\psi^{\\text{class}} \\circ q).'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logit_D_agg = {t: -100*np.einsum('c,cl->cl', 1./np.diag(dq_dp_Logit_agg[t]), dq_dp_Logit_agg[t]) for t in np.arange(T)}\n",
    "IPDL_D_agg = {t: -100*np.einsum('c,cl->cl', 1./np.diag(dq_dp_IPDL_agg[t]), dq_dp_IPDL_agg[t]) for t in np.arange(T)}\n",
    "\n",
    "D0, D1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    D0[t,:,:] = Logit_D_agg[t]\n",
    "    D1[t,:,:] = IPDL_D_agg[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D0p = {j : (D0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D0p[j], num_bins, range = (np.quantile(D0p[j], 0.10), np.quantile(D0p[j], 0.90)), color = 'r', alpha = 1) # Logit is red\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of weigthed sums of Logit (red) and IPDL (blue) price diversion ratios by class')\n",
    "fig.supxlabel('Weigthed sum of diversion ratios wrt. classes')\n",
    "fig.supylabel('Weigthed sum of diversion ratios of classes')\n",
    "fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D1p = {j : (D1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg, sharex=False, sharey=False)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D1p[j], num_bins, range = (np.quantile(D1p[j], 0.10), np.quantile(D1p[j], 0.90)), color = 'b', alpha = 1) # IPDL is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of weigthed sums of Logit (red) and IPDL (blue) price diversion ratios by class')\n",
    "fig.supxlabel('Weigthed sum of diversion ratios wrt. classes')\n",
    "fig.supylabel('Weigthed sum of diversion ratios of classes')\n",
    "fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also calculate the mean diversion ratios within each class. For the Logit model these are given as..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(D0.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. product', index = 'Mean diversion ratio of product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the IPDL model the mean diversion ratios are..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(D1.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. product', index = 'Mean diversion ratio of product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR = 2*(IPDL_loglikelihood(IPDL_theta, y, x, pop_share, Psi, Nest_count).sum() - logit.logit_loglikehood(logit_beta, y, x, pop_share).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scstat.chi2.sf(LR, df = G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the corresponding choice probabilities implied by the MLE $\\hat \\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_14188\\1295911045.py:42: RuntimeWarning: divide by zero encountered in log\n",
      "  log_psiq =  np.log(psi_q) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n"
     ]
    }
   ],
   "source": [
    "q_hat = IPDL_ccp(IPDL_theta, x, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market $t=1$ the choice probabilites $\\hat q_t$ are: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also find the IPDL price elasticities $\\mathcal{E}$:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
