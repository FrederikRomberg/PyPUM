{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Demand for Cars with the IPDL model\n",
    "\n",
    "In this notebook, we will introduce and estimate the Inverse Product Differentiation Logit (IPDL) model of Fosgerau et al. (2023) using publically available data on the European car market from Frank Verboven's website at https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market. We begin by introducing the data set. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "The dataset consists of approximately 110 vehicle makes per year in the period 1970-1999 in five European markets (Belgium, France, Germany, Italy, and the United Kingdom). The data set includes 47 variables in total. The first four columns are market and product codes for the year, country, and make as well as quantity sold (No. of new registrations) which will be used in computing observed market shares. The remaining variables consist of car characteristics such as prices, horse power, weight and other physical car characteristics as well as macroeconomic variables such as GDP per capita which have been used to construct estimates of the average wage income and purchasing power.\n",
    "\n",
    "We have in total 30 years and 5 countries, totalling $T=150$ year-country combinations, indexed by $t$, and we refer to each simply as market $t$. In market $t$, the choice set is $\\mathcal{J}_t$ which includes the set of available makes as well as an outside option. Let $\\mathcal{J} := \\bigcup_{t=1}^T \\mathcal{J}_t$ be the full choice set and \n",
    " $J:=\\#\\mathcal{J}$ the number of choices which were available in at least one market, for this data set there are $J=357$ choices.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the dataset `eurocars.csv` we thus have a dataframe of $\\sum_{t=1}^T \\#\\mathcal{J}_t = 11459$ rows and $47$ columns. The `ye` column runs through $y=70,\\ldots,99$, the `ma` column runs through $m=1,\\ldots,M$, and the ``co`` column takes values $j\\in \\mathcal{J}$. \n",
    "\n",
    "Because we consider a country-year pair as the level of observation, we construct a `market` column taking values $t=1,\\ldots,T$. In Python, this variable will take values $t=0,\\ldots,T-1$. We construct an outside option $j=0$ in each market $t$ by letting the 'sales' of $j=0$ be determined as \n",
    "\n",
    "$$\\mathrm{sales}_{0t} = \\mathrm{pop}_t - \\sum_{j=1}^J \\mathrm{sales}_{jt}$$\n",
    "\n",
    "where $\\mathrm{pop}_t$ is the total population in market $t$, and the car characteristics of the outside option is set to zero. The market shares of each product in market $t$ can then be found as\n",
    "$$\n",
    "\\textrm{market share}_{jt}=\\frac{\\mathrm{sales_{jt}}}{\\mathrm{pop}_t}.\n",
    "$$\n",
    "We also read in the variable description of the dataset contained in `eurocars.dta`. We will use the list `x_vars` throughout to work with our explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variable names                                        description\n",
      "0              cy            cylinder volume or displacement (in cc)\n",
      "1              hp                                 horsepower (in kW)\n",
      "2              we                                     weight (in kg)\n",
      "3              le                                     length (in cm)\n",
      "4              wi                                      width (in cm)\n",
      "5              he                                     height (in cm)\n",
      "6              li          average of li1, li2, li3 (used in papers)\n",
      "7              sp                            maximum speed (km/hour)\n",
      "8              ac  time to acceleration (in seconds from 0 to 100...\n",
      "9              pr   price (in destination currency including V.A.T.)\n",
      "10          brand                                      name of brand\n",
      "11           home  domestic car dummy (appropriate interaction of...\n",
      "12            cla                              class or segment code\n",
      "x has full rank\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "\n",
    "# Files\n",
    "import Logit_file as logit\n",
    "import Eurocarsdata_file as eurocarsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and variable names\n",
    "# os.chdir('../GREENCAR_notebooks/') # Assigns work directory\n",
    "\n",
    "input_path = os.getcwd() # Assigns input path as current working directory (cwd)\n",
    "descr = (pd.read_stata('eurocars.dta', iterator = True)).variable_labels() # Obtain variable descriptions\n",
    "dat_file = pd.read_csv(os.path.join(input_path, 'eurocars.csv')) # reads in the data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ye</td>\n",
       "      <td>year (=first dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ma</td>\n",
       "      <td>market (=second dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>co</td>\n",
       "      <td>model code (=third dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zcode</td>\n",
       "      <td>alternative model code (predecessors and succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brd</td>\n",
       "      <td>brand code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type</td>\n",
       "      <td>name of brand and model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>name of model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>org</td>\n",
       "      <td>origin code (demand side, country with which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>loc</td>\n",
       "      <td>location code (production side, country where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frm</td>\n",
       "      <td>firm code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qu</td>\n",
       "      <td>sales (number of new car registrations)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pl</td>\n",
       "      <td>places (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>doors (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>li1</td>\n",
       "      <td>measure 1 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>li2</td>\n",
       "      <td>measure 2 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>li3</td>\n",
       "      <td>measure 3 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>princ</td>\n",
       "      <td>=pr/(ngdp/pop): price relative to per capita i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eurpr</td>\n",
       "      <td>=pr/avdexr: price in common currency (in SDR t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>exppr</td>\n",
       "      <td>=pr/avexr: price in exporter currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>avexr</td>\n",
       "      <td>av. exchange rate of exporter country (exporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>avdexr</td>\n",
       "      <td>av. exchange rate of destination country (dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>avcpr</td>\n",
       "      <td>av. consumer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>avppr</td>\n",
       "      <td>av. producer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>avdcpr</td>\n",
       "      <td>av. consumer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>avdppr</td>\n",
       "      <td>av. producer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xexr</td>\n",
       "      <td>avdexr/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tax</td>\n",
       "      <td>percentage VAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pop</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ngdp</td>\n",
       "      <td>nominal gross domestic product of destination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rgdp</td>\n",
       "      <td>real gross domestic product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>engdp</td>\n",
       "      <td>=ngdp/avdexr: nominal gdp in common currency (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ergdp</td>\n",
       "      <td>=rgdp/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>engdpc</td>\n",
       "      <td>=engdp/pop: nominal gdp per capita in common c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ergdpc</td>\n",
       "      <td>=ergdp/pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              ye                   year (=first dimension of panel)\n",
       "1              ma                market (=second dimension of panel)\n",
       "2              co             model code (=third dimension of panel)\n",
       "3           zcode  alternative model code (predecessors and succe...\n",
       "4             brd                                         brand code\n",
       "5            type                            name of brand and model\n",
       "6           brand                                      name of brand\n",
       "7           model                                      name of model\n",
       "8             org  origin code (demand side, country with which c...\n",
       "9             loc  location code (production side, country where ...\n",
       "10            cla                              class or segment code\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            frm                                          firm code\n",
       "13             qu            sales (number of new car registrations)\n",
       "14             cy            cylinder volume or displacement (in cc)\n",
       "15             hp                                 horsepower (in kW)\n",
       "16             we                                     weight (in kg)\n",
       "17             pl             places (number, not reliable variable)\n",
       "18             do              doors (number, not reliable variable)\n",
       "19             le                                     length (in cm)\n",
       "20             wi                                      width (in cm)\n",
       "21             he                                     height (in cm)\n",
       "22            li1  measure 1 for fuel efficiency (liter per km, a...\n",
       "23            li2  measure 2 for fuel efficiency (liter per km, a...\n",
       "24            li3  measure 3 for fuel efficiency (liter per km, a...\n",
       "25             li          average of li1, li2, li3 (used in papers)\n",
       "26             sp                            maximum speed (km/hour)\n",
       "27             ac  time to acceleration (in seconds from 0 to 100...\n",
       "28             pr   price (in destination currency including V.A.T.)\n",
       "29          princ  =pr/(ngdp/pop): price relative to per capita i...\n",
       "30          eurpr  =pr/avdexr: price in common currency (in SDR t...\n",
       "31          exppr              =pr/avexr: price in exporter currency\n",
       "32          avexr  av. exchange rate of exporter country (exporte...\n",
       "33         avdexr  av. exchange rate of destination country (dest...\n",
       "34          avcpr       av. consumer price index of exporter country\n",
       "35          avppr       av. producer price index of exporter country\n",
       "36         avdcpr    av. consumer price index of destination country\n",
       "37         avdppr    av. producer price index of destination country\n",
       "38           xexr                                       avdexr/avexr\n",
       "39            tax                                     percentage VAT\n",
       "40            pop                                         population\n",
       "41           ngdp  nominal gross domestic product of destination ...\n",
       "42           rgdp                        real gross domestic product\n",
       "43          engdp  =ngdp/avdexr: nominal gdp in common currency (...\n",
       "44          ergdp                                        =rgdp/avexr\n",
       "45         engdpc  =engdp/pop: nominal gdp per capita in common c...\n",
       "46         ergdpc                                         =ergdp/pop"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(descr, index=['description']).transpose().reset_index().rename(columns={'index' : 'variable names'}) # Prints data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              cy            cylinder volume or displacement (in cc)\n",
       "1              hp                                 horsepower (in kW)\n",
       "2              we                                     weight (in kg)\n",
       "3              le                                     length (in cm)\n",
       "4              wi                                      width (in cm)\n",
       "5              he                                     height (in cm)\n",
       "6              li          average of li1, li2, li3 (used in papers)\n",
       "7              sp                            maximum speed (km/hour)\n",
       "8              ac  time to acceleration (in seconds from 0 to 100...\n",
       "9              pr   price (in destination currency including V.A.T.)\n",
       "10          brand                                      name of brand\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            cla                              class or segment code"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside option is included if OO == True, otherwise analysis is done on the inside options only.\n",
    "OO = True\n",
    "\n",
    "# Choose which variables to include in the analysis, and assign them either as discrete variables or continuous.\n",
    "\n",
    "x_discretevars = [ 'brand', 'home', 'cla']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "z_IV_contvars = ['xexr']\n",
    "z_IV_discretevars = []\n",
    "x_allvars =  [*x_contvars, *x_discretevars]\n",
    "z_allvars = [*z_IV_contvars, *z_IV_discretevars]\n",
    "\n",
    "if OO:\n",
    "    nest_vars = [var for var in ['in_out', *x_allvars] if (var != 'pr')] # We nest over all variables other than price, but an alternative list can be specified here if desired.\n",
    "else:\n",
    "    nest_vars = [var for var in x_allvars if (var != 'pr')] # See above\n",
    "\n",
    "nest_cont_vars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac'] # The list of continuous variables, from which nests will be created according to the deciles of the distribution.\n",
    "\n",
    "G = len(nest_vars)\n",
    "\n",
    "# Print list of chosen variables as a dataframe\n",
    "pd.DataFrame(descr, index=['description'])[x_allvars].transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the data to fit our setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, dat_org, x_vars, z_vars, N, pop_share, T, J, K = eurocarsdata.Eurocars_cleandata(dat_file, x_contvars, x_discretevars, z_IV_contvars, z_IV_discretevars, outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of numpy arrays for each market. This allows the size of the data set to vary over markets.\n",
    "\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)} # Dict of explanatory variables\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)} # Dict of market shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function tests whether the utility parameters are identified, by looking at the rank of the stacked matrix of explanatory variables.\n",
    "\n",
    "def rank_test(x):\n",
    "    x_stacked = np.concatenate([x[t] for t in np.arange(T)], axis = 0)\n",
    "    eigs=la.eig(x_stacked.T@x_stacked)[0]\n",
    "\n",
    "    if np.min(eigs)<1.0e-8:\n",
    "        print('x does not have full rank')\n",
    "    else:\n",
    "        print('x has full rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has full rank\n"
     ]
    }
   ],
   "source": [
    "rank_test(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed utility, logit and nested logit\n",
    "\n",
    "In the following, a vector $z\\in \\mathbb R^d$ is always a column vector. The IPDL model is a discrete choice model, where the probability vector over the alternatives is given by the solution to a utility maximization problem of the form\n",
    "$$\n",
    "p=\\arg\\max_{q\\in \\Delta} q'u-\\Omega(q)\n",
    "$$\n",
    "where $\\Delta$ is the probability simplex over the set of discrete choices, $u$ is a vector of payoffs for each option, $\\Omega$ is a convex function and $q'$ denotes the transpose of $q$. All additive random utility models can be represented in this way (Fosgerau and Sørensen (2021)). For example, the logit choice probabilities result from the perturbation function $\\Omega(q)=q'\\ln q$ where $\\ln q$ is the elementwise logarithm.\n",
    "\n",
    "In the nested logit model, the choice set is divided into a partition $\\mathcal C=\\left\\{C_1,\\ldots,C_L\\right\\}$, and the perturbation function is given by\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\sum_{\\ell =1}^L \\left( \\sum_{j\\in C_\\ell}q_j\\right)\\ln \\left( \\sum_{j\\in C}q_j\\right),\n",
    "$$\n",
    "where $\\lambda\\in [0,1)$ is a parameter. This function can be written equivalently as\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\left(\\psi q\\right)'\\ln \\left( \\psi q\\right),\n",
    "$$\n",
    "where $\\psi$ is a $J \\times L$ matrix, where $\\psi_{j\\ell}=1$ if option $j$ belongs to nest $C_\\ell$ and zero otherwise.\n",
    " This specification generates nested logit choice probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The IPDL model\n",
    "\n",
    "In the IPDL model, we allow for multiple nesting structures. For each $g=1,\\ldots, G$, let $\\mathcal C_g$ and $\\psi^g$ be constructed as described for the nested logit, and let $L_g$ be the number of nests in group $g$. The IPDL perturbation function is then\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\sum_g \\lambda_g) q'\\ln q +\\sum_g \\lambda_g \\left(\\psi^g q \\right)'\\ln \\left( \\psi^g q\\right),\n",
    "$$\n",
    "where $\\lambda=(\\lambda_1,\\ldots,\\lambda_G)$ is a parameter vector satisfying $\\lambda_g \\geq 0$ and $\\sum_g \\lambda_g<1$. In this model, each option belongs to $G\\geq 1$ nests. When $G=1$, it simplifies to the nested logit model, and when $\\sum_g \\lambda_g=0$, it simplifies to the logit model. The IPDL model therefore allows more flexibility than a single nested logit model in the types of substitution patterns it can represent, without having to specify a hierarchical structure over the nests.\n",
    "\n",
    "In this note, the nesting is done according to a subset of the explanatory variables. For categorical variables, each category is a nest. For continuous variables, the data set is partitioned according to the deciles of the variable, resulting in `at most` 10 nests of roughly equal size, as well as a nest for the outside option. This construction implies that $\\Omega$ is a function of the data.\n",
    "\n",
    "## Similarity and negative values of $\\lambda$\n",
    "\n",
    "Bla bla bla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The utility function\n",
    "\n",
    "Let $x_{tj}$ be the vector of product characteristics for option $j$ in market $t$, and let $X_t$ denote the $J_t\\times K $ matrix with elements $x_{tjk}$. The payoff of option $j$ is a linear function of the characteristics $x_{tj}$ of product $j$, which means that the vector of utilities may be written\n",
    "$$\n",
    "u(X_t,\\beta)=X_t\\beta.\n",
    "$$\n",
    "\n",
    "Letting $\\theta=(\\beta',\\lambda')'$ denote the full parameter vector of length $D=K+G$, the choice probabilities in market $t$ may be written as\n",
    "$$\n",
    "p_t(\\theta)=\\arg \\max_{q\\in \\Delta_{J_t}} \\left\\{q'X_t \\beta-(1-\\sum_g \\lambda_g)q'\\ln q +\\sum_{g=1}^G\\lambda_g \\left(\\psi^{gt} q \\right)'\\ln \\left(\\psi^{gt} q\\right)\\right\\}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-rescaling for numerical stability\n",
    "\n",
    "Let $\\alpha$ be a scalar, and let $\\iota$ be the all-ones vector in $\\mathbb R^J$. Note that $q'(u+\\alpha\\iota)=q'u+(q'\\iota)\\alpha=q'u+\\alpha$, since $q$ sums to one. For this reason, $\\alpha$ does not enter into the utility maximization when calculating $P(u+\\alpha\\iota|\\lambda)$, and we have $P(u+\\alpha\\iota|\\lambda)=P(u|\\lambda)$.\n",
    "\n",
    "This allows us to re-scale the utilities just as in the logit model, since $P(u-(\\max_{j}u_j)\\iota|\\lambda)=P(u|\\lambda)$. The numerical benefits of this approach carry over to the IPDL model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient and Hessian\n",
    "\n",
    "The gradient of $\\Omega$ with respect to the choice probabilities is\n",
    "\n",
    "$$\n",
    "\\nabla_q \\Omega_t(q|\\lambda)=(1-\\sum_g \\lambda_g)\\ln q+ \\sum_g \\lambda_g(\\psi^{gt})'\\ln \\left( \\psi^{gt}q\\right)+\\iota=\\ln q-Z_t(q)\\lambda+\\iota\n",
    "$$\n",
    "where $\\iota$ is the all-ones vector and\n",
    "$$Z_{tg}(q)=\\ln q - (\\psi^{tg})' \\ln (\\psi^{tg}q)$$\n",
    "\n",
    "The Hessian of $\\Omega$ is\n",
    "$$\n",
    "\\nabla_{qq}^2 \\Omega_t(q|\\lambda)=(1-\\sum_g \\lambda_g) \\mathrm{diag}(q)^{-1}+\\sum_g\\lambda_g (\\psi^{gt})'\\mathrm{diag}(\\psi^{gt}q)^{-1}\\psi^{gt}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\Gamma$, we can show that\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(\\Gamma q)'\\ln (\\Gamma q)+c\\\\\n",
    "\\nabla_q \\Omega(q|\\lambda)=\\Gamma'\\ln (\\Gamma q)+\\iota\\\\\n",
    "\\nabla^2_{qq}\\Omega(q|\\lambda)=\\Gamma'\\mathrm{diag}(\\Gamma q)^{-1}\\Gamma,\n",
    "$$\n",
    "where $c$ is a scalar that depends on $\\lambda$ but not on $q$ and therefore does not affect the utility maximization problem, $\\iota=(1,\\ldots,1)'\\in \\mathbb R^J$ is the all-ones vector and $\\mathrm{diag}(z)$ is a diagonal matrix with the elements of the vector $z$ on the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "For purposes of computing the gradient and Hessian of $\\Omega$, it is convenient to define\n",
    "$$\n",
    "\\Gamma=\\left(\\begin{array}{c}\n",
    "(1-\\sum_g \\lambda_g)I_J\\\\\n",
    "\\lambda_1 \\Psi^1\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_G \\Psi^G\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "where $I_J$ is the identity matrix in $\\mathbb R^J$. The matrix $\\Gamma$ is a block matrix with $J+\\sum_g C_g$ rows and $J$ columns. Note that \n",
    "\n",
    "$$\n",
    "\\Gamma q=\\left(\\begin{array}{c}\n",
    "(1-\\sum_g\\lambda_g)q \\\\\n",
    "\\lambda_1\\Psi^g q\\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_G \\Psi^Gq\n",
    "\\end{array}\\right)>0\n",
    "$$\n",
    "if $q>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_nests(data, markets_id, products_id, columns, cont_var = None, cont_var_bins = None, outside_option = True):\n",
    "    '''\n",
    "    This function creates the nest matrices \\Psi^{gt}, and stack them over g for each t.\n",
    "\n",
    "    Args.\n",
    "        data: a pandas DataFrame\n",
    "        markets_id: a string denoting the column of 'data' containing an enumeration t=0,1,...,T-1 of markets\n",
    "        products_id: a string denoting the column of 'data' containing product codes which uniquely identifies products\n",
    "        columns: a list containing the column names of columns in 'data' from which nest groupings g=0,1,...,G-1 for each market t are to be generated\n",
    "        cont_var: a list of the continuous variables in 'columns'\n",
    "        cont_var_bins: a list containing the number of bins to make for each continuous variable in 'columns'\n",
    "        outside_option: a boolean indicating whether the model is estimated with or without an outside option. Default is set to 'True' i.e. with an outside option.\n",
    "\n",
    "    Returns\n",
    "        Psi: a dictionary of length T of the J[t] by J[t] identity stacked on top of the Psi_g matrices for each market t and each gropuing g\n",
    "        nest_dict: a dictionary of length T of pandas series describing the structure of each nest for each market t and each grouping g\n",
    "        nest_count: a dictionary of length T of (G,) numpy arrays containing the amount of nests in each category g\n",
    "    '''\n",
    "\n",
    "    T = data[markets_id].nunique()\n",
    "    J = np.array([data[data[markets_id] == t][products_id].nunique() for t in np.arange(T)])\n",
    "    \n",
    "    # We include nest on outside vs. inside options. The amount of categories varies if the outside option is included in the analysis.\n",
    "    dat = data.sort_values(by = [markets_id, products_id]) # We sort the data in ascending, first according to market and then according to the product id\n",
    "    \n",
    "    Psi = {}\n",
    "    nest_dict = {}\n",
    "    nest_counts = {}\n",
    "\n",
    "    # Assign nests for products in each market t\n",
    "    for t in np.arange(T):\n",
    "        data_t = dat[dat[markets_id] == t] # Subset data on market t\n",
    "\n",
    "\n",
    "        ### Bin continuous variables according to quantiles of the variable\n",
    "\n",
    "        if cont_var == None:\n",
    "            None\n",
    "        else:\n",
    "            for var,n_bins in zip(cont_var,cont_var_bins):\n",
    "                if outside_option:\n",
    "                    q_dat = np.unique(np.quantile(data_t[var].rank(method = 'min'), q = np.arange(1,n_bins + 1) / n_bins)) # Get the unique 'n_bins' equally spaced quantiles of each continuous variable given in the cont_var list\n",
    "                    data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = [0.99,1, *q_dat], labels=False) # Quantiles are equally spaced with 'n_bins' quantiles for the variable. The outside option gets its own bin (0.99,1].\n",
    "                else:\n",
    "                    q_dat = np.unique(np.quantile(data_t[var].rank(method = 'min'), q = np.arange(1,n_bins + 1) / n_bins)) # Get the unique 'n_bins' equally spaced quantiles of each continuous variable given in the cont_var list\n",
    "                    data_t[var] = pd.cut(data_t[var].rank(method = 'min'), bins = q_dat, labels=False) # Bin the variable according to 'n_bins' equally spaced quantiles.\n",
    "\n",
    "        nest_dict[t] = data_t[columns].apply(lambda col: list(np.unique(col))) # Get the unique values of each 'col' in columns\n",
    "        nest_counts[t] = data_t[columns].nunique().values # Find the number of unique values in each column in columns and output as a numpy array\n",
    "\n",
    "        nest_count_total = data_t[columns].nunique().sum() # Find the sum of nest counts L_g\n",
    "        nests = pd.get_dummies(data_t[columns], columns = columns).values.reshape((J[t], nest_count_total)).transpose() # Finds dummies for each category in columns, and converts these to numpy arrays of the appropiate size. Note that the data has been sorted according to market and then product.\n",
    "        Psi_t = np.concatenate([np.eye(J[t]), nests], axis = 0) # Stack a J[t] by J[t] identity on top of the stacked \\Psi^g matrices for each market t\n",
    "\n",
    "        Psi[t] = Psi_t\n",
    "\n",
    "    return Psi, nest_dict, nest_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_bins=[np.int64(10) for i in range(len(nest_cont_vars))] # Sets the number of bins to 10 for each continuous variable.\n",
    "Psi, Nest_descr, Nest_count = Create_nests(dat, 'market', 'co', nest_vars, nest_cont_vars, cont_bins , outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Gamma(Lambda, Psi, nest_count):\n",
    "    '''\n",
    "    This function \n",
    "    '''\n",
    "\n",
    "    T = len(Psi)\n",
    "    \n",
    "    Gamma = {}\n",
    "    lambda0 = np.array([1 - sum(Lambda)])\n",
    "    Lambda_full = np.concatenate((lambda0, Lambda)) # create vector (1- sum(lambda), lambda_1, ..., lambda_G)\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        C,J = Psi[t].shape # The amount of alternatives in market t\n",
    "        Lambda_long = np.empty((C,)) # Initialize a row vector with as many rows as psi_stack\n",
    "        indices = np.concatenate((np.array([J]) , nest_count[t])).cumsum().astype('int64') # Get the indices of where the identity and the nests in psi_stack are located along the rows of psi_stack.\n",
    "\n",
    "        for i in np.arange(len(indices)):\n",
    "            if i == 0:\n",
    "                Lambda_long[0:(indices[i])] = Lambda_full[i] # Assign 1-sum(lambda) to the first J coordinates of Lambda_long\n",
    "            else:\n",
    "                Lambda_long[indices[i-1]:indices[i]] = Lambda_full[i] # Assign lambda_g to the coordinates of Lambda_long corresponding to the rows of psi_stack equal to the block matrix \\psi^g \n",
    "    \n",
    "        Gamma[t] =  np.einsum('c,cj->cj', Lambda_long, Psi[t]) # Compute hadamard product of lambda parameters and psi_stack\n",
    "\n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda0 = np.ones((G,))/(2*(G+1))\n",
    "Gamma0 = Create_Gamma(lambda0, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "Suppose we are evaluating the choice probability function $p_t(\\theta)$ at some parameter vector $\\theta$. While it is possible to solve for the choice probabilities explicitly by numerical maximization, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Let $u_t=X_t\\beta$ and let $q_t^0$ be an initial guess of the choice probabilities, e.g. $q_t^0\\propto \\exp(X_t\\beta)$. Define further\n",
    "$$\n",
    "a=\\sum_{g:\\lambda_g\\geq 0} \\lambda_g   \\qquad b=\\sum_{g:\\lambda_g<0} |\\lambda_g|.\n",
    "$$\n",
    "\n",
    "The choice probabilities are then updated iteratively as\n",
    "$$\n",
    "q_t^{r} = \\frac{e^{v_t^{r}}}{\\sum_{j\\in \\mathcal J_t} e^{v_{tj}^{r}}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "v_t^{r} =\\ln q_t^{r-1}+\\left(u_t-\\nabla_q \\Omega_t(q^{r-1}_t|\\lambda)\\right)/(1+b).\n",
    "$$\n",
    "Using the definition of $Z_{gt}$ above, this becomes\n",
    "$$\n",
    "v^r_t=\\ln q_t^{r-1}+\\left(u_t+Z_{t}(q^{r-1})\\lambda-\\ln q_t^{r-1}  \\right)/(1+b) =  \\left( u_t+ b\\ln q^{r-1}_t+Z_{t}(q^{r-1})\\lambda\\right)/(1+b)\n",
    "$$\n",
    "\n",
    "\n",
    "For numerical stability, it can be a good idea to also do max-rescaling of $v^r_t$ at every iteration. The Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p_t(\\theta)||q_t^{r})\\leq \\frac{a+b}{1+b}D_{KL}(p_t(\\theta)||q^{r-1}_t).\n",
    "$$\n",
    "This is implemeneted in the function \"IPDL_ccp\" below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_ccp(Theta, x, psi, nest_count, tol = 1.0e-15, maximum_iterations = 1000):\n",
    "    '''\n",
    "    This function finds approximations to the true conditional choice probabilities given parameters.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        tol: tolerated approximation error\n",
    "        maximum_iterations: a no. of maximum iterations which if reached will stop the algorithm\n",
    "\n",
    "    Output\n",
    "        q_1: a dictionary of T numpy arrays (J[t],) of IPDL choice probabilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x) # Number of markets\n",
    "    K = x[0].shape[1] # Number of car characteristics\n",
    "\n",
    "    # Parameters\n",
    "    Beta = Theta[:K]\n",
    "    Lambda = Theta[K:]\n",
    "    G = len(Lambda)  # Number of groups\n",
    "\n",
    "    #print(Lambda)\n",
    "    # Calculate small beta\n",
    "    C_minus = np.array([True if Lambda[g] < 0 else False for g in np.arange(G)])\n",
    "    #print(C_minus) # Find the categories g with negative a negative parameter lambda_g\n",
    "    if C_minus.all() == False:\n",
    "        b = 0\n",
    "    else:    \n",
    "        b = np.abs(Lambda[C_minus]).sum() # sum of absolute value of negative lambda parameters.\n",
    "\n",
    "    Gamma = Create_Gamma(Lambda, psi, nest_count) # Find the Gamma matrix\n",
    "\n",
    "    u = {t: np.einsum('jk,k->j', x[t], Beta) for t in np.arange(T)} # Calculate linear utilities\n",
    "    q = {t: np.exp(u[t]) / np.exp(u[t]).sum() for t in np.arange(T)}\n",
    "    q0 = q\n",
    "    Epsilon = 1.0e-10\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        q1 = {}\n",
    "        for t in np.arange(T):\n",
    "            # Calculate v\n",
    "            psi_q = np.einsum('cj,j->c', psi[t], q0[t]) # Compute matrix product\n",
    "            log_psiq =  np.log(np.abs(psi_q) + Epsilon) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "            gamma_log_prod = np.einsum('cj,c->j', Gamma[t], log_psiq) # Compute matrix product\n",
    "            v = np.log(q0[t] + Epsilon) + (u[t] - gamma_log_prod)/(1 + b) # Calculate v = log(q) + (u - Gamma^T %o% log(Gamma %o% q) %o% Gamma)/(1 + b)\n",
    "            v -= v.max(keepdims = True) # Do max rescaling wrt. alternatives\n",
    "\n",
    "            # Calculate iterated ccp q^k\n",
    "            numerator = np.exp(v)\n",
    "            denom = numerator.sum()\n",
    "            q1[t] = numerator/denom\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.array([np.sum((q1[t]-q0[t])**2/q[t]) for t in np.arange(T)])) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "        \n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand derivatives and price Elasticity\n",
    "\n",
    "While the demand derivatives in the IPDL model are not quite as simple as in the logit model, they are still easy to compute. \n",
    "Let $q=P(u|\\lambda)$, then\n",
    "$$\n",
    "\\nabla_u P(u|\\lambda)=\\left(\\nabla^2_{qq}\\Omega(q|\\lambda)\\right)^{-1}-qq'\n",
    "$$\n",
    "where the $()^{-1}$ denotes the matrix inverse. The derivatives with respect to any $x_{ij\\ell}$ can now easily be computed by the chain rule,\n",
    "$$\n",
    "    \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{\\partial u_{ik}}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell,\n",
    "$$\n",
    "\n",
    "Finally, moving to price elasticity is the same as in the logit model, if $x_{ik\\ell}$ is the log price of product $k$ for individual $i$, then\n",
    "$$\n",
    "    \\mathcal{E}_{jk}= \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}\\frac{1}{P_j(u_i|\\lambda)}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{1}{P_j(u_i|\\lambda)}\\beta_\\ell=\\frac{\\partial \\ln P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell$$\n",
    "we can also write this compactly as\n",
    "$$\n",
    "\\nabla_u \\ln P(u|\\lambda)=\\mathrm{diag}(P(u|\\lambda))^{-1}\\nabla_u P(u|\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pertubation_hessian(q, x, Theta, psi, nest_count):\n",
    "    '''\n",
    "    This function calucates the hessian of the pertubation function \\Omega\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Hess: a dictionary of T numpy arrays (J[t],J[t]) of second partial derivatives of the pertubation function \\Omega for each market t\n",
    "    '''\n",
    "    \n",
    "    T = len(q.keys())\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    Gamma = Create_Gamma(Theta[K:], psi, nest_count) # Find the \\Gamma matrices \n",
    "    #Hess = {}\n",
    "    Hess={}\n",
    "    for t in np.arange(T):\n",
    "        psi_q = np.einsum('cj,j->c', psi[t], q[t]) # Compute a matrix product\n",
    "        Hess[t] = np.einsum('cj,c,cl->jl', Gamma[t], 1/psi_q, psi[t]) # Computes the product \\Gamma' diag(\\psi q)^{-1} \\psi (but faster)\n",
    "\n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_gradient(q, x, Theta, psi_stack, nest_count):\n",
    "    \n",
    "    '''\n",
    "    This function calucates the gradient of the choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K) of partial derivatives of the choice proabilities wrt. utilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Grad = {}\n",
    "    Hess = compute_pertubation_hessian(q, x, Theta, psi_stack, nest_count) # Compute the hessian of the pertubation function\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        inv_omega_hess = la.inv(Hess[t]) # (J,J) for each t=1,...,T , computes the inverse of the Hessian\n",
    "        qqT = q[t][:,None]*q[t][None,:] # (J,J) outerproduct of ccp's for each market t\n",
    "        Grad[t] = inv_omega_hess - qqT  # Compute IPDL gradient of ccp's wrt. utilities\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calucates the gradient of the log choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Epsilon: a dictionary of T numpy arrays (J[t],J[t]) of partial derivatives of the log choice proabilities of products j wrt. utilites of products k for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack, nest_count) # Find the gradient of ccp's wrt. utilities\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        #ccp_grad = Grad[t]\n",
    "        #inv_diagq = np.divide(1, q[t], out = np.inf*np.ones_like(q[t]), where = (q[t] > 0)) # Find the inverse of the ccp's and assign infinity to any entry if that entry has q = 0\n",
    "        Epsilon[t] = Grad[t]/q[t][:,None] # Computes diag(q)^{-1}Grad[t]\n",
    "        #np.einsum('j,jk->jk', inv_diagq, ccp_grad) # Computes a Hadamard product. Is equivalent to:   diag(q)^-1 %o% ccp_grad\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_elasticity(q, x, Theta, psi_stack, nest_count, char_number = K-1):\n",
    "    ''' \n",
    "    This function calculates the elasticity of choice probabilities wrt. any characteristic or nest grouping of products\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        char_number: an integer which is an index of the parameter in theta wrt. which we wish calculate the elasticity. Default is the index for the parameter of 'pr'.\n",
    "\n",
    "    Returns\n",
    "        a dictionary of T numpy arrays (J[t],J[t]) of choice probability semi-elasticities for each market t\n",
    "    '''\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count) # Find the gradient of log ccp's wrt. utilities\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = Grad[t]*Theta[char_number] # Calculate semi-elasticities\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of IPDL\n",
    "\n",
    "The log-likelihood contribution is\n",
    "$$\n",
    "\\ell_t(\\theta)=y_t'\\ln p(\\mathbf{X}_t,\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_t$ and $\\theta$ - calculates $u_t=\\mathbf{X}_t\\beta$ and constructs $\\Gamma$, and then calls the fixed point routine described above. That routine will return $p(\\mathbf{X}_t,\\theta)$, and we can then evaluate $\\ell_t(\\theta)$. Using our above defined functions we now construct precisely such an estimation procedure.\n",
    "\n",
    "For maximizing the likelihood, we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_t=p(\\mathbf{X}_t,\\theta)$, then we have\n",
    "$$\n",
    "\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)=\\mathrm{diag}(q_t)^{-1}\\left(\\nabla_{qq}^2\\Omega(q_t|\\lambda)^{-1}-q_tq_t' \\right)\\left[\\mathbf{X}_t,-\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P(u|\\lambda)$ and the last term is a block matrix of size $J\\times dim(\\theta)$. Note that the latter cross derivative $\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)$ is given by $\\nabla_{q,\\lambda} \\Omega(q_t|\\lambda)_g = \\ln(q) - (\\Psi^g)' \\ln(\\Psi^g q)$ for each row $g=1,\\ldots,G$. The derivative of the log-likelihood function can be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_t(\\theta)=\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)' y_t \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (T,) of IPDL loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "    ccp_hat = IPDL_ccp(Theta, x, psi_stack, nest_count)\n",
    "    sum_lambdaplus = np.array([theta for theta in Theta[K:] if theta >0]).sum()\n",
    "\n",
    "    '''if sum_lambdaplus >= 1:\n",
    "        ll = np.NINF*np.ones((T,))'''\n",
    "\n",
    "    \n",
    "    ll=np.empty((T,))\n",
    "    for t in np.arange(T):\n",
    "        ll[t] = sample_share[t]*(y[t].T@np.log(ccp_hat[t]))#np.einsum('j,j', y[t], np.log(ccp_hat[t], out = -np.inf*np.ones_like(ccp_hat[t]), where = (ccp_hat[t] > 0)))\n",
    "\n",
    "    print([sum_lambdaplus, -ll.mean()])\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' The negative loglikelihood criterion to minimize\n",
    "    '''\n",
    "    Q = -IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the derivative of the loglikehood wrt. parameters $\\nabla_\\theta \\ell_t(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation(q, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)} # Determine log(q), and set entries equal minus inifinity if entry <= 0\n",
    "    Z = {}\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        G = len(nest_count[t])\n",
    "        indices = np.int64(np.cumsum(nest_count[t])) # Find the indices of the categories g used in the psi_stack matrices\n",
    "        J = np.int64(psi_stack[t].shape[0] - np.sum(nest_count[t])) # Find the number of alternatives\n",
    "        Z_t = np.empty((J,G)) # Initialize a J[t] by G numpy matrix for market t\n",
    "\n",
    "        for g in np.arange(G):\n",
    "\n",
    "            # Find the \\psi^g matrix for category g\n",
    "            if g == 0:\n",
    "                Psi = psi_stack[t][J:J+indices[g],:] \n",
    "            else:\n",
    "                Psi = psi_stack[t][J+indices[g-1]:J+indices[g],:]\n",
    "\n",
    "            Psi_q = np.einsum('cj,j->c', Psi, q[t]) # Compute a matrix product\n",
    "            log_Psiq = np.log(Psi_q, out = -np.inf*np.ones_like(Psi_q), where = (Psi_q > 0)) # Determine log of Psi_q, and set entries equal to minus infinity if entry <= 0.\n",
    "            Psi_logPsiq = np.einsum('cj,c->j', Psi, log_Psiq) # Compute matrix product\n",
    "\n",
    "            Z_t[:,g] = log_q[t] - Psi_logPsiq # Compute cross differential\n",
    "        \n",
    "        Z[t] = Z_t\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_theta_grad_log_ccp(Theta, x, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the derivative of the IPDL log ccp's wrt. parameters theta\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K+G) of derivatives of the IPDL log ccp's wrt. parameters theta for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "\n",
    "    q = IPDL_ccp(Theta, x, psi_stack, nest_count) # Find choice probabilities\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack, nest_count) # Find cross differentials of the pertubation function\n",
    "    u_grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count)  # Find the gradient of log ccp's wrt. utilities\n",
    "    Grad={}\n",
    "\n",
    "    for t in range(T):\n",
    "        G=np.concatenate((x[t], Z[t]), axis=1)\n",
    "        Grad[t]=u_grad[t]@G\n",
    "   \n",
    "   # G = [np.concatenate((x[t], Z[t]), axis=1) for t in np.arange(T)] # Construct the block matrix of the covariates and the cross differentials as block matrices\n",
    "    #Grad = {t: np.einsum('jk,kd->jd', u_grad[t], G[t]) for t in np.arange(T)} # Compute the derivative by matrix multiplication.\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the score of the IPDL loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of IPDL scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = IPDL_theta_grad_log_ccp(Theta, x, psi_stack, nest_count) # Find derivatives of the IPDL log ccp's wrt. parameters theta\n",
    "    D = log_ccp_grad[0].shape[1] # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] =sample_share[t]*(log_ccp_grad[t].T@y[t]) #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' The derivative of the negative loglikelihood criterion\n",
    "    '''\n",
    "    return -IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analyticgrad(y, x, theta, sample_share, Psi, Nest_count, delta = 1.0e-8):\n",
    "\n",
    "    numgrad = np.empty((T, K+G))\n",
    "\n",
    "    for i in np.arange(K+G):\n",
    "        vec = np.zeros((K+G,))\n",
    "        vec[i] = 1\n",
    "        numgrad[:,i] = (IPDL_loglikelihood(theta + delta*vec, y, x, sample_share, Psi, Nest_count) - IPDL_loglikelihood(theta, y, x, sample_share, Psi, Nest_count)) / delta\n",
    "\n",
    "    angrad = IPDL_score(theta, y, x, sample_share, Psi, Nest_count)\n",
    "\n",
    "    normdiff = la.norm(angrad - numgrad)\n",
    "    \n",
    "    return normdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.02532802581592098]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025773423423]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.0253280257742625]\n",
      "[0.2, 0.025328025752644607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.025328025785555663]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025804910493]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025808185297]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025807773717]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025781644066]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025793748807]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025772319317]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802575448933]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752663914]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802575692615]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025754346756]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025755706987]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025756416794]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752998543]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752940065]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025753080664]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802575848033]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802575708758]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025753779498]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025753202272]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025753024355]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752901828]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025755696468]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025754097806]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025754486894]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025753455247]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802575363106]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025757681796]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025756813435]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802575587119]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025757473855]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025753914615]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025754009522]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752657576]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752909954]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752676214]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752872504]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752709725]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025753157835]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025752663238]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025754017366]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025753850587]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802575468668]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802575280782]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025769193775]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802576750713]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.02532802576628219]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025764678137]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.2, 0.025328025758767303]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.025328025495423803]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.02532802563983325]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.025328025637431768]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.0253280256404797]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.02532802564075714]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.025328025638966866]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.02532802563386272]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.025328025640239893]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.025328025637472572]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.025328025640235903]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.025328025674415233]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.02532802552938457]\n",
      "[0.2, 0.025328025752644607]\n",
      "[0.20000001, 0.02532802559349725]\n",
      "[0.2, 0.025328025752644607]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.265717055373921e-07"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta0 = np.ones((K+G,))/(K+G)\n",
    "test_analyticgrad(y, x, theta0, pop_share, Psi, Nest_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors in Maximum Likelihood estimation\n",
    "\n",
    "As usual we may consistently estimate the Covariance Matrix  of the IPDL maximum likelihood estimator for some estimate $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'\\in \\mathbb{R}^{K+G}$ as:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\left( \\sum_{i=1}^N \\nabla_\\theta \\ell_i (\\hat \\theta) \\nabla_\\theta \\ell_i (\\hat \\theta)' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Thereby we may find the estimated standard error of parameter $d$ as the squareroot of the d'th diagonal entry of $\\hat \\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_d = \\sqrt{\\hat \\Sigma_{dd}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_se(score, N):\n",
    "    '''\n",
    "    This function computes the asymptotic standard errors of the MLE.\n",
    "\n",
    "    Args.\n",
    "        score: a numpy array (T,K+G) of IPDL scores\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns\n",
    "        SE: a numpy array (K+G,) of asymptotic IPDL MLE standard errors\n",
    "    '''\n",
    "\n",
    "    SE = np.sqrt(np.diag(la.inv(np.einsum('td,tm->dm', score, score))) / N)\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        SE: a numpy array (K+G,) of asymptotic IPDL MLE standard errors\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        N: an integer giving the number of observations\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated asymptotic p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE\n",
    "    p = 2*scstat.t.sf(T, df = N-1)\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_IPDL(f, Theta0, y, x, sample_share, psi_stack, nest_count, N, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given starting values and variables necessary in the IPDL model specification.\n",
    "\n",
    "    Args:\n",
    "        f: a function to minimize,\n",
    "        Theta0 : a numpy array (K+G,) of initial guess parameters (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests', \n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t,\n",
    "        N: an integer giving the number of observations,\n",
    "        Analytic_jac: a boolean. Default value is 'True'. If 'True' the analytic jacobian of the IPDL loglikelihood function is used in estimation. Else the numerical jacobian is used.\n",
    "        options: dictionary with options for the optimizer (e.g. disp=True which tells it to display information at termination.)\n",
    "    \n",
    "    Returns:\n",
    "        res: a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(f(Theta, y, x, sample_share, psi_stack, nest_count))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Theta0.tolist(), options=options, jac=Grad, **kwargs) # optimize.minimize takes a list of parameters Theta0 (not a numpy array) as initial guess.\n",
    "    se = IPDL_se(IPDL_score(result.x, y, x, sample_share, psi_stack, nest_count), N)\n",
    "    T,p = IPDL_t_p(se, result.x, N)\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'theta': result.x,\n",
    "        'se': se,\n",
    "        't': T,\n",
    "        'p': p,\n",
    "        'success':  result.success, # bool, whether convergence was succesful 1\n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001529\n",
      "         Iterations: 26\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.ones((K,))\n",
    "\n",
    "# Estimate the model\n",
    "Logit_beta = logit.estimate_logit(logit.q_logit, beta_0, y, x, sample_share=pop_share, Analytic_jac=True)['beta']\n",
    "Logit_SE = logit.logit_se(logit.logit_score(Logit_beta, y, x, pop_share), N)\n",
    "Logit_t, Logit_p = logit.logit_t_p(Logit_beta, logit.logit_score(Logit_beta, y, x, pop_share), N)\n",
    "\n",
    "# Initialize \\theta^0\n",
    "theta0 = np.append(Logit_beta,lambda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.0015948261527286335]\n",
      "[0.46079084676294746, 0.0015937636986622294]\n",
      "[0.4468113766718806, 0.0015896014134653246]\n",
      "[0.39089349630761283, 0.0015743076291167553]\n",
      "[0.2281243669677471, 0.0015417555205683043]\n",
      "[0.060600342124995175, 0.0015297514814028087]\n",
      "[0.09194301569734192, 0.001528945875853086]\n",
      "[0.08754792352009409, 0.0015289219523688684]\n",
      "[0.08723707804525717, 0.001528921024770911]\n",
      "[0.08688372100505572, 0.0015289192371360878]\n",
      "[0.08547029284424992, 0.0015289138110982522]\n",
      "[0.08332761495059388, 0.0015289014132569831]\n",
      "[0.0805135351421179, 0.0015288788144729574]\n",
      "[0.07629566513484756, 0.0015288368116710463]\n",
      "[0.06989057552662799, 0.0015287587250286808]\n",
      "[0.06095085530565152, 0.0015286155025599883]\n",
      "[0.054366692823034704, 0.0015283604366217673]\n",
      "[0.06038624280887057, 0.0015279297008842464]\n",
      "[0.08223355459784573, 0.0015272631183534952]\n",
      "[0.12600614965671908, 0.001526469491098254]\n",
      "[0.18517484909978454, 0.0015256905464002854]\n",
      "[0.22932076232684934, 0.0015252220771737796]\n",
      "[0.23594919844511492, 0.0015251223326176399]\n",
      "[0.22922221252571498, 0.0015251024768491727]\n",
      "[0.22346114567252362, 0.0015250989146173095]\n",
      "[0.22168645122659097, 0.0015250979094109625]\n",
      "[0.21990236205806635, 0.0015250960934294422]\n",
      "[0.2178531369871465, 0.0015250927257274586]\n",
      "[0.21525356687037056, 0.001525086440814981]\n",
      "[0.21175133502749338, 0.0015250747696022072]\n",
      "[0.2068959460342804, 0.0015250533994059115]\n",
      "[0.20017843954397002, 0.0015250151804258923]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001525\n",
      "         Iterations: 28\n",
      "         Function evaluations: 32\n",
      "         Gradient evaluations: 32\n"
     ]
    }
   ],
   "source": [
    "resbla2 = estimate_IPDL(q_IPDL, theta0, y, x, pop_share, Psi, Nest_count, N, Analytic_jac=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.0015948261527286335]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0015948261527286335"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-IPDL_loglikelihood(theta0, y, x, pop_share, Psi, Nest_count).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_table(theta,se,N,x_vars,nest_vars):\n",
    "    IPDL_t, IPDL_p = IPDL_t_p(se, theta, N)\n",
    "\n",
    "    if OO:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "    else:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "\n",
    "    table  = pd.DataFrame({'theta': [ str(np.round(theta[i], decimals = 4)) + '***' if IPDL_p[i] <0.01 else str(np.round(theta[i], decimals = 3)) + '**' if IPDL_p[i] <0.05 else str(np.round(theta[i], decimals = 3)) + '*' if IPDL_p[i] <0.1 else np.round(theta[i], decimals = 3) for i in range(len(theta))], \n",
    "                'se' : np.round(se, decimals = 5),\n",
    "                't (theta == 0)': np.round(IPDL_t, decimals = 3),\n",
    "                'p': np.round(IPDL_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.534</td>\n",
       "      <td>4.42209</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.314</td>\n",
       "      <td>2.85188</td>\n",
       "      <td>0.110</td>\n",
       "      <td>0.912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.453</td>\n",
       "      <td>2.66961</td>\n",
       "      <td>0.170</td>\n",
       "      <td>0.865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.939</td>\n",
       "      <td>2.34671</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.944</td>\n",
       "      <td>3.27157</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.083</td>\n",
       "      <td>4.52762</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.071</td>\n",
       "      <td>3.79520</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.739</td>\n",
       "      <td>1.45557</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.382</td>\n",
       "      <td>3.07874</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>-0.052</td>\n",
       "      <td>1.18038</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.844</td>\n",
       "      <td>1.10182</td>\n",
       "      <td>0.766</td>\n",
       "      <td>0.444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>0.994</td>\n",
       "      <td>2.02041</td>\n",
       "      <td>0.492</td>\n",
       "      <td>0.623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>1.054</td>\n",
       "      <td>1.57219</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>0.797</td>\n",
       "      <td>1.56270</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>0.822</td>\n",
       "      <td>1.74077</td>\n",
       "      <td>0.472</td>\n",
       "      <td>0.637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>0.726</td>\n",
       "      <td>1.57295</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0.645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>0.952</td>\n",
       "      <td>2.43383</td>\n",
       "      <td>0.391</td>\n",
       "      <td>0.696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>0.971</td>\n",
       "      <td>3.70785</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>0.909</td>\n",
       "      <td>1.79459</td>\n",
       "      <td>0.506</td>\n",
       "      <td>0.613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.827</td>\n",
       "      <td>1.59384</td>\n",
       "      <td>0.519</td>\n",
       "      <td>0.604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>1.051</td>\n",
       "      <td>1.51171</td>\n",
       "      <td>0.695</td>\n",
       "      <td>0.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>0.866</td>\n",
       "      <td>2.09271</td>\n",
       "      <td>0.414</td>\n",
       "      <td>0.679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>0.918</td>\n",
       "      <td>3.48849</td>\n",
       "      <td>0.263</td>\n",
       "      <td>0.792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>0.924</td>\n",
       "      <td>1.55877</td>\n",
       "      <td>0.593</td>\n",
       "      <td>0.553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>0.96</td>\n",
       "      <td>3.00502</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>0.633</td>\n",
       "      <td>1.45506</td>\n",
       "      <td>0.435</td>\n",
       "      <td>0.663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>0.835</td>\n",
       "      <td>2.52316</td>\n",
       "      <td>0.331</td>\n",
       "      <td>0.741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.949</td>\n",
       "      <td>2.10137</td>\n",
       "      <td>0.452</td>\n",
       "      <td>0.651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.96196</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>0.907</td>\n",
       "      <td>2.54086</td>\n",
       "      <td>0.357</td>\n",
       "      <td>0.721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>0.933</td>\n",
       "      <td>1.54323</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.861</td>\n",
       "      <td>1.48223</td>\n",
       "      <td>0.581</td>\n",
       "      <td>0.561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>1.035</td>\n",
       "      <td>1.50657</td>\n",
       "      <td>0.687</td>\n",
       "      <td>0.492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>0.523</td>\n",
       "      <td>1.54527</td>\n",
       "      <td>0.338</td>\n",
       "      <td>0.735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>0.872</td>\n",
       "      <td>3.73484</td>\n",
       "      <td>0.234</td>\n",
       "      <td>0.815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>0.807</td>\n",
       "      <td>1.95541</td>\n",
       "      <td>0.413</td>\n",
       "      <td>0.680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>0.999</td>\n",
       "      <td>12.60202</td>\n",
       "      <td>0.079</td>\n",
       "      <td>0.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>0.972</td>\n",
       "      <td>6.00145</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>0.989</td>\n",
       "      <td>2.69247</td>\n",
       "      <td>0.367</td>\n",
       "      <td>0.713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>0.947</td>\n",
       "      <td>1.66503</td>\n",
       "      <td>0.569</td>\n",
       "      <td>0.569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>0.993</td>\n",
       "      <td>2.43869</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>0.967</td>\n",
       "      <td>1.76176</td>\n",
       "      <td>0.549</td>\n",
       "      <td>0.583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>0.998</td>\n",
       "      <td>3.87805</td>\n",
       "      <td>0.257</td>\n",
       "      <td>0.797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>0.871</td>\n",
       "      <td>1.47967</td>\n",
       "      <td>0.589</td>\n",
       "      <td>0.556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>0.875</td>\n",
       "      <td>2.20524</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>0.804</td>\n",
       "      <td>2.06632</td>\n",
       "      <td>0.389</td>\n",
       "      <td>0.697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>0.971</td>\n",
       "      <td>3.13374</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.5849***</td>\n",
       "      <td>0.53093</td>\n",
       "      <td>2.985</td>\n",
       "      <td>0.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.31</td>\n",
       "      <td>0.38989</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.349</td>\n",
       "      <td>0.66626</td>\n",
       "      <td>0.524</td>\n",
       "      <td>0.600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.097</td>\n",
       "      <td>0.98906</td>\n",
       "      <td>0.098</td>\n",
       "      <td>0.922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.453</td>\n",
       "      <td>1.60107</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.53185</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.17061</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.15690</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.18101</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.20766</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>0.035</td>\n",
       "      <td>0.18672</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.12911</td>\n",
       "      <td>0.209</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.059</td>\n",
       "      <td>0.16816</td>\n",
       "      <td>0.350</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.16214</td>\n",
       "      <td>0.189</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.14290</td>\n",
       "      <td>0.697</td>\n",
       "      <td>0.486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.049</td>\n",
       "      <td>0.18287</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.036</td>\n",
       "      <td>0.25112</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>0.092</td>\n",
       "      <td>0.45236</td>\n",
       "      <td>0.203</td>\n",
       "      <td>0.840</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables         theta        se  t (theta == 0)      p\n",
       "in_out           -2.534   4.42209           0.573  0.567\n",
       "cy               -0.314   2.85188           0.110  0.912\n",
       "hp               -0.453   2.66961           0.170  0.865\n",
       "we               -0.939   2.34671           0.400  0.689\n",
       "le               -1.944   3.27157           0.594  0.552\n",
       "wi               -2.083   4.52762           0.460  0.645\n",
       "he               -2.071   3.79520           0.546  0.585\n",
       "li               -0.739   1.45557           0.507  0.612\n",
       "sp               -1.382   3.07874           0.449  0.654\n",
       "ac               -0.052   1.18038           0.044  0.965\n",
       "pr                0.844   1.10182           0.766  0.444\n",
       "brand_2           0.994   2.02041           0.492  0.623\n",
       "brand_3           1.054   1.57219           0.670  0.503\n",
       "brand_4           0.797   1.56270           0.510  0.610\n",
       "brand_5           0.822   1.74077           0.472  0.637\n",
       "brand_6           0.726   1.57295           0.461  0.645\n",
       "brand_7           0.952   2.43383           0.391  0.696\n",
       "brand_8           0.971   3.70785           0.262  0.793\n",
       "brand_9           0.909   1.79459           0.506  0.613\n",
       "brand_10          0.827   1.59384           0.519  0.604\n",
       "brand_11          1.051   1.51171           0.695  0.487\n",
       "brand_12          0.866   2.09271           0.414  0.679\n",
       "brand_13          0.918   3.48849           0.263  0.792\n",
       "brand_14          0.924   1.55877           0.593  0.553\n",
       "brand_15           0.96   3.00502           0.319  0.749\n",
       "brand_16          0.633   1.45506           0.435  0.663\n",
       "brand_17          0.835   2.52316           0.331  0.741\n",
       "brand_18          0.949   2.10137           0.452  0.651\n",
       "brand_19          0.885   1.96196           0.451  0.652\n",
       "brand_20          0.907   2.54086           0.357  0.721\n",
       "brand_21          0.933   1.54323           0.605  0.545\n",
       "brand_22          0.861   1.48223           0.581  0.561\n",
       "brand_23          1.035   1.50657           0.687  0.492\n",
       "brand_24          0.523   1.54527           0.338  0.735\n",
       "brand_25          0.872   3.73484           0.234  0.815\n",
       "brand_26          0.807   1.95541           0.413  0.680\n",
       "brand_27          0.999  12.60202           0.079  0.937\n",
       "brand_28          0.972   6.00145           0.162  0.871\n",
       "brand_29          0.989   2.69247           0.367  0.713\n",
       "brand_30          0.947   1.66503           0.569  0.569\n",
       "brand_31          0.993   2.43869           0.407  0.684\n",
       "brand_32          0.967   1.76176           0.549  0.583\n",
       "brand_33          0.998   3.87805           0.257  0.797\n",
       "brand_34          0.871   1.47967           0.589  0.556\n",
       "brand_35          0.875   2.20524           0.397  0.691\n",
       "brand_36          0.804   2.06632           0.389  0.697\n",
       "brand_37          0.971   3.13374           0.310  0.757\n",
       "home_2        1.5849***   0.53093           2.985  0.003\n",
       "cla_2              0.31   0.38989           0.795  0.427\n",
       "cla_3             0.349   0.66626           0.524  0.600\n",
       "cla_4             0.097   0.98906           0.098  0.922\n",
       "cla_5             0.453   1.60107           0.283  0.777\n",
       "group_in_out      0.025   0.53185           0.047  0.963\n",
       "group_cy         -0.037   0.17061           0.219  0.827\n",
       "group_hp         -0.059   0.15690           0.375  0.708\n",
       "group_we         -0.002   0.18101           0.011  0.991\n",
       "group_le         -0.002   0.20766           0.007  0.994\n",
       "group_wi          0.035   0.18672           0.188  0.851\n",
       "group_he         -0.027   0.12911           0.209  0.834\n",
       "group_li         -0.059   0.16816           0.350  0.726\n",
       "group_sp         -0.031   0.16214           0.189  0.850\n",
       "group_ac           -0.1   0.14290           0.697  0.486\n",
       "group_brand       0.049   0.18287           0.266  0.790\n",
       "group_home       -0.036   0.25112           0.145  0.885\n",
       "group_cla         0.092   0.45236           0.203  0.840"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IPDL_theta = resbla2['theta']\n",
    "IPDL_SE = resbla2['se']\n",
    "IPDL_t, IPDL_p = IPDL_t_p(IPDL_SE, IPDL_theta, N)\n",
    "reg_table(IPDL_theta, IPDL_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20017843954397002"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in IPDL_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative approach\n",
    "\n",
    "The log-likelihood function is not globally concave, and finding the global optimum can be difficult. Using the estimation procedure of Fosgerau et. al. (2023 working paper), we can instead fit the parameters using the first-order conditions for optimality. The estimator takes the form\n",
    "\n",
    "$$\n",
    "\\hat \\theta^0=\\arg \\min_{\\theta} \\sum_t s_t \\hat \\varepsilon^0_t(\\theta)'\\hat W^0_t\\hat \\varepsilon^0 _t(\\theta),\n",
    "$$\n",
    "where $\\hat W^0_t$ is a positive semidefinite weight matrix, $s_t$ is market $t$'s share of the total population and \n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t(u(X_t,\\beta)- \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)) ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat D^0_t=\\textrm{diag}(\\hat q^0_t)-\\hat q^0_t (\\hat q^0_t)'.\n",
    "$$\n",
    "Using equation (...) above, we have that $\\hat \\epsilon_t$ is a linear function of $\\theta$,\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t \\left(\\hat G^0_t\\theta- \\ln \\hat q^0_t\\right)\\equiv \\hat A^0_t\\theta-\\hat r^0_t.\n",
    "$$\n",
    "Using linearity, the weighted least squares criterion has a unique closed form solution,\n",
    "$$\n",
    "\\hat \\theta^0 =\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat A^0_t \\right)^{-1}\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat r_t^0 \\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_array(q, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function calculates the G block matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        G: a dictionary  of T numpy arrays (J[t],K+G): a G matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack, nest_count) # Find the cross derivative of the pertubation function \\Omega wrt. lambda and ccp's q\n",
    "    G = {t: np.concatenate((x[t],Z[t]), axis=1) for t in np.arange(T)} # Join block matrices along 2nd dimensions  s.t. last dimension is K+G (same dimension as theta)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_array(q):\n",
    "    '''\n",
    "    This function calculates the D matrix - the logit derivative of ccp's wrt. utilities\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "\n",
    "    Returns\n",
    "        D: a dictionary of T numpy arrays (J[t],J[t]) of logit derivatives of ccp's wrt. utilities for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = {t: np.diag(q[t]) - np.einsum('j,k->jk', q[t], q[t]) for t in np.arange(T)}\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_array(q, x, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the A matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        A: a dictionary  of T numpy arrays (J[t],K+G): an A matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    D = D_array(q)\n",
    "    G = G_array(q, x, psi_stack, nest_count)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_array(q):\n",
    "    '''\n",
    "    This function calculates 'r'; the logarithm of observed or nonparametrically estimated market shares\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "    \n",
    "    Returns\n",
    "        r: a dictionary of T numpy arrays (J[t],) of the log of ccp's for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = D_array(q) \n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) for t in np.arange(T)}\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS_init(q, x, sample_share, psi_stack, nest_count, N):\n",
    "    ''' \n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the initial FKN parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "\n",
    "    #W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "    A = A_array(q, x, psi_stack, nest_count)\n",
    "    r = r_array(q)\n",
    "\n",
    "    d = A[0].shape[1]\n",
    "    \n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1/q[t], A[t], optimize = True) # Fast product using that the weights are diagonal.\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1/q[t], r[t], optimize = True)\n",
    "    \n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    #se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "    \n",
    "    return theta_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the observed market shares we may thus find initial parameter estimates $\\hat \\theta^0$ as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFKN0 = WLS_init(y, x, pop_share, Psi, Nest_count, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.038008441930833"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in thetaFKN0[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for parameter bounds\n",
    "\n",
    "As we see above, the least squares estimator is not guaranteed to respect the parameter bounds $\\sum_g \\hat \\lambda_g<1$. We can use that if we replace $\\hat q^0_t$ with the choice probabilities from the maximum likelihood estimator of the logit model, $\\hat q^{logit}_t\\propto \\exp\\{X_t\\hat \\beta^{logit}\\}$, and plug these choice probabilities into the WLS estimator described above, it will return $\\hat \\theta=(\\hat \\beta^{logit},0,\\ldots,0)$ as the parameter estimate. Let $\\hat q_t(\\alpha)$ denote the weighted average of the logit probabilites and the market shares,\n",
    "$$\n",
    "\\hat q_t(\\alpha) =(1-\\alpha) \\hat q^{logit}_t+\\alpha \\hat q^0_t.\n",
    "$$\n",
    " Let $\\hat \\theta^0(\\alpha)$ denote the resulting parameter vector. We perform a line search for values of $\\alpha$, $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\ldots)$ until $\\hat \\theta^0(\\alpha)$ yields a feasible parameter vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogL(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' A function giving the mean IPDL loglikehood evaluated at data and an array of parameters 'Theta'\n",
    "    '''\n",
    "    return np.mean(IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, nest_count, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta0.shape[0]\n",
    "    K = x[0].shape[1]\n",
    "    G = d-K\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit.logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    #alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    alpha0=0.5\n",
    "    #LogL_alpha = np.empty((num_alpha,))\n",
    "    #theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in range(1,100):\n",
    "\n",
    "        alpha = alpha0**k\n",
    "\n",
    "      \n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha = WLS_init(q_alpha, x, sample_share, psi_stack, nest_count, N)[0]\n",
    "\n",
    "        lambda_alpha = theta_alpha[K:]\n",
    "        \n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() <1:\n",
    "            break\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, nest_count, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = len(Theta0)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit.logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    LogL_alpha = np.empty((num_alpha,))\n",
    "    theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in np.arange(len(alpha_line)):\n",
    "\n",
    "        alpha = alpha_line[k]\n",
    "\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha[k,:] = WLS_init(q_alpha, x, sample_share, psi_stack, nest_count, N)\n",
    "\n",
    "        lambda_alpha = theta_alpha[k,K:]\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() >= 1:\n",
    "            LogL_alpha[k] = np.NINF\n",
    "        else:\n",
    "            LogL_alpha[k] = LogL(theta_alpha[k,:], y, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "    alpha_star = np.argmax(LogL_alpha)\n",
    "    theta_hat_star = theta_alpha[alpha_star,:]\n",
    "\n",
    "    return theta_hat_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the grid search method we find corressponding parameters $\\hat \\theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.263098792430562e-10, 0.08299327576305793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9860191438985264, 0.010038474247656253]\n",
      "[0.9845099998650955, 0.00560785010006522]\n",
      "[0.9831527479509522, 0.0030761593628499083]\n"
     ]
    }
   ],
   "source": [
    "theta_alpha = GridSearch(thetaFKN0, beta_0, y, x, pop_share, Psi, Nest_count, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9831527479509522"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in theta_alpha[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9831527479509522, 0.0030761593628499083]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0030761593628499083"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_IPDL(theta_alpha, y, x, pop_share, Psi, Nest_count).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated FKN estimator\n",
    "\n",
    "The iterated estimator is as the initial one, except there is an additional term on $\\hat \\varepsilon$. First, we update the choice probabilities,\n",
    "$$\n",
    "\\hat q^k_i=p(\\mathbf X_i,\\hat \\theta^{k-1})\\\\\n",
    "$$\n",
    "Then we assign\n",
    "$$\n",
    "\\hat D^k_i=\\nabla^2_{qq}\\Omega(\\hat q_i^k|\\hat \\lambda^{k-1})^{-1}-(\\hat q^k_i \\hat q^k_i)'\n",
    "$$\n",
    "and then construct the residual\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)=\\hat D^k_i\\left( u(x_i,\\beta)-\\nabla_q \\Omega(\\hat q_i^k|\\lambda)\\right) -y_i+\\hat q_i^k,\n",
    "$$\n",
    "Which can once again be simplified as\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)= \\hat A_i^k \\theta-\\hat r^k_i,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat A^k_i=\\hat D_i^k\\hat G^k_i, \\hat r_i^k =\\hat D^k_i\\ln \\hat q_i^k-y_i\n",
    "$$\n",
    "and where $\\hat G^k_i$ is constructed as in the initial estimator. Using the weighted least squares estimator with weights $\\hat W_i^k=\\textrm{diag}(\\hat q^k_i)^{-1}$, we get the estimator\n",
    "$$\n",
    "\\hat \\theta^k = \\arg \\min_{\\theta}\\frac{1}{n}\\sum_i \\hat \\varepsilon^k_i(\\theta)'\\hat W_i^k \\hat \\varepsilon^k_i(\\theta).\n",
    "$$\n",
    "We can once again solve it in closed form as\n",
    "$$\n",
    "\\hat \\theta^k =\\left( \\frac{1}{n}\\sum_i \\hat (A^k_i)'\\hat W_i^k \\hat A^k_i)\\right)^{-1}\\left( \\frac{1}{n}\\sum_i (\\hat A_i^k)'\\hat W_i^k \\hat r_i^k\\right)\n",
    "$$\n",
    "Now we implement this procedure and iterate starting from our initial guess $\\hat \\theta^{*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS(Theta, y, x, sample_share, psi_stack, nest_count, N):\n",
    "    '''\n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the iterated parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta.shape[0]\n",
    "    \n",
    "    # Get ccp's\n",
    "    q = IPDL_ccp(Theta, x, psi_stack, nest_count)\n",
    "\n",
    "    # Construct A\n",
    "    D = ccp_gradient(q, x, Theta, psi_stack, nest_count) # A is here constructed using the IPDL derivative of ccp's wrt. utilities instead of teh Logit derivative\n",
    "    G = G_array(q, x, psi_stack, nest_count)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "    W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "\n",
    "    # Construct r\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where=(q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) + y[t] for t in np.arange(T)}\n",
    "\n",
    "    # Estimate parameters\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,jk,kp->dp', A[t], W[t], A[t], optimize = True)\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,jk,k->d', A[t], W[t], r[t], optimize = True)\n",
    "\n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "\n",
    "    return theta_hat,se_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKN_estimator(logit_beta, q_obs, x, sample_share, psi_stack, nest_count, N, tol = 1.0e-15, max_iters = 1000):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    theta_init = WLS_init(q_obs, x, sample_share, psi_stack, nest_count, N) #WLS_init(q_obs, x, sample_share, psi_stack, nest_count,  N)\n",
    "    \n",
    "    if np.array([p for p in theta_init[K:] if p>0]).sum() >= 1:\n",
    "        theta_hat_star = GridSearch(theta_init, logit_beta, q_obs, x, sample_share, psi_stack, nest_count, N)\n",
    "        theta0 = theta_hat_star\n",
    "    else:\n",
    "        theta0 = theta_init\n",
    "\n",
    "    #logl0 = LogL(theta0, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    for k in np.arange(max_iters):\n",
    "        theta1, se1 = WLS(theta0, q_obs, x, sample_share, psi_stack, nest_count, N)\n",
    "\n",
    "        '''logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "        \n",
    "        for m in range(10):\n",
    "            if logl1<logl0:\n",
    "                theta1=0.5*theta0+0.5*theta1\n",
    "                logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "            else:\n",
    "                break'''\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.abs(theta1 - theta0))\n",
    "\n",
    "        if dist<tol:\n",
    "            succes = True\n",
    "            iter = k\n",
    "            break\n",
    "        elif k==max_iters:\n",
    "            succes = False\n",
    "            iter = max_iters\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        theta0 = theta1\n",
    "\n",
    "    res = {'theta': theta1,\n",
    "           'se': se1,\n",
    "           'fun': -LogL(theta1, y, x, sample_share, psi_stack, nest_count),\n",
    "           'iter': iter,\n",
    "           'succes': succes}\n",
    "    \n",
    "    return res \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6.263098792430562e-10, 0.08299327576305793]\n",
      "[0.9860191438985264, 0.010038474247656253]\n",
      "[0.9845099998650955, 0.00560785010006522]\n",
      "[0.9831527479509522, 0.0030761593628499083]\n",
      "[0.926333697938532, 0.0014941817309736777]\n"
     ]
    }
   ],
   "source": [
    "res = FKN_estimator(beta_0, y, x, pop_share, Psi, Nest_count, N, tol=1.0e-8, max_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-10.0484***</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>3266.601</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.9218***</td>\n",
       "      <td>0.00150</td>\n",
       "      <td>612.936</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-2.8092***</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>1455.524</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.0536***</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>39.013</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.5624***</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>1089.602</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.5314***</td>\n",
       "      <td>0.00299</td>\n",
       "      <td>1850.329</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-0.0604***</td>\n",
       "      <td>0.00179</td>\n",
       "      <td>33.676</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.8833***</td>\n",
       "      <td>0.00103</td>\n",
       "      <td>859.291</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>2.8631***</td>\n",
       "      <td>0.00202</td>\n",
       "      <td>1414.538</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.3022***</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>475.615</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.3285***</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>324.332</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-1.6152***</td>\n",
       "      <td>0.00357</td>\n",
       "      <td>451.927</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>0.1247***</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>268.432</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.6278***</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>997.982</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.3345***</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>752.529</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.2971***</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>650.401</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.7184***</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>524.172</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.5536***</td>\n",
       "      <td>0.00156</td>\n",
       "      <td>355.475</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7631***</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>800.315</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.2176***</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>410.922</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.0279***</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>62.687</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.4581***</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>606.226</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-0.9733***</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>738.765</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-1.6554***</td>\n",
       "      <td>0.00133</td>\n",
       "      <td>1241.480</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.7403***</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>675.572</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-0.7117***</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>1336.209</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.624***</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>868.573</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.2706***</td>\n",
       "      <td>0.00042</td>\n",
       "      <td>638.220</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-0.9988***</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>913.331</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.1378***</td>\n",
       "      <td>0.00061</td>\n",
       "      <td>226.123</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.0482***</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>111.969</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.0334***</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>73.917</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>0.102***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>215.753</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.3409***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>732.432</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.7656***</td>\n",
       "      <td>0.00120</td>\n",
       "      <td>639.770</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.6629***</td>\n",
       "      <td>0.00071</td>\n",
       "      <td>929.061</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.9641***</td>\n",
       "      <td>0.03498</td>\n",
       "      <td>84.728</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.7208***</td>\n",
       "      <td>0.00121</td>\n",
       "      <td>593.423</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.8435***</td>\n",
       "      <td>0.01335</td>\n",
       "      <td>212.978</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.4481***</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>585.819</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-0.9625***</td>\n",
       "      <td>0.00221</td>\n",
       "      <td>436.070</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.38***</td>\n",
       "      <td>0.00077</td>\n",
       "      <td>492.138</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-2.6995***</td>\n",
       "      <td>0.01131</td>\n",
       "      <td>238.604</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.6818***</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>1094.758</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.3467***</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>517.141</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1918***</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>308.472</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.6284***</td>\n",
       "      <td>0.00290</td>\n",
       "      <td>561.773</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.0783***</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>2482.335</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>-0.0053***</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>41.631</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.0693***</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>346.504</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>-0.0125***</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>39.977</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.0545***</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>122.802</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.6457***</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>2726.037</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.0269***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>295.169</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>-0.0042***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>48.400</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0191***</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>200.720</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0205***</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>245.343</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>0.0006***</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>7.850</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.0206***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>234.072</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0199***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>230.656</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0242***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>276.551</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.28***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>1230.063</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.2158***</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>986.270</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.0773***</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>633.541</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)      p\n",
       "in_out        -10.0484***  0.00308        3266.601  0.000\n",
       "cy             -0.9218***  0.00150         612.936  0.000\n",
       "hp             -2.8092***  0.00193        1455.524  0.000\n",
       "we             -0.0536***  0.00137          39.013  0.000\n",
       "le             -1.5624***  0.00143        1089.602  0.000\n",
       "wi              5.5314***  0.00299        1850.329  0.000\n",
       "he             -0.0604***  0.00179          33.676  0.000\n",
       "li             -0.8833***  0.00103         859.291  0.000\n",
       "sp              2.8631***  0.00202        1414.538  0.000\n",
       "ac              0.3022***  0.00064         475.615  0.000\n",
       "pr             -0.3285***  0.00101         324.332  0.000\n",
       "brand_2        -1.6152***  0.00357         451.927  0.000\n",
       "brand_3         0.1247***  0.00046         268.432  0.000\n",
       "brand_4        -0.6278***  0.00063         997.982  0.000\n",
       "brand_5        -0.3345***  0.00044         752.529  0.000\n",
       "brand_6        -0.2971***  0.00046         650.401  0.000\n",
       "brand_7        -0.7184***  0.00137         524.172  0.000\n",
       "brand_8        -0.5536***  0.00156         355.475  0.000\n",
       "brand_9        -1.7631***  0.00220         800.315  0.000\n",
       "brand_10        0.2176***  0.00053         410.922  0.000\n",
       "brand_11       -0.0279***  0.00044          62.687  0.000\n",
       "brand_12       -0.4581***  0.00076         606.226  0.000\n",
       "brand_13       -0.9733***  0.00132         738.765  0.000\n",
       "brand_14       -1.6554***  0.00133        1241.480  0.000\n",
       "brand_15       -1.7403***  0.00258         675.572  0.000\n",
       "brand_16       -0.7117***  0.00053        1336.209  0.000\n",
       "brand_17        -0.624***  0.00072         868.573  0.000\n",
       "brand_18        0.2706***  0.00042         638.220  0.000\n",
       "brand_19       -0.9988***  0.00109         913.331  0.000\n",
       "brand_20       -0.1378***  0.00061         226.123  0.000\n",
       "brand_21       -0.0482***  0.00043         111.969  0.000\n",
       "brand_22        0.0334***  0.00045          73.917  0.000\n",
       "brand_23         0.102***  0.00047         215.753  0.000\n",
       "brand_24       -0.3409***  0.00047         732.432  0.000\n",
       "brand_25       -0.7656***  0.00120         639.770  0.000\n",
       "brand_26       -0.6629***  0.00071         929.061  0.000\n",
       "brand_27       -2.9641***  0.03498          84.728  0.000\n",
       "brand_28       -0.7208***  0.00121         593.423  0.000\n",
       "brand_29       -2.8435***  0.01335         212.978  0.000\n",
       "brand_30       -1.4481***  0.00247         585.819  0.000\n",
       "brand_31       -0.9625***  0.00221         436.070  0.000\n",
       "brand_32         -0.38***  0.00077         492.138  0.000\n",
       "brand_33       -2.6995***  0.01131         238.604  0.000\n",
       "brand_34       -0.6818***  0.00062        1094.758  0.000\n",
       "brand_35       -0.3467***  0.00067         517.141  0.000\n",
       "brand_36       -0.1918***  0.00062         308.472  0.000\n",
       "brand_37       -1.6284***  0.00290         561.773  0.000\n",
       "home_2          1.0783***  0.00043        2482.335  0.000\n",
       "cla_2          -0.0053***  0.00013          41.631  0.000\n",
       "cla_3           0.0693***  0.00020         346.504  0.000\n",
       "cla_4          -0.0125***  0.00031          39.977  0.000\n",
       "cla_5           0.0545***  0.00044         122.802  0.000\n",
       "group_in_out    0.6457***  0.00024        2726.037  0.000\n",
       "group_cy       -0.0269***  0.00009         295.169  0.000\n",
       "group_hp       -0.0042***  0.00009          48.400  0.000\n",
       "group_we              0.0  0.00010           0.735  0.463\n",
       "group_le       -0.0191***  0.00010         200.720  0.000\n",
       "group_wi       -0.0205***  0.00008         245.343  0.000\n",
       "group_he        0.0006***  0.00007           7.850  0.000\n",
       "group_li       -0.0206***  0.00009         234.072  0.000\n",
       "group_sp       -0.0199***  0.00009         230.656  0.000\n",
       "group_ac       -0.0242***  0.00009         276.551  0.000\n",
       "group_brand       0.28***  0.00023        1230.063  0.000\n",
       "group_home     -0.2158***  0.00022         986.270  0.000\n",
       "group_cla      -0.0773***  0.00012         633.541  0.000"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKN_theta = res['theta']\n",
    "FKN_SE = res['se']\n",
    "FKN_t, FKN_p = IPDL_t_p(FKN_SE, FKN_theta, N)\n",
    "reg_table(FKN_theta, FKN_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP Estimation and instruments\n",
    "\n",
    "The setting is now a bit different. Instead of the noise coming from random sampling of individuals, we now have an additional source of uncertainty, stemming frm the random sampling of the fixed effects $\\xi_{tj}$ for each market and each product. The number of ”observations” is therefore\n",
    "\n",
    "$$\n",
    "S = T \\cdot \\sum_t J_t\n",
    "$$\n",
    "\n",
    "Note that while random sampling of individuals choices (number of observations\n",
    "in the hundreds of millions) still has an effect on the estimated parameters in\n",
    "principle, this effect is completely drowned out by the sampling variance of the\n",
    "fixed effects (number of observations T ≈ 15000?), so we choose to ignore it\n",
    "here. When estimating random coefficients models, there is also a third source\n",
    "of uncertainty stemming from approximation of numerical integrals. This is not\n",
    "an issue in IPDL, as we have the inverse demand in closed form.\n",
    "\n",
    "The principles are pretty similar to what we have been doing already. When\n",
    "applicable, I will use the same notation as in the FKN section. Define the\n",
    "residual,\n",
    "\n",
    "$$\\xi_m(\\theta) = u(X_m, \\beta) − \\nabla_q \\Omega(q^0|\\lambda)$$\n",
    "\n",
    "In the IPDL model, this residual is a linear function of $\\theta$ which has the form\n",
    "\n",
    "$$\\xi_m(\\theta) =  G^0_m \\theta − r_m^0$$\n",
    "\n",
    "where $ G^0_m=[X_m, Z_m^0]$, where $Z_m^0 = \\nabla_{q,\\lambda}\\Omega(q_m^0|\\lambda)$ and $r^0_m = \\ln q^0_m$ as in the FKN section with $q^0_m$ being e.g. the observed market shares in market $m$. For the BLP estimator, we set this residual orthogonal to a matrix of instruments $\\hat Z_m$ of size $J_m \\times d$, and find the estimator $ \\hat \\theta^{IV}$ which solves the moment conditions\n",
    "\n",
    "$$\\frac{1}{T} \\sum_m \\hat Z_m' \\xi(\\hat \\theta^{IV}) = 0$$\n",
    "\n",
    "Since $\\hat \\xi$ is linear, the moment equations have a unique solution,\n",
    "\n",
    "$$\\hat \\theta^{IV} = \\left(\\frac{1}{T}\\sum_m \\hat Z_m' G^0_m \\right)^{-1}\\left(\\frac{1}{T}\\sum_m \\hat Z_m' r^0_m \\right)$$\n",
    "\n",
    "We require an instrument for the price of the goods. This is something which is correlated with the price, but uncorrelated with the error term $\\xi_m$ (in the BLP model, $\\xi_{mj}$ represents unobserved components of car quality). A standard instrument in this case would be a measure of marginal cost (or something which is correlated with marginal cost, like a production price index). For everything other than price, we can simply use the regressor itself as the instrument i.e. $ \\hat Z^{mjd} = G^0_{mjd}$, for all other dimensions than price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct our instruments $\\hat Z$. We'll use the average exchange rate of the destination country relative to average exchange rate of the origin country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "xexr = {t: dat[dat['market'] == t][z_vars[0]].values for t in np.arange(T)}\n",
    "G0 = G_array(y, x, Psi, Nest_count)\n",
    "pr_index = len(x_contvars)\n",
    "for t in np.arange(T):\n",
    "    G0[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z = G0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the moment estimator $\\hat \\theta^{IV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_estimator(y, z, x, sample_share, psi_stack, nest_count):\n",
    "    '''\n",
    "    Args.\n",
    "        y: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        z: a dictionary of T numpy arrays (J[t],K+G) of instruments for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a numpy array (K+G,) of BLP parameter estimates\n",
    "    '''\n",
    "    T = len(z)\n",
    "\n",
    "    G = G_array(y, x, psi_stack, nest_count)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t], out = np.NINF*np.ones_like((y[t])), where = (y[t] > 0)) for t in np.arange(T)}\n",
    "    \n",
    "    sZG = np.empty((T,d,d))\n",
    "    sZr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sZG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', z[t], G[t])\n",
    "        sZr[t,:] = sample_share[t]*np.einsum('jd,j->d', z[t], r[t])\n",
    "\n",
    "    theta_hat = la.solve(sZG.sum(axis=0), sZr.sum(axis=0))\n",
    "    \n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLP_theta = BLP_estimator(y, z, x, np.ones((T,)), Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0955190867828002"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in BLP_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Logit model we get the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_logit = x\n",
    "for t in np.arange(T):\n",
    "    G_logit[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z_logit = G_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.92920752,  -2.3589754 ,  -6.76421995,   0.02963003,\n",
       "        -2.05176127,  10.84731336,  -1.04140126,  -0.58331478,\n",
       "         5.15289118,   0.51808091,  -0.17336342,  -2.037768  ,\n",
       "        -0.81720168,  -1.44357757,  -1.04059281,  -1.16245013,\n",
       "        -1.74530433,  -0.85123531,  -2.72300281,  -1.08758839,\n",
       "        -0.68958989,  -0.95909482,  -2.11727698,  -2.93039275,\n",
       "        -2.90655875,  -2.05527142,  -1.82107985,   0.51974428,\n",
       "        -2.02980519,  -0.79701277,  -0.86356478,  -0.86816254,\n",
       "        -0.81661044,  -1.48858878,  -0.9378501 ,  -1.87621854,\n",
       "        -3.7657435 ,  -1.52567201,  -3.14936663,  -2.07998398,\n",
       "        -1.85954898,  -0.7631942 ,  -1.94891051,  -1.60837966,\n",
       "        -1.15784827,  -0.48973547,  -2.57588437,   1.56903974,\n",
       "         0.0275757 ,   0.04982496,  -0.30342661,  -0.3829885 ])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta = logit.LogitBLP_estimator(y, z_logit, x, np.ones((T,)))\n",
    "LogitBLP_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLP approximation to optimal instruments\n",
    "\n",
    "BLP propose an algorithm for constructing an approximation to the optimal instruments. It is described in simple terms in Reynaert & Verboven (2014), and it has the following steps.\n",
    "It requires an initial parameter estimator $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'$, here we can just usethe MLE we have already computed. Let $W_m$ denote the matrix of instruments (this is the matrix $X_m$ with the price replaced by the exchange rate). The steps are then as follows:\n",
    "\n",
    "First we form the regression equation of the covariates on the instruments:\n",
    "$$\n",
    "X_m = W_m \\Pi + E_m\n",
    "$$\n",
    "\n",
    "The OLS estimate is then given as:\n",
    "$$\n",
    "\\hat \\Pi = \\left( \\frac{1}{T}\\sum_m W_m' W_m \\right)^{-1}\\left( \\frac{1}{T}\\sum_m W_m' X_m\\right)\n",
    "$$\n",
    "\n",
    "Thus the predicted covariates given the instruments $W$ are:\n",
    "$$\n",
    "\\hat X_m = W_m \\hat \\Pi\n",
    "$$\n",
    "\n",
    "Having constructed $\\hat X_m$ (which consists of the exogenous regressors, and the predicted price given $W_m$), we compute the predicted mean utility:\n",
    "\n",
    "$$\n",
    "\\hat u_m = \\hat X_m \\hat \\beta\n",
    "$$\n",
    "\n",
    "and then the predicted market shares at the mean utility:\n",
    "\n",
    "$$\n",
    "\\hat q_m^{*} = P(\\hat u_m | \\hat \\lambda)\n",
    "$$\n",
    "\n",
    "Computationally, here we just use $\\hat X_m$ in place of $X_m$ in the CCP function.\n",
    "Given the predicted market shares, we compute\n",
    "\n",
    "$$\n",
    "\\hat G_m^{*} = \\left[\\hat X_m, \\nabla_{q,\\lambda} \\Omega (\\hat q_m^{*} | \\hat \\lambda)\\right]\n",
    "$$\n",
    "\n",
    "which is the same as the function $\\hat G_m^0$ we already have constructed, except we evaluate it at the\n",
    "predictions $\\hat X_m$ and $\\hat q_m^{*}$ instead of at $X_m$ and $\\hat q_m^0$.\n",
    "\n",
    "The procedure above gives an approximation to the optimal instruments. We also require a weight matrix. The optimal weight matrix is the (generalized) inverse of the conditional (on the instruments) covariance of the fixed effects. Assuming $\\xi_{jm}$ is independetly and identically distributed over j and m, the conditional covariance simplifies to a scalar $\\sigma^2$ times an identity matrix (of size $J_m$).\n",
    "This means that all fixed effects are weighted equally, and the weights therefore drop out of the IV regression. The optimal IV estimator is therefore\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} = \\left(\\frac{1}{T}\\sum_m (\\hat G_m^*)'\\hat G_m^0\\right)^{-1}\\left( \\frac{1}{T}\\sum_m (\\hat G_m^*)'\\hat r_m^0 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat \\xi^*$ denote the estimated residual evaluated at the new parameter estimates,\n",
    "\n",
    "$$\n",
    "\\hat \\xi_{mj}^* = \\hat \\xi_{mj}(\\hat \\theta^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "We may estimate the constant $\\sigma^2$ by\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{T}\\sum_{m}\\sum_{j = 1}^{J_m} \\left(\\hat \\xi_{mj}^*\\right)^2 \n",
    "$$\n",
    "\n",
    "The distribution of the estimator $\\hat \\theta^{\\text{IV}}$ is then\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} \\sim \\mathcal{N}(\\theta_0, \\Sigma^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "which can be consistently estimated by\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma^{\\text{IV}} = \\hat \\sigma^2 \\left( \\sum_m (\\hat G_m^*)'\\hat G_m^0 \\right)^{-1}\n",
    "$$\n",
    "\n",
    "and the standard errors are then the square root of the diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_x(x, w, sample_share):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(w)\n",
    "    K = w[0].shape[1]\n",
    "\n",
    "    sWW = np.empty((T,K,K))\n",
    "    sWX = np.empty((T,K,K))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sWW[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], w[t])\n",
    "        sWX[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], x[t])\n",
    "\n",
    "    Pi_hat = la.solve(sWW.sum(axis=0), sWX.sum(axis=0))\n",
    "    X_hat = {t: np.einsum('jl,lk->jk', w[t], Pi_hat) for t in np.arange(T)}\n",
    "\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_se(Theta, y, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    S = T * np.array([x[t].shape[0] for t in np.arange(T)]).sum()\n",
    "\n",
    "    G = G_array(y, x, psi_stack, nest_count)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t]) for t in np.arange(T)}\n",
    "    \n",
    "    # We calculate \\sigma^2\n",
    "    xi = {t: np.einsum('jd,d->j', G[t], Theta) - r[t] for t in np.arange(T)}\n",
    "    sum_xij2 = np.empty((T,))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sum_xij2[t] = (xi[t]**2).sum()\n",
    "    \n",
    "    sigma2 = np.sum(sum_xij2) / S\n",
    "\n",
    "    # We calculate GG for each market t\n",
    "    GG = np.empty((T,d,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        GG[t,:,:] = np.einsum('jd,jp->dp', G[t], G[t])\n",
    "\n",
    "    # Finally we compute \\Sigma and the standard errors\n",
    "    Sigma = sigma2*la.inv(GG.sum(axis=0))\n",
    "    SE = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalBLP_estimator(Theta0, q_obs, w, x, sample_share, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    beta0 = Theta0[:K]\n",
    "    lambda0 = Theta0[K:]\n",
    "    \n",
    "    X_hat = predict_x(x, w, sample_share)\n",
    "    q0 = IPDL_ccp(Theta0, X_hat, psi_stack, nest_count)\n",
    "    G_star = G_array(q0, X_hat, psi_stack, nest_count)\n",
    "    G0 = G_array(q_obs, x, psi_stack, nest_count)\n",
    "    \n",
    "    r = {t: np.log(q_obs[t]) for t in np.arange(T)}\n",
    "\n",
    "    d = G0[0].shape[1]\n",
    "\n",
    "    sGG = np.empty((T,d,d))\n",
    "    sGr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sGG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', G_star[t], G0[t])\n",
    "        sGr[t,:] = sample_share[t]*np.einsum('jd,j->d', G_star[t], r[t])\n",
    "\n",
    "    Theta_IV = la.solve(sGG.sum(axis=0), sGr.sum(axis=0))\n",
    "    SE_IV = BLP_se(Theta_IV, q_obs, x, psi_stack, nest_count)\n",
    "\n",
    "    return Theta_IV, SE_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThetaOptBLP, SEOptBLP = OptimalBLP_estimator(FKN_theta, y, z_logit, x, np.ones((T,)), Psi, Nest_count)\n",
    "OptBLP_t, OptBLP_p = IPDL_t_p(SEOptBLP, ThetaOptBLP, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0791951142394631"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in ThetaOptBLP[K:]  if p > 0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-10.1369***</td>\n",
       "      <td>0.02973</td>\n",
       "      <td>341.008</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-1.2104***</td>\n",
       "      <td>0.01525</td>\n",
       "      <td>79.373</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-3.5582***</td>\n",
       "      <td>0.01782</td>\n",
       "      <td>199.716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.0919***</td>\n",
       "      <td>0.01693</td>\n",
       "      <td>5.425</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.1952***</td>\n",
       "      <td>0.01992</td>\n",
       "      <td>59.994</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.1567***</td>\n",
       "      <td>0.03083</td>\n",
       "      <td>167.261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.1346***</td>\n",
       "      <td>0.02412</td>\n",
       "      <td>5.579</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.6423***</td>\n",
       "      <td>0.01044</td>\n",
       "      <td>61.545</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>3.8483***</td>\n",
       "      <td>0.02156</td>\n",
       "      <td>178.469</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.4453***</td>\n",
       "      <td>0.00918</td>\n",
       "      <td>48.501</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.1582***</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>73.947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-1.2928***</td>\n",
       "      <td>0.02812</td>\n",
       "      <td>45.978</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>-0.1643***</td>\n",
       "      <td>0.00403</td>\n",
       "      <td>40.773</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.8878***</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>205.382</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.5127***</td>\n",
       "      <td>0.00390</td>\n",
       "      <td>131.583</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.4219***</td>\n",
       "      <td>0.00386</td>\n",
       "      <td>109.207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.9998***</td>\n",
       "      <td>0.00687</td>\n",
       "      <td>145.487</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.391***</td>\n",
       "      <td>0.00977</td>\n",
       "      <td>40.036</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7052***</td>\n",
       "      <td>0.00632</td>\n",
       "      <td>269.936</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>-0.2729***</td>\n",
       "      <td>0.00385</td>\n",
       "      <td>70.942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.1073***</td>\n",
       "      <td>0.00394</td>\n",
       "      <td>27.262</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.5033***</td>\n",
       "      <td>0.00468</td>\n",
       "      <td>107.564</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-1.2609***</td>\n",
       "      <td>0.00592</td>\n",
       "      <td>212.936</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-1.8665***</td>\n",
       "      <td>0.00820</td>\n",
       "      <td>227.575</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.8081***</td>\n",
       "      <td>0.00792</td>\n",
       "      <td>228.433</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-1.1454***</td>\n",
       "      <td>0.00386</td>\n",
       "      <td>296.484</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.9533***</td>\n",
       "      <td>0.00456</td>\n",
       "      <td>209.162</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.3122***</td>\n",
       "      <td>0.00418</td>\n",
       "      <td>74.664</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-1.2534***</td>\n",
       "      <td>0.00527</td>\n",
       "      <td>237.833</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.3031***</td>\n",
       "      <td>0.00477</td>\n",
       "      <td>63.474</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.2697***</td>\n",
       "      <td>0.00371</td>\n",
       "      <td>72.715</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>-0.2426***</td>\n",
       "      <td>0.00378</td>\n",
       "      <td>64.155</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>-0.1789***</td>\n",
       "      <td>0.00409</td>\n",
       "      <td>43.732</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.6858***</td>\n",
       "      <td>0.00384</td>\n",
       "      <td>178.376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.7068***</td>\n",
       "      <td>0.00457</td>\n",
       "      <td>154.682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.9844***</td>\n",
       "      <td>0.00459</td>\n",
       "      <td>214.397</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.2279***</td>\n",
       "      <td>0.04775</td>\n",
       "      <td>46.655</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.9277***</td>\n",
       "      <td>0.00784</td>\n",
       "      <td>118.328</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.3593***</td>\n",
       "      <td>0.01513</td>\n",
       "      <td>155.961</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.236***</td>\n",
       "      <td>0.00790</td>\n",
       "      <td>156.427</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-1.4692***</td>\n",
       "      <td>0.01640</td>\n",
       "      <td>89.588</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.3679***</td>\n",
       "      <td>0.00731</td>\n",
       "      <td>50.306</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-1.5784***</td>\n",
       "      <td>0.02779</td>\n",
       "      <td>56.805</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.7613***</td>\n",
       "      <td>0.00495</td>\n",
       "      <td>153.862</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.5443***</td>\n",
       "      <td>0.00463</td>\n",
       "      <td>117.483</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.148***</td>\n",
       "      <td>0.00393</td>\n",
       "      <td>37.669</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.6339***</td>\n",
       "      <td>0.00990</td>\n",
       "      <td>164.996</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>0.9322***</td>\n",
       "      <td>0.00176</td>\n",
       "      <td>530.101</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.04***</td>\n",
       "      <td>0.00214</td>\n",
       "      <td>18.670</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.119***</td>\n",
       "      <td>0.00288</td>\n",
       "      <td>41.311</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.0704***</td>\n",
       "      <td>0.00405</td>\n",
       "      <td>17.375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.0628***</td>\n",
       "      <td>0.00522</td>\n",
       "      <td>12.045</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.9163***</td>\n",
       "      <td>0.00160</td>\n",
       "      <td>571.541</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.0329***</td>\n",
       "      <td>0.00087</td>\n",
       "      <td>37.712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>-0.0188***</td>\n",
       "      <td>0.00083</td>\n",
       "      <td>22.554</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>-0.0244***</td>\n",
       "      <td>0.00090</td>\n",
       "      <td>26.956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0599***</td>\n",
       "      <td>0.00088</td>\n",
       "      <td>68.204</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0194***</td>\n",
       "      <td>0.00081</td>\n",
       "      <td>23.996</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0235***</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>35.377</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.0262***</td>\n",
       "      <td>0.00083</td>\n",
       "      <td>31.638</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0051***</td>\n",
       "      <td>0.00082</td>\n",
       "      <td>6.188</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.062***</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>74.207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.1629***</td>\n",
       "      <td>0.00088</td>\n",
       "      <td>185.560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.3336***</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>304.387</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.0355***</td>\n",
       "      <td>0.00131</td>\n",
       "      <td>27.074</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)    p\n",
       "in_out        -10.1369***  0.02973         341.008  0.0\n",
       "cy             -1.2104***  0.01525          79.373  0.0\n",
       "hp             -3.5582***  0.01782         199.716  0.0\n",
       "we             -0.0919***  0.01693           5.425  0.0\n",
       "le             -1.1952***  0.01992          59.994  0.0\n",
       "wi              5.1567***  0.03083         167.261  0.0\n",
       "he              0.1346***  0.02412           5.579  0.0\n",
       "li             -0.6423***  0.01044          61.545  0.0\n",
       "sp              3.8483***  0.02156         178.469  0.0\n",
       "ac              0.4453***  0.00918          48.501  0.0\n",
       "pr             -0.1582***  0.00214          73.947  0.0\n",
       "brand_2        -1.2928***  0.02812          45.978  0.0\n",
       "brand_3        -0.1643***  0.00403          40.773  0.0\n",
       "brand_4        -0.8878***  0.00432         205.382  0.0\n",
       "brand_5        -0.5127***  0.00390         131.583  0.0\n",
       "brand_6        -0.4219***  0.00386         109.207  0.0\n",
       "brand_7        -0.9998***  0.00687         145.487  0.0\n",
       "brand_8         -0.391***  0.00977          40.036  0.0\n",
       "brand_9        -1.7052***  0.00632         269.936  0.0\n",
       "brand_10       -0.2729***  0.00385          70.942  0.0\n",
       "brand_11       -0.1073***  0.00394          27.262  0.0\n",
       "brand_12       -0.5033***  0.00468         107.564  0.0\n",
       "brand_13       -1.2609***  0.00592         212.936  0.0\n",
       "brand_14       -1.8665***  0.00820         227.575  0.0\n",
       "brand_15       -1.8081***  0.00792         228.433  0.0\n",
       "brand_16       -1.1454***  0.00386         296.484  0.0\n",
       "brand_17       -0.9533***  0.00456         209.162  0.0\n",
       "brand_18        0.3122***  0.00418          74.664  0.0\n",
       "brand_19       -1.2534***  0.00527         237.833  0.0\n",
       "brand_20       -0.3031***  0.00477          63.474  0.0\n",
       "brand_21       -0.2697***  0.00371          72.715  0.0\n",
       "brand_22       -0.2426***  0.00378          64.155  0.0\n",
       "brand_23       -0.1789***  0.00409          43.732  0.0\n",
       "brand_24       -0.6858***  0.00384         178.376  0.0\n",
       "brand_25       -0.7068***  0.00457         154.682  0.0\n",
       "brand_26       -0.9844***  0.00459         214.397  0.0\n",
       "brand_27       -2.2279***  0.04775          46.655  0.0\n",
       "brand_28       -0.9277***  0.00784         118.328  0.0\n",
       "brand_29       -2.3593***  0.01513         155.961  0.0\n",
       "brand_30        -1.236***  0.00790         156.427  0.0\n",
       "brand_31       -1.4692***  0.01640          89.588  0.0\n",
       "brand_32       -0.3679***  0.00731          50.306  0.0\n",
       "brand_33       -1.5784***  0.02779          56.805  0.0\n",
       "brand_34       -0.7613***  0.00495         153.862  0.0\n",
       "brand_35       -0.5443***  0.00463         117.483  0.0\n",
       "brand_36        -0.148***  0.00393          37.669  0.0\n",
       "brand_37       -1.6339***  0.00990         164.996  0.0\n",
       "home_2          0.9322***  0.00176         530.101  0.0\n",
       "cla_2             0.04***  0.00214          18.670  0.0\n",
       "cla_3            0.119***  0.00288          41.311  0.0\n",
       "cla_4           0.0704***  0.00405          17.375  0.0\n",
       "cla_5           0.0628***  0.00522          12.045  0.0\n",
       "group_in_out    0.9163***  0.00160         571.541  0.0\n",
       "group_cy       -0.0329***  0.00087          37.712  0.0\n",
       "group_hp       -0.0188***  0.00083          22.554  0.0\n",
       "group_we       -0.0244***  0.00090          26.956  0.0\n",
       "group_le       -0.0599***  0.00088          68.204  0.0\n",
       "group_wi       -0.0194***  0.00081          23.996  0.0\n",
       "group_he       -0.0235***  0.00066          35.377  0.0\n",
       "group_li       -0.0262***  0.00083          31.638  0.0\n",
       "group_sp       -0.0051***  0.00082           6.188  0.0\n",
       "group_ac        -0.062***  0.00084          74.207  0.0\n",
       "group_brand     0.1629***  0.00088         185.560  0.0\n",
       "group_home     -0.3336***  0.00110         304.387  0.0\n",
       "group_cla      -0.0355***  0.00131          27.074  0.0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_table(ThetaOptBLP, SEOptBLP, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.91633759, -0.03292946, -0.01877064, -0.02439268, -0.05987561,\n",
       "       -0.01935042, -0.0235214 , -0.02619192, -0.00505587, -0.06204799,\n",
       "        0.16285753, -0.33359122, -0.03554067])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOptBLP[K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17336341520624832"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15824227223733603"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOptBLP[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.3285452745644795"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKN_theta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8471166690149027"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logit_beta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "qOpt = IPDL_ccp(ThetaOptBLP, z_logit, Psi, Nest_count, tol = 1.0e-30)\n",
    "HessOpt = compute_pertubation_hessian(qOpt, z_logit, ThetaOptBLP, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([qOpt[t].min() for t in np.arange(T)]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([la.eigvals(HessOpt[t]).min() for t in np.arange(T)]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market $t=1$ the price elasticities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Semi-elasticity wrt. product</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semi-elasticity of product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002154</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.000145</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.000076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.293504</td>\n",
       "      <td>0.003975</td>\n",
       "      <td>0.011917</td>\n",
       "      <td>0.008352</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.005558</td>\n",
       "      <td>-0.035478</td>\n",
       "      <td>-0.002193</td>\n",
       "      <td>-0.010791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>0.004965</td>\n",
       "      <td>0.008152</td>\n",
       "      <td>0.002458</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>0.008469</td>\n",
       "      <td>-0.000139</td>\n",
       "      <td>0.007459</td>\n",
       "      <td>-0.001840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.002403</td>\n",
       "      <td>-0.303546</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.006946</td>\n",
       "      <td>0.010956</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.014070</td>\n",
       "      <td>0.001239</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001436</td>\n",
       "      <td>-0.001658</td>\n",
       "      <td>-0.005176</td>\n",
       "      <td>-0.004613</td>\n",
       "      <td>-0.001881</td>\n",
       "      <td>0.000124</td>\n",
       "      <td>-0.010569</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.005609</td>\n",
       "      <td>-0.009268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004673</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>-0.313277</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.013743</td>\n",
       "      <td>-0.000387</td>\n",
       "      <td>0.012262</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.006517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000110</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>-0.010787</td>\n",
       "      <td>-0.001223</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.003906</td>\n",
       "      <td>0.006030</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.028073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.004892</td>\n",
       "      <td>0.025282</td>\n",
       "      <td>-0.309494</td>\n",
       "      <td>0.012510</td>\n",
       "      <td>0.002727</td>\n",
       "      <td>0.017941</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.003721</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003061</td>\n",
       "      <td>-0.005108</td>\n",
       "      <td>0.007339</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>0.002392</td>\n",
       "      <td>-0.001916</td>\n",
       "      <td>0.022684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.005155</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.008359</td>\n",
       "      <td>-0.270438</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>-0.002178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.006496</td>\n",
       "      <td>0.002373</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>-0.000020</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>-0.004658</td>\n",
       "      <td>0.007773</td>\n",
       "      <td>-0.006296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003556</td>\n",
       "      <td>0.004579</td>\n",
       "      <td>-0.000632</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>0.011662</td>\n",
       "      <td>-0.300352</td>\n",
       "      <td>0.063939</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>0.007720</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>0.009701</td>\n",
       "      <td>-0.006768</td>\n",
       "      <td>0.000517</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000262</td>\n",
       "      <td>-0.012263</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>-0.009067</td>\n",
       "      <td>0.009330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.007936</td>\n",
       "      <td>0.005206</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>0.009426</td>\n",
       "      <td>0.004925</td>\n",
       "      <td>0.022353</td>\n",
       "      <td>-0.320983</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>-0.001803</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020568</td>\n",
       "      <td>0.014223</td>\n",
       "      <td>0.008572</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>-0.000569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.006103</td>\n",
       "      <td>0.005704</td>\n",
       "      <td>0.011557</td>\n",
       "      <td>0.010364</td>\n",
       "      <td>0.004917</td>\n",
       "      <td>0.021054</td>\n",
       "      <td>0.011579</td>\n",
       "      <td>-0.297235</td>\n",
       "      <td>-0.008353</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019253</td>\n",
       "      <td>0.013353</td>\n",
       "      <td>0.007339</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>-0.002034</td>\n",
       "      <td>0.008214</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>0.008683</td>\n",
       "      <td>0.001458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.007045</td>\n",
       "      <td>0.005525</td>\n",
       "      <td>0.010852</td>\n",
       "      <td>0.005706</td>\n",
       "      <td>-0.005000</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>-0.005262</td>\n",
       "      <td>-0.001960</td>\n",
       "      <td>-0.287296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006869</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>0.007461</td>\n",
       "      <td>0.002436</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>-0.000391</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>-0.002944</td>\n",
       "      <td>0.007942</td>\n",
       "      <td>0.001230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.005124</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>-0.003383</td>\n",
       "      <td>-0.007570</td>\n",
       "      <td>0.018543</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.041560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005460</td>\n",
       "      <td>-0.001766</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>-0.001159</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>-0.001767</td>\n",
       "      <td>0.003729</td>\n",
       "      <td>0.007293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.004710</td>\n",
       "      <td>0.007482</td>\n",
       "      <td>0.003424</td>\n",
       "      <td>0.010372</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>-0.021621</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>-0.000042</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007070</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.029251</td>\n",
       "      <td>0.000126</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.007669</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>-0.002852</td>\n",
       "      <td>0.006628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.001568</td>\n",
       "      <td>-0.012835</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.004111</td>\n",
       "      <td>0.018093</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003812</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.025393</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>-0.000132</td>\n",
       "      <td>0.000184</td>\n",
       "      <td>0.004445</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>-0.007738</td>\n",
       "      <td>0.009092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004825</td>\n",
       "      <td>0.005237</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>0.012171</td>\n",
       "      <td>-0.002229</td>\n",
       "      <td>0.012556</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>-0.000058</td>\n",
       "      <td>-0.004920</td>\n",
       "      <td>0.021916</td>\n",
       "      <td>0.018523</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>-0.006681</td>\n",
       "      <td>0.005715</td>\n",
       "      <td>-0.000898</td>\n",
       "      <td>0.009472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.005310</td>\n",
       "      <td>0.005056</td>\n",
       "      <td>0.010430</td>\n",
       "      <td>0.005755</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.005625</td>\n",
       "      <td>0.003896</td>\n",
       "      <td>-0.001555</td>\n",
       "      <td>-0.013358</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.003634</td>\n",
       "      <td>0.006509</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>-0.000621</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>0.006920</td>\n",
       "      <td>-0.008243</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>-0.002946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.004552</td>\n",
       "      <td>-0.007456</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>0.012912</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.018775</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.005886</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002437</td>\n",
       "      <td>-0.006288</td>\n",
       "      <td>0.002329</td>\n",
       "      <td>-0.001623</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000214</td>\n",
       "      <td>0.003663</td>\n",
       "      <td>0.021458</td>\n",
       "      <td>0.038643</td>\n",
       "      <td>0.006268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003011</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.002781</td>\n",
       "      <td>-0.002992</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>-0.005654</td>\n",
       "      <td>0.017683</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.011554</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.006066</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.003361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.008781</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>-0.009314</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.011636</td>\n",
       "      <td>-0.002441</td>\n",
       "      <td>0.016522</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>-0.001828</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>-0.004167</td>\n",
       "      <td>-0.007834</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>-0.001344</td>\n",
       "      <td>0.000177</td>\n",
       "      <td>0.003970</td>\n",
       "      <td>0.004933</td>\n",
       "      <td>0.004236</td>\n",
       "      <td>0.009980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>-0.005755</td>\n",
       "      <td>0.011878</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.008954</td>\n",
       "      <td>0.003032</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>0.000114</td>\n",
       "      <td>-0.002934</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.003207</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>0.002526</td>\n",
       "      <td>-0.001913</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.007383</td>\n",
       "      <td>-0.007097</td>\n",
       "      <td>-0.000350</td>\n",
       "      <td>-0.008828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.014821</td>\n",
       "      <td>-0.011738</td>\n",
       "      <td>-0.000618</td>\n",
       "      <td>0.006908</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>-0.000271</td>\n",
       "      <td>-0.004389</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.006339</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004628</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>-0.001524</td>\n",
       "      <td>-0.005432</td>\n",
       "      <td>0.001724</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>-0.013901</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>0.009534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.016585</td>\n",
       "      <td>0.005199</td>\n",
       "      <td>-0.012529</td>\n",
       "      <td>0.008726</td>\n",
       "      <td>0.013839</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>0.018936</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.007351</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>-0.005386</td>\n",
       "      <td>0.001681</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>-0.000839</td>\n",
       "      <td>0.006511</td>\n",
       "      <td>0.008614</td>\n",
       "      <td>0.011096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>0.005650</td>\n",
       "      <td>0.012229</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>-0.017256</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>-0.003935</td>\n",
       "      <td>-0.009079</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005136</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.003491</td>\n",
       "      <td>0.002546</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>-0.000598</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.007165</td>\n",
       "      <td>-0.011169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.004699</td>\n",
       "      <td>0.007521</td>\n",
       "      <td>0.010711</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>0.012642</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.005283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.000185</td>\n",
       "      <td>0.003618</td>\n",
       "      <td>0.001971</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.000174</td>\n",
       "      <td>0.006071</td>\n",
       "      <td>-0.001899</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>0.000254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.001440</td>\n",
       "      <td>0.005597</td>\n",
       "      <td>0.009708</td>\n",
       "      <td>0.003986</td>\n",
       "      <td>-0.007239</td>\n",
       "      <td>0.005992</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>-0.000796</td>\n",
       "      <td>-0.008293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003118</td>\n",
       "      <td>0.003881</td>\n",
       "      <td>0.006608</td>\n",
       "      <td>0.002301</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.000165</td>\n",
       "      <td>0.006840</td>\n",
       "      <td>-0.006736</td>\n",
       "      <td>0.007998</td>\n",
       "      <td>0.005742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.009248</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.010026</td>\n",
       "      <td>0.005691</td>\n",
       "      <td>0.005024</td>\n",
       "      <td>-0.011479</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005804</td>\n",
       "      <td>0.002241</td>\n",
       "      <td>0.003971</td>\n",
       "      <td>0.002223</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>-0.001477</td>\n",
       "      <td>0.006456</td>\n",
       "      <td>0.004719</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.002431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.000898</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>0.010968</td>\n",
       "      <td>0.009843</td>\n",
       "      <td>-0.030640</td>\n",
       "      <td>0.006715</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>-0.012421</td>\n",
       "      <td>-0.000380</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006126</td>\n",
       "      <td>0.004603</td>\n",
       "      <td>0.006533</td>\n",
       "      <td>0.002559</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>-0.001518</td>\n",
       "      <td>0.006884</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.007922</td>\n",
       "      <td>-0.005311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004963</td>\n",
       "      <td>0.004150</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>0.006804</td>\n",
       "      <td>0.011791</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.016199</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.005474</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001623</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>-0.000341</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>-0.002396</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.006928</td>\n",
       "      <td>0.008455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.000660</td>\n",
       "      <td>0.020475</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>-0.014548</td>\n",
       "      <td>0.011149</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.018160</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.005290</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000923</td>\n",
       "      <td>-0.013535</td>\n",
       "      <td>0.006552</td>\n",
       "      <td>0.002455</td>\n",
       "      <td>-0.000812</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.007624</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.008831</td>\n",
       "      <td>0.004965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.000821</td>\n",
       "      <td>0.020331</td>\n",
       "      <td>0.009091</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.010804</td>\n",
       "      <td>0.001884</td>\n",
       "      <td>0.015087</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004656</td>\n",
       "      <td>-0.002649</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.007594</td>\n",
       "      <td>0.009017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.004736</td>\n",
       "      <td>0.021929</td>\n",
       "      <td>0.007201</td>\n",
       "      <td>0.009649</td>\n",
       "      <td>-0.002801</td>\n",
       "      <td>0.003926</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>-0.004806</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006014</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.008705</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.008384</td>\n",
       "      <td>-0.001093</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>-0.003802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.006104</td>\n",
       "      <td>0.020108</td>\n",
       "      <td>-0.008663</td>\n",
       "      <td>0.007387</td>\n",
       "      <td>0.012763</td>\n",
       "      <td>-0.001919</td>\n",
       "      <td>0.016974</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.006069</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003097</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>-0.002478</td>\n",
       "      <td>0.001540</td>\n",
       "      <td>0.001909</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>-0.005155</td>\n",
       "      <td>0.006338</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.010831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>-0.001937</td>\n",
       "      <td>0.024323</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.013080</td>\n",
       "      <td>0.003674</td>\n",
       "      <td>-0.000475</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.005041</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>-0.001023</td>\n",
       "      <td>0.007014</td>\n",
       "      <td>-0.001277</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.000164</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.002635</td>\n",
       "      <td>-0.001956</td>\n",
       "      <td>0.027800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.005632</td>\n",
       "      <td>0.006351</td>\n",
       "      <td>0.030115</td>\n",
       "      <td>0.028479</td>\n",
       "      <td>-0.015907</td>\n",
       "      <td>0.006267</td>\n",
       "      <td>0.003707</td>\n",
       "      <td>-0.003795</td>\n",
       "      <td>-0.009224</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.004631</td>\n",
       "      <td>0.007049</td>\n",
       "      <td>0.003123</td>\n",
       "      <td>-0.000757</td>\n",
       "      <td>-0.000339</td>\n",
       "      <td>0.007364</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.006575</td>\n",
       "      <td>0.001107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003656</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>-0.012266</td>\n",
       "      <td>0.007986</td>\n",
       "      <td>0.010034</td>\n",
       "      <td>0.003918</td>\n",
       "      <td>0.013924</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005358</td>\n",
       "      <td>-0.002164</td>\n",
       "      <td>-0.004114</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>-0.002296</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.005602</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.000076</td>\n",
       "      <td>0.008688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.001717</td>\n",
       "      <td>0.005143</td>\n",
       "      <td>0.004789</td>\n",
       "      <td>-0.002025</td>\n",
       "      <td>0.009794</td>\n",
       "      <td>-0.010715</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.007096</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012132</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>0.006794</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.003980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004891</td>\n",
       "      <td>-0.001660</td>\n",
       "      <td>-0.000195</td>\n",
       "      <td>-0.005026</td>\n",
       "      <td>0.010866</td>\n",
       "      <td>0.004256</td>\n",
       "      <td>0.064275</td>\n",
       "      <td>0.004836</td>\n",
       "      <td>0.007355</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.297823</td>\n",
       "      <td>0.008857</td>\n",
       "      <td>-0.002052</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>-0.000360</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.000003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>-0.002697</td>\n",
       "      <td>0.005016</td>\n",
       "      <td>-0.011796</td>\n",
       "      <td>0.012439</td>\n",
       "      <td>0.014907</td>\n",
       "      <td>0.062518</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.006978</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012459</td>\n",
       "      <td>-0.302092</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>-0.000535</td>\n",
       "      <td>0.000255</td>\n",
       "      <td>0.002587</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.006640</td>\n",
       "      <td>0.010025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>-0.004620</td>\n",
       "      <td>-0.014843</td>\n",
       "      <td>0.009301</td>\n",
       "      <td>0.012322</td>\n",
       "      <td>-0.005708</td>\n",
       "      <td>0.020676</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.006166</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001584</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>-0.298519</td>\n",
       "      <td>-0.000862</td>\n",
       "      <td>-0.001429</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>-0.013205</td>\n",
       "      <td>0.005561</td>\n",
       "      <td>-0.003834</td>\n",
       "      <td>0.008429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>-0.010640</td>\n",
       "      <td>-0.004350</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>0.011630</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.005203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>-0.002226</td>\n",
       "      <td>-0.270725</td>\n",
       "      <td>0.017660</td>\n",
       "      <td>0.001687</td>\n",
       "      <td>-0.008316</td>\n",
       "      <td>0.004931</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.009262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>-0.005868</td>\n",
       "      <td>-0.006339</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.009250</td>\n",
       "      <td>0.003217</td>\n",
       "      <td>0.017014</td>\n",
       "      <td>0.001220</td>\n",
       "      <td>0.005373</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000971</td>\n",
       "      <td>-0.001027</td>\n",
       "      <td>-0.004996</td>\n",
       "      <td>0.023891</td>\n",
       "      <td>-0.276269</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.005397</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.007097</td>\n",
       "      <td>-0.010982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.008289</td>\n",
       "      <td>0.004567</td>\n",
       "      <td>0.013365</td>\n",
       "      <td>0.005977</td>\n",
       "      <td>-0.001586</td>\n",
       "      <td>0.009124</td>\n",
       "      <td>-0.023373</td>\n",
       "      <td>-0.016319</td>\n",
       "      <td>-0.013386</td>\n",
       "      <td>...</td>\n",
       "      <td>0.008081</td>\n",
       "      <td>0.005796</td>\n",
       "      <td>0.009428</td>\n",
       "      <td>0.027016</td>\n",
       "      <td>0.019628</td>\n",
       "      <td>-0.282448</td>\n",
       "      <td>0.009738</td>\n",
       "      <td>0.002572</td>\n",
       "      <td>0.008351</td>\n",
       "      <td>0.001154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.004134</td>\n",
       "      <td>-0.008534</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.004116</td>\n",
       "      <td>0.011586</td>\n",
       "      <td>-0.009356</td>\n",
       "      <td>0.016970</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.005304</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.001285</td>\n",
       "      <td>-0.011947</td>\n",
       "      <td>-0.002912</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.000213</td>\n",
       "      <td>-0.244116</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>-0.004223</td>\n",
       "      <td>0.007473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.000108</td>\n",
       "      <td>0.005126</td>\n",
       "      <td>0.011900</td>\n",
       "      <td>0.004347</td>\n",
       "      <td>-0.012671</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>-0.003489</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.007888</td>\n",
       "      <td>-0.291878</td>\n",
       "      <td>0.027647</td>\n",
       "      <td>-0.004383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.003958</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>-0.001919</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>-0.006046</td>\n",
       "      <td>0.006323</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.005190</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004050</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>-0.003031</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.001605</td>\n",
       "      <td>0.000160</td>\n",
       "      <td>-0.003691</td>\n",
       "      <td>0.015242</td>\n",
       "      <td>-0.289167</td>\n",
       "      <td>0.006235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.156088</td>\n",
       "      <td>-0.000781</td>\n",
       "      <td>-0.006511</td>\n",
       "      <td>0.030408</td>\n",
       "      <td>0.022628</td>\n",
       "      <td>-0.009400</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>-0.001080</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.002473</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.006502</td>\n",
       "      <td>-0.002405</td>\n",
       "      <td>0.006207</td>\n",
       "      <td>-0.316027</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Semi-elasticity wrt. product        0         1         2         3   \\\n",
       "Semi-elasticity of product                                             \n",
       "0                            -0.002154  0.000032  0.000054  0.000083   \n",
       "1                             0.156088 -0.293504  0.003975  0.011917   \n",
       "2                             0.156088  0.002403 -0.303546  0.002601   \n",
       "3                             0.156088  0.004673  0.001687 -0.313277   \n",
       "4                             0.156088  0.003556  0.004892  0.025282   \n",
       "5                             0.156088  0.000005  0.005155  0.009970   \n",
       "6                             0.156088  0.003556  0.004579 -0.000632   \n",
       "7                             0.156088 -0.007936  0.005206  0.006995   \n",
       "8                             0.156088 -0.006103  0.005704  0.011557   \n",
       "9                             0.156088 -0.007045  0.005525  0.010852   \n",
       "10                            0.156088  0.004194  0.005124  0.001505   \n",
       "11                            0.156088  0.001769  0.004710  0.007482   \n",
       "12                            0.156088  0.003526  0.002486  0.001568   \n",
       "13                            0.156088  0.004825  0.005237  0.001958   \n",
       "14                            0.156088 -0.005310  0.005056  0.010430   \n",
       "15                            0.156088  0.004180  0.004552 -0.007456   \n",
       "16                            0.156088  0.003011  0.005050  0.002781   \n",
       "17                            0.156088  0.008781  0.001147 -0.009314   \n",
       "18                            0.156088  0.011017 -0.005755  0.011878   \n",
       "19                            0.156088  0.014821 -0.011738 -0.000618   \n",
       "20                            0.156088  0.016585  0.005199 -0.012529   \n",
       "21                            0.156088  0.011276  0.005650  0.012229   \n",
       "22                            0.156088  0.001219  0.002703  0.004699   \n",
       "23                            0.156088 -0.001440  0.005597  0.009708   \n",
       "24                            0.156088  0.001397  0.002703  0.009248   \n",
       "25                            0.156088  0.000898  0.005175  0.010968   \n",
       "26                            0.156088  0.004963  0.004150 -0.000352   \n",
       "27                            0.156088 -0.000660  0.020475  0.009918   \n",
       "28                            0.156088 -0.000821  0.020331  0.009091   \n",
       "29                            0.156088 -0.004736  0.021929  0.007201   \n",
       "30                            0.156088  0.006104  0.020108 -0.008663   \n",
       "31                            0.156088  0.003432 -0.001937  0.024323   \n",
       "32                            0.156088 -0.005632  0.006351  0.030115   \n",
       "33                            0.156088  0.003656  0.004909 -0.012266   \n",
       "34                            0.156088 -0.001717  0.005143  0.004789   \n",
       "35                            0.156088  0.004891 -0.001660 -0.000195   \n",
       "36                            0.156088  0.004881 -0.002697  0.005016   \n",
       "37                            0.156088  0.004399 -0.004620 -0.014843   \n",
       "38                            0.156088  0.003427 -0.010640 -0.004350   \n",
       "39                            0.156088  0.003111 -0.005868 -0.006339   \n",
       "40                            0.156088 -0.008289  0.004567  0.013365   \n",
       "41                            0.156088  0.004134 -0.008534  0.004863   \n",
       "42                            0.156088 -0.000108  0.005126  0.011900   \n",
       "43                            0.156088  0.003182  0.003958  0.000077   \n",
       "44                            0.156088 -0.000781 -0.006511  0.030408   \n",
       "\n",
       "Semi-elasticity wrt. product        4         5         6         7   \\\n",
       "Semi-elasticity of product                                             \n",
       "0                             0.000076  0.000114  0.000051  0.000145   \n",
       "1                             0.008352  0.000018  0.005558 -0.035478   \n",
       "2                             0.006946  0.010956  0.004327  0.014070   \n",
       "3                             0.023284  0.013743 -0.000387  0.012262   \n",
       "4                            -0.309494  0.012510  0.002727  0.017941   \n",
       "5                             0.008359 -0.270438  0.005185  0.006263   \n",
       "6                             0.004098  0.011662 -0.300352  0.063939   \n",
       "7                             0.009426  0.004925  0.022353 -0.320983   \n",
       "8                             0.010364  0.004917  0.021054  0.011579   \n",
       "9                             0.005706 -0.005000  0.007878 -0.005262   \n",
       "10                            0.000976 -0.003383 -0.007570  0.018543   \n",
       "11                            0.003424  0.010372  0.004873 -0.021621   \n",
       "12                           -0.012835  0.005532  0.004111  0.018093   \n",
       "13                            0.007723  0.012171 -0.002229  0.012556   \n",
       "14                            0.005755  0.005856  0.005625  0.003896   \n",
       "15                            0.007721  0.012912  0.000227  0.018775   \n",
       "16                           -0.002992  0.005311 -0.005654  0.017683   \n",
       "17                            0.003241  0.011636 -0.002441  0.016522   \n",
       "18                            0.000976  0.008954  0.003032  0.005766   \n",
       "19                            0.006908  0.012770 -0.000271 -0.004389   \n",
       "20                            0.008726  0.013839  0.003569  0.018936   \n",
       "21                            0.009376 -0.017256  0.000703  0.007166   \n",
       "22                            0.007521  0.010711  0.002261  0.012642   \n",
       "23                            0.003986 -0.007239  0.005992  0.000249   \n",
       "24                            0.008805  0.010026  0.005691  0.005024   \n",
       "25                            0.009843 -0.030640  0.006715  0.000183   \n",
       "26                            0.006804  0.011791  0.003315  0.016199   \n",
       "27                           -0.014548  0.011149  0.002189  0.018160   \n",
       "28                            0.001500  0.010804  0.001884  0.015087   \n",
       "29                            0.009649 -0.002801  0.003926  0.001245   \n",
       "30                            0.007387  0.012763 -0.001919  0.016974   \n",
       "31                            0.002366  0.013080  0.003674 -0.000475   \n",
       "32                            0.028479 -0.015907  0.006267  0.003707   \n",
       "33                            0.007986  0.010034  0.003918  0.013924   \n",
       "34                           -0.002025  0.009794 -0.010715  0.012304   \n",
       "35                           -0.005026  0.010866  0.004256  0.064275   \n",
       "36                           -0.011796  0.012439  0.014907  0.062518   \n",
       "37                            0.009301  0.012322 -0.005708  0.020676   \n",
       "38                            0.004339  0.011630  0.001126  0.000946   \n",
       "39                            0.002883  0.009250  0.003217  0.017014   \n",
       "40                            0.005977 -0.001586  0.009124 -0.023373   \n",
       "41                            0.004116  0.011586 -0.009356  0.016970   \n",
       "42                            0.004347 -0.012671  0.006352  0.005399   \n",
       "43                           -0.001919  0.011657 -0.006046  0.006323   \n",
       "44                            0.022628 -0.009400  0.006193 -0.001080   \n",
       "\n",
       "Semi-elasticity wrt. product        8         9   ...        35        36  \\\n",
       "Semi-elasticity of product                        ...                       \n",
       "0                             0.000012  0.000050  ...  0.000046  0.000033   \n",
       "1                            -0.002193 -0.010791  ...  0.006997  0.004965   \n",
       "2                             0.001239  0.005116  ... -0.001436 -0.001658   \n",
       "3                             0.001628  0.006517  ... -0.000110  0.002000   \n",
       "4                             0.001586  0.003721  ... -0.003061 -0.005108   \n",
       "5                             0.000503 -0.002178  ...  0.004422  0.003599   \n",
       "6                             0.004841  0.007720  ...  0.003896  0.009701   \n",
       "7                             0.000931 -0.001803  ...  0.020568  0.014223   \n",
       "8                            -0.297235 -0.008353  ...  0.019253  0.013353   \n",
       "9                            -0.001960 -0.287296  ...  0.006869  0.004634   \n",
       "10                            0.001732  0.041560  ... -0.005460 -0.001766   \n",
       "11                            0.000864 -0.000042  ...  0.007070  0.002878   \n",
       "12                            0.001364  0.005705  ... -0.003812  0.002467   \n",
       "13                            0.001502  0.005514  ...  0.004545 -0.000058   \n",
       "14                           -0.001555 -0.013358  ...  0.004869  0.003634   \n",
       "15                            0.001439  0.005886  ...  0.002437 -0.006288   \n",
       "16                            0.001492  0.003701  ... -0.011554  0.002050   \n",
       "17                            0.001476 -0.001828  ...  0.002440 -0.004167   \n",
       "18                            0.000114 -0.002934  ...  0.001436  0.003207   \n",
       "19                            0.001667  0.006339  ...  0.004628  0.000928   \n",
       "20                            0.001699  0.007351  ...  0.003360  0.002787   \n",
       "21                           -0.003935 -0.009079  ...  0.005136  0.004112   \n",
       "22                            0.001671  0.005283  ...  0.004322  0.000185   \n",
       "23                           -0.000796 -0.008293  ...  0.003118  0.003881   \n",
       "24                           -0.011479  0.003003  ...  0.005804  0.002241   \n",
       "25                           -0.012421 -0.000380  ...  0.006126  0.004603   \n",
       "26                            0.001373  0.005474  ... -0.001623  0.002889   \n",
       "27                            0.001441  0.005290  ... -0.000923 -0.013535   \n",
       "28                            0.001330  0.001659  ... -0.004656 -0.002649   \n",
       "29                            0.000467 -0.004806  ...  0.006014  0.003661   \n",
       "30                            0.001419  0.006069  ...  0.003097  0.004295   \n",
       "31                            0.001485  0.005041  ...  0.001473 -0.001023   \n",
       "32                           -0.003795 -0.009224  ...  0.006457  0.004631   \n",
       "33                            0.000970  0.000699  ...  0.005358 -0.002164   \n",
       "34                            0.001584  0.007096  ... -0.012132  0.002762   \n",
       "35                            0.004836  0.007355  ... -0.297823  0.008857   \n",
       "36                            0.004718  0.006978  ...  0.012459 -0.302092   \n",
       "37                            0.001423  0.006166  ... -0.001584  0.000819   \n",
       "38                            0.001487  0.005203  ...  0.000386 -0.000313   \n",
       "39                            0.001220  0.005373  ... -0.000971 -0.001027   \n",
       "40                           -0.016319 -0.013386  ...  0.008081  0.005796   \n",
       "41                            0.001441  0.005304  ...  0.000405  0.001285   \n",
       "42                            0.000757 -0.003489  ...  0.002308  0.004197   \n",
       "43                            0.001331  0.005190  ...  0.004050  0.002881   \n",
       "44                            0.000222  0.000800  ...  0.000002  0.004330   \n",
       "\n",
       "Semi-elasticity wrt. product        37        38        39        40  \\\n",
       "Semi-elasticity of product                                             \n",
       "0                             0.000060  0.000023  0.000017  0.000001   \n",
       "1                             0.008152  0.002458  0.001649 -0.000371   \n",
       "2                            -0.005176 -0.004613 -0.001881  0.000124   \n",
       "3                            -0.010787 -0.001223 -0.001318  0.000235   \n",
       "4                             0.007339  0.001325  0.000651  0.000114   \n",
       "5                             0.006496  0.002373  0.001395 -0.000020   \n",
       "6                            -0.006768  0.000517  0.001091  0.000262   \n",
       "7                             0.008572  0.000152  0.002018 -0.000234   \n",
       "8                             0.007339  0.002967  0.001801 -0.002034   \n",
       "9                             0.007461  0.002436  0.001860 -0.000391   \n",
       "10                            0.002222  0.001578 -0.001159  0.000208   \n",
       "11                            0.029251  0.000126  0.002129  0.000007   \n",
       "12                            0.025393  0.000826 -0.000132  0.000184   \n",
       "13                           -0.004920  0.021916  0.018523  0.001729   \n",
       "14                            0.006509  0.002545 -0.000621 -0.000045   \n",
       "15                            0.002329 -0.001623  0.000698  0.000214   \n",
       "16                            0.006066  0.001551  0.000008  0.000158   \n",
       "17                           -0.007834  0.001910 -0.001344  0.000177   \n",
       "18                            0.008116  0.002526 -0.001913  0.000131   \n",
       "19                           -0.001524 -0.005432  0.001724  0.000235   \n",
       "20                           -0.005386  0.001681  0.001736  0.000288   \n",
       "21                            0.003491  0.002546  0.001610 -0.000598   \n",
       "22                            0.003618  0.001971  0.001330  0.000174   \n",
       "23                            0.006608  0.002301  0.001436  0.000165   \n",
       "24                            0.003971  0.002223  0.001566 -0.001477   \n",
       "25                            0.006533  0.002559  0.001579 -0.001518   \n",
       "26                           -0.000341  0.001727  0.001572  0.000228   \n",
       "27                            0.006552  0.002455 -0.000812  0.000065   \n",
       "28                            0.007235  0.002342  0.000342  0.000056   \n",
       "29                            0.008705  0.002572  0.001705  0.000047   \n",
       "30                           -0.002478  0.001540  0.001909  0.000255   \n",
       "31                            0.007014 -0.001277  0.002123  0.000164   \n",
       "32                            0.007049  0.003123 -0.000757 -0.000339   \n",
       "33                           -0.004114  0.000197 -0.002296  0.000168   \n",
       "34                            0.006794  0.001701  0.000339  0.000153   \n",
       "35                           -0.002052  0.000193 -0.000360  0.000253   \n",
       "36                            0.001492 -0.000221 -0.000535  0.000255   \n",
       "37                           -0.298519 -0.000862 -0.001429  0.000228   \n",
       "38                           -0.002226 -0.270725  0.017660  0.001687   \n",
       "39                           -0.004996  0.023891 -0.276269  0.001658   \n",
       "40                            0.009428  0.027016  0.019628 -0.282448   \n",
       "41                           -0.011947 -0.002912  0.001397  0.000213   \n",
       "42                            0.007975  0.002737  0.001332  0.000089   \n",
       "43                           -0.003031  0.001140  0.001605  0.000160   \n",
       "44                            0.006635  0.002821 -0.002473  0.000022   \n",
       "\n",
       "Semi-elasticity wrt. product        41        42        43        44  \n",
       "Semi-elasticity of product                                            \n",
       "0                             0.000066  0.000042  0.000076  0.000076  \n",
       "1                             0.008469 -0.000139  0.007459 -0.001840  \n",
       "2                            -0.010569  0.004004  0.005609 -0.009268  \n",
       "3                             0.003906  0.006030  0.000071  0.028073  \n",
       "4                             0.003590  0.002392 -0.001916  0.022684  \n",
       "5                             0.006751 -0.004658  0.007773 -0.006296  \n",
       "6                            -0.012263  0.005252 -0.009067  0.009330  \n",
       "7                             0.007776  0.001561  0.003315 -0.000569  \n",
       "8                             0.008214  0.002723  0.008683  0.001458  \n",
       "9                             0.007094 -0.002944  0.007942  0.001230  \n",
       "10                            0.005267 -0.001767  0.003729  0.007293  \n",
       "11                            0.007669  0.001051 -0.002852  0.006628  \n",
       "12                            0.004445  0.002673 -0.007738  0.009092  \n",
       "13                           -0.006681  0.005715 -0.000898  0.009472  \n",
       "14                            0.006920 -0.008243  0.007772 -0.002946  \n",
       "15                            0.003663  0.021458  0.038643  0.006268  \n",
       "16                            0.005204  0.000787  0.000070  0.003361  \n",
       "17                            0.003970  0.004933  0.004236  0.009980  \n",
       "18                            0.007383 -0.007097 -0.000350 -0.008828  \n",
       "19                           -0.013901  0.005025  0.005509  0.009534  \n",
       "20                           -0.000839  0.006511  0.008614  0.011096  \n",
       "21                            0.001861  0.002229  0.007165 -0.011169  \n",
       "22                            0.006071 -0.001899  0.003569  0.000254  \n",
       "23                            0.006840 -0.006736  0.007998  0.005742  \n",
       "24                            0.006456  0.004719  0.006400  0.002431  \n",
       "25                            0.006884  0.002170  0.007922 -0.005311  \n",
       "26                           -0.002396  0.004905  0.006928  0.008455  \n",
       "27                            0.007624  0.002823  0.008831  0.004965  \n",
       "28                            0.007492  0.002298  0.007594  0.009017  \n",
       "29                            0.008384 -0.001093  0.003511 -0.003802  \n",
       "30                           -0.005155  0.006338  0.007500  0.010831  \n",
       "31                            0.002983  0.002635 -0.001956  0.027800  \n",
       "32                            0.007364  0.000552  0.006575  0.001107  \n",
       "33                            0.005602  0.002808  0.000076  0.008688  \n",
       "34                            0.003543  0.001318  0.003942  0.003980  \n",
       "35                            0.000579  0.002085  0.006636  0.000003  \n",
       "36                            0.002587  0.005333  0.006640  0.010025  \n",
       "37                           -0.013205  0.005561 -0.003834  0.008429  \n",
       "38                           -0.008316  0.004931  0.003724  0.009262  \n",
       "39                            0.005397  0.003247  0.007097 -0.010982  \n",
       "40                            0.009738  0.002572  0.008351  0.001154  \n",
       "41                           -0.244116  0.004976 -0.004223  0.007473  \n",
       "42                            0.007888 -0.291878  0.027647 -0.004383  \n",
       "43                           -0.003691  0.015242 -0.289167  0.006235  \n",
       "44                            0.006502 -0.002405  0.006207 -0.316027  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_hat = IPDL_elasticity(qOpt, z_logit, ThetaOptBLP, Psi, Nest_count, char_number = pr_index)\n",
    "pd.DataFrame(E_hat[0]).rename_axis(index = 'Semi-elasticity of product', columns = 'Semi-elasticity wrt. product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios for the IPDL model\n",
    "\n",
    "The diversion ratio to product j from product k is the fraction of consumers leaving product k and switching to product j following a one percent increase in the price of product k. Hence we have:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{jk}^i = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial x_{ik\\ell}}{\\partial P_k(u_i|\\lambda) / \\partial x_{ik\\ell}} = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial u_{ik}}{\\partial P_k(u_i|\\lambda) / \\partial u_{ik}}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{D}^i = \\left( \\mathcal{D}_{jk}^i \\right)_{j,k \\in \\{0,1,\\ldots ,5\\}}$ is the matrix of diversion ratios for individual i. This can be written more compactly as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}^i = -100 \\cdot  (\\nabla_u P(u|\\lambda) \\circ I_J)^{-1}\\nabla_u P(u|\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_diversion_ratio(q, x, Theta, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates diversion ratios from the IPDL model\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Diversion_ratio: a dictionary of T numpy arrays (J,J) of diversion ratios from product j to product k for each individual i\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack, nest_count) # Find the derivatives of ccp's wrt. utilities\n",
    "    inv_diaggrad = {t: np.divide(1, np.diag(Grad[t]), out = np.zeros_like(np.diag(Grad[t])), where = (np.diag(Grad[t]) != 0)) for t in np.arange(T)}  # Compute the inverse of the 'own'-derivatives of ccp's\n",
    "    DR = {t: np.multiply(-100, np.einsum('j,jk->jk', inv_diaggrad[t], Grad[t])) for t in np.arange(T)} # Compute diversion ratios as a hadamard product.\n",
    "    \n",
    "    return DR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the implied diversion ratios $\\mathcal{ D}^i$ from our estimates $\\hat \\theta^{\\text{IPDL}}$, we find for market $t=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Diversion ratio wrt. product</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diversion ratio of product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.501946</td>\n",
       "      <td>2.484405</td>\n",
       "      <td>3.830382</td>\n",
       "      <td>3.527633</td>\n",
       "      <td>5.279894</td>\n",
       "      <td>2.347399</td>\n",
       "      <td>6.714418</td>\n",
       "      <td>0.539727</td>\n",
       "      <td>2.300454</td>\n",
       "      <td>...</td>\n",
       "      <td>2.148663</td>\n",
       "      <td>1.527512</td>\n",
       "      <td>2.783564</td>\n",
       "      <td>1.077207</td>\n",
       "      <td>0.796285</td>\n",
       "      <td>0.067275</td>\n",
       "      <td>3.076710</td>\n",
       "      <td>1.940927</td>\n",
       "      <td>3.520549</td>\n",
       "      <td>3.536350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>53.180933</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.354341</td>\n",
       "      <td>4.060417</td>\n",
       "      <td>2.845522</td>\n",
       "      <td>0.006099</td>\n",
       "      <td>1.893809</td>\n",
       "      <td>-12.087737</td>\n",
       "      <td>-0.747189</td>\n",
       "      <td>-3.676577</td>\n",
       "      <td>...</td>\n",
       "      <td>2.383930</td>\n",
       "      <td>1.691481</td>\n",
       "      <td>2.777415</td>\n",
       "      <td>0.837353</td>\n",
       "      <td>0.561882</td>\n",
       "      <td>-0.126507</td>\n",
       "      <td>2.885576</td>\n",
       "      <td>-0.047511</td>\n",
       "      <td>2.541467</td>\n",
       "      <td>-0.626764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>51.421606</td>\n",
       "      <td>0.791680</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.856965</td>\n",
       "      <td>2.288246</td>\n",
       "      <td>3.609468</td>\n",
       "      <td>1.425411</td>\n",
       "      <td>4.635319</td>\n",
       "      <td>0.408250</td>\n",
       "      <td>1.685438</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.473059</td>\n",
       "      <td>-0.546303</td>\n",
       "      <td>-1.705125</td>\n",
       "      <td>-1.519769</td>\n",
       "      <td>-0.619609</td>\n",
       "      <td>0.040742</td>\n",
       "      <td>-3.481821</td>\n",
       "      <td>1.319186</td>\n",
       "      <td>1.847728</td>\n",
       "      <td>-3.053215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>49.824302</td>\n",
       "      <td>1.491654</td>\n",
       "      <td>0.538566</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>7.432408</td>\n",
       "      <td>4.386795</td>\n",
       "      <td>-0.123604</td>\n",
       "      <td>3.914069</td>\n",
       "      <td>0.519823</td>\n",
       "      <td>2.080403</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034966</td>\n",
       "      <td>0.638460</td>\n",
       "      <td>-3.443210</td>\n",
       "      <td>-0.390524</td>\n",
       "      <td>-0.420669</td>\n",
       "      <td>0.074928</td>\n",
       "      <td>1.246837</td>\n",
       "      <td>1.924739</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>8.961202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.433291</td>\n",
       "      <td>1.148932</td>\n",
       "      <td>1.580569</td>\n",
       "      <td>8.168915</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>4.042226</td>\n",
       "      <td>0.881074</td>\n",
       "      <td>5.796901</td>\n",
       "      <td>0.512326</td>\n",
       "      <td>1.202232</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.989099</td>\n",
       "      <td>-1.650315</td>\n",
       "      <td>2.371325</td>\n",
       "      <td>0.428106</td>\n",
       "      <td>0.210237</td>\n",
       "      <td>0.036830</td>\n",
       "      <td>1.159854</td>\n",
       "      <td>0.772850</td>\n",
       "      <td>-0.618921</td>\n",
       "      <td>7.329512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>57.716678</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>1.906321</td>\n",
       "      <td>3.686586</td>\n",
       "      <td>3.090742</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.917215</td>\n",
       "      <td>2.315914</td>\n",
       "      <td>0.185839</td>\n",
       "      <td>-0.805507</td>\n",
       "      <td>...</td>\n",
       "      <td>1.635051</td>\n",
       "      <td>1.330699</td>\n",
       "      <td>2.402010</td>\n",
       "      <td>0.877367</td>\n",
       "      <td>0.515841</td>\n",
       "      <td>-0.007474</td>\n",
       "      <td>2.496398</td>\n",
       "      <td>-1.722324</td>\n",
       "      <td>2.874049</td>\n",
       "      <td>-2.328093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>51.968471</td>\n",
       "      <td>1.184098</td>\n",
       "      <td>1.524649</td>\n",
       "      <td>-0.210371</td>\n",
       "      <td>1.364367</td>\n",
       "      <td>3.882824</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>21.287949</td>\n",
       "      <td>1.611695</td>\n",
       "      <td>2.570349</td>\n",
       "      <td>...</td>\n",
       "      <td>1.297091</td>\n",
       "      <td>3.229753</td>\n",
       "      <td>-2.253411</td>\n",
       "      <td>0.171996</td>\n",
       "      <td>0.363307</td>\n",
       "      <td>0.087065</td>\n",
       "      <td>-4.082982</td>\n",
       "      <td>1.748596</td>\n",
       "      <td>-3.018814</td>\n",
       "      <td>3.106333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>48.628197</td>\n",
       "      <td>-2.472425</td>\n",
       "      <td>1.621945</td>\n",
       "      <td>2.179260</td>\n",
       "      <td>2.936579</td>\n",
       "      <td>1.534356</td>\n",
       "      <td>6.964029</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.289976</td>\n",
       "      <td>-0.561708</td>\n",
       "      <td>...</td>\n",
       "      <td>6.407939</td>\n",
       "      <td>4.430986</td>\n",
       "      <td>2.670436</td>\n",
       "      <td>0.047306</td>\n",
       "      <td>0.628622</td>\n",
       "      <td>-0.072960</td>\n",
       "      <td>2.422562</td>\n",
       "      <td>0.486252</td>\n",
       "      <td>1.032803</td>\n",
       "      <td>-0.177176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>52.513452</td>\n",
       "      <td>-2.053170</td>\n",
       "      <td>1.919108</td>\n",
       "      <td>3.888235</td>\n",
       "      <td>3.486654</td>\n",
       "      <td>1.654082</td>\n",
       "      <td>7.083144</td>\n",
       "      <td>3.895635</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-2.810074</td>\n",
       "      <td>...</td>\n",
       "      <td>6.477342</td>\n",
       "      <td>4.492246</td>\n",
       "      <td>2.469066</td>\n",
       "      <td>0.998333</td>\n",
       "      <td>0.605792</td>\n",
       "      <td>-0.684361</td>\n",
       "      <td>2.763536</td>\n",
       "      <td>0.916014</td>\n",
       "      <td>2.921425</td>\n",
       "      <td>0.490450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>54.329998</td>\n",
       "      <td>-2.452269</td>\n",
       "      <td>1.923161</td>\n",
       "      <td>3.777238</td>\n",
       "      <td>1.986006</td>\n",
       "      <td>-1.740280</td>\n",
       "      <td>2.741986</td>\n",
       "      <td>-1.831712</td>\n",
       "      <td>-0.682099</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.391002</td>\n",
       "      <td>1.612821</td>\n",
       "      <td>2.597009</td>\n",
       "      <td>0.847985</td>\n",
       "      <td>0.647350</td>\n",
       "      <td>-0.136259</td>\n",
       "      <td>2.469188</td>\n",
       "      <td>-1.024743</td>\n",
       "      <td>2.764528</td>\n",
       "      <td>0.428071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>55.295802</td>\n",
       "      <td>1.485901</td>\n",
       "      <td>1.815210</td>\n",
       "      <td>0.533294</td>\n",
       "      <td>0.345597</td>\n",
       "      <td>-1.198327</td>\n",
       "      <td>-2.681873</td>\n",
       "      <td>6.569168</td>\n",
       "      <td>0.613734</td>\n",
       "      <td>14.722927</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.934305</td>\n",
       "      <td>-0.625794</td>\n",
       "      <td>0.787173</td>\n",
       "      <td>0.559059</td>\n",
       "      <td>-0.410455</td>\n",
       "      <td>0.073860</td>\n",
       "      <td>1.865816</td>\n",
       "      <td>-0.626123</td>\n",
       "      <td>1.321018</td>\n",
       "      <td>2.583729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>52.028173</td>\n",
       "      <td>0.589745</td>\n",
       "      <td>1.569865</td>\n",
       "      <td>2.493923</td>\n",
       "      <td>1.141319</td>\n",
       "      <td>3.457168</td>\n",
       "      <td>1.624140</td>\n",
       "      <td>-7.206899</td>\n",
       "      <td>0.287948</td>\n",
       "      <td>-0.013852</td>\n",
       "      <td>...</td>\n",
       "      <td>2.356747</td>\n",
       "      <td>0.959327</td>\n",
       "      <td>9.749976</td>\n",
       "      <td>0.042165</td>\n",
       "      <td>0.709810</td>\n",
       "      <td>0.002188</td>\n",
       "      <td>2.556394</td>\n",
       "      <td>0.350391</td>\n",
       "      <td>-0.950652</td>\n",
       "      <td>2.209296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>52.898183</td>\n",
       "      <td>1.194825</td>\n",
       "      <td>0.842562</td>\n",
       "      <td>0.531302</td>\n",
       "      <td>-4.349668</td>\n",
       "      <td>1.874730</td>\n",
       "      <td>1.393365</td>\n",
       "      <td>6.131868</td>\n",
       "      <td>0.462271</td>\n",
       "      <td>1.933443</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.291975</td>\n",
       "      <td>0.835987</td>\n",
       "      <td>8.605726</td>\n",
       "      <td>0.280073</td>\n",
       "      <td>-0.044768</td>\n",
       "      <td>0.062407</td>\n",
       "      <td>1.506251</td>\n",
       "      <td>0.905739</td>\n",
       "      <td>-2.622559</td>\n",
       "      <td>3.081377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>56.830151</td>\n",
       "      <td>1.756590</td>\n",
       "      <td>1.906735</td>\n",
       "      <td>0.712762</td>\n",
       "      <td>2.811744</td>\n",
       "      <td>4.431314</td>\n",
       "      <td>-0.811385</td>\n",
       "      <td>4.571367</td>\n",
       "      <td>0.546801</td>\n",
       "      <td>2.007772</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654641</td>\n",
       "      <td>-0.021250</td>\n",
       "      <td>-1.791166</td>\n",
       "      <td>7.979310</td>\n",
       "      <td>6.744125</td>\n",
       "      <td>0.629570</td>\n",
       "      <td>-2.432373</td>\n",
       "      <td>2.080844</td>\n",
       "      <td>-0.326978</td>\n",
       "      <td>3.448601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>66.562963</td>\n",
       "      <td>-2.264634</td>\n",
       "      <td>2.156221</td>\n",
       "      <td>4.447625</td>\n",
       "      <td>2.454328</td>\n",
       "      <td>2.497236</td>\n",
       "      <td>2.398839</td>\n",
       "      <td>1.661356</td>\n",
       "      <td>-0.663084</td>\n",
       "      <td>-5.696310</td>\n",
       "      <td>...</td>\n",
       "      <td>2.076546</td>\n",
       "      <td>1.549745</td>\n",
       "      <td>2.775633</td>\n",
       "      <td>1.085098</td>\n",
       "      <td>-0.264701</td>\n",
       "      <td>-0.019287</td>\n",
       "      <td>2.950808</td>\n",
       "      <td>-3.515183</td>\n",
       "      <td>3.314542</td>\n",
       "      <td>-1.256208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>51.708795</td>\n",
       "      <td>1.384584</td>\n",
       "      <td>1.508039</td>\n",
       "      <td>-2.469886</td>\n",
       "      <td>2.557696</td>\n",
       "      <td>4.277594</td>\n",
       "      <td>0.075207</td>\n",
       "      <td>6.219879</td>\n",
       "      <td>0.476608</td>\n",
       "      <td>1.949793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.807351</td>\n",
       "      <td>-2.083090</td>\n",
       "      <td>0.771624</td>\n",
       "      <td>-0.537777</td>\n",
       "      <td>0.231152</td>\n",
       "      <td>0.070734</td>\n",
       "      <td>1.213636</td>\n",
       "      <td>7.108456</td>\n",
       "      <td>12.801563</td>\n",
       "      <td>2.076559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>68.915991</td>\n",
       "      <td>1.329491</td>\n",
       "      <td>2.229641</td>\n",
       "      <td>1.227702</td>\n",
       "      <td>-1.320934</td>\n",
       "      <td>2.344814</td>\n",
       "      <td>-2.496314</td>\n",
       "      <td>7.807237</td>\n",
       "      <td>0.658614</td>\n",
       "      <td>1.634202</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.101321</td>\n",
       "      <td>0.905025</td>\n",
       "      <td>2.678088</td>\n",
       "      <td>0.684872</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.069717</td>\n",
       "      <td>2.297520</td>\n",
       "      <td>0.347505</td>\n",
       "      <td>0.030739</td>\n",
       "      <td>1.484063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>53.886597</td>\n",
       "      <td>3.031435</td>\n",
       "      <td>0.395813</td>\n",
       "      <td>-3.215416</td>\n",
       "      <td>1.119009</td>\n",
       "      <td>4.017008</td>\n",
       "      <td>-0.842826</td>\n",
       "      <td>5.704076</td>\n",
       "      <td>0.509593</td>\n",
       "      <td>-0.631209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842210</td>\n",
       "      <td>-1.438608</td>\n",
       "      <td>-2.704378</td>\n",
       "      <td>0.659476</td>\n",
       "      <td>-0.464026</td>\n",
       "      <td>0.061241</td>\n",
       "      <td>1.370742</td>\n",
       "      <td>1.703044</td>\n",
       "      <td>1.462572</td>\n",
       "      <td>3.445279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>52.885917</td>\n",
       "      <td>3.732771</td>\n",
       "      <td>-1.949905</td>\n",
       "      <td>4.024644</td>\n",
       "      <td>0.330587</td>\n",
       "      <td>3.033824</td>\n",
       "      <td>1.027155</td>\n",
       "      <td>1.953662</td>\n",
       "      <td>0.038697</td>\n",
       "      <td>-0.994116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486488</td>\n",
       "      <td>1.086442</td>\n",
       "      <td>2.749728</td>\n",
       "      <td>0.855721</td>\n",
       "      <td>-0.648012</td>\n",
       "      <td>0.044429</td>\n",
       "      <td>2.501370</td>\n",
       "      <td>-2.404630</td>\n",
       "      <td>-0.118445</td>\n",
       "      <td>-2.991000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>53.448369</td>\n",
       "      <td>5.074931</td>\n",
       "      <td>-4.019452</td>\n",
       "      <td>-0.211689</td>\n",
       "      <td>2.365565</td>\n",
       "      <td>4.372612</td>\n",
       "      <td>-0.092745</td>\n",
       "      <td>-1.502968</td>\n",
       "      <td>0.570804</td>\n",
       "      <td>2.170765</td>\n",
       "      <td>...</td>\n",
       "      <td>1.584823</td>\n",
       "      <td>0.317658</td>\n",
       "      <td>-0.521907</td>\n",
       "      <td>-1.860050</td>\n",
       "      <td>0.590259</td>\n",
       "      <td>0.080381</td>\n",
       "      <td>-4.759951</td>\n",
       "      <td>1.720852</td>\n",
       "      <td>1.886274</td>\n",
       "      <td>3.264644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>50.593025</td>\n",
       "      <td>5.375629</td>\n",
       "      <td>1.685183</td>\n",
       "      <td>-4.060990</td>\n",
       "      <td>2.828282</td>\n",
       "      <td>4.485684</td>\n",
       "      <td>1.156826</td>\n",
       "      <td>6.137846</td>\n",
       "      <td>0.550652</td>\n",
       "      <td>2.382588</td>\n",
       "      <td>...</td>\n",
       "      <td>1.089178</td>\n",
       "      <td>0.903423</td>\n",
       "      <td>-1.745668</td>\n",
       "      <td>0.544869</td>\n",
       "      <td>0.562568</td>\n",
       "      <td>0.093374</td>\n",
       "      <td>-0.271993</td>\n",
       "      <td>2.110353</td>\n",
       "      <td>2.791984</td>\n",
       "      <td>3.596427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>52.277363</td>\n",
       "      <td>3.776515</td>\n",
       "      <td>1.892169</td>\n",
       "      <td>4.095665</td>\n",
       "      <td>3.140298</td>\n",
       "      <td>-5.779453</td>\n",
       "      <td>0.235285</td>\n",
       "      <td>2.400160</td>\n",
       "      <td>-1.317916</td>\n",
       "      <td>-3.040826</td>\n",
       "      <td>...</td>\n",
       "      <td>1.720204</td>\n",
       "      <td>1.377164</td>\n",
       "      <td>1.169115</td>\n",
       "      <td>0.852753</td>\n",
       "      <td>0.539262</td>\n",
       "      <td>-0.200154</td>\n",
       "      <td>0.623352</td>\n",
       "      <td>0.746665</td>\n",
       "      <td>2.399749</td>\n",
       "      <td>-3.740716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>53.010312</td>\n",
       "      <td>0.413876</td>\n",
       "      <td>0.917966</td>\n",
       "      <td>1.596009</td>\n",
       "      <td>2.554417</td>\n",
       "      <td>3.637564</td>\n",
       "      <td>0.767950</td>\n",
       "      <td>4.293363</td>\n",
       "      <td>0.567574</td>\n",
       "      <td>1.794250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.467665</td>\n",
       "      <td>0.062960</td>\n",
       "      <td>1.228816</td>\n",
       "      <td>0.669416</td>\n",
       "      <td>0.451662</td>\n",
       "      <td>0.059052</td>\n",
       "      <td>2.061731</td>\n",
       "      <td>-0.645002</td>\n",
       "      <td>1.212264</td>\n",
       "      <td>0.086423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>52.864049</td>\n",
       "      <td>-0.487618</td>\n",
       "      <td>1.895667</td>\n",
       "      <td>3.287847</td>\n",
       "      <td>1.349909</td>\n",
       "      <td>-2.451785</td>\n",
       "      <td>2.029470</td>\n",
       "      <td>0.084459</td>\n",
       "      <td>-0.269729</td>\n",
       "      <td>-2.808796</td>\n",
       "      <td>...</td>\n",
       "      <td>1.055994</td>\n",
       "      <td>1.314373</td>\n",
       "      <td>2.238085</td>\n",
       "      <td>0.779216</td>\n",
       "      <td>0.486275</td>\n",
       "      <td>0.055807</td>\n",
       "      <td>2.316410</td>\n",
       "      <td>-2.281517</td>\n",
       "      <td>2.708853</td>\n",
       "      <td>1.944607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>50.747953</td>\n",
       "      <td>0.454104</td>\n",
       "      <td>0.878818</td>\n",
       "      <td>3.006869</td>\n",
       "      <td>2.862650</td>\n",
       "      <td>3.259771</td>\n",
       "      <td>1.850311</td>\n",
       "      <td>1.633481</td>\n",
       "      <td>-3.732136</td>\n",
       "      <td>0.976438</td>\n",
       "      <td>...</td>\n",
       "      <td>1.886966</td>\n",
       "      <td>0.728545</td>\n",
       "      <td>1.291091</td>\n",
       "      <td>0.722780</td>\n",
       "      <td>0.509025</td>\n",
       "      <td>-0.480275</td>\n",
       "      <td>2.099093</td>\n",
       "      <td>1.534183</td>\n",
       "      <td>2.080891</td>\n",
       "      <td>0.790241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>52.667726</td>\n",
       "      <td>0.303147</td>\n",
       "      <td>1.746272</td>\n",
       "      <td>3.700743</td>\n",
       "      <td>3.321373</td>\n",
       "      <td>-10.338787</td>\n",
       "      <td>2.265780</td>\n",
       "      <td>0.061687</td>\n",
       "      <td>-4.191252</td>\n",
       "      <td>-0.128265</td>\n",
       "      <td>...</td>\n",
       "      <td>2.067161</td>\n",
       "      <td>1.553183</td>\n",
       "      <td>2.204225</td>\n",
       "      <td>0.863411</td>\n",
       "      <td>0.532897</td>\n",
       "      <td>-0.512080</td>\n",
       "      <td>2.322770</td>\n",
       "      <td>0.732353</td>\n",
       "      <td>2.673057</td>\n",
       "      <td>-1.792186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>64.747912</td>\n",
       "      <td>2.058784</td>\n",
       "      <td>1.721339</td>\n",
       "      <td>-0.146079</td>\n",
       "      <td>2.822534</td>\n",
       "      <td>4.890954</td>\n",
       "      <td>1.375058</td>\n",
       "      <td>6.719665</td>\n",
       "      <td>0.569591</td>\n",
       "      <td>2.270629</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.673179</td>\n",
       "      <td>1.198472</td>\n",
       "      <td>-0.141613</td>\n",
       "      <td>0.716331</td>\n",
       "      <td>0.651913</td>\n",
       "      <td>0.094512</td>\n",
       "      <td>-0.994042</td>\n",
       "      <td>2.034684</td>\n",
       "      <td>2.873946</td>\n",
       "      <td>3.507110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>50.136507</td>\n",
       "      <td>-0.212140</td>\n",
       "      <td>6.576705</td>\n",
       "      <td>3.185644</td>\n",
       "      <td>-4.673015</td>\n",
       "      <td>3.581127</td>\n",
       "      <td>0.703175</td>\n",
       "      <td>5.833183</td>\n",
       "      <td>0.462788</td>\n",
       "      <td>1.699295</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296375</td>\n",
       "      <td>-4.347679</td>\n",
       "      <td>2.104673</td>\n",
       "      <td>0.788572</td>\n",
       "      <td>-0.260771</td>\n",
       "      <td>0.020940</td>\n",
       "      <td>2.448961</td>\n",
       "      <td>0.906926</td>\n",
       "      <td>2.836441</td>\n",
       "      <td>1.594644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>51.510376</td>\n",
       "      <td>-0.271011</td>\n",
       "      <td>6.709294</td>\n",
       "      <td>2.999950</td>\n",
       "      <td>0.495011</td>\n",
       "      <td>3.565530</td>\n",
       "      <td>0.621730</td>\n",
       "      <td>4.978713</td>\n",
       "      <td>0.438760</td>\n",
       "      <td>0.547619</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.536385</td>\n",
       "      <td>-0.874064</td>\n",
       "      <td>2.387710</td>\n",
       "      <td>0.772728</td>\n",
       "      <td>0.113001</td>\n",
       "      <td>0.018365</td>\n",
       "      <td>2.472354</td>\n",
       "      <td>0.758419</td>\n",
       "      <td>2.506047</td>\n",
       "      <td>2.975718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>53.286296</td>\n",
       "      <td>-1.616961</td>\n",
       "      <td>7.486161</td>\n",
       "      <td>2.458333</td>\n",
       "      <td>3.294148</td>\n",
       "      <td>-0.956295</td>\n",
       "      <td>1.340333</td>\n",
       "      <td>0.425148</td>\n",
       "      <td>0.159508</td>\n",
       "      <td>-1.640802</td>\n",
       "      <td>...</td>\n",
       "      <td>2.053247</td>\n",
       "      <td>1.249648</td>\n",
       "      <td>2.971834</td>\n",
       "      <td>0.877932</td>\n",
       "      <td>0.582071</td>\n",
       "      <td>0.015990</td>\n",
       "      <td>2.862154</td>\n",
       "      <td>-0.373227</td>\n",
       "      <td>1.198608</td>\n",
       "      <td>-1.297905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>50.916829</td>\n",
       "      <td>1.991164</td>\n",
       "      <td>6.559362</td>\n",
       "      <td>-2.825983</td>\n",
       "      <td>2.409593</td>\n",
       "      <td>4.163363</td>\n",
       "      <td>-0.625827</td>\n",
       "      <td>5.537006</td>\n",
       "      <td>0.462866</td>\n",
       "      <td>1.979861</td>\n",
       "      <td>...</td>\n",
       "      <td>1.010211</td>\n",
       "      <td>1.401139</td>\n",
       "      <td>-0.808398</td>\n",
       "      <td>0.502393</td>\n",
       "      <td>0.622833</td>\n",
       "      <td>0.083302</td>\n",
       "      <td>-1.681451</td>\n",
       "      <td>2.067350</td>\n",
       "      <td>2.446416</td>\n",
       "      <td>3.533270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>51.158051</td>\n",
       "      <td>1.124937</td>\n",
       "      <td>-0.634859</td>\n",
       "      <td>7.971992</td>\n",
       "      <td>0.775526</td>\n",
       "      <td>4.286860</td>\n",
       "      <td>1.204247</td>\n",
       "      <td>-0.155830</td>\n",
       "      <td>0.486708</td>\n",
       "      <td>1.652307</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482741</td>\n",
       "      <td>-0.335333</td>\n",
       "      <td>2.298961</td>\n",
       "      <td>-0.418416</td>\n",
       "      <td>0.695871</td>\n",
       "      <td>0.053606</td>\n",
       "      <td>0.977654</td>\n",
       "      <td>0.863643</td>\n",
       "      <td>-0.641028</td>\n",
       "      <td>9.111635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>50.092737</td>\n",
       "      <td>-1.807326</td>\n",
       "      <td>2.038359</td>\n",
       "      <td>9.664832</td>\n",
       "      <td>9.139645</td>\n",
       "      <td>-5.105107</td>\n",
       "      <td>2.011236</td>\n",
       "      <td>1.189647</td>\n",
       "      <td>-1.217840</td>\n",
       "      <td>-2.960091</td>\n",
       "      <td>...</td>\n",
       "      <td>2.072154</td>\n",
       "      <td>1.486369</td>\n",
       "      <td>2.262214</td>\n",
       "      <td>1.002165</td>\n",
       "      <td>-0.243064</td>\n",
       "      <td>-0.108791</td>\n",
       "      <td>2.363173</td>\n",
       "      <td>0.177162</td>\n",
       "      <td>2.110009</td>\n",
       "      <td>0.355404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>60.333037</td>\n",
       "      <td>1.413267</td>\n",
       "      <td>1.897610</td>\n",
       "      <td>-4.741128</td>\n",
       "      <td>3.086799</td>\n",
       "      <td>3.878416</td>\n",
       "      <td>1.514544</td>\n",
       "      <td>5.382004</td>\n",
       "      <td>0.375063</td>\n",
       "      <td>0.270233</td>\n",
       "      <td>...</td>\n",
       "      <td>2.071180</td>\n",
       "      <td>-0.836427</td>\n",
       "      <td>-1.590378</td>\n",
       "      <td>0.076255</td>\n",
       "      <td>-0.887419</td>\n",
       "      <td>0.064793</td>\n",
       "      <td>2.165313</td>\n",
       "      <td>1.085485</td>\n",
       "      <td>0.029383</td>\n",
       "      <td>3.358195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>59.994109</td>\n",
       "      <td>-0.659977</td>\n",
       "      <td>1.976911</td>\n",
       "      <td>1.840698</td>\n",
       "      <td>-0.778493</td>\n",
       "      <td>3.764370</td>\n",
       "      <td>-4.118527</td>\n",
       "      <td>4.729100</td>\n",
       "      <td>0.608698</td>\n",
       "      <td>2.727511</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.663175</td>\n",
       "      <td>1.061451</td>\n",
       "      <td>2.611332</td>\n",
       "      <td>0.653699</td>\n",
       "      <td>0.130155</td>\n",
       "      <td>0.058901</td>\n",
       "      <td>1.361742</td>\n",
       "      <td>0.506419</td>\n",
       "      <td>1.515005</td>\n",
       "      <td>1.529610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>52.409725</td>\n",
       "      <td>1.642235</td>\n",
       "      <td>-0.557488</td>\n",
       "      <td>-0.065567</td>\n",
       "      <td>-1.687522</td>\n",
       "      <td>3.648368</td>\n",
       "      <td>1.429094</td>\n",
       "      <td>21.581525</td>\n",
       "      <td>1.623843</td>\n",
       "      <td>2.469433</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>2.974075</td>\n",
       "      <td>-0.688980</td>\n",
       "      <td>0.064901</td>\n",
       "      <td>-0.120769</td>\n",
       "      <td>0.084959</td>\n",
       "      <td>0.194510</td>\n",
       "      <td>0.699994</td>\n",
       "      <td>2.228277</td>\n",
       "      <td>0.001162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>51.669004</td>\n",
       "      <td>1.615887</td>\n",
       "      <td>-0.892804</td>\n",
       "      <td>1.660276</td>\n",
       "      <td>-3.904617</td>\n",
       "      <td>4.117648</td>\n",
       "      <td>4.934711</td>\n",
       "      <td>20.695028</td>\n",
       "      <td>1.561754</td>\n",
       "      <td>2.309965</td>\n",
       "      <td>...</td>\n",
       "      <td>4.124335</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.493900</td>\n",
       "      <td>-0.073104</td>\n",
       "      <td>-0.177210</td>\n",
       "      <td>0.084496</td>\n",
       "      <td>0.856522</td>\n",
       "      <td>1.765301</td>\n",
       "      <td>2.198163</td>\n",
       "      <td>3.318363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>52.287458</td>\n",
       "      <td>1.473450</td>\n",
       "      <td>-1.547495</td>\n",
       "      <td>-4.972338</td>\n",
       "      <td>3.115684</td>\n",
       "      <td>4.127575</td>\n",
       "      <td>-1.911981</td>\n",
       "      <td>6.926260</td>\n",
       "      <td>0.476686</td>\n",
       "      <td>2.065588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.530590</td>\n",
       "      <td>0.274277</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-0.288603</td>\n",
       "      <td>-0.478800</td>\n",
       "      <td>0.076330</td>\n",
       "      <td>-4.423463</td>\n",
       "      <td>1.862882</td>\n",
       "      <td>-1.284318</td>\n",
       "      <td>2.823771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>57.655512</td>\n",
       "      <td>1.265752</td>\n",
       "      <td>-3.930032</td>\n",
       "      <td>-1.606906</td>\n",
       "      <td>1.602724</td>\n",
       "      <td>4.295828</td>\n",
       "      <td>0.415823</td>\n",
       "      <td>0.349604</td>\n",
       "      <td>0.549188</td>\n",
       "      <td>1.921781</td>\n",
       "      <td>...</td>\n",
       "      <td>0.142414</td>\n",
       "      <td>-0.115674</td>\n",
       "      <td>-0.822330</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>6.523302</td>\n",
       "      <td>0.623222</td>\n",
       "      <td>-3.071677</td>\n",
       "      <td>1.821536</td>\n",
       "      <td>1.375722</td>\n",
       "      <td>3.421212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>56.498517</td>\n",
       "      <td>1.125931</td>\n",
       "      <td>-2.124040</td>\n",
       "      <td>-2.294613</td>\n",
       "      <td>1.043380</td>\n",
       "      <td>3.348172</td>\n",
       "      <td>1.164364</td>\n",
       "      <td>6.158542</td>\n",
       "      <td>0.441769</td>\n",
       "      <td>1.944830</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.351303</td>\n",
       "      <td>-0.371716</td>\n",
       "      <td>-1.808533</td>\n",
       "      <td>8.647570</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.600249</td>\n",
       "      <td>1.953576</td>\n",
       "      <td>1.175175</td>\n",
       "      <td>2.568826</td>\n",
       "      <td>-3.975059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>55.262676</td>\n",
       "      <td>-2.934868</td>\n",
       "      <td>1.616943</td>\n",
       "      <td>4.731762</td>\n",
       "      <td>2.116149</td>\n",
       "      <td>-0.561637</td>\n",
       "      <td>3.230466</td>\n",
       "      <td>-8.275190</td>\n",
       "      <td>-5.777834</td>\n",
       "      <td>-4.739304</td>\n",
       "      <td>...</td>\n",
       "      <td>2.861147</td>\n",
       "      <td>2.051945</td>\n",
       "      <td>3.337929</td>\n",
       "      <td>9.564820</td>\n",
       "      <td>6.949269</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>3.447795</td>\n",
       "      <td>0.910772</td>\n",
       "      <td>2.956489</td>\n",
       "      <td>0.408603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>63.940202</td>\n",
       "      <td>1.693629</td>\n",
       "      <td>-3.495993</td>\n",
       "      <td>1.992040</td>\n",
       "      <td>1.685997</td>\n",
       "      <td>4.745971</td>\n",
       "      <td>-3.832762</td>\n",
       "      <td>6.951564</td>\n",
       "      <td>0.590277</td>\n",
       "      <td>2.172779</td>\n",
       "      <td>...</td>\n",
       "      <td>0.165724</td>\n",
       "      <td>0.526236</td>\n",
       "      <td>-4.893881</td>\n",
       "      <td>-1.192672</td>\n",
       "      <td>0.572202</td>\n",
       "      <td>0.087227</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>2.038463</td>\n",
       "      <td>-1.729899</td>\n",
       "      <td>3.061210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>53.477228</td>\n",
       "      <td>-0.036970</td>\n",
       "      <td>1.756072</td>\n",
       "      <td>4.076921</td>\n",
       "      <td>1.489433</td>\n",
       "      <td>-4.341085</td>\n",
       "      <td>2.176186</td>\n",
       "      <td>1.849871</td>\n",
       "      <td>0.259397</td>\n",
       "      <td>-1.195496</td>\n",
       "      <td>...</td>\n",
       "      <td>0.790697</td>\n",
       "      <td>1.437914</td>\n",
       "      <td>2.732428</td>\n",
       "      <td>0.937682</td>\n",
       "      <td>0.456345</td>\n",
       "      <td>0.030549</td>\n",
       "      <td>2.702558</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>9.472053</td>\n",
       "      <td>-1.501538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>53.978613</td>\n",
       "      <td>1.100510</td>\n",
       "      <td>1.368756</td>\n",
       "      <td>0.026604</td>\n",
       "      <td>-0.663762</td>\n",
       "      <td>4.031153</td>\n",
       "      <td>-2.090714</td>\n",
       "      <td>2.186498</td>\n",
       "      <td>0.460373</td>\n",
       "      <td>1.794759</td>\n",
       "      <td>...</td>\n",
       "      <td>1.400673</td>\n",
       "      <td>0.996382</td>\n",
       "      <td>-1.048305</td>\n",
       "      <td>0.394094</td>\n",
       "      <td>0.555108</td>\n",
       "      <td>0.055184</td>\n",
       "      <td>-1.276276</td>\n",
       "      <td>5.271034</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>2.156211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>49.390801</td>\n",
       "      <td>-0.247226</td>\n",
       "      <td>-2.060274</td>\n",
       "      <td>9.621836</td>\n",
       "      <td>7.160312</td>\n",
       "      <td>-2.974507</td>\n",
       "      <td>1.959683</td>\n",
       "      <td>-0.341677</td>\n",
       "      <td>0.070403</td>\n",
       "      <td>0.253151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>1.370154</td>\n",
       "      <td>2.099539</td>\n",
       "      <td>0.892748</td>\n",
       "      <td>-0.782467</td>\n",
       "      <td>0.006947</td>\n",
       "      <td>2.057295</td>\n",
       "      <td>-0.761145</td>\n",
       "      <td>1.964133</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Diversion ratio wrt. product          0           1           2           3   \\\n",
       "Diversion ratio of product                                                     \n",
       "0                            -100.000000    1.501946    2.484405    3.830382   \n",
       "1                              53.180933 -100.000000    1.354341    4.060417   \n",
       "2                              51.421606    0.791680 -100.000000    0.856965   \n",
       "3                              49.824302    1.491654    0.538566 -100.000000   \n",
       "4                              50.433291    1.148932    1.580569    8.168915   \n",
       "5                              57.716678    0.001883    1.906321    3.686586   \n",
       "6                              51.968471    1.184098    1.524649   -0.210371   \n",
       "7                              48.628197   -2.472425    1.621945    2.179260   \n",
       "8                              52.513452   -2.053170    1.919108    3.888235   \n",
       "9                              54.329998   -2.452269    1.923161    3.777238   \n",
       "10                             55.295802    1.485901    1.815210    0.533294   \n",
       "11                             52.028173    0.589745    1.569865    2.493923   \n",
       "12                             52.898183    1.194825    0.842562    0.531302   \n",
       "13                             56.830151    1.756590    1.906735    0.712762   \n",
       "14                             66.562963   -2.264634    2.156221    4.447625   \n",
       "15                             51.708795    1.384584    1.508039   -2.469886   \n",
       "16                             68.915991    1.329491    2.229641    1.227702   \n",
       "17                             53.886597    3.031435    0.395813   -3.215416   \n",
       "18                             52.885917    3.732771   -1.949905    4.024644   \n",
       "19                             53.448369    5.074931   -4.019452   -0.211689   \n",
       "20                             50.593025    5.375629    1.685183   -4.060990   \n",
       "21                             52.277363    3.776515    1.892169    4.095665   \n",
       "22                             53.010312    0.413876    0.917966    1.596009   \n",
       "23                             52.864049   -0.487618    1.895667    3.287847   \n",
       "24                             50.747953    0.454104    0.878818    3.006869   \n",
       "25                             52.667726    0.303147    1.746272    3.700743   \n",
       "26                             64.747912    2.058784    1.721339   -0.146079   \n",
       "27                             50.136507   -0.212140    6.576705    3.185644   \n",
       "28                             51.510376   -0.271011    6.709294    2.999950   \n",
       "29                             53.286296   -1.616961    7.486161    2.458333   \n",
       "30                             50.916829    1.991164    6.559362   -2.825983   \n",
       "31                             51.158051    1.124937   -0.634859    7.971992   \n",
       "32                             50.092737   -1.807326    2.038359    9.664832   \n",
       "33                             60.333037    1.413267    1.897610   -4.741128   \n",
       "34                             59.994109   -0.659977    1.976911    1.840698   \n",
       "35                             52.409725    1.642235   -0.557488   -0.065567   \n",
       "36                             51.669004    1.615887   -0.892804    1.660276   \n",
       "37                             52.287458    1.473450   -1.547495   -4.972338   \n",
       "38                             57.655512    1.265752   -3.930032   -1.606906   \n",
       "39                             56.498517    1.125931   -2.124040   -2.294613   \n",
       "40                             55.262676   -2.934868    1.616943    4.731762   \n",
       "41                             63.940202    1.693629   -3.495993    1.992040   \n",
       "42                             53.477228   -0.036970    1.756072    4.076921   \n",
       "43                             53.978613    1.100510    1.368756    0.026604   \n",
       "44                             49.390801   -0.247226   -2.060274    9.621836   \n",
       "\n",
       "Diversion ratio wrt. product          4           5           6           7   \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               3.527633    5.279894    2.347399    6.714418   \n",
       "1                               2.845522    0.006099    1.893809  -12.087737   \n",
       "2                               2.288246    3.609468    1.425411    4.635319   \n",
       "3                               7.432408    4.386795   -0.123604    3.914069   \n",
       "4                            -100.000000    4.042226    0.881074    5.796901   \n",
       "5                               3.090742 -100.000000    1.917215    2.315914   \n",
       "6                               1.364367    3.882824 -100.000000   21.287949   \n",
       "7                               2.936579    1.534356    6.964029 -100.000000   \n",
       "8                               3.486654    1.654082    7.083144    3.895635   \n",
       "9                               1.986006   -1.740280    2.741986   -1.831712   \n",
       "10                              0.345597   -1.198327   -2.681873    6.569168   \n",
       "11                              1.141319    3.457168    1.624140   -7.206899   \n",
       "12                             -4.349668    1.874730    1.393365    6.131868   \n",
       "13                              2.811744    4.431314   -0.811385    4.571367   \n",
       "14                              2.454328    2.497236    2.398839    1.661356   \n",
       "15                              2.557696    4.277594    0.075207    6.219879   \n",
       "16                             -1.320934    2.344814   -2.496314    7.807237   \n",
       "17                              1.119009    4.017008   -0.842826    5.704076   \n",
       "18                              0.330587    3.033824    1.027155    1.953662   \n",
       "19                              2.365565    4.372612   -0.092745   -1.502968   \n",
       "20                              2.828282    4.485684    1.156826    6.137846   \n",
       "21                              3.140298   -5.779453    0.235285    2.400160   \n",
       "22                              2.554417    3.637564    0.767950    4.293363   \n",
       "23                              1.349909   -2.451785    2.029470    0.084459   \n",
       "24                              2.862650    3.259771    1.850311    1.633481   \n",
       "25                              3.321373  -10.338787    2.265780    0.061687   \n",
       "26                              2.822534    4.890954    1.375058    6.719665   \n",
       "27                             -4.673015    3.581127    0.703175    5.833183   \n",
       "28                              0.495011    3.565530    0.621730    4.978713   \n",
       "29                              3.294148   -0.956295    1.340333    0.425148   \n",
       "30                              2.409593    4.163363   -0.625827    5.537006   \n",
       "31                              0.775526    4.286860    1.204247   -0.155830   \n",
       "32                              9.139645   -5.105107    2.011236    1.189647   \n",
       "33                              3.086799    3.878416    1.514544    5.382004   \n",
       "34                             -0.778493    3.764370   -4.118527    4.729100   \n",
       "35                             -1.687522    3.648368    1.429094   21.581525   \n",
       "36                             -3.904617    4.117648    4.934711   20.695028   \n",
       "37                              3.115684    4.127575   -1.911981    6.926260   \n",
       "38                              1.602724    4.295828    0.415823    0.349604   \n",
       "39                              1.043380    3.348172    1.164364    6.158542   \n",
       "40                              2.116149   -0.561637    3.230466   -8.275190   \n",
       "41                              1.685997    4.745971   -3.832762    6.951564   \n",
       "42                              1.489433   -4.341085    2.176186    1.849871   \n",
       "43                             -0.663762    4.031153   -2.090714    2.186498   \n",
       "44                              7.160312   -2.974507    1.959683   -0.341677   \n",
       "\n",
       "Diversion ratio wrt. product          8           9   ...          35  \\\n",
       "Diversion ratio of product                            ...               \n",
       "0                               0.539727    2.300454  ...    2.148663   \n",
       "1                              -0.747189   -3.676577  ...    2.383930   \n",
       "2                               0.408250    1.685438  ...   -0.473059   \n",
       "3                               0.519823    2.080403  ...   -0.034966   \n",
       "4                               0.512326    1.202232  ...   -0.989099   \n",
       "5                               0.185839   -0.805507  ...    1.635051   \n",
       "6                               1.611695    2.570349  ...    1.297091   \n",
       "7                               0.289976   -0.561708  ...    6.407939   \n",
       "8                            -100.000000   -2.810074  ...    6.477342   \n",
       "9                              -0.682099 -100.000000  ...    2.391002   \n",
       "10                              0.613734   14.722927  ...   -1.934305   \n",
       "11                              0.287948   -0.013852  ...    2.356747   \n",
       "12                              0.462271    1.933443  ...   -1.291975   \n",
       "13                              0.546801    2.007772  ...    1.654641   \n",
       "14                             -0.663084   -5.696310  ...    2.076546   \n",
       "15                              0.476608    1.949793  ...    0.807351   \n",
       "16                              0.658614    1.634202  ...   -5.101321   \n",
       "17                              0.509593   -0.631209  ...    0.842210   \n",
       "18                              0.038697   -0.994116  ...    0.486488   \n",
       "19                              0.570804    2.170765  ...    1.584823   \n",
       "20                              0.550652    2.382588  ...    1.089178   \n",
       "21                             -1.317916   -3.040826  ...    1.720204   \n",
       "22                              0.567574    1.794250  ...    1.467665   \n",
       "23                             -0.269729   -2.808796  ...    1.055994   \n",
       "24                             -3.732136    0.976438  ...    1.886966   \n",
       "25                             -4.191252   -0.128265  ...    2.067161   \n",
       "26                              0.569591    2.270629  ...   -0.673179   \n",
       "27                              0.462788    1.699295  ...   -0.296375   \n",
       "28                              0.438760    0.547619  ...   -1.536385   \n",
       "29                              0.159508   -1.640802  ...    2.053247   \n",
       "30                              0.462866    1.979861  ...    1.010211   \n",
       "31                              0.486708    1.652307  ...    0.482741   \n",
       "32                             -1.217840   -2.960091  ...    2.072154   \n",
       "33                              0.375063    0.270233  ...    2.071180   \n",
       "34                              0.608698    2.727511  ...   -4.663175   \n",
       "35                              1.623843    2.469433  ... -100.000000   \n",
       "36                              1.561754    2.309965  ...    4.124335   \n",
       "37                              0.476686    2.065588  ...   -0.530590   \n",
       "38                              0.549188    1.921781  ...    0.142414   \n",
       "39                              0.441769    1.944830  ...   -0.351303   \n",
       "40                             -5.777834   -4.739304  ...    2.861147   \n",
       "41                              0.590277    2.172779  ...    0.165724   \n",
       "42                              0.259397   -1.195496  ...    0.790697   \n",
       "43                              0.460373    1.794759  ...    1.400673   \n",
       "44                              0.070403    0.253151  ...    0.000665   \n",
       "\n",
       "Diversion ratio wrt. product          36          37          38          39  \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               1.527512    2.783564    1.077207    0.796285   \n",
       "1                               1.691481    2.777415    0.837353    0.561882   \n",
       "2                              -0.546303   -1.705125   -1.519769   -0.619609   \n",
       "3                               0.638460   -3.443210   -0.390524   -0.420669   \n",
       "4                              -1.650315    2.371325    0.428106    0.210237   \n",
       "5                               1.330699    2.402010    0.877367    0.515841   \n",
       "6                               3.229753   -2.253411    0.171996    0.363307   \n",
       "7                               4.430986    2.670436    0.047306    0.628622   \n",
       "8                               4.492246    2.469066    0.998333    0.605792   \n",
       "9                               1.612821    2.597009    0.847985    0.647350   \n",
       "10                             -0.625794    0.787173    0.559059   -0.410455   \n",
       "11                              0.959327    9.749976    0.042165    0.709810   \n",
       "12                              0.835987    8.605726    0.280073   -0.044768   \n",
       "13                             -0.021250   -1.791166    7.979310    6.744125   \n",
       "14                              1.549745    2.775633    1.085098   -0.264701   \n",
       "15                             -2.083090    0.771624   -0.537777    0.231152   \n",
       "16                              0.905025    2.678088    0.684872    0.003626   \n",
       "17                             -1.438608   -2.704378    0.659476   -0.464026   \n",
       "18                              1.086442    2.749728    0.855721   -0.648012   \n",
       "19                              0.317658   -0.521907   -1.860050    0.590259   \n",
       "20                              0.903423   -1.745668    0.544869    0.562568   \n",
       "21                              1.377164    1.169115    0.852753    0.539262   \n",
       "22                              0.062960    1.228816    0.669416    0.451662   \n",
       "23                              1.314373    2.238085    0.779216    0.486275   \n",
       "24                              0.728545    1.291091    0.722780    0.509025   \n",
       "25                              1.553183    2.204225    0.863411    0.532897   \n",
       "26                              1.198472   -0.141613    0.716331    0.651913   \n",
       "27                             -4.347679    2.104673    0.788572   -0.260771   \n",
       "28                             -0.874064    2.387710    0.772728    0.113001   \n",
       "29                              1.249648    2.971834    0.877932    0.582071   \n",
       "30                              1.401139   -0.808398    0.502393    0.622833   \n",
       "31                             -0.335333    2.298961   -0.418416    0.695871   \n",
       "32                              1.486369    2.262214    1.002165   -0.243064   \n",
       "33                             -0.836427   -1.590378    0.076255   -0.887419   \n",
       "34                              1.061451    2.611332    0.653699    0.130155   \n",
       "35                              2.974075   -0.688980    0.064901   -0.120769   \n",
       "36                           -100.000000    0.493900   -0.073104   -0.177210   \n",
       "37                              0.274277 -100.000000   -0.288603   -0.478800   \n",
       "38                             -0.115674   -0.822330 -100.000000    6.523302   \n",
       "39                             -0.371716   -1.808533    8.647570 -100.000000   \n",
       "40                              2.051945    3.337929    9.564820    6.949269   \n",
       "41                              0.526236   -4.893881   -1.192672    0.572202   \n",
       "42                              1.437914    2.732428    0.937682    0.456345   \n",
       "43                              0.996382   -1.048305    0.394094    0.555108   \n",
       "44                              1.370154    2.099539    0.892748   -0.782467   \n",
       "\n",
       "Diversion ratio wrt. product          40          41          42          43  \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               0.067275    3.076710    1.940927    3.520549   \n",
       "1                              -0.126507    2.885576   -0.047511    2.541467   \n",
       "2                               0.040742   -3.481821    1.319186    1.847728   \n",
       "3                               0.074928    1.246837    1.924739    0.022570   \n",
       "4                               0.036830    1.159854    0.772850   -0.618921   \n",
       "5                              -0.007474    2.496398   -1.722324    2.874049   \n",
       "6                               0.087065   -4.082982    1.748596   -3.018814   \n",
       "7                              -0.072960    2.422562    0.486252    1.032803   \n",
       "8                              -0.684361    2.763536    0.916014    2.921425   \n",
       "9                              -0.136259    2.469188   -1.024743    2.764528   \n",
       "10                              0.073860    1.865816   -0.626123    1.321018   \n",
       "11                              0.002188    2.556394    0.350391   -0.950652   \n",
       "12                              0.062407    1.506251    0.905739   -2.622559   \n",
       "13                              0.629570   -2.432373    2.080844   -0.326978   \n",
       "14                             -0.019287    2.950808   -3.515183    3.314542   \n",
       "15                              0.070734    1.213636    7.108456   12.801563   \n",
       "16                              0.069717    2.297520    0.347505    0.030739   \n",
       "17                              0.061241    1.370742    1.703044    1.462572   \n",
       "18                              0.044429    2.501370   -2.404630   -0.118445   \n",
       "19                              0.080381   -4.759951    1.720852    1.886274   \n",
       "20                              0.093374   -0.271993    2.110353    2.791984   \n",
       "21                             -0.200154    0.623352    0.746665    2.399749   \n",
       "22                              0.059052    2.061731   -0.645002    1.212264   \n",
       "23                              0.055807    2.316410   -2.281517    2.708853   \n",
       "24                             -0.480275    2.099093    1.534183    2.080891   \n",
       "25                             -0.512080    2.322770    0.732353    2.673057   \n",
       "26                              0.094512   -0.994042    2.034684    2.873946   \n",
       "27                              0.020940    2.448961    0.906926    2.836441   \n",
       "28                              0.018365    2.472354    0.758419    2.506047   \n",
       "29                              0.015990    2.862154   -0.373227    1.198608   \n",
       "30                              0.083302   -1.681451    2.067350    2.446416   \n",
       "31                              0.053606    0.977654    0.863643   -0.641028   \n",
       "32                             -0.108791    2.363173    0.177162    2.110009   \n",
       "33                              0.064793    2.165313    1.085485    0.029383   \n",
       "34                              0.058901    1.361742    0.506419    1.515005   \n",
       "35                              0.084959    0.194510    0.699994    2.228277   \n",
       "36                              0.084496    0.856522    1.765301    2.198163   \n",
       "37                              0.076330   -4.423463    1.862882   -1.284318   \n",
       "38                              0.623222   -3.071677    1.821536    1.375722   \n",
       "39                              0.600249    1.953576    1.175175    2.568826   \n",
       "40                           -100.000000    3.447795    0.910772    2.956489   \n",
       "41                              0.087227 -100.000000    2.038463   -1.729899   \n",
       "42                              0.030549    2.702558 -100.000000    9.472053   \n",
       "43                              0.055184   -1.276276    5.271034 -100.000000   \n",
       "44                              0.006947    2.057295   -0.761145    1.964133   \n",
       "\n",
       "Diversion ratio wrt. product          44  \n",
       "Diversion ratio of product                \n",
       "0                               3.536350  \n",
       "1                              -0.626764  \n",
       "2                              -3.053215  \n",
       "3                               8.961202  \n",
       "4                               7.329512  \n",
       "5                              -2.328093  \n",
       "6                               3.106333  \n",
       "7                              -0.177176  \n",
       "8                               0.490450  \n",
       "9                               0.428071  \n",
       "10                              2.583729  \n",
       "11                              2.209296  \n",
       "12                              3.081377  \n",
       "13                              3.448601  \n",
       "14                             -1.256208  \n",
       "15                              2.076559  \n",
       "16                              1.484063  \n",
       "17                              3.445279  \n",
       "18                             -2.991000  \n",
       "19                              3.264644  \n",
       "20                              3.596427  \n",
       "21                             -3.740716  \n",
       "22                              0.086423  \n",
       "23                              1.944607  \n",
       "24                              0.790241  \n",
       "25                             -1.792186  \n",
       "26                              3.507110  \n",
       "27                              1.594644  \n",
       "28                              2.975718  \n",
       "29                             -1.297905  \n",
       "30                              3.533270  \n",
       "31                              9.111635  \n",
       "32                              0.355404  \n",
       "33                              3.358195  \n",
       "34                              1.529610  \n",
       "35                              0.001162  \n",
       "36                              3.318363  \n",
       "37                              2.823771  \n",
       "38                              3.421212  \n",
       "39                             -3.975059  \n",
       "40                              0.408603  \n",
       "41                              3.061210  \n",
       "42                             -1.501538  \n",
       "43                              2.156211  \n",
       "44                           -100.000000  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DR_hat = IPDL_diversion_ratio(qOpt, z_logit, ThetaOptBLP, Psi, Nest_count)\n",
    "pd.DataFrame(DR_hat[0]).rename_axis(index = 'Diversion ratio of product', columns = 'Diversion ratio wrt. product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticities and Diversion Ratios visualization\n",
    "\n",
    "Finally we advance our exposition towards visualing the IPDL semi-elasticities $\\mathcal{E}$ and diversion ratios $\\mathcal{D}$ compared to those implied by a multinomial Logit model. Since the number of products varies across markets $t$ we aggregate our results according to the categorical variable `cla` describing the class or segment of each vehicle $j$. This variable takes values 'subcompact', 'compact', 'intermediate', 'standard', and 'luxury' encoded as the integers $0,1,\\ldots, 5$ in our dataset. To this end we consider the 'pooled' elasticities and diversion ratios calculated using the directional derivative $\\frac{\\partial q_c}{\\partial u_{\\ell}} = \\sum_{j: x_{j,\\text{cla}} = c} \\sum_{k: x_{k,\\text{cla}} = \\ell} \\frac{\\partial q_j}{\\partial u_k}$ of class $c$ wrt. the utility of class $\\ell$, where $q_c = \\sum_{j: x_{j,\\text{cla}} = c} q_j$ denotes the within-group choice proabbilitity of choosing a car of class $c$. \n",
    "\n",
    "The pooled choice probability semi-elasticity $\\mathcal{E}_{c\\ell}$ of class $c$ wrt. the prices of cars of class $\\ell$ can then be computed as $\\mathcal{E}_{c\\ell} = \\frac{\\partial q_c}{\\partial u_{\\ell}}\\frac{1}{q_c}\\theta^{\\text{price}}$. \n",
    "\n",
    "Similarly, we may compute the pooled diversion ratio $\\mathcal{D}_{c\\ell}$ following a unit increase in the price of cars of class $\\ell$ from class $\\ell$ to cars of class $c$ as $\\mathcal{D}_{c\\ell} = -100\\cdot\\frac{\\partial q_c / \\partial u_{\\ell}}{\\partial q_{\\ell} / \\partial u_{\\ell}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_directionalgrad(data, Theta, q, x, psi, nest_count, direction_var, market_id = 'market', product_id = 'co', model = 'IPDL', outside_option = True):\n",
    "    '''\n",
    "    '''\n",
    "    T = len(q)\n",
    "    J = np.array([q[t].shape[0] for t in np.arange(T)])\n",
    "    data = data.sort_values([market_id, product_id])\n",
    "\n",
    "    vec = {}\n",
    "    q_agg = {}\n",
    "    dq_du_agg = {}\n",
    "\n",
    "    if model == 'IPDL':\n",
    "        Grad = ccp_gradient(q, x, Theta, psi, nest_count)\n",
    "    else:\n",
    "        Grad = {t: (np.diag(q[t]) - q[t][:,None]*q[t][None,:]) for t in np.arange(T)} \n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        G_t = data[data[market_id] == t][direction_var].nunique()\n",
    "        vec[t] = pd.get_dummies(data[data[market_id] == t][direction_var], columns = direction_var).values.reshape((J[t], G_t)).transpose()\n",
    "        \n",
    "        # Calculate the sum of within-group probabilities\n",
    "        q_agg[t] = vec[t]@q[t]\n",
    "\n",
    "        # Calculate directional derivatives\n",
    "        dq_du_agg[t] = np.einsum('cj,jk,lk->cl', vec[t], Grad[t], vec[t])\n",
    "    \n",
    "    return q_agg, dq_du_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elasticity_agg(data, Theta, q, x, psi, nest_count, direction_var, market_id = 'market', product_id = 'co', char_number = K-1, model = 'IPDL', outside_option = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    q_agg, dq_du_agg = ccp_directionalgrad(data, Theta, q, x, psi, nest_count, direction_var, market_id, product_id, model, outside_option)\n",
    "    E_agg = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        E_agg[t] = Theta[char_number]*np.einsum('cl,c->cl', dq_du_agg[t], 1./q_agg[t])\n",
    "\n",
    "    return E_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IPDLagg = Elasticity_agg(dat, ThetaOptBLP, qOpt, z_logit, Psi, Nest_count, 'cla', char_number = pr_index)\n",
    "E_Logitagg = Elasticity_agg(dat, LogitBLP_beta, logit.logit_ccp(LogitBLP_beta, z_logit), z_logit, Psi, Nest_count, 'cla', char_number = pr_index, model = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_agg = E_IPDLagg[0].shape[0]\n",
    "\n",
    "E0, E1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    E0[t,:,:] = E_Logitagg[t]\n",
    "    E1[t,:,:] = E_IPDLagg[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot histograms of our results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E0p = {j : (E0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E0p[j], num_bins, range = (np.quantile(E0p[j], 0.10), np.quantile(E0p[j], 0.90)), color = 'r', alpha = 1) # Logit is blue\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E1p = {j : (E1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E1p[j], num_bins, range = (np.quantile(E1p[j], 0.10), np.quantile(E1p[j], 0.90)), color = 'b', alpha = 1) # IPDL is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled IPDL price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mean elasticities for the logit model are given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean elasticity wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean elasticity of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>-0.172311</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>-0.172365</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>-0.172652</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>-0.173053</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>-0.173175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean elasticity wrt. class         0         1         2         3         4  \\\n",
       "Mean elasticity of class                                                       \n",
       "0                          -0.003261  0.001052  0.000998  0.000711  0.000311   \n",
       "1                           0.170102 -0.172311  0.000998  0.000711  0.000311   \n",
       "2                           0.170102  0.001052 -0.172365  0.000711  0.000311   \n",
       "3                           0.170102  0.001052  0.000998 -0.172652  0.000311   \n",
       "4                           0.170102  0.001052  0.000998  0.000711 -0.173053   \n",
       "5                           0.170102  0.001052  0.000998  0.000711  0.000311   \n",
       "\n",
       "Mean elasticity wrt. class         5  \n",
       "Mean elasticity of class              \n",
       "0                           0.000189  \n",
       "1                           0.000189  \n",
       "2                           0.000189  \n",
       "3                           0.000189  \n",
       "4                           0.000189  \n",
       "5                          -0.173175  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(E0.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. class', index = 'Mean elasticity of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For IPDL the mean elasticities are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean elasticity wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean elasticity of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003563</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.154679</td>\n",
       "      <td>-0.355211</td>\n",
       "      <td>0.076783</td>\n",
       "      <td>0.074719</td>\n",
       "      <td>0.033596</td>\n",
       "      <td>0.015434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.154679</td>\n",
       "      <td>0.082262</td>\n",
       "      <td>-0.315598</td>\n",
       "      <td>0.038811</td>\n",
       "      <td>0.027140</td>\n",
       "      <td>0.012706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.154679</td>\n",
       "      <td>0.105016</td>\n",
       "      <td>0.052393</td>\n",
       "      <td>-0.334010</td>\n",
       "      <td>0.015338</td>\n",
       "      <td>0.006584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.154679</td>\n",
       "      <td>0.107891</td>\n",
       "      <td>0.079243</td>\n",
       "      <td>0.033330</td>\n",
       "      <td>-0.362370</td>\n",
       "      <td>-0.012773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.154679</td>\n",
       "      <td>0.087504</td>\n",
       "      <td>0.070788</td>\n",
       "      <td>0.029351</td>\n",
       "      <td>-0.033131</td>\n",
       "      <td>-0.309191</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean elasticity wrt. class         0         1         2         3         4  \\\n",
       "Mean elasticity of class                                                       \n",
       "0                          -0.003563  0.001096  0.001076  0.000811  0.000361   \n",
       "1                           0.154679 -0.355211  0.076783  0.074719  0.033596   \n",
       "2                           0.154679  0.082262 -0.315598  0.038811  0.027140   \n",
       "3                           0.154679  0.105016  0.052393 -0.334010  0.015338   \n",
       "4                           0.154679  0.107891  0.079243  0.033330 -0.362370   \n",
       "5                           0.154679  0.087504  0.070788  0.029351 -0.033131   \n",
       "\n",
       "Mean elasticity wrt. class         5  \n",
       "Mean elasticity of class              \n",
       "0                           0.000220  \n",
       "1                           0.015434  \n",
       "2                           0.012706  \n",
       "3                           0.006584  \n",
       "4                          -0.012773  \n",
       "5                          -0.309191  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(E1.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. class', index = 'Mean elasticity of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios\n",
    "\n",
    "We now visualize the implied diversion ratios $\\mathcal{D}$. If $\\bar D_{c\\ell}$ denotes the sum of choice probability weigthed diversion ratios, then we have as above that $\\bar D_{c\\ell} = \\sum_{j}\\sum_{k} \\mathrm{1}_{\\{j\\in c\\}} \\mathrm{1}_{\\{k\\in \\ell\\}} q_j q_k \\mathcal{D}_{jk}$ i.e. more generally $\\bar D = (\\psi^{\\text{class}} \\circ q) \\mathcal{D} (\\psi^{\\text{class}} \\circ q).'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiversionRatio_agg(data, Theta, q, x, psi, nest_count, direction_var, market_id = 'market', product_id = 'co', char_number = K-1, model = 'IPDL', outside_option = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    dq_du_agg = ccp_directionalgrad(data, Theta, q, x, psi, nest_count, direction_var, market_id, product_id, model, outside_option)[1]\n",
    "    D_agg = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        D_agg[t] = -100*np.einsum('cl,c->cl', dq_du_agg[t], 1./np.diag(dq_du_agg[t]))\n",
    "\n",
    "    return D_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_IPDLagg = DiversionRatio_agg(dat, ThetaOptBLP, qOpt, z_logit, Psi, Nest_count, 'cla', char_number = pr_index)\n",
    "D_Logitagg = DiversionRatio_agg(dat, LogitBLP_beta, logit.logit_ccp(LogitBLP_beta, z_logit), z_logit, Psi, Nest_count, 'cla', char_number = pr_index, model = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0, D1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    D0[t,:,:] = D_Logitagg[t]\n",
    "    D1[t,:,:] = D_IPDLagg[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D0p = {j : (D0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D0p[j], num_bins, range = (np.quantile(D0p[j], 0.10), np.quantile(D0p[j], 0.90)), color = 'r', alpha = 1) # Logit is red\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit diversion ratios by class')\n",
    "fig.supxlabel('Diversion ratio from class')\n",
    "fig.supylabel('Diversion ratio to class')\n",
    "\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAHlCAYAAAD/f1iPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABvmUlEQVR4nO3deXgT5doG8DvdF9oCLQUKbcEiyI6IrLLJdtgEBZW9KIICHkVEQVApiOAuqCyiLHI4LOqRRTyi7IIstiAqi4hKbT1lKeWDltJWmr7fH7Fp0qZtJnknM0nu33VFh8lk8uTOO+mTyWRiEEIIEBEREREAwEfrAoiIiIj0hM0RERERkQU2R0REREQW2BwRERERWWBzRERERGSBzRERERGRBTZHRERERBbYHBERERFZYHNEREREZIHNkc6tXr0aBoMBKSkpNq8fMGAA6tWrZzWvXr16GDt2rKL7OXjwIJKSknD16lXHCiWbnn/+ecTFxcHPzw9Vq1bVuhybxo4dW2YMOatbt27o1q1bpcvVq1cPAwYMsJpnMBisLhEREejWrRu++OKLMrctXsbHxwcRERFo3LgxxowZg6+//trm/RkMBjz++OMOP67SbGVnMBiQlJQk7T5kS0pKgsFg0LoMhy1ZsgSrV68uMz81NRUGg8HmdWqp7PXZlYprSU1N1boUj+CndQEk36ZNmxAeHq7oNgcPHsScOXMwduxY3f4RdzdbtmzByy+/jFmzZqFv374IDAzUuiS3MXToUDz99NMoKirC77//jnnz5mHgwIH4/PPP0b9/f/NynTp1whtvvAEAuH79Os6cOYMNGzagT58+GDJkCNavXw9/f3+X1n7o0CHUrVvXpfepxCOPPIJ//OMfWpfhsCVLliAqKqrMG8DatWvj0KFDSEhI0KYw8ihsjjzQ7bffrnUJit28eRMGgwF+fp4zJE+cOAEAeOKJJxAdHa1xNe6lZs2aaN++PQCgY8eO6NChAxo0aICFCxdaNUdVq1Y1LwcAPXv2xOTJk5GUlIQ5c+bg+eefx6uvvurS2i3rcRUl20/dunV107wJIZCfn4/g4GCn1xUYGKhJ9uSZ+LGaByr9sVpRURHmzZuHRo0aITg4GFWrVkWLFi2waNEiAKbd7M888wwAoH79+uaPKvbu3Wu+/WuvvYbbbrsNgYGBiI6OxpgxY/Dnn39a3a8QAvPnz0d8fDyCgoLQpk0b7Nixo8xHLHv37oXBYMC//vUvPP3006hTpw4CAwPx66+/IjMzE5MmTUKTJk1QpUoVREdH4+6778b+/fut7qt4F/rrr7+OV199FfXq1UNwcDC6deuGX375BTdv3sSMGTMQExODiIgI3Hvvvbh06ZLVOnbv3o1u3bohMjISwcHBiIuLw5AhQ3Djxo0K87Unj3r16uH5558HYPpDX9lHLWPHjkWVKlVw8uRJ9OjRA6GhoahRowYef/zxMvXk5+fjueeeQ/369REQEIA6depg8uTJZT4Stfd5s0UIgSVLlqBVq1YIDg5GtWrVMHToUPz+++9llnvttdfMz3nr1q3x5ZdfVrp+pRISElCjRg388ccfdi2flJSEpk2b4r333kN+fr6UGlavXo1GjRohMDAQjRs3xpo1a2wuZ/lc//DDDzAYDFixYkWZ5b788ksYDAZs3brVPO/s2bMYMWIEoqOjzfezePFiq9tVtP3cuHED06ZNQ/369REUFITq1aujTZs2WL9+vfn2tj5Ws3esdOvWDc2aNUNycjI6d+6MkJAQ3HLLLXjllVdQVFRUaYbFH2suW7YMjRs3RmBgID766CMAwJw5c9CuXTtUr14d4eHhaN26NVasWAHL30avV68eTp48iX379plfp4o/1izvY7UDBw6gR48eCAsLQ0hICDp27FjmI1p7cqvI//3f/+Ghhx5C9erVERoaioEDB1ptKy+99BL8/PyQnp5e5rYPP/wwIiMjKx2nR44cwcCBAxEZGYmgoCAkJCRgypQpFd5mx44dGDRoEOrWrYugoCA0aNAAjz76KC5fvmy1XGZmJiZMmIDY2FgEBgaiRo0a6NSpE3bu3Gle5vvvv8eAAQPMYzMmJgb9+/e36/XELQnStVWrVgkA4vDhw+LmzZtlLv369RPx8fFWt4mPjxeJiYnmfy9YsED4+vqK2bNni127dont27eLhQsXiqSkJCGEEOnp6eKf//ynACA+++wzcejQIXHo0CFx7do1IYQQEyZMEADE448/LrZv3y6WLVsmatSoIWJjY0VmZqb5fp577jkBQEyYMEFs375dfPDBByIuLk7Url1bdO3a1bzcnj17BABRp04dMXToULF161axbds2kZWVJX7++WcxceJEsWHDBrF3716xbds2MW7cOOHj4yP27NljXse5c+cEABEfHy8GDhwotm3bJtauXStq1qwpGjZsKEaPHi0efvhh8eWXX4ply5aJKlWqiIEDB1rdPigoSPTq1Uts3rxZ7N27V/z73/8Wo0ePFv/3f/9X4XNiTx7Hjh0T48aNEwDE9u3bxaFDh0R6enq560xMTBQBAQEiLi5OvPzyy+Lrr78WSUlJws/PTwwYMMC8XFFRkejTp4/w8/MTL7zwgvj666/FG2+8IUJDQ8Xtt98u8vPzFdVZfN+lx9D48eOFv7+/ePrpp8X27dvFunXrxG233SZq1qwpLly4YF5u9uzZAoAYN26c+PLLL8Xy5ctFnTp1RK1ataye8/LEx8eL/v37W80DICZPnmw178qVK8LHx0d07NixwttamjFjhgAg9u/fX+G67VG8HQ4aNEh8/vnnYu3ataJBgwYiNja2THYAxOzZs83/vv3220WnTp3KrPOBBx4Q0dHR4ubNm0IIIU6ePCkiIiJE8+bNxZo1a8TXX38tnn76aeHj42PeVoWoePt59NFHRUhIiHjrrbfEnj17xLZt28Qrr7wi3n33XfPti58zS/aOla5du4rIyEhx6623imXLlokdO3aISZMmCQDio48+qjTH4rpbtGgh1q1bJ3bv3i1OnDghhBBi7NixYsWKFWLHjh1ix44d4qWXXhLBwcFizpw55tsfO3ZM3HLLLeL22283v04dO3ZMCFHymrBq1Srz8nv37hX+/v7ijjvuEBs3bhSbN28WvXv3FgaDQWzYsMG8nD252VI8LmJjY82vN8uXLxfR0dEiNjbW/Fpy8eJFERgYKGbNmmV1+6ysLBEcHCyeeeaZCu9n+/btwt/fX7Ro0UKsXr1a7N69W6xcuVIMGzasTC3nzp0zz1u6dKlYsGCB2Lp1q9i3b5/46KOPRMuWLUWjRo3EX3/9ZV6uT58+okaNGmL58uVi7969YvPmzeLFF180Z3T9+nURGRkp2rRpIz7++GOxb98+sXHjRvHYY4+JU6dOVVi7u2JzpHPFA76iS2XN0YABA0SrVq0qvJ/XX3+9zIYlhBCnT58WAMSkSZOs5h85ckQAEDNnzhRCmP54BQYGigcffNBquUOHDgkANpujLl26VPr4CwsLxc2bN0WPHj3Evffea55f/ELYsmVLYTQazfMXLlwoAIh77rnHaj1TpkwRAMwN36effioAiOPHj1dagyV78xCi5I+Q5R+X8iQmJgoAYtGiRVbzX375ZQFAHDhwQAhhepEEIF577TWr5TZu3CgAiOXLlyuus3RzVPycvfnmm1a3TU9PF8HBweLZZ58VQgjxf//3fyIoKMjqeRFCiG+//bbMc16e8pqjSZMmiZs3b4q//vpLnD59WvTt21cAEIsXL67wtpaWLl0qAIiNGzdarVtpc2Q0GkVMTIxo3bq1KCoqMs9PTU0V/v7+lTZH77zzjgAgzpw5Y55XvL08/fTT5nl9+vQRdevWNY/RYo8//rgICgoSV65cEUJUvP00a9ZMDB48uMLHU7o5UjJWunbtKgCII0eOWC3bpEkT0adPnwrvVwhTNhEREebHUh6j0Shu3rwp5s6dKyIjI61yb9q0qc2xZas5at++vYiOjhY5OTnmeYWFhaJZs2aibt265vXak5stxa/P5W0D8+bNM89LTEwU0dHRoqCgwDzv1VdfFT4+PmVed0tLSEgQCQkJIi8vr9JayltXUVGRuHnzpvjjjz8EALFlyxbzdVWqVBFTpkwpd90pKSkCgNi8eXOFdXoSfqzmJtasWYPk5OQyl7vuuqvS27Zt2xY//PADJk2ahK+++grZ2dl23++ePXsAoMzBj23btkXjxo2xa9cuAMDhw4dRUFCABx54wGq59u3bl/tNqCFDhticv2zZMrRu3RpBQUHw8/ODv78/du3ahdOnT5dZtl+/fvDxKRnGjRs3BgCr41Is56elpQEAWrVqhYCAAEyYMAEfffRRmY+LymNvHo4aOXKk1b9HjBhhdb+7d++2ef/3338/QkNDzffvTJ3btm2DwWDAqFGjUFhYaL7UqlULLVu2NH/ceujQIeTn55epuWPHjoiPj7f/QduwZMkS+Pv7IyAgAI0bN8bBgwcxd+5cTJo0ye51CIuPY5xx5swZZGRkYMSIEVYfR8XHx6Njx46V3n7kyJEIDAy0+rhn/fr1KCgowEMPPQTA9FHprl27cO+99yIkJMQq9379+iE/Px+HDx+2Wq+t7adt27b48ssvMWPGDOzduxd5eXmV1qd0rNSqVQtt27a1mteiRQu7P/K8++67Ua1atTLzd+/ejZ49eyIiIgK+vr7w9/fHiy++iKysrDIfidsjNzcXR44cwdChQ1GlShXzfF9fX4wePRp//vknzpw5A8Cx3CyVtw0UZwsATz75JC5duoRPPvkEgOmjzKVLl6J///4Vflv0l19+wW+//YZx48YhKChIUV2XLl3CY489htjYWPNrafG2afl62rZtW6xevRrz5s3D4cOHcfPmTav1NGjQANWqVcP06dOxbNkynDp1SlEd7ojNkZto3Lgx2rRpU+YSERFR6W2fe+45vPHGGzh8+DD69u2LyMhI9OjRw66vn2ZlZQEwfROktJiYGPP1xf+vWbNmmeVszStvnW+99RYmTpyIdu3a4T//+Q8OHz6M5ORk/OMf/7D5glW9enWrfwcEBFQ4v/hz/YSEBOzcuRPR0dGYPHkyEhISkJCQYD4Oqzz25uEIPz8/REZGWs2rVauW1f1mZWXBz88PNWrUsFrOYDCgVq1aZZ4PR+q8ePEihBCoWbMm/P39rS6HDx82H69QvI7iGm3V7agHHngAycnJSElJwZkzZ5CVlYUXXnhB0TqK/1jHxMQ4VYuzj7N69eq45557sGbNGhiNRgCm45fatm2Lpk2bmu+jsLAQ7777bpnM+/XrBwBljhOx9dy+8847mD59OjZv3ozu3bujevXqGDx4MM6ePVvp47N3rJQeo4DpYGh7Gwpb9/Pdd9+hd+/eAIAPPvgA3377LZKTkzFr1iwAUNysAKbjgIQQ5T4uoOSxO5KbpfLGhmV2t99+Ozp37mw+hmzbtm1ITU2t9NQSmZmZAKD4IPqioiL07t0bn332GZ599lns2rUL3333nbnJtsx048aNSExMxIcffogOHTqgevXqGDNmDC5cuAAAiIiIwL59+9CqVSvMnDkTTZs2RUxMDGbPnl2mkfIUnvPVICqXn58fpk6diqlTp+Lq1avYuXMnZs6ciT59+iA9PR0hISHl3rb4hfD8+fNlNs6MjAxERUVZLXfx4sUy67hw4YLNd0a2zrWydu1adOvWDUuXLrWan5OTU/GDdEDnzp3RuXNnGI1GpKSk4N1338WUKVNQs2ZNDBs2zOZt7M3DEYWFhcjKyrL641P84lQ8LzIyEoWFhcjMzLRqkIQQuHDhAu68806n64yKioLBYMD+/fttnn6geF7xfRTXaKm859xeNWrUQJs2bRy+vRACn3/+OUJDQ51aD1D547THQw89hE8++QQ7duxAXFwckpOTrcZ4tWrVzHs0Jk+ebHMd9evXt/q3re0nNDQUc+bMwZw5c3Dx4kXz3pCBAwfi559/rvDxqTGmbbFV94YNG+Dv749t27ZZ7R3ZvHmzw/dTrVo1+Pj44Pz582Wuy8jIAADzY3MkN0vljY0GDRpYzXviiSdw//3349ixY3jvvffQsGFD9OrVq8J1F2/nSg98PnHiBH744QesXr0aiYmJ5vm//vprmWWjoqKwcOFCLFy4EGlpadi6dStmzJiBS5cuYfv27QCA5s2bY8OGDRBC4Mcff8Tq1asxd+5cBAcHY8aMGYpqcwfcc+RlqlatiqFDh2Ly5Mm4cuWK+YRhxX/wSr9Du/vuuwGYmhZLycnJOH36NHr06AEAaNeuHQIDA7Fx40ar5Q4fPmz37nbA9MJZ+g/yjz/+iEOHDtm9DqV8fX3Rrl078zu6Y8eOlbusvXk46t///rfVv9etWwcA5m/7Fa+/9P3/5z//QW5urvl6Z+ocMGAAhBD43//+Z3NvZfPmzQGYPjINCgoqU/PBgwcVPedqmDNnDk6dOoUnn3xS8UcRpTVq1Ai1a9fG+vXrrT6q++OPP3Dw4EG71tG7d2/UqVMHq1atwqpVqxAUFIThw4ebrw8JCUH37t3x/fffo0WLFjZzt7XHpiI1a9bE2LFjMXz4cJw5c6bcb2GqPabtUXwaAl9fX/O8vLw8/Otf/yqzrL17qUJDQ9GuXTt89tlnVssXFRVh7dq1qFu3Lho2bFjmdvbmZqm8baD0iVDvvfdexMXF4emnn8bOnTsxadKkSk/I2bBhQyQkJGDlypUoKCiotJZixest/Xr6/vvvV3i7uLg4PP744+jVq5fN10KDwYCWLVvi7bffRtWqVSt8vXRn3HPkBQYOHIhmzZqhTZs25q9DL1y4EPHx8bj11lsBwPwHb9GiRUhMTIS/vz8aNWqERo0aYcKECXj33Xfh4+ODvn37IjU1FS+88AJiY2Px1FNPATB9dDB16lQsWLAA1apVw7333os///wTc+bMQe3ata2OC6rIgAED8NJLL2H27Nno2rUrzpw5g7lz56J+/fooLCyUlsmyZcuwe/du9O/fH3FxccjPz8fKlSsBmM6VUx5783BEQEAA3nzzTVy/fh133nknDh48iHnz5qFv377mY8t69eqFPn36YPr06cjOzkanTp3w448/Yvbs2bj99tsxevRop+vs1KkTJkyYgIceeggpKSno0qULQkNDcf78eRw4cADNmzfHxIkTUa1aNUybNg3z5s3DI488gvvvvx/p6elISkpy+mM1e129etX8MUFubq75JJD79+/HAw88gDlz5pS5zW+//YZPP/20zPwmTZqgSZMmZeb7+PjgpZdewiOPPIJ7770X48ePx9WrVxU9Tl9fX4wZMwZvvfUWwsPDcd9995X5SHzRokW466670LlzZ0ycOBH16tVDTk4Ofv31V3z++efm480q0q5dOwwYMAAtWrRAtWrVcPr0afzrX/9Chw4dyt1DrOaYtlf//v3x1ltvYcSIEZgwYQKysrLwxhtv2NxzWbwHY+PGjbjlllsQFBRkfv0qbcGCBejVqxe6d++OadOmISAgAEuWLMGJEyewfv16cwPhSG6WUlJSrLaBWbNmoU6dOmWOkfP19cXkyZMxffp0hIaG2v1LBosXL8bAgQPRvn17PPXUU4iLi0NaWhq++uqrMo1Zsdtuuw0JCQmYMWMGhBCoXr06Pv/8c+zYscNquWvXrqF79+4YMWIEbrvtNoSFhSE5ORnbt2/HfffdB8D0EeCSJUswePBg3HLLLRBC4LPPPsPVq1cr3fPltrQ6EpzsU/wNhOTkZJvX9+/fv9Jvq7355puiY8eOIioqyvx18XHjxonU1FSr2z333HMiJiZG+Pj4CADmr84bjUbx6quvioYNGwp/f38RFRUlRo0aVear6UVFRWLevHmibt26IiAgQLRo0UJs27ZNtGzZ0urbHMXftvnkk0/KPJ6CggIxbdo0UadOHREUFCRat24tNm/eXOYbVcXfTHn99detbl/eukvneOjQIXHvvfeK+Ph4ERgYKCIjI0XXrl3F1q1bbeZsyd48lH5bLTQ0VPz444+iW7duIjg4WFSvXl1MnDhRXL9+3WrZvLw8MX36dBEfHy/8/f1F7dq1xcSJE8ucgsDeOm19lV8IIVauXCnatWsnQkNDRXBwsEhISBBjxowRKSkp5mWKiorEggULRGxsrPk5//zzz0XXrl2lfpW/vNvi729sGgwGUaVKFdGoUSMxevRo8dVXX9m8TfHyti6W3zCz5cMPPxS33nqrCAgIEA0bNhQrV660mV156/rll1/M97Vjxw6b93Hu3Dnx8MMPizp16gh/f39Ro0YN0bFjR6tvPVW0/cyYMUO0adNGVKtWTQQGBopbbrlFPPXUU+Ly5cvmZWx9ld/esdK1a1fRtGnTMvdb3hgqraLnduXKlaJRo0bmuhcsWCBWrFhR5htYqamponfv3iIsLMzq27q2vq0mhBD79+8Xd999t3kct2/fXnz++edWy9iTmy3Frytff/21GD16tKhataoIDg4W/fr1E2fPnrV5m9TUVAFAPPbYYxWHVcqhQ4dE3759RUREhAgMDBQJCQniqaeeKlOLZVanTp0SvXr1EmFhYaJatWri/vvvF2lpaVZjND8/Xzz22GOiRYsWIjw8XAQHB4tGjRqJ2bNni9zcXCGEED///LMYPny4SEhIEMHBwSIiIkK0bdtWrF69WtFjcCcGISR9pYPIhnPnzuG2227D7NmzMXPmTK3L0a2xY8fi008/xfXr17UuhYhU9O677+KJJ57AiRMnzAfkk/7wYzWS5ocffsD69evRsWNHhIeH48yZM3jttdcQHh6OcePGaV0eEZFmvv/+e5w7dw5z587FoEGD2BjpHJsjkiY0NBQpKSlYsWIFrl69av419Zdffrncr/MTEXmDe++9FxcuXEDnzp2xbNkyrcuhSvBjNSIiIiIL/Co/ERERkQU2R0REREQW2BwRERERWWBzRERERGSBzRERERGRBTZHRERERBbYHBERERFZYHNEREREZIHNEREREZEFNkdEREREFtgcEREREVlgc0RERERkgc0RERERkQU2R0REREQW2BwRERERWWBzRERERGSBzZEbW7JkCerXr4+goCDccccd2L9/v9YleZRvvvkGAwcORExMDAwGAzZv3qx1SR5nwYIFuPPOOxEWFobo6GgMHjwYZ86c0bosXVu6dClatGiB8PBwhIeHo0OHDvjyyy+1LsujLViwAAaDAVOmTNG6FN1KSkqCwWCwutSqVUvrshzG5shNbdy4EVOmTMGsWbPw/fffo3Pnzujbty/S0tK0Ls1j5ObmomXLlnjvvfe0LsVj7du3D5MnT8bhw4exY8cOFBYWonfv3sjNzdW6NN2qW7cuXnnlFaSkpCAlJQV33303Bg0ahJMnT2pdmkdKTk7G8uXL0aJFC61L0b2mTZvi/Pnz5stPP/2kdUkOMwghhNZFkHLt2rVD69atsXTpUvO8xo0bY/DgwViwYIGGlXkmg8GATZs2YfDgwVqX4tEyMzMRHR2Nffv2oUuXLlqX4zaqV6+O119/HePGjdO6FI9y/fp1tG7dGkuWLMG8efPQqlUrLFy4UOuydCkpKQmbN2/G8ePHtS5FCu45ckN//fUXjh49it69e1vN7927Nw4ePKhRVUTOu3btGgDTH3uqnNFoxIYNG5Cbm4sOHTpoXY7HmTx5Mvr374+ePXtqXYpbOHv2LGJiYlC/fn0MGzYMv//+u9YlOcxP6wJIucuXL8NoNKJmzZpW82vWrIkLFy5oVBWRc4QQmDp1Ku666y40a9ZM63J07aeffkKHDh2Qn5+PKlWqYNOmTWjSpInWZXmUDRs24OjRo0hJSdG6FLfQrl07rFmzBg0bNsTFixcxb948dOzYESdPnkRkZKTW5SnG5siNGQwGq38LIcrMI3IXjz/+OH788UccOHBA61J0r1GjRjh+/DiuXr2K//znP0hMTMS+ffvYIEmSnp6OJ598El9//TWCgoK0Lsct9O3b1zzdvHlzdOjQAQkJCfjoo48wdepUDStzDJsjNxQVFQVfX98ye4kuXbpUZm8SkTv45z//ia1bt+Kbb75B3bp1tS5H9wICAtCgQQMAQJs2bZCcnIxFixbh/fff17gyz3D06FFcunQJd9xxh3me0WjEN998g/feew8FBQXw9fXVsEL9Cw0NRfPmzXH27FmtS3EIjzlyQwEBAbjjjjuwY8cOq/k7duxAx44dNaqKSDkhBB5//HF89tln2L17N+rXr691SW5JCIGCggKty/AYPXr0wE8//YTjx4+bL23atMHIkSNx/PhxNkZ2KCgowOnTp1G7dm2tS3EI9xy5qalTp2L06NFo06YNOnTogOXLlyMtLQ2PPfaY1qV5jOvXr+PXX381//vcuXM4fvw4qlevjri4OA0r8xyTJ0/GunXrsGXLFoSFhZn3hkZERCA4OFjj6vRp5syZ6Nu3L2JjY5GTk4MNGzZg79692L59u9aleYywsLAyx72FhoYiMjKSx8OVY9q0aRg4cCDi4uJw6dIlzJs3D9nZ2UhMTNS6NIewOXJTDz74ILKysjB37lycP38ezZo1w3//+1/Ex8drXZrHSElJQffu3c3/Lv7cPDExEatXr9aoKs9SfCqKbt26Wc1ftWoVxo4d6/qC3MDFixcxevRonD9/HhEREWjRogW2b9+OXr16aV0aebE///wTw4cPx+XLl1GjRg20b98ehw8fdtu/STzPEREREZEFHnNEREREZIHNEREREZEFNkdEREREFnhAtkJFRUXIyMhAWFgYT7hYihACOTk5iImJgY+P/X03My0fM5WPmcrHTOVjpvIpyZTNkUIZGRmIjY3VugxdS09PV3QiP2ZaOWYqHzOVj5nKx0zlsydTNkcKhYWFATCFGx4ernE1+pKdnY3Y2FhzRvZipuVjpvIxU/mYqXzMVD4lmbI5Uqh4N2V4eDgHXjmU7splppVjpvIxU/mYqXzMVD57MmVzRPpRWAhs3myaHjAA8OPwdBozVUdhIbBtm2maucrBTOVjpg5jUqQffn7A4MFaV+FZmKk6mKt8zFQ+ZuowfpWfiIiIyAL3HJF+GI3A3r2m6c6dAf7ytfOYqTqMRmD/ftM0c5WDmcrHTB3G5oj0Iz8fKP6h1+vXgdBQbevxBMxUHcxVPmYqHzN1GJsj0g+DAWjSpGSanMdM1cFc5WOm8jFTh7E5Iv0ICQFOntS6Cs/CTNXBXOVjpvIxU4fxgGwiIiIiC2yOiIiIiCywOSL9yMsDevUyXfLytK7GMzBTdTBX+ZipfMzUYTzmiPSjqAjYubNkmpzHTNXBXOVjpvIxU4exOSL9CAwE1q4tmSbnMVN1MFf5mKl8zNRhbI5IP/z8gJEjta7CszBTdTBX+ZipfMzUYTzmiIiIiMgC9xyRfhiNQHKyabp1a57qXgZmqg6jETh2zDTNXOVgpvLZmWnx+SGFcFFdboDNEelHfj7Qtq1pmqe6l4OZqoO5ysdM5WOmDmNzRPphMADx8SXT5Dxmqg7mKh8zlY+ZOozNEelHSAiQmqp1FZ6FmaqDucrHTOVjpg7jAdlEREREFtgcEREREVlgc0T6kZ8PDB5suuTna12NZ2Cm6mCu8jFT+Zipw3jM0d+WLFmC119/HefPn0fTpk2xcOFCdO7cWeuyvIvRCGzZUjJNzmOm6mCu8jFT+Zipw9gcAdi4cSOmTJmCJUuWoFOnTnj//ffRt29fnDp1CnFxcVqX5z0CAoDly0umyXnMVB3MVT5mKh8zdRibIwBvvfUWxo0bh0ceeQQAsHDhQnz11VdYunQpFixYoHF1XsTfHxg/XusqPAszVQdzlY+ZysdMHeb1xxz99ddfOHr0KHr37m01v3fv3jh48KBGVRERycNT3BAp4/V7ji5fvgyj0YiaNWtaza9ZsyYuXLigUVVeqqgIOHnSNN24MeDj9b2785ipOoqKgNOnTdPMVQ5mKh8zdZjXN0fFDKXeWgkhyswjleXlAc2amaZ5qns5mKk6mKt8zFQ+Zuowr2+OoqKi4OvrW2Yv0aVLl8rsTSIXiIrSugLPw0zVwVzlY6byMVOHeH1zFBAQgDvuuAM7duzAvffea56/Y8cODBo0SMPKvFBoKJCZqXUV5XLLX67WeaZui7nKx0zlk5Cpktc9t3yNLIfXN0cAMHXqVIwePRpt2rRBhw4dsHz5cqSlpeGxxx7TujQiIiJyMTZHAB588EFkZWVh7ty5OH/+PJo1a4b//ve/iC/+NWMiBTzp3RORp+B2qYy3H3LLQ9f/NmnSJKSmpqKgoABHjx5Fly5dtC7J++TnAyNHmi481b0czFQdzFU+ZiofM3UYmyPSD6MRWLfOdOGp7uXwoEwNBh29m/WgXG3RJGc3y1RX47E8OsrULfKywI/VSD8CAoC33y6ZJucxU3UwV/mYqXzM1GFsjkg//P2BKVO0rsKK2x+noMNMPYKH5Kqr8a2TTHWVibNUyNRy7095GdmzjN7xYzUiIiIiC9xzRPpRVASkppqm4+J4qnsZ3ChTg6Hsu8zy5gEavyMtKgLS0kzTOs/VXraydikXZ1rR2Co97a57P9TOVBfbokrYHJF+5OUB9eubpnmqezmYqTqYq3zMVD5m6jA2R6QvISFaVwDA/m9VVPTOyfI6Td9h6SRTLaj67t9Nci09livbY6IpjTJ15vHrZjsvj4JMHc1BN+NHIjZHpB+hoUBurtZVeBZmqg7mKh8zlY+ZOozNEXkVGe/slK7D1rsqzY/vcKHK8rK1Z8Pe9er23bqGSufiyG29BfeUuJ67bK/ufxQhERERkURsjshxBQXA+PGmS0GB/tZnh/L26iidZ+/ZX20tVzxPlXejGmRaHntylXHsh0voKFdbysva3jFa+jYyn6dy6ThTyxzKe+wVZa7qNl4RHWSq5ZmxnblvNkfkuMJC4MMPTZfCQv2tj5ipWpirfMxUPmbqMB5zRI7z9wfmzSuZdva3e0qvT0VqvZNxdr3SP493Yab2kJ27S/Zo2KKzXJXS5bFFLspUD8cLuey4G4mZ6iE3V2JzRI4LCABmzSr5t7O/+lx6feQ8ZqoO5iofM5WPmTqMzRF5PS0/D6fKMSd1VJZreedH4vNROUe/KQjocI+eShx9vK7Kic0ROU4I4PJl03RUlJz1ZWaWrI+vws5jpuooPfaZq/OYqXzM1GFsjshxN24A0dGm6evX5awvJqZkfTzVvfOYqTpKj30vybX0N9mkvnv3kkzL+/02VbhBpvacxd7ebxDLxOZIIfH3s5edna1xJTpgeebV7Gxk/31AtlD4imnONCfHan1OH+DtxoqHV/E4Y6bOk5Zpdjbg62u9Yi/NlZnK582ZqvVn1ZFM2RwplPP3H5vY2FiNK9GZ4r0TMGUUERFh903NmTZqZHN93qh0fMzUedIyLb3te3GuzFQ+b85UwcN0ar32ZGoQSttSL1dUVISMjAyEhYXBwM9vrQghkJOTg5iYGPj42H8KLWZaPmYqHzOVj5nKx0zlU5IpmyMiIiIiCzxDNhEREZEFNkdEREREFtgcEREREVlgc0RERERkgc0RERERkQU2R0REREQW2BwRERERWeAZshXiCbbKx5OWycdM5WOm8jFT+ZipfEoyZXOkUEZGBn86pBLp6emoW7eu3csz08oxU/mYqXzMVD5mKp89mbI5UigsLAyAKdzw8HCNq9GX7OxsxMbGmjOyFzMtHzOVj5nKx0zlY6byKcmUzZFCxbspw319EV78w3XXrwOhoRpWpS9Kd+WaMw0P58ZcDocz5Tgtl1Pj1NcXqFLFdAVzNWOm8jFT+ezJlM2RowIDgU2bSqaJ9IjjVB3MVT5mKh8zdRibI0f5+QGDB2tdBVHFOE7VwVzlY6byMVOH8av8RERERBa458hRRiOwd69punNnwNdX03KIbOI4VYfRCOzfb5pmrnIwU/mYqcPYHDkqPx/o3t00zQPdSK84TtXBXOVjpvIxU4exOXKUwQA0aVIyTaRHHKfqYK7yMVP5mKnD2Bw5KiQEOHlS6yqIKsZxqg7mKh8zlY+ZOowHZBMRERFZYHNEREREZIHNkaPy8oBevUyXvDytqyGyjeNUHcxVPmYqHzN1GI85clRREbBzZ8k0kR5xnKqDucrHTOVjpg5jc+SowEBg7dqSaSI94jhVB3OVj5nKx0wdxubIUX5+wMiRWldBVDGOU3UwV/mYqXzM1GE85oiIiIjIAvccOcpoBJKTTdOtW/O07KRPHKfqMBqBY8dM08xVDmYqHzN1GJsjR+XnA23bmqZ5WnbSK45TdTBX+ZipfMzUYWyOHGUwAPHxJdNEesRxqg7mKh8zlY+ZOozNkaNCQoDUVK2rIKoYx6k6mKt8zFQ+ZuowHpBNREREZIHNEREREZEFNkeOys8HBg82XfLzta6GyDaOU3UwV/mYqXzM1GE85shRRiOwZUvJNJEecZyqg7nKx0zlY6YOU9wcZWRkICcnB40aNQIAGI1GvPnmmzh27Bh69+6Nhx9+WHqRuhQQACxfXjJNpEccp+pgrvIxU/mYqcMUN0ePPvoo4uLisHjxYgDASy+9hLlz56Jq1ar45JNPEBAQgFGjRkkvVHf8/YHx47WugqhiHKfqYK7yMVP5dJRp8ZkEhNC2DnspPubo2LFj6N69u/nfH3zwAZ566ilcuXIFEyZMMDdN7uSbb77BwIEDERMTA4PBgM2bN2tdEhEREWlEcXOUlZWFWrVqAQBOnz6N8+fPY+zYsQCAIUOG4MyZM1ILdIXc3Fy0bNkS7733nv03KioCTp40XYqK1CuOyBkeNE51dQ47D8rVkqYZe2immmKmDlP8sVpERAQuXboEwLTHpXr16mjevDkAwGAw4K+//pJboQv07dsXffv2VXajvDygWTPTNE/LTnrFcaoO5iofM5WPmTpMcXPUtm1bvPrqq/D398eiRYvQu3dv83W///47YmJipBaoa1FRWlegCoPBfT4XJjt46DjVHHOVj5nK52SmzhwrpKu9vQopbo5eeukl9OrVC4MGDUK1atUwa9Ys83WbN29G2+IfufN0oaFAZqbWVRBVjONUHcxVPmYqHzN1mOLmqFWrVvjjjz/w888/o0GDBggPDzdfN2nSJNx6661SCyQiKuZu33gh91d6TzrHoHPcJT+HTgIZEhKC1q1bl5nfv39/pwsiIiIi0pLib6vt3r0bn3zyifnfFy9eRL9+/VCrVi2MGTMG+d5yivL8fGDkSNPFWx4zuR+OU3UwV/l0lKnSY2V0e2yNipkaDCUXT6S4OXrxxRdx6tQp87+fffZZ7N+/Hx07dsSnn36K119/XWqBrnD9+nUcP34cx48fBwCcO3cOx48fR1paWvk3MhqBdetMF56WnfSK41QdzFU+ZiofM3WY4o/VfvnlF0yfPh0AUFhYiE2bNuHVV1/FpEmT8MYbb2DlypV44YUXpBeqppSUFKsTW06dOhUAkJiYiNWrV9u+UUAA8PbbJdNurvhzdVvvAvjtNTfmpuPUchzqcuy5aa7F7N2mXbrtu2Gmut9r4oaZ6oXi5ig7OxtVq1YFABw9ehS5ubm45557AJi+5p+UlCSzPpfo1q0bhNJXAH9/YMoUVeohkobjVB3MVT5mKh8zdZjij9Wio6Nx9uxZAMDOnTsRHx+PunXrAgBycnLg7+8vt0LSBd2/QyIip1hu4+XtQfZUnvzYHGHreCJPPr7IFsV7jv7xj39g5syZOHnyJFavXo3ExETzdT///DPq1asnsz79KioCUlNN03FxgI/iPpNIfRyn6igqAoqPSWSucjBT+ZipwxQ3R/Pnz0daWho++OADtG3bFs8//7z5unXr1qFjx45SC9StvDygfn3TtIeclr30u4TyjkEiN+IG49TyvCdus8fCDXItT3Geltu4Lo7r0iBTe8eWjLNEa5KxnZlquY3p9fhCxc1RVFQUtm/fbvO6PXv2ICgoyOmi3EZIiNYVEFWO41QdzFU+ZiofM3WIQyeBLI/l2bI9XmgokJurdRVEFeM4VQdzlY+ZysdMHeZQc2Q0GvHll1/i9OnTyMvLs7rOYDC43Vf5yTZdfpxBZKG8j4RsfVSnp132rlTRx2aOnuzQk7O0/NixvOvsuT3g2TlZKp2LJzxuxc1RVlYWOnfujJ9//hkGg8H8FXiDRTpsjoiIiMhdKT50fdasWQgKCsIff/wBIQSOHDmCs2fPYurUqWjYsGHFZ5X2JAUFwPjxpktBgdbVOMTbvprpFmSPKxePU3vHlMwDry2/duyyMe1m239ludj6Gr/l/11yoLzGmSodO46Oc0fvzyEqZGpP3Z7wt0Vxc7Rr1y5MnToVMTExphX4+CAhIQGvv/46evbsiWnTpkkvUpcKC4EPPzRdCgu1roY8hexxxXGqDuYqHzOVj5k6TPHHan/++Sfq1asHX19f+Pj4INfiYK+BAwdixIgRUgvULX9/YN68kmkvUfr4Bd18DdhTlB5Xzv4ekovGael3ieUdd1HZiQYdvT9nl1PMS7d/Vbl5prrcU6LjTB3dS6fk740zx8g59FX+a9euAQBiYmJw4sQJdOnSBQBw5coVFHpLdxoQAMyapXUV5GlKjytnf0mb41QdzFU+ZiofM3WY4ubojjvuwMmTJ9G/f3/069cPc+fORXh4OAICAjBz5ky0b99ejTpJMkfewct610/ehWPF9dT+VhmfU8dU9Lx4wzcB7WVPFmrnpbg5evzxx/Hbb78BAF566SUcPnwYY8aMAQAkJCRg0aJFcivUKyGAzEzTdFQUXy1IDiGAy5dN01FRctbHcSpf6eeJuTqPmcrHTB2muDnq2bMnevbsCQCoUaMGvv/+e5w4cQIGgwG33XYb/PyknldSv27cAP4+KN3dfj6AdOzGDSA62jR9/bqc9XGcylf6edJRrhXt4dX130YdZyqby54HN8vUkVzUOseS052MwWBA8+bNZdTiForP65Sdk1MyMzvb+QNn3Uh2tu1/Z/89IRSOTnOmpVfsjSzPZpudjey/x5XDmXrxOC1N6jj19bVesZfmykztV97Lm6qvp26eqa3MKvszUdFtlGRqV3Ok9NxFcXFxipZ3Jzl//7GJbdSoZGbxO3MvERFR8b9zcnIQUXpmBcyZxsY6W5pnsRhXDmfqxeO0NNXGqRfnykztV14sLns9dcNMbT3syqKw5zb2ZGoQdrRQPj4+VmfArozRzbpTJYqKipCRkYGwsDBFmXgDIQRycnIQExMDHx/7T6HFTMvHTOVjpvIxU/mYqXxKMrWrOVq9erWikBMTE+1eloiIiEhP7GqOiIiIiLyF4p8PISIiIvJkipujqVOnYuTIkTavGzVqFJ555hmniyIiIiLSiuLmaOvWrejdu7fN63r37o0tW7Y4XRQRERGRVhQ3R//73/9Qr149m9fFx8fjzz//dLYmIiIiIs0obo5CQ0ORnp5u87q0tDQEBQU5XRQRERGRVhR/W23gwIH4888/8d1338Hf3988/+bNm2jXrh1iYmKwbds26YXqBc8hUT6el0M+ZiofM5WPmcrHTOVTlKlQ6PDhwyIgIEA0bNhQvPrqq2Lt2rXilVdeEQ0bNhSBgYHiyJEjSlfpVtLT0wUAXiq4pKenM1NmqvsLM2Wm7nBhptpkqvi31dq1a4etW7di8uTJmDFjhnl+QkICtm7dirZt2ypdpVsJCwsDAKSnpyM8PFzjavQlOzsbsbGx5ozsxUzLx0zlY6byMVP5mKl8SjJ16Idn+/Tpg19//RVnz55FZmYmatSogVtvvdWRVbmd4t2U4eHhHHjlULorl5lWjpnKx0zlY6byMVP57MnUoeao2K233uo1TVEZhYXA5s2m6QEDAD+noiSAmZL7KCwEio+t5FiVg5nKx0wdxqQc5ecHDB6sdRWehZmSu+BYlY+ZysdMHcafDyEiIiKywD1HjjIagb17TdOdOwO+vpqW4xGYKbkLoxHYv980zbEqBzOVj5k6jM2Ro/Lzge7dTdPXrwOhodrW4wmYKbkLjlX5mKl8zNRhbI4cZTAATZqUTJPzmCm5C45V+ZipfMzUYQ41Rzdv3sSaNWuwa9cuZGVlISoqCj179sSoUaOszprt0UJCgJMnta7CszBTchccq/IxU/mYqcMUN0fXrl1Djx49cOzYMYSGhqJWrVo4ePAg1q9fjyVLlmDXrl08twIRERG5LcXfVps1axbOnDmDjRs3IicnB2fPnkVOTg4+/vhjnDlzBrNmzVKjTiIiIiKXUNwcbd68GXPnzsX9999vNX/o0KFISkrCpk2bpBWna3l5QK9epktentbVeAZmSu6CY1U+ZiofM3WY4o/VMjMz0aJFC5vXtWzZEpcvX3a6KLdQVATs3FkyTc5jpuQuOFblY6byMVOHKW6O6tSpgwMHDqBHjx5lrvv2228RExMjpTDdCwwE1q4tmSbnMVNyFxyr8jFT+ZipwxQ3Rw8++CDmz5+PsLAwJCYmIjIyEllZWVi7di3mz5+PqVOnqlGn/vj5ASNHal2FZ2Gm5C44VuVjpvIxU4cpbo6SkpLw/fffY9q0aXjmmWfg5+eHwsJCCCHQp08fJCUlqVAmERERkWsobo4CAwOxfft2fPXVV9izZw+ysrIQGRmJHj16oFevXmrUqE9GI5CcbJpu3ZqnZZeBmZK7MBqBY8dM0xyrcjBT+ZipwxQ3R2lpaahduzb69OmDPn36WF1XWFiIjIwMxMXFSStQt/LzgbZtTdM8LbsczJTcBceqfMxUPmbqMMXNUf369XHo0CG0LQ7cwg8//IC2bdvCaDRKKU7XDAYgPr5kmpzHTMldcKzKx0zlY6YOU9wcCSHKvc5oNMLgLU9ASAiQmqp1FZ6FmZK74FiVj5nKx0wdpvgkkABsNkAFBQX48ssvERUV5XRRRERERFqxqzmaM2cOfH194evrC4PBgPbt25v/XXwJCQnB3LlzMWjQILVrlmrBggW48847ERYWhujoaAwePBhnzpzRuiwiIiLSiF0fq7Vt2xaTJk2CEAJLlizB0KFDUbNmTatlAgMD0bx5c4wYMUKVQtWyb98+TJ48GXfeeScKCwsxa9Ys9O7dG6dOnUJoRQev5ecDY8aYpjdsAIKCXFOwJ2Om5C7y84Fhw0zTHKtyMFP5mKnD7GqO+vbti759+wIAcnNz8eKLL6J+/fqqFuYq27dvt/r3qlWrEB0djaNHj6JLly7l39BoBLZsKZkm5zFTchccq/IxU/mYqcMUH5C9atUqNerQjWvXrgEAqlevXvGCAQHA8uUl0+Q8ZkrugmNVPmYqHzN1mOLmyJMJITB16lTcddddaNasWcUL+/sD48e7pjBvwUzJXXCsysdM5WOmDmNzZOHxxx/Hjz/+iAMHDmhdiq4YDEAFZ3AgIiICYH06JXf+u8Hm6G///Oc/sXXrVnzzzTeoW7du5TcoKgJOnjRNN24M+Dh0VgSyxEzJXRQVAadPm6Y5VuVgpvIxU4d5fXMkhMA///lPbNq0CXv37rX/QPO8PKD4ozeell0OZkrugmNVPmYqn0qZFu8dcuc9Q5Xx+uZo8uTJWLduHbZs2YKwsDBcuHABABAREYHg4OCKb8wTXsrHTMldcKzKx0zlY6YOcag5unnzJtasWYNdu3YhKysLUVFR6NmzJ0aNGgV/f3/ZNapq6dKlAIBu3bpZzV+1ahXGjh1b/g1DQ4HMTPUK80Y6ztQb3imRAjoeq25LB5l63Haug0zdleLm6Nq1a+jRoweOHTuG0NBQ1KpVCwcPHsT69euxZMkS7Nq1C+Hh4WrUqoqKfiuOiIiIvI/io7NmzZqFM2fOYOPGjcjJycHZs2eRk5ODjz/+GGfOnMGsWbPUqJOIiIgkMxhKLlRCcXO0efNmzJ07F/fff7/V/KFDhyIpKQmbNm2SVpyu5ecDI0eaLvn5WlfjGZgpuQuOVfmYqXzM1GGKm6PMzEy0aNHC5nUtW7bE5cuXnS7KLRiNwLp1pgtPyy6HB2Va3jsxvjvzECqN1dLjxqve0XvQ9q8bEjKVMQYt9065y5hWfMxRnTp1cODAAfTo0aPMdd9++y1iYmKkFKZ7AQHA22+XTJPzmCm5C45V+ZipfMzUYYqbowcffBDz589HWFgYEhMTERkZiaysLKxduxbz58/H1KlT1ahTf/z9gSlTtK7CZYrPkq3q2bLdIFOP+zYLOcYNxqrbkZxpRduq0u24vOV1/3rAceowxc1RUlISvv/+e0ybNg3PPPMM/Pz8UFhYCCEE+vTpg6SkJBXKJCIiInINxc1RYGAgtm/fjq+++gp79uxBVlYWIiMj0aNHD/Tq1UuNGvWpqAhITTVNx8V5xWnZVf+c2AszJTdVVASkpZmmXTxW7dlbofs9GrZomKlsuvk9SoWZ2rO3zVs4fIbsPn36oE+fPjJrcS95eUDxT43wVPdyMFNyFxyr8jFT+Zipw7z+50OcEhKidQWqqeidj6rvilyQqYz6dfPOkLSj8fZf+p188Xgs/W03y+t0z01eU8vbi1LRN1Q1ew7cJFO9sas5uuWWW7Bp0ya0bNkS9evXh6GC/WsGgwG//fabtAJ1KzQUyM3VugrPwkzJXXCsysdM5WOmDrOrOeratav5J0G6du1aYXNE3sWT9qBU9o0Upbdz5jrybqXHnKyxUt7epsruyxPGqpLHpfS1wFv/JFo+bnceG7bY1RytWrXKPL169Wq1aiEiIiLSnOKvA6xZswZZWVk2r7ty5QrWrFnjdFFuoaAAGD/edCko0Loa1bj0nZILM7XnLK2VLaPkbMaVXaekLlJIjXGlwjrted4dHRuyx5Uq41Sl7d+rtykHM3XkTNa2ltcie1ln4zYIhT9L7+vri0OHDqFt27Zlrjt69Cjatm0Lowef+j07OxsRERG4lpGB8OKzgXvgtwAsT/pYntInhTRnc+2a+WNYe7g60/J2BZfele7oRmVPbuXVUboGpzNVeDuPlJsLVKlimr5+HdlGo/OZ+vparVPGWHX2j4gj49bRj9VUGaeSMy0vB1vbZ3nZOfNaUF5W9nJVplo1jmp9DGfr8TgyThV/W62iXio/Px++vr5KV+me/P2BefNKpj2QyzcalTN11TdJHMnNa9/ZukLpcSXjzZsOt39nx53SY+2kk5Sp7D1ksm/r0rNt28hUL681ej+2za7mKC0tDanFJ+cD8P333yO/1C/85uXlYfny5YiLi5NaoG4FBACzZmldhWdhpqSG0uNKxq+Tc6zKx0zlY6YOs/uA7Dlz5sBgMMBgMGDSpElllineo7Ro0SK5FZJu6eUdiAy2zg3jivsi0gtvGZcV7UF29f266r7dmVZ7mOxqjh544AE0a9YMQgg88MADmD9/Pm699VarZQIDA9GsWTPUq1dPjTr1RwggM9M0HRXFES4DMyU1CAFcvmyajopSZ50cq85jpvIxU4fZ1Rw1btwYjRs3BmDaizRgwABERkaqWpju3bgBePAB2ZpgpqSGGzeA6GjT9PXr6qzTS8aqqn9bvTRTVblppvbu3VPzPEuKD8hOTEyUW4GbKf74MDsnp2RmdracgzzdVHZ28f9NEwq/AMlMbZCWafGKvJnlGYKzs5H997hyKlPLL5548ViVOk69LNPyNk1vy1TWS1RF63EkU4d+W+3KlStYt24dTp8+jby8PKvrDAYDVqxY4chq3ULO33/AYxs1KplZvLfDS0VEWP87JycHEaVnVoCZliUt09hYmWW5P4txJS1TLx6rqo1TL8i0vJi8LVMFD83h9TiSqeLzHKWlpeHOO+/EjRs3cOPGDURFReHKlSswGo2oVq0aIiIi8PvvvytZpVspKipCRkYGwsLC+DMqpQghkJOTg5iYGPj42H9+UWZaPmYqHzOVj5nKx0zlU5Kp4uZoxIgRuHDhArZt24YqVaogJSUFzZo1wwcffID58+dj586d5uOTiIiIiNyN4p8POXToECZOnIigoCAApk4sICAAkydPxrhx4/DMM89IL5KIiIjIVRQ3RxcvXkTt2rXh4+MDX19fqwM+u3btigMHDkgtkIiIiMiVFDdHNWvWxJUrVwAA9erVQ0pKivm61NRU+Pk5dIw3ERERkS4o7mTat2+P77//Hvfccw/uu+8+zJ07FwUFBQgICMDrr7+Ou+++W406iYiIiFxC8QHZR48eRWpqKoYMGYLc3FwMHz4cX3zxBYQQ6NKlC9avX4/atWurVS8RERGRqhQ3R7ZkZ2fDYDAgLCxMRk1EREREmlHUHOXl5aFBgwZYtmwZBg4cqGZdusVzSJSP5+WQj5nKx0zlY6byMVP5lGSq6Jij4OBg5OXlIdRNfp9FDRkZGTzrcCXS09NRt25du5dnppVjpvIxU/mYqXzMVD57MlV8QHaPHj2wc+dOrz3wuvijw/T0dISHh2tcjb5kZ2cjNjZW8cerzLR8zFQ+ZiofM5WPmcqnJFPFzdHMmTMxZMgQBAUF4b777kPt2rXL7LqrXr260tW6jeLHGh4ezoFXDqW7cplp5ZipfMxUPmYqHzOVz55MFTdHd9xxBwAgKSkJc+bMsbmMUYe//CtdYSGwebNpesAAgOd3ch4zlY+ZkrsoLAS2bTNNc6yqgxnbTXEyL774Ig/yAkyDavBgravwLMxUPmZK7oJjVX3M2G6Km6OkpCQVyiAiIiLSB+5Tc5TRCOzda5ru3Bnw9dW0HI/ATOVjpuQujEZg/37TNMeqOpix3dgcOSo/H+je3TR9/Trgxac3kIaZysdMyV1wrKqPGduNzZGjDAagSZOSaXIeM5WPmZK74FhVHzO2G5sjR4WEACdPal2FZ2Gm8jFTchccq+pjxnaz/5zkRERERF6AzRERERGRBYc+VhNCIDk5GX/88Qfy8vLKXD9mzBinC9O9vDxgyBDT9NatQHCwtvV4AmYqHzMld5GXB9xzj2maY1UdzNhuipujX375Bffccw/Onj0LIUSZ6w0Gg3c0R0VFwM6dJdPkPGYqHzMld8Gxqj5mbDfFzdHkyZORn5+PjRs3okWLFggMDFSjLv0LDATWri2ZJucxU/mYKbkLjlX1MWO7KW6OvvvuO3zwwQcYOnSoGvW4Dz8/YORIravwLMxUPmZK7oJjVX3M2G6KD8iuUqUKf+mXiIiIPJbi5uihhx7CunXr1KjFvRiNQHKy6WI0al2NZ2Cm8jFTchccq+pjxnZT/LFas2bNsH79etxzzz0YOHAgIiMjyyxz3333SSlO1/LzgbZtTdM8DbsczFQ+ZkrugmNVfczYboqboxEjRgAAzp07h23btpW53mAwwOgNHanBAMTHl0yT85ipfMyU3AXHqvqYsd0UN0d79uxRow73ExICpKZqXYVnYabyMVNyFxyr6mPGdlPcHHXt2lWNOoiIiIh0weEfns3JycGhQ4eQlZWFqKgotG/fHmFhYTJrIyIiInI5h35b7Y033kBMTAz69u2LkSNHok+fPoiJicFbb70luz79ys8HBg82XfLzta7GMzBT+ZgpuQuOVfUxY7sp3nO0Zs0aPPvss+jbty/Gjh2LmJgYZGRk4KOPPsIzzzyDGjVqYPTo0WrUqi9GI7BlS8k0OY+ZysdMyV1wrKqPGdtNcXP09ttvY8SIEVhbfAryv91///0YNWoU3n77be9ojgICgOXLS6bJecxUPmZK7oJjVX3M2G6Km6Off/4ZCxYssHndqFGjcO+99zpdlCstXboUS5cuRerfR/A3bdoUL774Ivr27VvxDf39gfHj1S/QmzBT+ZgpuQuOVfUxY7spPuYoODgYV65csXndlStXEBwc7HRRrlS3bl288sorSElJQUpKCu6++24MGjQIJ0+e1Lo0IiIi0oDi5qhz585ISkpCRkaG1fwLFy5g7ty56NKli7TiXGHgwIHo168fGjZsiIYNG+Lll19GlSpVcPjw4YpvWFQEnDxpuhQVuaZYT8dM5WOm5C44VtXHjO2m+GO1+fPno2PHjmjQoAF69OiB2rVr4/z589i9ezf8/f3x2WefqVGnSxiNRnzyySfIzc1Fhw4dKl44Lw9o1sw0zdOwy8FM5WOm5C4cHKuWJ3oWQoW6PAlfD+ymuDlq2rQpkpOTMXv2bOzZswdZWVmIjIzE4MGDMXv2bDRs2FCNOlX1008/oUOHDsjPz0eVKlWwadMmNGnSpPIbRkWpX5y3YabyMVNyFxyr6mPGdnHoJJANGzbE+vXrZdeimUaNGuH48eO4evUq/vOf/yAxMRH79u2ruEEKDQUyM11XpDdgpvJpkGnxO3m+iydFNN7+vWIPFF9j7ebwGbI9SUBAABo0aAAAaNOmDZKTk7Fo0SK8//77GldGRERErmZXczR37lw88sgjiImJwdy5cytc1mAw4IUXXpBSnFaEECgoKNC6DCKPxD1LRO7DK/ao2WBXc5SUlIR//OMfiImJQVJSUoXLultzNHPmTPTt2xexsbHIycnBhg0bsHfvXmzfvr3iG+bnAxMnmqZXrACCgtQv1tMxU/mYKbmL/Hxg3DjTNMeqOpix3exqjoosvvJX5GFf/7t48SJGjx6N8+fPIyIiAi1atMD27dvRq1evim9oNALr1pmmi884Ss7RcaaW756Ait9BGQw6eoflBpnqJivSlo7HqsewM+PSr3cV8dTt2OuPOVqxYoVjNwwIAN5+u2SanMdM5WOm5C44VtXHjO2muDnKz8/HX3/9hfDwcPO8jz/+GMeOHUPPnj3Rs2dPqQXqlr8/MGWK1lWoqngPiMv2hOgsU494R+TCTJW827RnPW6dOylnx1hVcvwLx5ENTr4eeFOmis+QPXr0aDzxxBPmf7/zzjsYNmwYXnvtNfTp0wf//e9/pRZIRERE5EqKm6PvvvsO//jHP8z/fueddzBq1ChcvXoV9913H9544w2pBepWURGQmmq6eNhxWLbI2itQIZ1m6pLHrhadZkpUhsKxajAo3zYduY1HsZFxcSaWF1nUWKerKP5YLTMzE3Xq1AEAnDt3Dr///jvWr1+P8PBwjBs3DmPGjJFepC7l5QH165umeRp2OZipfMyU3AXHqvqYsd0UN0chISG4du0aAGD//v2oUqUK2rRpAwAICgrC9evX5VaoZyEhWlfgUi45BslFmcr87NzWuspbvybfZFM5U3d8V0g6pcJYdWR8evS5fVR6PZCVmV6yV9wcNW/eHIsXL0Z8fDyWLFmC7t27w/D3o0lLS0OtWrWkF6lLoaFAbq7WVXgWZiofMyV3wbGqPmZsN8XN0QsvvIABAwagVatWCAgIwM6dO83XffHFF2jdurXUAkkb3rI3oPQeHtl7diraQ6Vkj5OeKK2xsrGk9mN2h0ypYva+HuntdUsve0Fs0UNWleWjZX6Km6O7774bp0+fxtGjR9GqVSvccsstVte1atVKZn1ERERELqXo22p5eXkYMWIE0tPTcd9991k1RgDw6KOPol27dlIL1K2CAmD8eNPFi36HrbiTV+VdhxtlausbGLr8VobETLV6fI7ery6fD0+hxrb69zo/MIxHoEE/27+948gdxlugoQAfGEwZB8DxjCv7Fpqa3yR01TfgFDVHwcHB2LJli8f9hIhDCguBDz80XQoLta7GMzBT+ZgpqUGNcfX3OsfjQ/iBY1UNfijEeDBjeyj+WK1Vq1Y4ceIEunTpokY97sPfH5g3r2TaQ1XUnUv/5pWLMtVq70d5969qPRIyrejdoSO3s3f58v5t6xuA5GKlx5XRKG2ds54HbkLd11Rnx4w9xxI6ep9qHFtTfN/+8McsmJ43tTMurwatbq+U4ubolVdewejRo9G0aVN07dpVjZrcQ0AAMGuW1lV4FmYqHzMlNZQeV/n50tY5/3nnV0W23UQA5oOvB/ZQ3BxNmjQJ169fx913341q1aqhdu3a5q/yA4DBYMAPP/wgtUjSL139An0l1Hjnwb0WrsfMSQu2xh3Hoj6o8a02xc1RZGQkoqKi5Ny7OxMCyMw0TUdFcSuRgZnKx0xJDUIAly+bpmX9Pfh7nVEATP/lWJVPIAqm540ZV0xxc7R3714VynBDN24AMTGmaZ6GXQ4NM63omCC3xnFKarhxA4iONk3L+lWEv9eZCSAU1wFwrMoWghvIhOl5C8V13HDzjCs7JraYI3uTFDdH3k78nXJ2Tk7JzOxsOQckuqns7OL/myaEwpHITMtipvJJy7R4Rd7M8izL2dnI/ntcOZWpr2/JfGQjO9s7x6qa41QgF9nm6WwAnpexrc3ToUyFAy5duiRmzJgh2rdvLxo0aCBOnDghhBBi2bJl4tixY46s0m2kp6cLALxUcElPT2emzFT3F2bKTN3hwky1ydQghLK29Ny5c+jUqROuXbuGli1b4siRI0hOTkbr1q0xefJk3LhxA6tWrVKySrdSVFSEjIwMhIWFWR2IToAQAjk5OYiJiYGPj/2n0GKm5WOm8jFT+ZipfMxUPiWZKm6O7r//fpw8eRI7d+5EdHQ0AgICkJKSgtatW2P9+vWYPXs2fvnlF6ceABEREZFWFB9ztGvXLixduhQxMTEwljp+oXbt2sjIyJBWHBEREZGrKfr5EADIz89H9erVbV6Xm5uraPcfERERkd4o7mQaNWqEnTt32rzum2++QbNmzZwuioiIiEgrij9WGz9+PKZOnYqYmBiMHDkSAPDXX3/h008/xZIlS/Dee+9JL5KIiIjIVRQfkA0AEyZMwIcffggfHx8UFRXBx8cHQgiMHz8ey5YtU6NOIiIiIpdwqDkCgMOHD+OLL77AxYsXERUVhQEDBqBjx46y6yMiIiJyKYebIyIiIiJPpPiYozZt2uDhhx/G8OHDUa1aNTVq0jWeYKt8PGmZfMxUPmYqHzOVj5nKpyhTReclF0K0bdtWGAwGERQUJIYNGya++uorUVRUpHQ1bounZpdzanZmyky1vjBTZuoOF2aqTaaK9xwdOXIEZ86cwcqVK7F27Vp8/PHHiImJwdixY5GYmIgGDRooXaVbCQsLAwCkp6cjPDxc42r0JTs7G7GxseaM7MVMy8dM5WOm8jFT+ZipfEoyVdwcAaZzHb366qtYsGABtm/fjlWrVuGNN97A/Pnzcdddd2Hfvn2OrNYtFO+mDPf1RXhEhGnm9etAaKiGVemL0l25zLRyzFQ+hzMND0e4ry9QpYrpCuZq5lSm/ENuEzO1ITfXqe3Pnkwdao6K+fj4oF+/fujXrx++/fZbDB8+HAcOHHBmle4jMBDYtKlkmpzHTOVjpupgrkTaccH251RzlJOTgw0bNmDVqlU4cuQIgoKCMHz4cFm16ZufHzB4sNZVeBZmKh8zVQdzJdKOC7Y/h34Ibffu3Rg9ejRq1aqFRx99FEVFRViyZAnOnz+PtWvXyq6RiIiIyGUU7zmqV68e0tPTER0djUmTJuHhhx9G48aN1ahN34xGYO9e03TnzoCvr6bleARmKh8zVYfRCOzfb5pmrkSu5YLtT3FzdPvtt+Pdd99Fv3794OvNLwj5+UD37qZpHpApBzOVj5mqg7kSaccF25/i5mhT8UFQ3s5gAJo0KZkm5zFT+ZipOpgrkXZcsP05dUC2VwsJAU6e1LoKz8JM5WOm6mCuRNpxwfZn1wHZvr6++O6770w38PGBr69vuRc/P/ZbRERE5L7s6mRefPFF1K1b1zzN32shIiIiT2VXczR79mzzdFJSklq1uJe8PGDIENP01q1AcLC29XgCZiofM1VHXh5wzz2maeZK5Fou2P74GZijioqAnTtLpsl5zFQ+ZqoO5kqkHRdsf4qao8zMTLz//vv45ptvkJGRAQCIiYlB9+7dMWHCBERGRqpSpC4FBgLFJ7zkzwfIwUzlY6bqYK5E2nHB9md3c7Rr1y4MGTIE2dnZ8PX1RVRUFIQQOHPmDHbu3Ik33ngDmzZtQpcuXVQpVHf8/ICRI7WuwrMwU/mYqTqYK5F2XLD92fVttczMTDz44IOIiIjAxx9/jGvXruH8+fO4cOECrl27hg0bNiA0NBRDhw5FVlaWqgUTERERqcmu5mjFihUwGo349ttvMXToUISEhJivCwkJwQMPPIADBw7g5s2bWLFihWrF6orRCCQnmy5Go9bVeAZmKh8zVQdzJdKOC7Y/uz5W+/rrr/Hwww+bv85vS1xcHB566CFs374dzz77rLQCdSs/H2jb1jTNnw+Qg5nKx0zVwVyJtOOC7c+uPUenT5/GXXfdVelynTt3xunTp50uSksLFiyAwWDAlClTKl7QYADi400XnvdJDmYqHzNVB3Ml0o4Ltj+79hxdvXoV0dHRlS4XHR2Nq1evOluTZpKTk7F8+XK0aNGi8oVDQoDUVNVr8irMVD5mqg7mSqQdF2x/du05KigogL+/f6XL+fn54a+//nK6KC1cv34dI0eOxAcffIBq1appXQ4RERFpxO6v8p85c6bS3037+eefnS5IK5MnT0b//v3Rs2dPzJs3T+tyiIiISCN2N0djx46tdBkhhFv+7tqGDRtw9OhRpKSk2H+j/HxgzJjiFQBBQeoU502YqXzMVB35+cCwYaZp5krkWi7Y/uxqjlatWiX9jvUiPT0dTz75JL7++msEKQnYaAS2bCmZJucxU/mYqTqYK5F2XLD92dUcJSYmqnLnenD06FFcunQJd9xxh3me0WjEN998g/feew8FBQXw9fUte8OAAGD58pJpch4zlY+ZqoO5EmnHBduf1//wbI8ePfDTTz9ZzXvooYdw2223Yfr06bYbIwDw9wfGj3dBhV6EmcrHTNXBXIm044Ltz+ubo7CwMDRr1sxqXmhoKCIjI8vMJyIiIs/n9c2Rw4qKgJMnTdONGwM+dp0VgSrCTOVjpuooKgKKT3jLXIlcywXbH5sjG/bu3Vv5Qnl5QPGeJf58gBzMVD5mqg7mSgSg5ATVQrjwTl2w/bE5ckZUlNYVeB5mKh8zVQdzJdKOytsfmyNHhYYCmZlaV+FZmKl8zFQdzJVIOy7Y/vhBOREREZEF7jkiItKYJsdtEKnM8gcz3G1sc8+Ro/LzgZEjTZf8fK2r8QzMVD5mqg7mSqQdF2x/bI4cZTQC69aZLvz5ADl0kqnBUPKOx3LaLekkU0tunyng0lyL8yovM8vrbV2IPI4Ltj9+rOaogADg7bdLpsl5zFQ+ZqoO5kqkHRdsf2yOHOXvD0yZonUVmjEYVPgMWWeZVvau29ZxIro7dkQnmaoyXrSkk1yJ1FDZa5/m27ILtj9+rEZERERkgXuOHFVUBKSmmqbj4rzi5wNU3yuiUaa629sjk47HqeXeJHu+1WL5PGn+nBUVAWlppmmd5UqkNlt7llz6zTQXbH9sjhyVlwfUr2+a5s8HyMFM5WOm6mCuRNpxwfbH5sgZISFaVyBNeceEuPxYEZ1m6qo9Farcj4sz1XyvjqsoyLWi49Mquw2Rs/S2TUqpR+XXNTZHjgoNBXJzta7CszBT+ZipOpgrkXZcsP2xOaJKld57ZHkOIL28E7FX6Xcslu/OlbxTt3dZvb1jU5OsPSG2lqvoOKSKsnXHMSpbRbkrycabxjK5npLt3hV4FCERERGRBTZHjiooAMaPN10KCrSuRhqlxzlIPS7ChZk6evbgys5UXNntXH4ciYbj1J7Ham8mju7hU00Fudp7Nmtnr5cxnnhWbR3RaFtV4/lXsl0rvU+DAQg0FOADg7pZsTlyVGEh8OGHpkthodbVeAZmKh8zVQdzJdk4puzmh0KMh7pZ8ZgjR/n7A/PmlUx7gMr2fKjOTTN1dG+bSz5Pd9NMlSidvytznfU88HoVf/zF43DIWaW3VZ38FqIsMvZyFrsJf8zCPLw8D+W+rjl73iU2R44KCABmzdK6Cs/CTOVjpur4O9f5z2tdCHmM0tuqSr827wluIgDzMQsvq/jSxuaIbPK04w5c/XiUHEdjecZnT6OHx8VvWcnl0jMhk1R62B6VcOQbrrLGJI85cpQQQGam6cJXCDmYqXzMVB1/5xqFTADMlSTgtqqAMG17KmbFPUeOunEDiIkxTXvYzwdo9u7CgzPVjBdm6pJzG924AURHIxNAKK7DYCibq7u9S7fFEx6D2/h7TAEwbatUrhDcQCaigWjT9gfw50M0J/5+1c3OySmZmZ3tcQfPKZGdXfx/04RQ+JfJ2zMtzs/WPGbqGFUzzc4GfH1L5iMbgGflaiu/ipaTkqm3szzjc3Y2sv/eVplpWQK5yDZPW29/srZ9NkcK5fz9xya2UaOSmcXvzL1URIT1v3NychBRemYFvD1TW1ExU+eommlsbKlrPC9Xe2NRL1MvZ7GtMtOy8gCUJGK9/cna9g1CaVvq5YqKipCRkYGwsDAYuM/ZihACOTk5iImJgY+P/YezMdPyMVP5mKl8zFQ+ZiqfkkzZHBERERFZ4LfViIiIiCywOSIiIiKywOaIiIiIyAKbIyIiIiILbI6IiIiILLA5IiIiIrLAk0AqxHNIlI/n5ZCPmcrHTOVjpvIxU/mUZMrmSKGMjAyPPvOoDOnp6ahbt67dyzPTyjFT+ZipfMxUPmYqnz2ZsjlSKCwsDIAp3PDwcI2r0Zfs7GzExsaaM7IXMy0fM5WPmcrHTOVjpvIpyZTNkULFuynDw8M58MqhdFcuM60cM5WPmcrHTOVjpvLZkymbI0cVFgKbN5umBwwA/Bil05ipfMzUtQoLgW3bTNPMmyrD8aJbfCYc5ecHDB6sdRWehZnKx0xdi3mTEhwvusWv8hMRERFZ4J4jRxmNwN69punOnQFfX03L8QjMVD5m6lpGI7B/v2maeVNlOF50i82Ro/Lzge7dTdPXrwOhodrW4wmYqXzM1LWYNynB8aJbbI4cZTAATZqUTJPzmKl8zNS1mDcpwfGiW2yOHBUSApw8qXUVnoWZysdMXYt5kxIcL7rFA7KJiIiILLA5IiIiIrLA5shReXlAr16mS16e1tV4BmYqHzN1LeZNSnC86BaPOXJUURGwc2fJNDmPmcrHTF2LeZMSHC+6xebIUYGBwNq1JdPkPGYqHzN1LeZNSnC86BabI0f5+QEjR2pdhWdhpvIxU9di3qQEx4tu8ZgjIiIiIgvcc+QooxFITjZNt27N077LwEzlY6auZTQCx46Zppk3VYbjRbfYHDkqPx9o29Y0zdO+y8FM5WOmrsW8SQmOF91ic+QogwGIjy+ZJucxU/mYqWsxb1KC40W32Bw5KiQESE3VugrPwkzlY6auxbxJCY4X3eIB2UREREQW2BwRERERWWBz5Kj8fGDwYNMlP1/rajwDM5WPmboW8yYlOF50y+uPOUpKSsKcOXOs5tWsWRMXLlyo+IZGI7BlS8k0OY+ZysdMXYt5kxIcL7rl9c0RADRt2hQ7i3/fBoCvPeeaCAgAli8vmSbnMVP5mKlrMW9SguNFt9gcAfDz80OtWrWU3cjfHxg/Xp2CvBUzlY+ZuhbzJiU4XnSLxxwBOHv2LGJiYlC/fn0MGzYMv//+u9YlERERkUa8vjlq164d1qxZg6+++goffPABLly4gI4dOyIrK6viGxYVASdPmi5FRa4p1tMxU/mYqWsxb1KC40W3vP5jtb59+5qnmzdvjg4dOiAhIQEfffQRpk6dWv4N8/KAZs1M0zztuxwuzrT4hLRCqHo32uI4dS3mTUpwvOiW1zdHpYWGhqJ58+Y4e/Zs5QtHRalfkLdhpvIxU9di3qQEx4susTkqpaCgAKdPn0bnzp0rXjA0FMjMdE1R3kJipl6xV8geGo1Tr83fRXlb/gyXZcZem7u74t8R3fL6Y46mTZuGffv24dy5czhy5AiGDh2K7OxsJCYmal0aERERacDr9xz9+eefGD58OC5fvowaNWqgffv2OHz4MOKLfymZbDIYPOfdKd9tkx5xXOoXnxvP5/XN0YYNGxy7YX4+MHGiaXrFCiAoSF5R3oqZysdMXSs/Hxg3zjTNvKkyHC+65fUfqznMaATWrTNdeNp3OXSSqcFgfUyHW9NJpsU8KltbVMi7ODOPzk3HVM1eZ9snlfD6PUcOCwgA3n67ZJqcx0zlY6auxbxJCY4X3WJz5Ch/f2DKFK2r8CwqZOr1xwa4eJyWfoddWf6ll3f754mvC7pT3jf7dIHjRbf4sRoRERGRBe45clRREZCaapqOiwN82Gc6zUWZetWxGy7I1JE8y7uN2+/pKyoC0tJM05XkLWsc2lqPrveWuIHKMrU1z6GcFYwXci02R47KywPq1zdN87TvcjBT+ZipazFvUoLjRbfYHDkjJETrClxO9fMbqZSpK/cW6W7vhw4y9aq9dRXk7VU5kBVbrwuhVYBLCEGo9/0p0T02R44KDQVyc7WuwrMwU/mYqWsxb1LgBkJRBbkQHDK6w+aIHKbHs2Sr8c68vHXa+9h5/Idyutv75mHcNd/KtiV7tzU1tklnMuVrhP7w6C8iIiIiC2yOHFVQAIwfb7oUFGhdjepccoZeN81U12cvdtNM3ZbKeTsy1nQ9Pu0k63FXtB5ZZyJXsp4AFGA5xmM5xiMA3D6lc2J7ZHPkqMJC4MMPTZfCQq2r8QzMVD5m6lrMmxTwQyHG40OMx4fwA8eLdE5sjzzmyFH+/sC8eSXTXsTyHZHU446cyFTGO2NH3pk6e1+qH1+gcaauUDrLyv6tZg3+8MczMOX9ehV/3IT6920vR75dqEXdrr5vV49zy/vzhz9m/T1ebsK7/o64ROnXPwW/X8fmyFEBAcCsWVpX4VmYqXzM1KVuIgDzwbzJPhwvKiv9+pefb/dN2RxRpdxlD4JsMo49cHRZPexp0IPycnG3MekO9dp7Bmi9fLPKXTMl98DmyFFCAJmZpumoKG4FMjBT+ZipiwlE4TIA4DKiADBvqgjHi6qEAC6b8kVUlKKbsjly1I0bQEyMaZqnfZfDzTO1p+9weW/i5pmWVlF+tr6ZpLbS9xGCG8hENAAgFNdxA+6dtyX21fJ58njRhRs3gGhTvrh+XdFN2RwpJP7ej5ydk1MyMztb0YFeniY7u/j/pgmhcF+7N2danF1585mpPNIyLe9JAyCQi2zzdDYAfeddwUNRdHs1MnW2Nndga7y4Ypx6Dcuz1WdnI/vv1z97MmVzpFDO339sYhs1KplZ/M7cS0VEWP87JycHEaVnVsCbMy0vJmYqn7RMY2PLXSYPQMka9Z+3godv1+1lZupsbe7A1nhxxTj1Shavf/ZkahBK21IvV1RUhIyMDISFhcHA/cxWhBDIyclBTEwMfHzsP4UWMy0fM5WPmcrHTOVjpvIpyZTNEREREZEFniGbiIiIyAKbIyIiIiILbI6IiIiILLA5IiIiIrLA5oiIiIjIApsjIiIiIgtsjoiIiIgssDkiIiIissDmiIiIiMgCmyMiIiIiC2yOiIiIiCywOSIiIiKywOaIiIiIyAKbIyIiIiILbI6IiIiILLA5IiIiIrLA5oiIiIjIApsjIiIiIgtsjoiIiIgssDkiIiIissDmiIiIiMgCmyMiIiIiC2yOiIiIiCywOSIiIiKywOaIiIiIyAKbIyIiIiILbI6IiIiILLA5IiIiIrLA5oiIiIjIApsjIiIiIgtsjoiIiIgssDkikmT16tUwGAzmS1BQEGrVqoXu3btjwYIFuHTpUpnbJCUlwWAwaFCt/QwGA5KSkrQuwyGnTp1CUlISUlNTy1w3duxY1KtXT+r9vfvuu2jQoAECAgJgMBhw9epVqet3teIxbSs/Ik/G5ohIslWrVuHQoUPYsWMHFi9ejFatWuHVV19F48aNsXPnTqtlH3nkERw6dEijSu1z6NAhPPLII1qX4ZBTp05hzpw5Nv+4v/DCC9i0aZO0+zp+/DieeOIJdO/eHbt378ahQ4cQFhYmbf1E5Dp+WhdA5GmaNWuGNm3amP89ZMgQPPXUU7jrrrtw33334ezZs6hZsyYAoG7duqhbt67La7xx4wZCQkLsWrZ9+/YqV2M/JXVXJiEhQcp6ip08eRIAMH78eLRt27bCZWU+DiKSj3uOiFwgLi4Ob775JnJycvD++++b55f+WG3w4MGIj49HUVFRmXW0a9cOrVu3Nv9bCIElS5agVatWCA4ORrVq1TB06FD8/vvvVrfr1q0bmjVrhm+++QYdO3ZESEgIHn74YQDA7t270a1bN0RGRiI4OBhxcXEYMmQIbty4Yb69rY/VTpw4gUGDBqFatWoICgpCq1at8NFHH1kts3fvXhgMBqxfvx6zZs1CTEwMwsPD0bNnT5w5c6bSzIqzOXbsGIYOHYpq1aqZG5qUlBQMGzYM9erVQ3BwMOrVq4fhw4fjjz/+MN9+9erVuP/++wEA3bt3N3/cuXr1agC2P1bLz8/Hc889h/r16yMgIAB16tTB5MmTK/14rFu3bhg1ahQA0/NkMBgwduzYSvNPS0vDqFGjEB0djcDAQDRu3Bhvvvmm1fOfmpoKg8GA119/Ha+++qr5MXfr1g2//PILbt68iRkzZiAmJgYRERG49957bX6Ea8uRI0cwcOBAREZGIigoCAkJCZgyZUqFt9mxYwcGDRqEunXrIigoCA0aNMCjjz6Ky5cvWy2XmZmJCRMmIDY2FoGBgahRowY6depktff0+++/x4ABA8yPPyYmBv3798eff/5pV/1EauGeIyIX6devH3x9ffHNN9+Uu8zDDz+MQYMGYffu3ejZs6d5/s8//4zvvvsO77zzjnneo48+itWrV+OJJ57Aq6++iitXrmDu3Lno2LEjfvjhB/PeKQA4f/48Ro0ahWeffRbz58+Hj48PUlNT0b9/f3Tu3BkrV65E1apV8b///Q/bt2/HX3/9Ve6ejTNnzqBjx46Ijo7GO++8g8jISKxduxZjx47FxYsX8eyzz1otP3PmTHTq1AkffvghsrOzMX36dAwcOBCnT5+Gr69vpbndd999GDZsGB577DHk5uYCMDUMjRo1wrBhw1C9enWcP38eS5cuxZ133olTp04hKioK/fv3x/z58zFz5kwsXrzY3FiWt8dICIHBgwdj165deO6559C5c2f8+OOPmD17Ng4dOoRDhw4hMDDQ5m2XLFmC9evXY968eVi1ahVuu+021KhRo8L8MzMz0bFjR/z111946aWXUK9ePWzbtg3Tpk3Db7/9hiVLlljdx+LFi9GiRQssXrwYV69exdNPP42BAweiXbt28Pf3x8qVK/HHH39g2rRpeOSRR7B169YKc/3qq68wcOBANG7cGG+99Rbi4uKQmpqKr7/+usLb/fbbb+jQoQMeeeQRREREIDU1FW+99Rbuuusu/PTTT/D39wcAjB49GseOHcPLL7+Mhg0b4urVqzh27BiysrIAALm5uejVqxfq16+PxYsXo2bNmrhw4QL27NmDnJycCmsgUp0gIilWrVolAIjk5ORyl6lZs6Zo3Lix+d+zZ88WlpvhzZs3Rc2aNcWIESOsbvfss8+KgIAAcfnyZSGEEIcOHRIAxJtvvmm1XHp6uggODhbPPvuseV7Xrl0FALFr1y6rZT/99FMBQBw/frzCxwVAzJ492/zvYcOGicDAQJGWlma1XN++fUVISIi4evWqEEKIPXv2CACiX79+Vst9/PHHAoA4dOhQhfdbnM2LL75Y4XJCCFFYWCiuX78uQkNDxaJFi8zzP/nkEwFA7Nmzp8xtEhMTRXx8vPnf27dvFwDEa6+9ZrXcxo0bBQCxfPnyCmso7/kvL/8ZM2YIAOLIkSNW8ydOnCgMBoM4c+aMEEKIc+fOCQCiZcuWwmg0mpdbuHChACDuueceq9tPmTJFABDXrl2rsN6EhASRkJAg8vLyKn1M586ds3l9UVGRuHnzpvjjjz8EALFlyxbzdVWqVBFTpkwpd90pKSkCgNi8eXOFdRJpgR+rEbmQEKLC6/38/DBq1Ch89tlnuHbtGgDAaDTiX//6FwYNGoTIyEgAwLZt22AwGDBq1CgUFhaaL7Vq1ULLli2xd+9eq/VWq1YNd999t9W8Vq1aISAgABMmTMBHH31U5uO48uzevRs9evRAbGys1fyxY8fixo0bZQ4wv+eee6z+3aJFCwCw+gisIkOGDCkz7/r165g+fToaNGgAPz8/+Pn5oUqVKsjNzcXp06ftWm9pu3fvBgDzx2HF7r//foSGhmLXrl0OrRewnf/u3bvRpEmTMscnjR07FkIIcz3F+vXrBx+fkpfsxo0bAwD69+9vtVzx/LS0tHLr+eWXX/Dbb79h3LhxCAoKUvRYLl26hMceewyxsbHw8/ODv78/4uPjAcAq+7Zt22L16tWYN28eDh8+jJs3b1qtp0GDBqhWrRqmT5+OZcuW4dSpU4rqIFITmyMiF8nNzUVWVhZiYmIqXO7hhx9Gfn4+NmzYAMD08cf58+fx0EMPmZe5ePEihBCoWbMm/P39rS6HDx8uc/xH7dq1y9xPQkICdu7ciejoaEyePBkJCQlISEjAokWLKqwvKyvL5vqKH1fxxybFihu6YsUfTeXl5VV4PxXVPmLECLz33nt45JFH8NVXX+G7775DcnIyatSoYfd6S8vKyoKfn5/Vx2GA6ZirWrVqlXlcSth6DEpzrF69utW/AwICKpyfn59fbj2ZmZkAoPjLAEVFRejduzc+++wzPPvss9i1axe+++47HD58GID1c7px40YkJibiww8/RIcOHVC9enWMGTMGFy5cAABERERg3759aNWqFWbOnImmTZsiJiYGs2fPLtNIEbkajzkicpEvvvgCRqMR3bp1q3C54r0Jq1atwqOPPopVq1YhJiYGvXv3Ni8TFRUFg8GA/fv32zwOpvS88s6l1LlzZ3Tu3BlGoxEpKSl49913MWXKFNSsWRPDhg2zeZvIyEicP3++zPyMjAxzbTKVrv3atWvYtm0bZs+ejRkzZpjnFxQU4MqVKw7fT2RkJAoLC5GZmWnVIAkhcOHCBdx5550Or9tW/q7O0VLx41N64POJEyfwww8/YPXq1UhMTDTP//XXX8ssGxUVhYULF2LhwoVIS0vD1q1bMWPGDFy6dAnbt28HADRv3hwbNmyAEAI//vgjVq9ejblz5yI4ONjquSVyNe45InKBtLQ0TJs2DREREXj00UcrXf6hhx7CkSNHcODAAXz++edITEy0Onh5wIABEELgf//7H9q0aVPm0rx5c0X1+fr6ol27dli8eDEA4NixY+Uu26NHD+zevdv8R7zYmjVrEBISovpX/w0GA4QQZRrADz/8EEaj0Wqekr1UPXr0AACsXbvWav5//vMf5Obmmq+XpUePHjh16lSZrNesWQODwYDu3btLvT9LDRs2REJCAlauXImCggK7b1fc5JXO3vIbmLbExcXh8ccfR69evWyOLYPBgJYtW+Ltt99G1apVKxx/RK7APUdEkp04ccJ8DNClS5ewf/9+rFq1Cr6+vti0aVOZj21sGT58OKZOnYrhw4ejoKCgzHEwnTp1woQJE/DQQw8hJSUFXbp0QWhoKM6fP48DBw6gefPmmDhxYoX3sWzZMuzevRv9+/dHXFwc8vPzsXLlSgCw+qZcabNnz8a2bdvQvXt3vPjii6hevTr+/e9/44svvsBrr72GiIiIykNyQnh4OLp06YLXX38dUVFRqFevHvbt24cVK1agatWqVss2a9YMALB8+XKEhYUhKCgI9evXL/NRHwD06tULffr0wfTp05GdnY1OnTqZv612++23Y/To0VIfx1NPPYU1a9agf//+mDt3LuLj4/HFF19gyZIlmDhxIho2bCj1/kpbvHgxBg4ciPbt2+Opp55CXFwc0tLS8NVXX+Hf//63zdvcdtttSEhIwIwZMyCEQPXq1fH5559jx44dVstdu3YN3bt3x4gRI3DbbbchLCwMycnJ2L59O+677z4ApuPmlixZgsGDB+OWW26BEAKfffYZrl69il69eqn62Ikqw+aISLLiY4MCAgJQtWpVNG7cGNOnT8cjjzxiV2MEwHy+mnXr1qFTp042/1C+//77aN++Pd5//30sWbIERUVFiImJQadOnSo9CSFgOiD766+/xuzZs3HhwgVUqVIFzZo1w9atW60+wiutUaNGOHjwIGbOnInJkycjLy8PjRs3xqpVq8o0cWpZt24dnnzySTz77LMoLCxEp06dsGPHjjIHJ9evXx8LFy7EokWL0K1bNxiNxnLrNBgM2Lx5M5KSkrBq1Sq8/PLLiIqKwujRozF//vxyv8bvqBo1auDgwYN47rnn8NxzzyE7Oxu33HILXnvtNUydOlXqfdnSp08ffPPNN5g7dy6eeOIJ5Ofno27dumUOoLfk7++Pzz//HE8++SQeffRR+Pn5oWfPnti5cyfi4uLMywUFBaFdu3b417/+hdTUVNy8eRNxcXGYPn26+VQPt956K6pWrYrXXnsNGRkZCAgIQKNGjcp8ZEekBYOo7OszRERERF6ExxwRERERWWBzRERERGSBzRERERGRBTZHRERERBbYHBERERFZYHNEREREZIHNEREREZEFNkdEREREFtgcEREREVlgc0RERERkgc0RERERkYX/B3WAPx/9bAhJAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D1p = {j : (D1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg, sharex=False, sharey=False)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D1p[j], num_bins, range = (np.quantile(D1p[j], 0.10), np.quantile(D1p[j], 0.90)), color = 'b', alpha = 1) # IPDL is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled IPDL diversion ratios by class')\n",
    "fig.supxlabel('Diversion ratio from class')\n",
    "fig.supylabel('Diversion ratio to class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also calculate the mean diversion ratios within each class. For the Logit model these are given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean diversion ratio wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean diversion ratio of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>32.977441</td>\n",
       "      <td>30.037606</td>\n",
       "      <td>21.805955</td>\n",
       "      <td>9.610813</td>\n",
       "      <td>5.568185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.716873</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.579788</td>\n",
       "      <td>0.413296</td>\n",
       "      <td>0.180329</td>\n",
       "      <td>0.109713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.685594</td>\n",
       "      <td>0.611145</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.413182</td>\n",
       "      <td>0.180340</td>\n",
       "      <td>0.109739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.522081</td>\n",
       "      <td>0.609950</td>\n",
       "      <td>0.578479</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.179993</td>\n",
       "      <td>0.109496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.294337</td>\n",
       "      <td>0.608266</td>\n",
       "      <td>0.576919</td>\n",
       "      <td>0.411271</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.109208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98.225371</td>\n",
       "      <td>0.607820</td>\n",
       "      <td>0.576488</td>\n",
       "      <td>0.410944</td>\n",
       "      <td>0.179377</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean diversion ratio wrt. class           0           1           2  \\\n",
       "Mean diversion ratio of class                                         \n",
       "0                               -100.000000   32.977441   30.037606   \n",
       "1                                 98.716873 -100.000000    0.579788   \n",
       "2                                 98.685594    0.611145 -100.000000   \n",
       "3                                 98.522081    0.609950    0.578479   \n",
       "4                                 98.294337    0.608266    0.576919   \n",
       "5                                 98.225371    0.607820    0.576488   \n",
       "\n",
       "Mean diversion ratio wrt. class           3           4           5  \n",
       "Mean diversion ratio of class                                        \n",
       "0                                 21.805955    9.610813    5.568185  \n",
       "1                                  0.413296    0.180329    0.109713  \n",
       "2                                  0.413182    0.180340    0.109739  \n",
       "3                               -100.000000    0.179993    0.109496  \n",
       "4                                  0.411271 -100.000000    0.109208  \n",
       "5                                  0.410944    0.179377 -100.000000  "
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D0.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. class', index = 'Mean diversion ratio of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the IPDL model the mean diversion ratios are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean diversion ratio wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean diversion ratio of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>31.645728</td>\n",
       "      <td>29.781028</td>\n",
       "      <td>22.700876</td>\n",
       "      <td>10.083722</td>\n",
       "      <td>5.788647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44.323550</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>21.348908</td>\n",
       "      <td>20.894545</td>\n",
       "      <td>9.237334</td>\n",
       "      <td>4.195663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49.273352</td>\n",
       "      <td>25.956544</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>12.240898</td>\n",
       "      <td>8.558105</td>\n",
       "      <td>3.971100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>46.540857</td>\n",
       "      <td>31.204486</td>\n",
       "      <td>15.591277</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>4.657714</td>\n",
       "      <td>2.005667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43.108902</td>\n",
       "      <td>29.673367</td>\n",
       "      <td>21.694605</td>\n",
       "      <td>8.969545</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-3.446419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>50.208758</td>\n",
       "      <td>28.195159</td>\n",
       "      <td>22.883511</td>\n",
       "      <td>9.533396</td>\n",
       "      <td>-10.820823</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean diversion ratio wrt. class           0           1           2  \\\n",
       "Mean diversion ratio of class                                         \n",
       "0                               -100.000000   31.645728   29.781028   \n",
       "1                                 44.323550 -100.000000   21.348908   \n",
       "2                                 49.273352   25.956544 -100.000000   \n",
       "3                                 46.540857   31.204486   15.591277   \n",
       "4                                 43.108902   29.673367   21.694605   \n",
       "5                                 50.208758   28.195159   22.883511   \n",
       "\n",
       "Mean diversion ratio wrt. class           3           4           5  \n",
       "Mean diversion ratio of class                                        \n",
       "0                                 22.700876   10.083722    5.788647  \n",
       "1                                 20.894545    9.237334    4.195663  \n",
       "2                                 12.240898    8.558105    3.971100  \n",
       "3                               -100.000000    4.657714    2.005667  \n",
       "4                                  8.969545 -100.000000   -3.446419  \n",
       "5                                  9.533396  -10.820823 -100.000000  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D1.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. class', index = 'Mean diversion ratio of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "To conclude we compare parameter estimates from the above models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit_MLE</th>\n",
       "      <th>IPDL_MLE</th>\n",
       "      <th>FKN</th>\n",
       "      <th>IPDL_BLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.468*** (4e-05)</td>\n",
       "      <td>-2.534 (4.42209)</td>\n",
       "      <td>-10.048*** (0.00308)</td>\n",
       "      <td>-10.137*** (0.02973)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.318*** (2e-05)</td>\n",
       "      <td>-0.314 (2.85188)</td>\n",
       "      <td>-0.922*** (0.0015)</td>\n",
       "      <td>-1.21*** (0.01525)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.457*** (2e-05)</td>\n",
       "      <td>-0.453 (2.66961)</td>\n",
       "      <td>-2.809*** (0.00193)</td>\n",
       "      <td>-3.558*** (0.01782)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.948*** (2e-05)</td>\n",
       "      <td>-0.939 (2.34671)</td>\n",
       "      <td>-0.054*** (0.00137)</td>\n",
       "      <td>-0.092*** (0.01693)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.928*** (3e-05)</td>\n",
       "      <td>-1.944 (3.27157)</td>\n",
       "      <td>-1.562*** (0.00143)</td>\n",
       "      <td>-1.195*** (0.01992)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.056*** (3e-05)</td>\n",
       "      <td>-2.083 (4.52762)</td>\n",
       "      <td>5.531*** (0.00299)</td>\n",
       "      <td>5.157*** (0.03083)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.021*** (3e-05)</td>\n",
       "      <td>-2.071 (3.7952)</td>\n",
       "      <td>-0.06*** (0.00179)</td>\n",
       "      <td>0.135*** (0.02412)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.715*** (1e-05)</td>\n",
       "      <td>-0.739 (1.45557)</td>\n",
       "      <td>-0.883*** (0.00103)</td>\n",
       "      <td>-0.642*** (0.01044)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.373*** (3e-05)</td>\n",
       "      <td>-1.382 (3.07874)</td>\n",
       "      <td>2.863*** (0.00202)</td>\n",
       "      <td>3.848*** (0.02156)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.01*** (1e-05)</td>\n",
       "      <td>-0.052 (1.18038)</td>\n",
       "      <td>0.302*** (0.00064)</td>\n",
       "      <td>0.445*** (0.00918)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.847*** (1e-05)</td>\n",
       "      <td>0.844 (1.10182)</td>\n",
       "      <td>-0.329*** (0.00101)</td>\n",
       "      <td>-0.158*** (0.00214)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.025 (0.53185)</td>\n",
       "      <td>0.646*** (0.00024)</td>\n",
       "      <td>0.916*** (0.0016)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.037 (0.17061)</td>\n",
       "      <td>-0.027*** (9e-05)</td>\n",
       "      <td>-0.033*** (0.00087)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.059 (0.1569)</td>\n",
       "      <td>-0.004*** (9e-05)</td>\n",
       "      <td>-0.019*** (0.00083)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.002 (0.18101)</td>\n",
       "      <td>0.0 (0.0001)</td>\n",
       "      <td>-0.024*** (0.0009)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.002 (0.20766)</td>\n",
       "      <td>-0.019*** (0.0001)</td>\n",
       "      <td>-0.06*** (0.00088)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.035 (0.18672)</td>\n",
       "      <td>-0.021*** (8e-05)</td>\n",
       "      <td>-0.019*** (0.00081)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.027 (0.12911)</td>\n",
       "      <td>0.001*** (7e-05)</td>\n",
       "      <td>-0.024*** (0.00066)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.059 (0.16816)</td>\n",
       "      <td>-0.021*** (9e-05)</td>\n",
       "      <td>-0.026*** (0.00083)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.031 (0.16214)</td>\n",
       "      <td>-0.02*** (9e-05)</td>\n",
       "      <td>-0.005*** (0.00082)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.1 (0.1429)</td>\n",
       "      <td>-0.024*** (9e-05)</td>\n",
       "      <td>-0.062*** (0.00084)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.049 (0.18287)</td>\n",
       "      <td>0.28*** (0.00023)</td>\n",
       "      <td>0.163*** (0.00088)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.036 (0.25112)</td>\n",
       "      <td>-0.216*** (0.00022)</td>\n",
       "      <td>-0.334*** (0.0011)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.092 (0.45236)</td>\n",
       "      <td>-0.077*** (0.00012)</td>\n",
       "      <td>-0.036*** (0.00131)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Logit_MLE          IPDL_MLE                   FKN  \\\n",
       "in_out        -2.468*** (4e-05)  -2.534 (4.42209)  -10.048*** (0.00308)   \n",
       "cy            -0.318*** (2e-05)  -0.314 (2.85188)    -0.922*** (0.0015)   \n",
       "hp            -0.457*** (2e-05)  -0.453 (2.66961)   -2.809*** (0.00193)   \n",
       "we            -0.948*** (2e-05)  -0.939 (2.34671)   -0.054*** (0.00137)   \n",
       "le            -1.928*** (3e-05)  -1.944 (3.27157)   -1.562*** (0.00143)   \n",
       "wi            -2.056*** (3e-05)  -2.083 (4.52762)    5.531*** (0.00299)   \n",
       "he            -2.021*** (3e-05)   -2.071 (3.7952)    -0.06*** (0.00179)   \n",
       "li            -0.715*** (1e-05)  -0.739 (1.45557)   -0.883*** (0.00103)   \n",
       "sp            -1.373*** (3e-05)  -1.382 (3.07874)    2.863*** (0.00202)   \n",
       "ac              0.01*** (1e-05)  -0.052 (1.18038)    0.302*** (0.00064)   \n",
       "pr             0.847*** (1e-05)   0.844 (1.10182)   -0.329*** (0.00101)   \n",
       "group_in_out              - (-)   0.025 (0.53185)    0.646*** (0.00024)   \n",
       "group_cy                  - (-)  -0.037 (0.17061)     -0.027*** (9e-05)   \n",
       "group_hp                  - (-)   -0.059 (0.1569)     -0.004*** (9e-05)   \n",
       "group_we                  - (-)  -0.002 (0.18101)          0.0 (0.0001)   \n",
       "group_le                  - (-)  -0.002 (0.20766)    -0.019*** (0.0001)   \n",
       "group_wi                  - (-)   0.035 (0.18672)     -0.021*** (8e-05)   \n",
       "group_he                  - (-)  -0.027 (0.12911)      0.001*** (7e-05)   \n",
       "group_li                  - (-)  -0.059 (0.16816)     -0.021*** (9e-05)   \n",
       "group_sp                  - (-)  -0.031 (0.16214)      -0.02*** (9e-05)   \n",
       "group_ac                  - (-)     -0.1 (0.1429)     -0.024*** (9e-05)   \n",
       "group_brand               - (-)   0.049 (0.18287)     0.28*** (0.00023)   \n",
       "group_home                - (-)  -0.036 (0.25112)   -0.216*** (0.00022)   \n",
       "group_cla                 - (-)   0.092 (0.45236)   -0.077*** (0.00012)   \n",
       "\n",
       "                          IPDL_BLP  \n",
       "in_out        -10.137*** (0.02973)  \n",
       "cy              -1.21*** (0.01525)  \n",
       "hp             -3.558*** (0.01782)  \n",
       "we             -0.092*** (0.01693)  \n",
       "le             -1.195*** (0.01992)  \n",
       "wi              5.157*** (0.03083)  \n",
       "he              0.135*** (0.02412)  \n",
       "li             -0.642*** (0.01044)  \n",
       "sp              3.848*** (0.02156)  \n",
       "ac              0.445*** (0.00918)  \n",
       "pr             -0.158*** (0.00214)  \n",
       "group_in_out     0.916*** (0.0016)  \n",
       "group_cy       -0.033*** (0.00087)  \n",
       "group_hp       -0.019*** (0.00083)  \n",
       "group_we        -0.024*** (0.0009)  \n",
       "group_le        -0.06*** (0.00088)  \n",
       "group_wi       -0.019*** (0.00081)  \n",
       "group_he       -0.024*** (0.00066)  \n",
       "group_li       -0.026*** (0.00083)  \n",
       "group_sp       -0.005*** (0.00082)  \n",
       "group_ac       -0.062*** (0.00084)  \n",
       "group_brand     0.163*** (0.00088)  \n",
       "group_home      -0.334*** (0.0011)  \n",
       "group_cla      -0.036*** (0.00131)  "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_index = pr_index + 1\n",
    "Logit_nest = ['-' for i in np.arange(G)]\n",
    "Logitbeta_show = [*(np.round(Logit_beta[:beta_index], decimals = 3).astype('str')), *Logit_nest]\n",
    "Logitse_show = [*(np.round(Logit_SE[:beta_index], decimals=5).astype('str')), *Logit_nest]\n",
    "Logitp_show = [*Logit_p[:beta_index], *[1 for i in np.arange(G)]]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Logit_MLE' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip(Logitp_show, Logitbeta_show, Logitse_show)],\n",
    "    'IPDL_MLE': [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*IPDL_p[:beta_index], *IPDL_p[K:]], [*(np.round(IPDL_theta[:beta_index], decimals = 3).astype('str')), *(np.round(IPDL_theta[K:], decimals = 3).astype('str'))], [*(np.round(IPDL_SE[:beta_index], decimals=5).astype('str')), *(np.round(IPDL_SE[K:], decimals=5).astype('str'))])],\n",
    "    'FKN' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*FKN_p[:beta_index], *FKN_p[K:]], [*(np.round(FKN_theta[:beta_index], decimals = 3).astype('str')), *(np.round(FKN_theta[K:], decimals = 3).astype('str'))], [*(np.round(FKN_SE[:beta_index], decimals=5).astype('str')), *(np.round(FKN_SE[K:], decimals=5).astype('str'))])],\n",
    "    'IPDL_BLP' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*OptBLP_p[:beta_index], *OptBLP_p[K:]], [*(np.round(ThetaOptBLP[:beta_index], decimals = 3).astype('str')), *(np.round(ThetaOptBLP[K:], decimals = 3).astype('str'))], [*(np.round(SEOptBLP[:beta_index], decimals=5).astype('str')), *(np.round(SEOptBLP[K:], decimals=5).astype('str'))])]\n",
    "}, \n",
    "index = [*x_vars[:beta_index], *['group_' + par for par in nest_vars]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
