{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Demand for Cars with the IPDL model\n",
    "\n",
    "In this notebook, we will explore the dataset used in\n",
    "Goldberg & Verboven (2005). We will estimate the IPDL Model\n",
    "model given the available data using the functions defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1180,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "import numba as nb\n",
    "\n",
    "# Files\n",
    "import Logit_file as logit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "The dataset consists of approximately 110 vehicle makes per year in the period 1970-1999 in five european markets (Belgium, France, Germany, Italy, and the United Kingdom). Furthermore, the data contains information on various characteristics of the makes such as sales, prices, horse power, weight and other physical car characteristics. Also these characteristics may vary across markets. \n",
    "\n",
    "A observation in our analysis will be a market in a given year such that e.g. the French car market in 1995 counts as a single observation. If $Y = 30$ is the number of years, and $M = 5$ is the number of country-level markets, we thus have $T=Y\\cdot M = 150$ markets and observations. In addition, since the available vehicle makes vary across time and place, let $\\mathcal{J}_t$ denote the set of available makes in each market $t=1,\\ldots,T$, and let $\\mathcal{J} := \\bigcup_{t=1}^T \\mathcal{J}_t$ be the set of all makes which were available in some market. Then $J:=\\#\\mathcal{J}$ is the number of makes which were available at some point of time in the period in at least one country-level market. In our dataset there are $J = 356$ unique vehicle makes. Note also however that characteristics of vehicle makes vary across markets.\n",
    "\n",
    "Our dataset includes 47 variables in total. The first three columns are market and product codes for the year, country, and make. Another variable is quantity sold (No. of new registrations) which will be used in computing observed market shares. The remaining 43 variables are potential explanatory variables. We will only consider the subset of these which describes car characteristics such as brand, after-tax price, horse power, etc. which adds up to $K=20$ characteristics. The remaining 23 variables are mainly macroeconomic variables such as e.g. GDP per capita which have been used to construct estimates of e.g. the average wage income and purchasing power. Since we are only interested in utility-shifting variables, we will not consider the latter columns. \n",
    "\n",
    "Reading in the dataset `eurocars.csv` we thus have a dataframe of $\\sum_{t=1}^T \\#\\mathcal{J}_t = 11459$ rows and $47$ columns. The `ye` column runs through $y=70,\\ldots,99$, the `ma` column runs through $m=1,\\ldots,M$, and the ``co`` column takes values $j\\in \\mathcal{J}$. \n",
    "\n",
    "Because we consider a country-year pair as the level of observation, we construct a `market` column taking values $t=1,\\ldots,T$. In python, this variable will take values $t=0,\\ldots,T-1$. We also construct a `market_share` variable giving us the market share of any product $j$ in any market $t$; this will obviously take values in $[0,1]$. To deal with the fact that choice sets $\\mathcal{J}_t$ vary across markets, we expand the dataframe so that every car $j\\in \\mathcal{J}$ which was observed in some market $t$ is in the choice set of all other markets as well, i.e. we impose $\\mathcal{J}_t = \\mathcal{J}$ for all markets $t$. We then impute a market share of $q_{jt}=0$ for any car $j$ which in reality was not available in market $t$. To this end we first construct an outside option $j=0$ in each market $t$  of not buying a car by letting the 'sales' of $j=0$ being determined as \n",
    "\n",
    "$$\\mathrm{sales}_{0t} = \\mathrm{pop}_t - \\sum_{j=1}^J \\mathrm{sales}_{jt}$$\n",
    "\n",
    "where $\\mathrm{pop}_t$ is the total population in market $t$.\n",
    "\n",
    "We also read in the variable description of the dataset contained in `eurocars.dta`. We will use the list `x_vars` throughout to work with our explanatory variables.\n",
    "\n",
    "Lastly, we access the underlying 3-dimensional numpy array of the explonatory variables `x` by sorting on `market` and then `co`, and subsequently resizing the explanatory variables as\n",
    "\n",
    "> `x = dat[x_vars].values.resize((T,J,K))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and variable names\n",
    "os.chdir('../GREENCAR_notebooks/')\n",
    "input_path = os.getcwd() # Assigns input path as current working directory (cwd)\n",
    "descr = (pd.read_stata('eurocars.dta', iterator = True)).variable_labels()\n",
    "dat = pd.read_csv(os.path.join(input_path, 'eurocars.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ye</td>\n",
       "      <td>year (=first dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ma</td>\n",
       "      <td>market (=second dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>co</td>\n",
       "      <td>model code (=third dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zcode</td>\n",
       "      <td>alternative model code (predecessors and succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brd</td>\n",
       "      <td>brand code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type</td>\n",
       "      <td>name of brand and model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>name of model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>org</td>\n",
       "      <td>origin code (demand side, country with which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>loc</td>\n",
       "      <td>location code (production side, country where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frm</td>\n",
       "      <td>firm code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qu</td>\n",
       "      <td>sales (number of new car registrations)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pl</td>\n",
       "      <td>places (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>doors (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>li1</td>\n",
       "      <td>measure 1 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>li2</td>\n",
       "      <td>measure 2 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>li3</td>\n",
       "      <td>measure 3 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>princ</td>\n",
       "      <td>=pr/(ngdp/pop): price relative to per capita i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eurpr</td>\n",
       "      <td>=pr/avdexr: price in common currency (in SDR t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>exppr</td>\n",
       "      <td>=pr/avexr: price in exporter currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>avexr</td>\n",
       "      <td>av. exchange rate of exporter country (exporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>avdexr</td>\n",
       "      <td>av. exchange rate of destination country (dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>avcpr</td>\n",
       "      <td>av. consumer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>avppr</td>\n",
       "      <td>av. producer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>avdcpr</td>\n",
       "      <td>av. consumer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>avdppr</td>\n",
       "      <td>av. producer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xexr</td>\n",
       "      <td>avdexr/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tax</td>\n",
       "      <td>percentage VAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pop</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ngdp</td>\n",
       "      <td>nominal gross domestic product of destination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rgdp</td>\n",
       "      <td>real gross domestic product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>engdp</td>\n",
       "      <td>=ngdp/avdexr: nominal gdp in common currency (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ergdp</td>\n",
       "      <td>=rgdp/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>engdpc</td>\n",
       "      <td>=engdp/pop: nominal gdp per capita in common c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ergdpc</td>\n",
       "      <td>=ergdp/pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              ye                   year (=first dimension of panel)\n",
       "1              ma                market (=second dimension of panel)\n",
       "2              co             model code (=third dimension of panel)\n",
       "3           zcode  alternative model code (predecessors and succe...\n",
       "4             brd                                         brand code\n",
       "5            type                            name of brand and model\n",
       "6           brand                                      name of brand\n",
       "7           model                                      name of model\n",
       "8             org  origin code (demand side, country with which c...\n",
       "9             loc  location code (production side, country where ...\n",
       "10            cla                              class or segment code\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            frm                                          firm code\n",
       "13             qu            sales (number of new car registrations)\n",
       "14             cy            cylinder volume or displacement (in cc)\n",
       "15             hp                                 horsepower (in kW)\n",
       "16             we                                     weight (in kg)\n",
       "17             pl             places (number, not reliable variable)\n",
       "18             do              doors (number, not reliable variable)\n",
       "19             le                                     length (in cm)\n",
       "20             wi                                      width (in cm)\n",
       "21             he                                     height (in cm)\n",
       "22            li1  measure 1 for fuel efficiency (liter per km, a...\n",
       "23            li2  measure 2 for fuel efficiency (liter per km, a...\n",
       "24            li3  measure 3 for fuel efficiency (liter per km, a...\n",
       "25             li          average of li1, li2, li3 (used in papers)\n",
       "26             sp                            maximum speed (km/hour)\n",
       "27             ac  time to acceleration (in seconds from 0 to 100...\n",
       "28             pr   price (in destination currency including V.A.T.)\n",
       "29          princ  =pr/(ngdp/pop): price relative to per capita i...\n",
       "30          eurpr  =pr/avdexr: price in common currency (in SDR t...\n",
       "31          exppr              =pr/avexr: price in exporter currency\n",
       "32          avexr  av. exchange rate of exporter country (exporte...\n",
       "33         avdexr  av. exchange rate of destination country (dest...\n",
       "34          avcpr       av. consumer price index of exporter country\n",
       "35          avppr       av. producer price index of exporter country\n",
       "36         avdcpr    av. consumer price index of destination country\n",
       "37         avdppr    av. producer price index of destination country\n",
       "38           xexr                                       avdexr/avexr\n",
       "39            tax                                     percentage VAT\n",
       "40            pop                                         population\n",
       "41           ngdp  nominal gross domestic product of destination ...\n",
       "42           rgdp                        real gross domestic product\n",
       "43          engdp  =ngdp/avdexr: nominal gdp in common currency (...\n",
       "44          ergdp                                        =rgdp/avexr\n",
       "45         engdpc  =engdp/pop: nominal gdp per capita in common c...\n",
       "46         ergdpc                                         =ergdp/pop"
      ]
     },
     "execution_count": 1182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(descr, index=['description']).transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              cy            cylinder volume or displacement (in cc)\n",
       "1              hp                                 horsepower (in kW)\n",
       "2              we                                     weight (in kg)\n",
       "3              le                                     length (in cm)\n",
       "4              wi                                      width (in cm)\n",
       "5              he                                     height (in cm)\n",
       "6              li          average of li1, li2, li3 (used in papers)\n",
       "7              sp                            maximum speed (km/hour)\n",
       "8              ac  time to acceleration (in seconds from 0 to 100...\n",
       "9              pr   price (in destination currency including V.A.T.)\n",
       "10          brand                                      name of brand\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            cla                              class or segment code"
      ]
     },
     "execution_count": 1183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Determine explanatory variables and find variable description as 'x_lab'\n",
    "x_discretevars = [ 'brand', 'home', 'cla']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "x_orgvars =  [*x_contvars, *x_discretevars]\n",
    "\n",
    "# Outisde is included if OO == True, and is not included else...\n",
    "OO = False\n",
    "\n",
    "# variable descriptions:\n",
    "x_lab = (pd.DataFrame(descr, index=['description'])[x_orgvars].transpose().reset_index().rename(columns={'index' : 'variable names'}))\n",
    "x_lab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the data to fit our setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1184,
   "metadata": {},
   "outputs": [],
   "source": [
    "### First we create the 'market' column \n",
    "\n",
    "dat = dat.sort_values(by = ['ye', 'ma'], ascending = True)\n",
    "Used_cols = [*dat.keys()[:28], 'pr', 'princ', 'pop'] \n",
    "dat = dat[Used_cols]\n",
    "market_vals = [*iter.product(dat['ye'].unique(), dat['ma'].unique())]\n",
    "market_vals = pd.DataFrame({'year' : [val[0] for val in market_vals], 'country' : [val[1] for val in market_vals]})\n",
    "market_vals = market_vals.reset_index().rename(columns={'index' : 'market'})\n",
    "dat = dat.merge(market_vals, left_on=['ye', 'ma'], right_on=['year', 'country'], how='left')\n",
    "dat_org = dat # Save the original data with the 'market'-column added as 'dat_org'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also create an inside/outside-option column if the outside option is included\n",
    "if OO:\n",
    "    dat['in_out'] = 1\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Take care of this above `in_out` column in Create_Nests function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>cy</th>\n",
       "      <th>hp</th>\n",
       "      <th>we</th>\n",
       "      <th>le</th>\n",
       "      <th>wi</th>\n",
       "      <th>he</th>\n",
       "      <th>li</th>\n",
       "      <th>sp</th>\n",
       "      <th>ac</th>\n",
       "      <th>pr</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand</th>\n",
       "      <th>home</th>\n",
       "      <th>cla</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">BMW</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">volvo</th>\n",
       "      <th rowspan=\"4\" valign=\"top\">0</th>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yugo</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>186 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                cy  hp  we  le  wi  he  li  sp  ac   pr\n",
       "brand home cla                                         \n",
       "BMW   0    2     1   4   3   1   1   1   3   2   3   23\n",
       "           3     2   3   2   1   1   1   2   1   1    9\n",
       "           4     7   9  12   5   4   4  17  11  11  111\n",
       "           5     7   9  13   4   4   5  17  12  14  124\n",
       "      1    2     2   3   3   1   1   1   3   2   3    6\n",
       "...             ..  ..  ..  ..  ..  ..  ..  ..  ..  ...\n",
       "volvo 0    2     3   8   5   3   2   3   8   4   5   57\n",
       "           3     8  11   5   3   2   3   8   6   8   54\n",
       "           4    11  24   8   6   4   4  16  12   8  107\n",
       "           5     8  22  10   5   3   4  18  17  15  116\n",
       "yugo  0    1     2   2   5   3   2   2   5   1   3   23\n",
       "\n",
       "[186 rows x 10 columns]"
      ]
     },
     "execution_count": 1186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat_org.groupby(x_discretevars)[x_contvars].nunique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We drop rows which contain NaN values in any explanatory variable or in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1187,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat = dat.dropna()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also convert our discrete explanatory variables to integer valued variables and make sure our continuous variables are identified as floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].str.replace(',', '.').astype('float64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].str.replace(',', '.').astype('float64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].str.replace(',', '.').astype('float64')\n",
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\4143134781.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n"
     ]
    }
   ],
   "source": [
    "obj_columns = dat.select_dtypes(['object'])\n",
    "for col in obj_columns:\n",
    "    if col in x_contvars:\n",
    "        dat[col] = dat[col].str.replace(',', '.').astype('float64')\n",
    "    else:\n",
    "        dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\708423628.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')\n"
     ]
    }
   ],
   "source": [
    "# We reencode all variables such that only the outside option takes the value 0\n",
    "x_0vars = [var for var in x_discretevars if len(dat[(dat['co'] != 0)&(dat[var].isin([0]))]) > 0]\n",
    "\n",
    "for col in x_0vars:\n",
    "    dat[col] = dat[col].astype('category').cat.rename_categories(np.arange(1, dat[col].nunique() + 1)).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Second we construct an outside option for each market t\n",
    "if OO:\n",
    "    outside_shares = dat.groupby('market', as_index=False)['qu'].sum()\n",
    "    outside_shares = outside_shares.merge(dat[['market', 'pop']], on = 'market', how='left').dropna().drop_duplicates(subset = 'market', keep = 'first')\n",
    "    outside_shares['qu'] = outside_shares['pop'] - outside_shares['qu']\n",
    "    keys_add = [key for key in dat.keys() if (key!='market')&(key!='qu')&(key!='pop')]\n",
    "    for key in keys_add:\n",
    "        outside_shares[key] = 0\n",
    "\n",
    "    dat = pd.concat([dat, outside_shares])\n",
    "else:\n",
    "    None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\2220452115.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat['ms'] = dat.groupby('market')['qu'].transform(lambda x: x/x.sum())\n"
     ]
    }
   ],
   "source": [
    "### Third we compute market shares for each product j in each market t \n",
    "\n",
    "dat['ms'] = dat.groupby('market')['qu'].transform(lambda x: x/x.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the amount of markets and amount of alternatives for each market\n",
    "T = dat['market'].nunique()\n",
    "J = np.array([dat[dat['market'] == t]['co'].nunique() for t in np.arange(T)])\n",
    "\n",
    "# number of observations\n",
    "if OO:\n",
    "    N = np.array([dat[dat['market'] == t]['pop'].unique().sum() for t in np.arange(T)]).sum()\n",
    "else:\n",
    "    N = np.array([len(dat[dat['market'] == t]['co']) for t in np.arange(T)]).sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also scale values such that they lie in the interval $[-1,1]$. This has various numerical benefits. Also, this will not affect elasticities or diversion ratios, but semielasticities will be affected by the scaling.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mgq977\\AppData\\Local\\Temp\\ipykernel_17076\\363870548.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dat[x_contvars] = dat[x_contvars] / dat[x_contvars].abs().max()\n"
     ]
    }
   ],
   "source": [
    "dat[x_contvars] = dat[x_contvars] / dat[x_contvars].abs().max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We encode the discete variables as onehot encoded variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat_disc = pd.get_dummies(dat[x_discretevars], prefix = x_discretevars, columns=x_discretevars, drop_first=True)\n",
    "x_disc_ohkeys = dat_disc.keys()\n",
    "dat = pd.concat([dat, dat_disc], axis = 1)\n",
    "\n",
    "\n",
    "if OO:\n",
    "    x_vars = ['in_out', *x_contvars, *x_disc_ohkeys]\n",
    "else:\n",
    "    x_vars = [*x_contvars, *x_disc_ohkeys]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find also the number of explanatory variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = len(x_vars)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will primarily use numpy data types and numpy functions in this notebook. Hence we store our response variable 'y' and our explanatory variables 'x' as numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create numpy arrays of response and explanatory variables\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)}\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26238938, 0.2869469 , 0.35132743, 0.34800885, 0.39070796,\n",
       "       0.43915929, 0.13318584, 0.24977876, 0.14424779, 0.19977876,\n",
       "       0.35221239, 0.2869469 , 0.24690265, 0.21172566, 0.24269912,\n",
       "       0.37588496, 0.28628319, 0.27389381, 0.2869469 , 0.54955752,\n",
       "       0.43982301, 0.26460177, 0.6159292 , 0.26460177, 0.37566372,\n",
       "       0.6159292 , 0.27787611, 0.28539823, 0.21106195, 0.58938053,\n",
       "       0.39734513, 0.28539823, 0.28517699, 0.18761062, 0.24292035,\n",
       "       0.39778761, 0.38672566, 0.28207965, 0.41017699, 0.19800885,\n",
       "       0.24181416, 0.2869469 , 0.26371681, 0.24159292, 0.24734513,\n",
       "       0.24734513, 0.24181416, 0.27787611, 0.27787611, 0.78053097,\n",
       "       0.20884956, 0.40088496, 0.28628319])"
      ]
     },
     "execution_count": 1197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[40][:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_vars = [var for var in x_orgvars if (var != 'pr')] # ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac']  we will nest on variables which are not price, brand, model.\n",
    "nest_cont_vars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac']\n",
    "\n",
    "if OO:\n",
    "    G = len(nest_vars) + 1\n",
    "else:\n",
    "    G = len(nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'brand', 'home', 'cla']"
      ]
     },
     "execution_count": 1199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nest_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if our specififcation of the model is operational, we stack the design matrices $X_t$ of markets $t$ on top of each other to get the $\\sum_t J_t$ by $K$ matrix $X^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = np.concatenate([x[t] for t in np.arange(T)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.42018729e+04 2.16884555e+03 1.91497934e+03 1.54851463e+03\n",
      " 1.19428924e+03 8.68236168e+02 6.84475455e+02 6.56874709e+02\n",
      " 6.15048797e+02 5.87853053e+02 5.61260108e+02 5.26239898e+02\n",
      " 5.06419663e+02 4.21706899e+02 3.86205235e+02 3.05772948e+02\n",
      " 2.86073788e+02 2.48372752e+02 2.36471131e+02 2.22157363e+02\n",
      " 2.00195242e+02 1.86085699e+02 1.81499525e+02 1.74541917e+02\n",
      " 1.51069115e+02 1.42700064e+02 1.04994482e+02 8.91834982e+01\n",
      " 6.99517659e+01 6.67044392e+01 5.54086025e+01 5.20348872e+01\n",
      " 4.90926971e+01 4.78168450e+01 4.58784416e+01 4.40961869e+01\n",
      " 3.64159650e+01 3.02696198e+01 2.07738163e+01 1.26087306e+01\n",
      " 1.07164395e+01 9.71209738e+00 8.99896670e+00 6.68887072e+00\n",
      " 5.01676436e+00 9.94280653e-01 3.73777516e+00 2.29604445e+00\n",
      " 2.93766692e+00 2.71609460e+00]\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame(x_star)\n",
    "\n",
    "XTX=x_star.T@x_star\n",
    "[e,v]=la.eig(XTX)\n",
    "print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.89380531e-01, 3.48082596e-01, 5.49738220e-01, 9.07114625e-01,\n",
       "        9.20212766e-01, 8.81987578e-01, 4.94444423e-01, 6.31578947e-01,\n",
       "        2.70000000e-01, 1.28470526e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00],\n",
       "       [2.46902655e-01, 2.38938053e-01, 4.10994764e-01, 7.60869565e-01,\n",
       "        8.45744681e-01, 8.81987578e-01, 4.44444444e-01, 5.46558704e-01,\n",
       "        3.82000008e-01, 7.48964950e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [2.42920354e-01, 1.74041298e-01, 4.31937173e-01, 7.86561265e-01,\n",
       "        8.35106383e-01, 8.57142857e-01, 4.55555545e-01, 5.06072874e-01,\n",
       "        5.67999992e-01, 6.95390919e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [2.87168142e-01, 2.33038348e-01, 4.60732984e-01, 8.43873518e-01,\n",
       "        8.77659574e-01, 8.63354037e-01, 5.22222201e-01, 5.30364372e-01,\n",
       "        3.92000008e-01, 8.15932488e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [4.39823009e-01, 4.12979351e-01, 6.96335079e-01, 9.25889328e-01,\n",
       "        9.41489362e-01, 8.94409938e-01, 6.05555534e-01, 6.47773279e-01,\n",
       "        3.03999996e-01, 1.80008743e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00],\n",
       "       [2.38495575e-01, 1.94690265e-01, 3.90052356e-01, 8.11264822e-01,\n",
       "        8.35106383e-01, 8.69565217e-01, 4.87037023e-01, 5.06072874e-01,\n",
       "        5.20000000e-01, 7.16820531e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [3.30088496e-01, 1.74041298e-01, 5.36649215e-01, 8.99209486e-01,\n",
       "        9.30851064e-01, 9.06832298e-01, 5.83333333e-01, 5.78947368e-01,\n",
       "        4.40000000e-01, 9.63261073e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00],\n",
       "       [5.50884956e-01, 5.22123894e-01, 5.99476440e-01, 9.03162055e-01,\n",
       "        9.30851064e-01, 8.97515528e-01, 7.22222222e-01, 7.16599190e-01,\n",
       "        2.40000000e-01, 1.38863888e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00],\n",
       "       [3.97345133e-01, 3.30383481e-01, 6.28272251e-01, 8.87351779e-01,\n",
       "        8.98936170e-01, 9.06832298e-01, 6.44444466e-01, 6.31578947e-01,\n",
       "        2.96000004e-01, 1.31149227e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00],\n",
       "       [2.84955752e-01, 2.53687316e-01, 4.79057592e-01, 8.18181818e-01,\n",
       "        8.35106383e-01, 8.75776398e-01, 5.61111132e-01, 6.07287449e-01,\n",
       "        3.12000008e-01, 1.05112248e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [3.25221239e-01, 2.38938053e-01, 5.13089005e-01, 8.35968379e-01,\n",
       "        8.77659574e-01, 9.03726708e-01, 5.55555556e-01, 5.74898785e-01,\n",
       "        4.00000000e-01, 9.63261073e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [2.85176991e-01, 2.33038348e-01, 4.60732984e-01, 8.57707510e-01,\n",
       "        8.69680851e-01, 8.91304348e-01, 4.58333333e-01, 5.66801619e-01,\n",
       "        3.40000000e-01, 8.56113011e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [1.87610619e-01, 1.50442478e-01, 3.31937173e-01, 6.02766798e-01,\n",
       "        7.50000000e-01, 8.38509317e-01, 4.16666667e-01, 4.77732794e-01,\n",
       "        5.20000000e-01, 6.41816888e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [3.78097345e-01, 3.48082596e-01, 5.60209424e-01, 8.60671937e-01,\n",
       "        8.90957447e-01, 9.00621118e-01, 5.00000000e-01, 6.27530364e-01,\n",
       "        2.90000000e-01, 1.36613778e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00],\n",
       "       [2.63716814e-01, 1.47492625e-01, 3.97905759e-01, 8.04347826e-01,\n",
       "        8.24468085e-01, 9.31677019e-01, 4.16666667e-01, 4.65587045e-01,\n",
       "        7.40000000e-01, 6.41816888e-04, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [2.85398230e-01, 3.39233038e-01, 5.23560209e-01, 8.22134387e-01,\n",
       "        8.29787234e-01, 8.88198758e-01, 5.38888878e-01, 6.27530364e-01,\n",
       "        3.27999992e-01, 1.16737813e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [2.60398230e-01, 2.38938053e-01, 3.71727749e-01, 7.90513834e-01,\n",
       "        7.97872340e-01, 8.63354037e-01, 4.33333344e-01, 5.82995951e-01,\n",
       "        2.90000000e-01, 8.62541895e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 1.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [3.30973451e-01, 2.38938053e-01, 4.94764398e-01, 8.65612648e-01,\n",
       "        8.67021277e-01, 9.00621118e-01, 4.83333323e-01, 5.58704453e-01,\n",
       "        3.60000000e-01, 9.84690685e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 1.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [2.20353982e-01, 1.74041298e-01, 3.45549738e-01, 7.49011858e-01,\n",
       "        7.92553191e-01, 8.44720497e-01, 4.33333344e-01, 5.26315789e-01,\n",
       "        4.00000000e-01, 1.18934348e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00],\n",
       "       [1.32300885e-01, 1.29793510e-01, 2.90575916e-01, 6.79841897e-01,\n",
       "        7.92553191e-01, 8.44720497e-01, 3.33333333e-01, 4.85829960e-01,\n",
       "        6.20000000e-01, 5.88242858e-04, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 1.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "        0.00000000e+00, 0.00000000e+00]])"
      ]
     },
     "execution_count": 1202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_star[0:20,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 1203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XTX.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multinomial Logit - for comparison\n",
    "Estimating a Logit model via maximum likelihood with an initial guess of parameters $\\hat \\beta^0 = 0$ yields estimated parameters $\\hat \\beta^{\\text{logit}}$ given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 3.581803\n",
      "         Iterations: 344\n",
      "         Function evaluations: 345\n",
      "         Gradient evaluations: 345\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.ones((K,))\n",
    "\n",
    "# Estimate the model\n",
    "res_logit = logit.estimate_logit(logit.q_logit, beta_0, y, x, N, Analytic_jac=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parameters</th>\n",
       "      <th>se</th>\n",
       "      <th>t</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-2.176080</td>\n",
       "      <td>0.004396</td>\n",
       "      <td>494.986645</td>\n",
       "      <td>7.979885e-242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-6.193511</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>1192.948413</td>\n",
       "      <td>9.907691e-299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.008929</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>1.962982</td>\n",
       "      <td>2.575524e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.979094</td>\n",
       "      <td>0.004890</td>\n",
       "      <td>609.190746</td>\n",
       "      <td>2.984160e-255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>12.297456</td>\n",
       "      <td>0.009518</td>\n",
       "      <td>1291.995557</td>\n",
       "      <td>6.842337e-304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.007362</td>\n",
       "      <td>0.005422</td>\n",
       "      <td>1.357772</td>\n",
       "      <td>8.829442e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-1.215902</td>\n",
       "      <td>0.003071</td>\n",
       "      <td>395.888089</td>\n",
       "      <td>2.223980e-227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>5.524244</td>\n",
       "      <td>0.006998</td>\n",
       "      <td>789.364467</td>\n",
       "      <td>5.175140e-272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.848903</td>\n",
       "      <td>0.001879</td>\n",
       "      <td>451.893182</td>\n",
       "      <td>6.197119e-236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-3.809732</td>\n",
       "      <td>0.008047</td>\n",
       "      <td>473.460594</td>\n",
       "      <td>5.988484e-239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-1.936547</td>\n",
       "      <td>0.106605</td>\n",
       "      <td>18.165719</td>\n",
       "      <td>6.472049e-40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>-0.399402</td>\n",
       "      <td>0.001638</td>\n",
       "      <td>243.888569</td>\n",
       "      <td>4.405841e-196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-1.017099</td>\n",
       "      <td>0.001758</td>\n",
       "      <td>578.607417</td>\n",
       "      <td>6.403958e-252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.846573</td>\n",
       "      <td>0.002096</td>\n",
       "      <td>403.857495</td>\n",
       "      <td>1.144518e-228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.872886</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>547.393791</td>\n",
       "      <td>2.473470e-248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-1.239700</td>\n",
       "      <td>0.010005</td>\n",
       "      <td>123.902567</td>\n",
       "      <td>1.724079e-152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.644308</td>\n",
       "      <td>0.003382</td>\n",
       "      <td>190.503574</td>\n",
       "      <td>3.790631e-180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-2.502221</td>\n",
       "      <td>0.022641</td>\n",
       "      <td>110.518125</td>\n",
       "      <td>3.586452e-145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>-0.648820</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>424.499142</td>\n",
       "      <td>6.851010e-232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.490169</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>309.524065</td>\n",
       "      <td>1.788747e-211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.786989</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>293.051611</td>\n",
       "      <td>6.103471e-208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-1.609924</td>\n",
       "      <td>0.011948</td>\n",
       "      <td>134.743654</td>\n",
       "      <td>7.186751e-158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-2.446274</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>1067.961558</td>\n",
       "      <td>1.435421e-291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-2.620754</td>\n",
       "      <td>0.023950</td>\n",
       "      <td>109.425578</td>\n",
       "      <td>1.548020e-144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-1.624901</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>776.676754</td>\n",
       "      <td>5.783911e-271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-1.156847</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>441.884450</td>\n",
       "      <td>1.739601e-234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.498125</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>375.630044</td>\n",
       "      <td>5.530376e-224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-1.598730</td>\n",
       "      <td>0.004778</td>\n",
       "      <td>334.602868</td>\n",
       "      <td>1.653105e-216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.488281</td>\n",
       "      <td>0.002520</td>\n",
       "      <td>193.768724</td>\n",
       "      <td>3.043697e-181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.588298</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>375.772608</td>\n",
       "      <td>5.226674e-224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>-0.447344</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>294.271541</td>\n",
       "      <td>3.290176e-208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>-0.446229</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>290.070608</td>\n",
       "      <td>2.792831e-207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.973294</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>652.064982</td>\n",
       "      <td>1.189497e-259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.904991</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>130.965308</td>\n",
       "      <td>4.804834e-156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-1.511116</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>477.748090</td>\n",
       "      <td>1.564391e-239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-3.599333</td>\n",
       "      <td>0.645708</td>\n",
       "      <td>5.574240</td>\n",
       "      <td>5.673945e-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-1.294108</td>\n",
       "      <td>0.009888</td>\n",
       "      <td>130.881571</td>\n",
       "      <td>5.280945e-156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-3.128017</td>\n",
       "      <td>0.111522</td>\n",
       "      <td>28.048317</td>\n",
       "      <td>1.268023e-61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.812559</td>\n",
       "      <td>0.012007</td>\n",
       "      <td>150.960209</td>\n",
       "      <td>3.597450e-165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-1.682195</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>389.239658</td>\n",
       "      <td>2.767113e-226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.714819</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>436.003824</td>\n",
       "      <td>1.278582e-233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-2.498331</td>\n",
       "      <td>0.009918</td>\n",
       "      <td>251.904194</td>\n",
       "      <td>3.601721e-198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-1.223949</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>722.360027</td>\n",
       "      <td>2.834567e-266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.594001</td>\n",
       "      <td>0.002625</td>\n",
       "      <td>226.282262</td>\n",
       "      <td>3.017498e-191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.326946</td>\n",
       "      <td>0.002099</td>\n",
       "      <td>155.737831</td>\n",
       "      <td>3.567770e-167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.695893</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>6346.748294</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>-0.034154</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>72.188920</td>\n",
       "      <td>3.960753e-118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>-0.001837</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>3.030451</td>\n",
       "      <td>1.439959e-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>-0.271905</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>274.262896</td>\n",
       "      <td>1.162708e-203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>-0.222985</td>\n",
       "      <td>0.001898</td>\n",
       "      <td>117.454756</td>\n",
       "      <td>4.570564e-149</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          parameters        se            t              p\n",
       "cy         -2.176080  0.004396   494.986645  7.979885e-242\n",
       "hp         -6.193511  0.005192  1192.948413  9.907691e-299\n",
       "we          0.008929  0.004549     1.962982   2.575524e-02\n",
       "le         -2.979094  0.004890   609.190746  2.984160e-255\n",
       "wi         12.297456  0.009518  1291.995557  6.842337e-304\n",
       "he          0.007362  0.005422     1.357772   8.829442e-02\n",
       "li         -1.215902  0.003071   395.888089  2.223980e-227\n",
       "sp          5.524244  0.006998   789.364467  5.175140e-272\n",
       "ac          0.848903  0.001879   451.893182  6.197119e-236\n",
       "pr         -3.809732  0.008047   473.460594  5.988484e-239\n",
       "brand_2    -1.936547  0.106605    18.165719   6.472049e-40\n",
       "brand_3    -0.399402  0.001638   243.888569  4.405841e-196\n",
       "brand_4    -1.017099  0.001758   578.607417  6.403958e-252\n",
       "brand_5    -0.846573  0.002096   403.857495  1.144518e-228\n",
       "brand_6    -0.872886  0.001595   547.393791  2.473470e-248\n",
       "brand_7    -1.239700  0.010005   123.902567  1.724079e-152\n",
       "brand_8    -0.644308  0.003382   190.503574  3.790631e-180\n",
       "brand_9    -2.502221  0.022641   110.518125  3.586452e-145\n",
       "brand_10   -0.648820  0.001528   424.499142  6.851010e-232\n",
       "brand_11   -0.490169  0.001584   309.524065  1.788747e-211\n",
       "brand_12   -0.786989  0.002685   293.051611  6.103471e-208\n",
       "brand_13   -1.609924  0.011948   134.743654  7.186751e-158\n",
       "brand_14   -2.446274  0.002291  1067.961558  1.435421e-291\n",
       "brand_15   -2.620754  0.023950   109.425578  1.548020e-144\n",
       "brand_16   -1.624901  0.002092   776.676754  5.783911e-271\n",
       "brand_17   -1.156847  0.002618   441.884450  1.739601e-234\n",
       "brand_18    0.498125  0.001326   375.630044  5.530376e-224\n",
       "brand_19   -1.598730  0.004778   334.602868  1.653105e-216\n",
       "brand_20   -0.488281  0.002520   193.768724  3.043697e-181\n",
       "brand_21   -0.588298  0.001566   375.772608  5.226674e-224\n",
       "brand_22   -0.447344  0.001520   294.271541  3.290176e-208\n",
       "brand_23   -0.446229  0.001538   290.070608  2.792831e-207\n",
       "brand_24   -0.973294  0.001493   652.064982  1.189497e-259\n",
       "brand_25   -0.904991  0.006910   130.965308  4.804834e-156\n",
       "brand_26   -1.511116  0.003163   477.748090  1.564391e-239\n",
       "brand_27   -3.599333  0.645708     5.574240   5.673945e-08\n",
       "brand_28   -1.294108  0.009888   130.881571  5.280945e-156\n",
       "brand_29   -3.128017  0.111522    28.048317   1.268023e-61\n",
       "brand_30   -1.812559  0.012007   150.960209  3.597450e-165\n",
       "brand_31   -1.682195  0.004322   389.239658  2.767113e-226\n",
       "brand_32   -0.714819  0.001639   436.003824  1.278582e-233\n",
       "brand_33   -2.498331  0.009918   251.904194  3.601721e-198\n",
       "brand_34   -1.223949  0.001694   722.360027  2.834567e-266\n",
       "brand_35   -0.594001  0.002625   226.282262  3.017498e-191\n",
       "brand_36   -0.326946  0.002099   155.737831  3.567770e-167\n",
       "home_2      1.695893  0.000267  6346.748294   0.000000e+00\n",
       "cla_2      -0.034154  0.000473    72.188920  3.960753e-118\n",
       "cla_3      -0.001837  0.000606     3.030451   1.439959e-03\n",
       "cla_4      -0.271905  0.000991   274.262896  1.162708e-203\n",
       "cla_5      -0.222985  0.001898   117.454756  4.570564e-149"
      ]
     },
     "execution_count": 1205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_beta, logit_se, logit_t, logit_p = res_logit['beta'], res_logit['se'], res_logit['t'], res_logit['p'] # maybe use 'logit.' functions from Logit_file instead of including e.g. standard errors in logit.estimate_logit function\n",
    "pd.DataFrame({'parameters': logit_beta, 'se' : logit_se, 't': logit_t, 'p': logit_p}, index = x_vars) # Our estimates"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compute the corresponding Logit choice probabilities. STILL FIX THIS PART IN LOGIT BOOK!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1206,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit_q = logit.logit_ccp(logit_beta, x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also find the elasticities and diversion ratios implied by the logit model as follows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1207,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon_logit = logit.logit_elasticity(logit_q, logit_beta, K-1) # Elasticities wrt. the price characteristic\n",
    "DR_logit_hat = logit.logit_diversion_ratio(logit_q, logit_beta)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The IPDL model - Nesting structure\n",
    "\n",
    "The IPDL model is a generalization of the nested logit model where each alternative may belong to more than one nest. Before fully introducing the model, we construct the nesting structure.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing nests\n",
    "\n",
    "Let $\\Delta=\\left\\{q\\in \\mathbb{R}^J_+: \\sum_{j=1}^J q_j=1\\right\\}$ denote the probability simplex. For each group of nests $g=1,\\ldots, G$, nest membership is denoted by the matrix $\\Psi^g\\in \\mathbb R^{C_g\\times J}$: $\\Psi^g_{cj}=1$ if product $j$ belongs to nest $c$ and zero otherwise, and each product can only belong to one nest within each group, meaning that $\\sum_{c=1}^{C_g}\\Psi^g_{cj}=1$ for all $j$ and all $g$. The matrix-vector product $\\Psi^gq$ is then\n",
    "$$\n",
    "\\Psi^g q=\\sum_j \\Psi^{g}_{cj}q_j=\\left(\\begin{array}{c}\n",
    "\\sum_{j:\\Psi^g_{1j}=1} q_j \\\\\n",
    "\\vdots \\\\\n",
    "\\sum_{j: \\Psi^g_{C_gj}=1}q_j\n",
    "\\end{array}\\right),\n",
    "$$\n",
    "and the vector $\\Psi^gq$ is a vector of nest-specific choice probabilities, i.e. the sum of the probabilities within each nest."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The perturbation function $\\Omega$\n",
    "\n",
    "In the following, a vector $z\\in \\mathbb R^d$ is always a column vector. We now construct the IPDL perturbation function which has the form (where for a vector $z$, the logarithm is applied elementwise and $z'$ denote the transpose)\n",
    "$$\n",
    "\\Omega(q|\\lambda)= (1-\\sum_{g=1}^G \\lambda_g) q'\\ln q +\\sum_{g=1}^{G} \\lambda_g \\left(\\Psi^g q \\right)'\\ln \\left(\\Psi^g q\\right).\n",
    "$$\n",
    "Note that since $\\Psi^g q$ denotes a probability distribution over the nests, the term $(\\Psi^gq)'\\ln (\\Psi^gq)$ is the (negative) entropy of the probability distribution $\\Psi^g q$. Similarly, $q'\\ln q$ is the negative entropy of q. Note also that as each nest has at least one member, and $q$ is strictly positive, $\\Psi^gq$ is also strictly positive. When the parameters $\\lambda_g$ satisfy $\\lambda_g>0$ and\n",
    "$$\n",
    "\\sum_g \\lambda_g<1,\n",
    "$$\n",
    "the function $\\Omega(\\cdot|\\lambda)$ is a strictly convex function of $q$, and the utility maximization problem has a unique interior (meaning strictly positive choice probabilities) solution. If $\\lambda_g = 0$ for all groupings $g$, we immediately see that the  IPDL becomes the standard multinomial Logit model for the choice probabilities $q$. When there is only one group of nests, $G=1$, then $\\Omega$ induces the nested logit choice probabilities (note though that the nested logit model is often parameterized in terms of the nesting parameter $\\mu=1-\\lambda$ instead!). \n",
    "\n",
    "It will be convenient to define a choice probability function for a given vector of payoffs $u$ as\n",
    "$$\n",
    "P(u|\\lambda)=\\arg \\max_{q\\in \\Delta}\\left\\{q'u-\\Omega(q|\\lambda)\\right\\}\n",
    "$$\n",
    "Letting $\\theta$ denote the full vector of parameters, $\\theta=(\\beta',\\lambda')'$, the individual choice probabilities is a function of the matrix $\\mathbf{X}_i$ and the parameters $\\theta$, as\n",
    "$$\n",
    "p(\\mathbf{X}_i,\\theta)=\\arg\\max_{q\\in \\Delta}\\left\\{q'\\mathbf{X}_i \\beta-(1-\\sum_{g=1}^G\\lambda_g)q'\\ln q-\\sum_{g=1}^G\\lambda_g \\left(\\Psi^g q \\right)'\\ln \\left(\\Psi^g q\\right)\\right\\}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max-rescaling for numerical stability\n",
    "\n",
    "Let $\\alpha$ be a scalar, and let $\\iota$ be the all-ones vector in $\\mathbb R^J$. Note that $q'(u+\\alpha\\iota)=q'u+(q'\\iota)\\alpha=q'u+\\alpha$, since $q$ sums to one. For this reason, $\\alpha$ does not enter into the utility maximization when calculating $P(u+\\alpha\\iota|\\lambda)$, and we have $P(u+\\alpha\\iota|\\lambda)=P(u|\\lambda)$.\n",
    "\n",
    "This allows us to re-scale the utilities just as in the logit model, since $P(u-(\\max_{j}u_j)\\iota|\\lambda)=P(u|\\lambda)$. The numerical benefits of this approach carry over to the IPDL model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient and Hessian\n",
    "\n",
    "For purposes of computing the gradient and Hessian of $\\Omega$, it is convenient to define\n",
    "$$\n",
    "\\Gamma=\\left(\\begin{array}{c}\n",
    "(1-\\sum_g \\lambda_g)I_J\\\\\n",
    "\\lambda_1 \\Psi^1\\\\\n",
    "\\vdots\\\\\n",
    "\\lambda_G \\Psi^G\n",
    "\\end{array}\\right)\n",
    "$$\n",
    "where $I_J$ is the identity matrix in $\\mathbb R^J$. The matrix $\\Gamma$ is a block matrix with $J+\\sum_g C_g$ rows and $J$ columns. Note that \n",
    "\n",
    "$$\n",
    "\\Gamma q=\\left(\\begin{array}{c}\n",
    "(1-\\sum_g\\lambda_g)q \\\\\n",
    "\\lambda_1\\Psi^g q\\\\\n",
    "\\vdots \\\\\n",
    "\\lambda_G \\Psi^Gq\n",
    "\\end{array}\\right)>0\n",
    "$$\n",
    "if $q>0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using $\\Gamma$, we can show that\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(\\Gamma q)'\\ln (\\Gamma q)+c\\\\\n",
    "\\nabla_q \\Omega(q|\\lambda)=\\Gamma'\\ln (\\Gamma q)+\\iota\\\\\n",
    "\\nabla^2_{qq}\\Omega(q|\\lambda)=\\Gamma'\\mathrm{diag}(\\Gamma q)^{-1}\\Gamma,\n",
    "$$\n",
    "where $c$ is a scalar that depends on $\\lambda$ but not on $q$ and therefore does not affect the utility maximization problem, $\\iota=(1,\\ldots,1)'\\in \\mathbb R^J$ is the all-ones vector and $\\mathrm{diag}(z)$ is a diagonal matrix with the elements of the vector $z$ on the diagonal."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we impose on all nests on all markets. We deal with this by setting $\\psi_{tcj} = 0$ for all products $j$ if the nest $c$ was not in fact observed in market $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_nests(data, markets_id, products_id, columns, cont_var = None, cont_var_bins = None, outside_option = True):\n",
    "    '''\n",
    "    This function creates the nest matrices \\Psi^g from any specified columns of the dataset\n",
    "\n",
    "    Args.\n",
    "        data: a pandas DataFrame\n",
    "        markets_id: a string denoting the column of 'data' containing an enumeration t=0,1,...,T-1 of markets\n",
    "        products_id: a string denoting the column of 'data' containing product codes which uniquely identifies products\n",
    "        columns: a list containing the column names of columns in 'data' from which nest groupings g=0,1,...,G-1 for each market t are to be generated\n",
    "        cont_var: a list of the continuous variables in 'columns'\n",
    "        cont_var_bins: a list containing the number of bins to make for each continuous variable in 'columns'\n",
    "        outside_option: a boolean indicating whether the model is estimated with or without an outside option. Default is set to 'True' i.e. with an outside option.\n",
    "\n",
    "    Returns\n",
    "        Psi_stack: a dictionary of length T of dictionaries of the Psi_g matrices for each market t and each gropuing g\n",
    "        nest_dict: a dictionary of length T of dictionaries of pandas dataframes describing the structure of each nest for each market t and each grouping g\n",
    "        nest_count: a dictionary of length T of (G,) numpy arrays containing the amount of nests in each category\n",
    "    '''\n",
    "\n",
    "    T = data[markets_id].nunique()\n",
    "    J = np.array([data[data[markets_id] == t][products_id].nunique() for t in np.arange(T)])\n",
    "    \n",
    "    # We nest on outside vs. inside options. The amount of categories varies if the outside option is included in the analysis.\n",
    "\n",
    "    if outside_option == True:\n",
    "        G = np.int64(len(columns) + 1)\n",
    "    else:\n",
    "        G = len(columns)\n",
    "\n",
    "    dat = data.sort_values(by = [markets_id, products_id]) # This is good :)\n",
    "    \n",
    "    Psi_dict = {}\n",
    "    Psi_stack = {}\n",
    "    nest_dict = {}\n",
    "    nest_counts = {}\n",
    "\n",
    "    ### Bin continuous variables according to quantiles of the variable\n",
    "\n",
    "    if cont_var == None:\n",
    "        None\n",
    "    else:\n",
    "        for var,n_bins in zip(cont_var,cont_var_bins):\n",
    "            if outside_option:\n",
    "                q_dat = np.quantile(dat[var].rank(method = 'first'), q = np.arange(1,n_bins + 1) / n_bins)\n",
    "                dat[var] = pd.cut(dat[var].rank(method = 'first'), bins = [0.99, 1, *q_dat], labels=False) # Quantiles are equally spaced with 'n_bins' quantiles for the variable. The outside option gets its own bin (0.99,1].\n",
    "            else:\n",
    "                dat[var] = pd.qcut(dat[var].rank(method = 'first'), q = n_bins, labels=False)\n",
    "\n",
    "    # Assign nests for products in each market t\n",
    "    for t in np.arange(T):\n",
    "        data_t = dat[dat[markets_id] == t] # Subset data on market t\n",
    "        Psi_dict_t = {}\n",
    "        nest_dict_t = {}\n",
    "        nest_counts_t = np.empty(G)\n",
    "\n",
    "        # For each category/group g we create a \\psi^g matrix plus related descriptions and the amount of nests in category g\n",
    "        for g in np.arange(G):\n",
    "            if (outside_option == True)&(g == 0): # If the outside option is included, we set the first category to be the outside/inside option nest.\n",
    "                mat = np.zeros((2, J[t]))\n",
    "                mat[0,0] = 1 # The outside option is set to one in the outside option nest\n",
    "                mat[1,1:] = 1 # All other products are set to one in the inside option nest\n",
    "\n",
    "                # Assign the matrix, nest descriptions, and nest count\n",
    "                Psi_dict_t[g] = mat\n",
    "                nest_dict_t[g] = pd.DataFrame({'nests': ['outside_option', 'inside_option']}).reset_index().rename(columns={'index' :'nest_index'}).rename_axis('outside/inside option', axis='columns') # Enumerate the two nests 'outside' and 'inside' by 0 and 1, and save the description of the category.\n",
    "                nest_counts_t[g] = 2\n",
    "\n",
    "            else:\n",
    "                if outside_option == True:\n",
    "                    col = columns[g-1] # If outside option is included, then the second category is the first variable in the specfied columns\n",
    "                else:\n",
    "                    col = columns[g] # If outside option is excluded, then the first category is the first variable in the specfied columns\n",
    "                    \n",
    "                vals = pd.DataFrame({'nests' : data_t[col].sort_values().unique()}).reset_index().rename(columns={'index' :'nest_index'}) # Enumerate the unique values of the variable 'col' by the integers 0,1,...,C_g-1\n",
    "                descr = vals.rename_axis(col, axis='columns') # Create a description of category g\n",
    "                \n",
    "                # Assign the description and counts\n",
    "                nest_dict_t[g] = descr\n",
    "                nest_counts_t[g] = len(vals['nests'])\n",
    "\n",
    "                # Constuct the \\psi^g matrix\n",
    "                product_enumeration = pd.DataFrame({products_id : data_t[products_id].sort_values().unique(), 'product_enumeration' : np.arange(J[t])}) # Enumerates products by j=0,...,J[t]-1\n",
    "                C_g = len(vals['nest_index']) # Find the amount of nests in category g\n",
    "\n",
    "                frame = data_t[[products_id, col]].merge(vals, left_on = col, right_on = 'nests') # Merge nest indices and nest enumerations onto the subsetted data for market t \n",
    "                allocation = frame[[products_id, 'nest_index']].merge(product_enumeration, on=products_id, how='left') # Merge the product enumeration onto frame\n",
    "\n",
    "                mat = np.zeros((C_g, J[t])) # Initialize zero matrix\n",
    "\n",
    "                for c,j in zip(allocation['nest_index'], allocation['product_enumeration']): \n",
    "                    mat[c, j] = 1 # Assigns a 1 to each pair of a nest index and a product index as specified by the eariler merges\n",
    "\n",
    "                Psi_dict_t[g] = mat # Assign the matrix\n",
    "\n",
    "        # For each market t assign the relevant information\n",
    "        Psi_dict[t] = Psi_dict_t\n",
    "        nest_dict[t] = nest_dict_t\n",
    "        nest_counts[t] = nest_counts_t\n",
    "        Psi_stack[t] = np.concatenate([np.eye(J[t]) if g==0 else Psi_dict[t][g-1] for g in np.arange(G + 1)]) # The top most block matrix is the J[t] by J[t] identity, and the next G block matrices are the \\psi^g matrices\n",
    "        \n",
    "    return Psi_stack, nest_dict, nest_counts"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We bin all the continuous explanatory variables different from `pr` (i.e. the price) in 10 bins, and the grouping of `pr` includes 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1169,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi_stack, Nest_descr, Nest_count = Create_nests(dat, 'market', 'co', nest_vars, nest_cont_vars, [*[np.int64(10) for i in range(len(nest_cont_vars))]], outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Gamma(Lambda, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function \n",
    "    '''\n",
    "\n",
    "    T = len(psi_stack)\n",
    "    \n",
    "    Gamma = {}\n",
    "    lambda0 = np.array([1 - sum(Lambda)])\n",
    "    Lambda_full = np.concatenate((lambda0, Lambda))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        J = psi_stack[t].shape[1]\n",
    "        C = np.int64(J + np.sum(nest_count[t]))\n",
    "        Lambda_long = np.empty((C,))\n",
    "        indices = np.concatenate((np.array([J]) , nest_count[t])).cumsum().astype('int64')\n",
    "\n",
    "        for i in np.arange(len(indices)):\n",
    "            if i == 0:\n",
    "                Lambda_long[0:(indices[i])] = Lambda_full[i]\n",
    "            else:\n",
    "                Lambda_long[indices[i-1]:indices[i]] = Lambda_full[i]\n",
    "    \n",
    "        Gamma[t] =  np.multiply(Lambda_long[:,None], psi_stack[t])\n",
    "\n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1171,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda0 = np.ones((G,))/(2*(G+1))\n",
    "Gamma0 = Create_Gamma(lambda0, Psi_stack, Nest_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "While it is possible to solve for the choice probabilities explicitly by maximizing utility, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Suppose we are evaluating the likelihood at some guess of the parameters $\\theta=(\\beta',\\lambda')$. Let $u_i=\\mathbf{X}_i\\beta$, and let $q_i^0$ denote some initial vector of choice probabilities e.g. $q^0_i=\\frac{e^{u_i}}{\\sum_{j'=1}^Je^{u_{ij'}}}$, we update the choice probabilities according to the formula\n",
    "$$\n",
    "v_i^{k} =u_i+\\ln q_i^{k-1}-\\Gamma'\\ln (\\Gamma q_i^{k-1})\\\\\n",
    "q_i^{k} = \\frac{e^{v_i^{k}}}{\\sum_{j=1}^J e^{v_{ij}^{k}}},\n",
    "$$\n",
    "they show that $\\lim_{k\\rightarrow \\infty}q_i^k=p(\\mathbf{X}_i,\\theta)$ for any starting value $q^0_i$ in the interior of $\\Delta$. For numerical stability, it can be a good idea to also do max-rescaling of $v^k_i$ at every iteration.\n",
    "\n",
    "Let $p$ denote the solution to the utility maximization problem. Formally, the Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p||q^{k+1})\\leq \\left(1- \\sum_g \\lambda_g \\right)D_{KL}(p||q^k),\n",
    "$$\n",
    "Noting that $(1-\\sum_g \\lambda_g)\\in [0,1)$ by assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_ccp(Theta, x, psi_stack, nest_count, tol = 1.0e-15, maximum_iterations = 1000, MAXRESCALE:bool = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "    Beta = Theta[:K]\n",
    "    Lambda = Theta[K:]\n",
    "    G = len(Lambda)\n",
    "\n",
    "    # Calculate small beta\n",
    "    C_minus = np.array([g for g in np.arange(G) if Lambda[g] < 0])\n",
    "    b = {t: np.max(np.dot(Lambda[C_minus], psi_stack[t][C_minus,:])) if (len(C_minus) != 0) else 0 for t in np.arange(T)} # Set the sum equal to 0 if C_minus is empty\n",
    "\n",
    "    Gamma = Create_Gamma(Lambda, psi_stack, nest_count)\n",
    "\n",
    "    u = {t: np.dot(x[t], Beta) for t in np.arange(T)}\n",
    "    q = {t: np.exp(u[t]) / np.exp(u[t]).sum() for t in np.arange(T)} # Find logit choice probabilities\n",
    "    q0 = q\n",
    "    \n",
    "    Epsilon = 1.0e-10\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        q1 = {}\n",
    "        for t in np.arange(T):\n",
    "            # Calculate v\n",
    "            gamma_q = np.dot(Gamma[t], q0[t])\n",
    "            log_gammaq = np.log(np.abs(gamma_q) + Epsilon) # Add epsilon to avoid zeros in log\n",
    "            gamma_log_prod = np.dot(np.transpose(Gamma[t]), log_gammaq) # Maybe multiply with active_mat???\n",
    "            v = np.log(q0[t], out = -np.inf*np.ones_like(q0[t]), where = (q0[t] > 0)) + np.divide(u[t] - gamma_log_prod, 1 + b[t])\n",
    "\n",
    "            if MAXRESCALE:\n",
    "                v -= v.max(keepdims = True)\n",
    "\n",
    "            # Calculate iterated ccp q^k\n",
    "            denom = np.exp(v).sum()\n",
    "            numerator = np.exp(v)\n",
    "            q1[t] = np.divide(numerator, denom) \n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.array([np.sum((q1[t]-q0[t])**2/q[t]) for t in np.arange(T)])) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "\n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta0 = np.append(logit_beta, lambda0)\n",
    "\n",
    "q0_hat = IPDL_ccp(theta0, x, Psi_stack, Nest_count)\n",
    "pd.DataFrame(q0_hat[0]).rename_axis(index='Products', columns='Markets').transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.array([np.sum(q0_hat[t]) for t in np.arange(T)]).all() == 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand derivatives and price Elasticity\n",
    "\n",
    "While the demand derivatives in the IPDL model are not quite as simple as in the logit model, they are still easy to compute. \n",
    "Let $q=P(u|\\lambda)$, then\n",
    "$$\n",
    "\\nabla_u P(u|\\lambda)=\\left(\\nabla^2_{qq}\\Omega(q|\\lambda)\\right)^{-1}-qq'\n",
    "$$\n",
    "where the $()^{-1}$ denotes the matrix inverse. The derivatives with respect to any $x_{ij\\ell}$ can now easily be computed by the chain rule,\n",
    "$$\n",
    "    \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{\\partial u_{ik}}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell,\n",
    "$$\n",
    "\n",
    "Finally, moving to price elasticity is the same as in the logit model, if $x_{ik\\ell}$ is the log price of product $k$ for individual $i$, then\n",
    "$$\n",
    "    \\mathcal{E}_{jk}= \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}\\frac{1}{P_j(u_i|\\lambda)}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{1}{P_j(u_i|\\lambda)}\\beta_\\ell=\\frac{\\partial \\ln P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell$$\n",
    "we can also write this compactly as\n",
    "$$\n",
    "\\nabla_u \\ln P(u|\\lambda)=\\mathrm{diag}(P(u|\\lambda))^{-1}\\nabla_u P(u|\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pertubation_hessian(q, x, Theta, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calucates the hessian of the pertubation function \\Omega\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        Lambda: a (G,) numpy array of nesting parameters\n",
    "        Psi: a dictionary of the \\Psi^g matrices as columns as outputted 'Create_incidence_matrix'\n",
    "    \n",
    "    Returns\n",
    "        Hess: a (N,J,J) numpy array of second partial derivatives of the pertubation function \\Omega\n",
    "    '''\n",
    "    \n",
    "    T = len(q.keys())\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    Gamma = Create_Gamma(Theta[K:], psi_stack, nest_count)\n",
    "    Hess = {}\n",
    "    eps = 1.0e-10\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        gamma_q = np.dot(Gamma[t], q[t])\n",
    "        inv_gamma_q = np.divide(1, gamma_q, out = np.inf*np.ones_like(gamma_q), where = (gamma_q!=0))\n",
    "        Hess[t] = np.dot(np.transpose(Gamma[t]), np.multiply(inv_gamma_q[:,None], Gamma[t]))\n",
    "\n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_gradient(q, x, Theta, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calucates the gradient of the choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        x: a (N,J,K) numpy array of covariates\n",
    "        Lambda: a (G,) numpy array of nesting parameters\n",
    "        Psi: a dictionary of the \\Psi^g matrices as columns as outputted 'Create_incidence_matrix'\n",
    "    \n",
    "    Returns\n",
    "        Grad: a (N,J,K) numpy array of partial derivatives of the choice proabilities wrt. characteristics\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Grad = {}\n",
    "    Hess = compute_pertubation_hessian(q, x, Theta, psi_stack, nest_count)\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        inv_omega_hess = la.inv(Hess[t]) # (N,J,J) # For each i=1,...,N , computes the inverse of the J*J Hessian\n",
    "        qqT = np.outer(q[t], q[t]) # (N,J,J) outerproduct\n",
    "        Grad[t] = inv_omega_hess - qqT\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calucates the gradient of the log choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        x: a (N,J,J) numpy array of covariates\n",
    "        Theta: a (K+G,) numpy array of IPDL parameters\n",
    "        Psi: a dictionary of the \\Psi^g matrices as columns as outputted 'Create_incidence_matrix'\n",
    "    \n",
    "    Returns\n",
    "        Epsilon: a (N,J,K) numpy array of partial derivatives of the log choice proabilities wrt. characteristics\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack, nest_count)\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        ccp_grad = Grad[t]\n",
    "        inv_diagq = np.divide(1, q[t], out = np.inf*np.ones_like(q[t]), where = (q[t] > 0))\n",
    "        Epsilon[t] = np.multiply(inv_diagq[:,None], ccp_grad) # Is equivalent to (1./q)[:,:,None]*ccp_grad an elementwise product. Einsum merely divides through by the nj'th elemnt of q in k'th row of ccp_grad.\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_elasticity(q, x, Theta, psi_stack, nest_count, char_number = K-1):\n",
    "    ''' \n",
    "    This function calculates the elasticity of choice probabilities wrt. any characteristic or nest grouping of products\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        x: a (N,J,K) numpy array of covariates\n",
    "        Theta: a (K+G,) numpy array of IPDL parameters\n",
    "        Psi: a dictionary of the \\Psi^g matrices as columns as outputted 'Create_incidence_matrix'\n",
    "        char_number: an integer which is an index of the parameter in theta wrt. which we wish calculate the elasticity \n",
    "\n",
    "    Returns\n",
    "        an (N,J,J) array of choice probability elasticities\n",
    "    '''\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count)\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = np.multiply(Grad[t], Theta[char_number])\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using guess parameters $\\hat \\theta^0$ we calculate price-to-log-income elasticities for individual $i=0$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(IPDL_elasticity(q0_hat, x, theta0, Psi_stack, Nest_count)[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of IPDL\n",
    "\n",
    "The log-likelihood contribution is\n",
    "$$\n",
    "\\ell_t(\\theta)=y_t'\\ln p(\\mathbf{X}_t,\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_t$ and $\\theta$ - calculates $u_t=\\mathbf{X}_t\\beta$ and constructs $\\Gamma$, and then calls the fixed point routine described above. That routine will return $p(\\mathbf{X}_t,\\theta)$, and we can then evaluate $\\ell_t(\\theta)$. Using our above defined functions we now construct precisely such an estimation procedure.\n",
    "\n",
    "For maximizing the likelihood, we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_t=p(\\mathbf{X}_t,\\theta)$, then we have\n",
    "$$\n",
    "\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)=\\mathrm{diag}(q_t)^{-1}\\left(\\nabla_{qq}^2\\Omega(q_t|\\lambda)^{-1}-q_tq_t' \\right)\\left[\\mathbf{X}_t,-\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P(u|\\lambda)$ and the last term is a block matrix of size $J\\times dim(\\theta)$. Note that the latter cross derivative $\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)$ is given by $\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)_g = \\ln(q) - \\Psi^g \\ln(\\Psi^g q)$ for each row $g=1,\\ldots,G$. The derivative of the log-likelihood function can be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_t(\\theta)=\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)' y_t \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_loglikelihood(Theta, y, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a numpy array (N,J) of observed choices in onehot encoding,\n",
    "        x: a numpy matrix (N,J,K) of covariates,\n",
    "        Psi: a dictionary of the matrices \\psi^g as columns as outputted by 'Create_incidence_matrix'\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (N,) of IPDL loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "    ccp_hat = IPDL_ccp(Theta, x, psi_stack, nest_count)\n",
    "    ll = np.empty(T)\n",
    "    \n",
    "    print(np.array([theta for theta in Theta[K:] if theta >0]).sum())\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        ll[t] = np.dot(y[t], np.log(ccp_hat[t], out = -np.inf*np.ones_like(ccp_hat[t]), where = (ccp_hat[t] > 0)))\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(IPDL_loglikelihood(theta0, y, x, Psi_stack, Nest_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL(Theta, y, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "    Q = -IPDL_loglikelihood(Theta, y, x, psi_stack, nest_count)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the derivative of the loglikehood wrt. parameters $\\nabla_\\theta \\ell_t(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation(q, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)}\n",
    "    Z = {}\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        G = len(nest_count[t])\n",
    "        indices = np.int64(np.cumsum(nest_count[t]))\n",
    "        J = np.int64(psi_stack[t].shape[0] - np.sum(nest_count[t]))\n",
    "        Z_t = np.empty((J,G))\n",
    "        for g in np.arange(G):\n",
    "\n",
    "            if g == 0:\n",
    "                Psi = psi_stack[t][J:J+indices[g],:]\n",
    "            else:\n",
    "                Psi = psi_stack[t][J+indices[g-1]:J+indices[g],:]\n",
    "\n",
    "            Psi_q = np.dot( Psi, q[t])\n",
    "            log_Psiq = np.log(Psi_q, out = -np.inf*np.ones_like(Psi_q), where = (Psi_q > 0)) # IS THIS THE RIGHT WAY TO HANDLE 0's ??? Should be set to 0 if input is 0 since no info is gained from the nest or car if they were not active in the market\n",
    "            Psi_logPsiq = np.dot(np.transpose(Psi), log_Psiq) # possibly hadamard multiply with active_mat ???\n",
    "\n",
    "            Z_t[:,g] = log_q[t] - Psi_logPsiq\n",
    "        \n",
    "        Z[t] = Z_t\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_theta_grad_log_ccp(Theta, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Gamma = Create_Gamma(Theta[K:], psi_stack, nest_count)\n",
    "    q = IPDL_ccp(Theta, x, psi_stack, nest_count)\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack, nest_count)\n",
    "    G = [np.concatenate((x[t], Z[t]), axis=1) for t in np.arange(T)]\n",
    "\n",
    "    u_grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count)\n",
    "\n",
    "    Grad = {t: np.dot(u_grad[t], G[t]) for t in np.arange(T)} # np.einsum('tjk,tkd->tjd', u_grad, G)\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_score(Theta, y, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = IPDL_theta_grad_log_ccp(Theta, x, psi_stack, nest_count)\n",
    "    D = log_ccp_grad[0].shape[1]\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] = np.dot(y[t], log_ccp_grad[t])\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL_score(Theta, y, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    return -IPDL_score(Theta, y, x, psi_stack, nest_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "delta = 1.0e-4\n",
    "numgrad = np.empty((T, K+G))\n",
    "\n",
    "for i in np.arange(K+G):\n",
    "    vec = np.zeros((K+G,))\n",
    "    vec[i] = 1\n",
    "    numgrad[:,i] = (IPDL_loglikelihood(theta0 + delta*vec, y, x, Psi_stack, Nest_count) - IPDL_loglikelihood(theta0, y, x, Psi_stack, Nest_count)) / delta"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "angrad = IPDL_score(theta0, y, x, Psi_stack, Nest_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numgrad.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(numgrad[0,:]).tranpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(angrad[0,:]).tranpose()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.mean(angrad - numgrad[0, :], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors in Maximum Likelihood estimation\n",
    "\n",
    "As usual we may consistently estimate the Covariance Matrix  of the IPDL maximum likelihood estimator for some estimate $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'\\in \\mathbb{R}^{K+G}$ as:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\left( \\sum_{i=1}^N \\nabla_\\theta \\ell_i (\\hat \\theta) \\nabla_\\theta \\ell_i (\\hat \\theta)' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Thereby we may find the estimated standard error of parameter $d$ as the squareroot of the d'th diagonal entry of $\\hat \\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_d = \\sqrt{\\hat \\Sigma_{dd}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_se(score, N):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    SE = np.sqrt(np.diag(la.inv(np.einsum('td,tm->dm', score, score)))) / N\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')'\n",
    "        y: a (N,J) array of onehot/dummy encoded observed choices over products j for each individual i \n",
    "        x: a numpy matrix (N,J,K) of covariates\n",
    "        Psi: a (J + sum(C_g),J) numpy array of the JxJ identity stacked on top of the \\Psi^g matrices\n",
    "        nest_indices: a (G+1,) array of indices locating the matrices \\Psi^g\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE\n",
    "    p = 2*scstat.t.sf(T, df = N-1)\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_IPDL(f, Theta0, y, x, psi_stack, nest_count, N, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given start values and \n",
    "    variables to calculate the residuals.\n",
    "\n",
    "    Args:\n",
    "    f: a function to minimize,\n",
    "    Theta0 : (K+G,) array of initial guess parameters,\n",
    "    y: a numpy array (N,J) of observed choices in onehot encoding,\n",
    "    x: array of observed explanatory variables (N,J,K),\n",
    "    Psi: dictionary of nesting distributions outputted by 'Create_incidence_matrix',\n",
    "    Analytic_jac: a boolean. Default value is 'True'. If 'True' the analytic jacobian of the IPDL loglikelihood function is used in estimation. Else the numerical jacobian is used.\n",
    "    options: dictionary with options for the optimizer (e.g. disp=True,\n",
    "        which tells it to display information at termination.)\n",
    "    \n",
    "    Returns:\n",
    "        res: a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(f(Theta, y, x, psi_stack, nest_count))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_IPDL_score(Theta, y, x, psi_stack, nest_count), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Theta0.tolist(), options=options, jac=Grad, **kwargs) # optimize.minimize takes a list of parameters Theta0 (not a numpy array) as initial guess.\n",
    "    se = IPDL_se( IPDL_score(result.x, y, x, psi_stack, nest_count) , N)\n",
    "    T,p = IPDL_t_p(se, result.x, N)\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'theta': result.x,\n",
    "        'se': se,\n",
    "        't': T,\n",
    "        'p': p,\n",
    "        'success':  result.success, # bool, whether convergence was succesful \n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resbla2 = estimate_IPDL(q_IPDL, theta0, y, x, Psi_stack, Nest_count, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPDL_theta, IPDL_se = resbla2['theta'], resbla2['se']\n",
    "IPDL_t, IPDL_p = IPDL_t_p(IPDL_se, IPDL_theta, N)\n",
    "\n",
    "if OO:\n",
    "    regdex = [*x_vars,'group outside/inside option', *['group_' + var for var in nest_vars]]\n",
    "else:\n",
    "    regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "\n",
    "pd.DataFrame({'theta': [ str(np.round(IPDL_theta[i], decimals = 4)) + '***' if IPDL_p[i] <0.01 else str(np.round(IPDL_theta[i], decimals = 3)) + '**' if IPDL_p[i] <0.05 else str(np.round(IPDL_theta[i], decimals = 3)) + '*' if IPDL_p[i] <0.1 else np.round(IPDL_theta[i], decimals = 3) for i in range(len(IPDL_theta))], \n",
    "              'se' : np.round(IPDL_se, decimals = 10),\n",
    "              't (theta == 0)': np.round(IPDL_t, decimals = 3),\n",
    "              'p': np.round(IPDL_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[*zip(regdex, IPDL_theta)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPDL_score(IPDL_theta, y, x, Psi_stack, Nest_count).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = 2*(IPDL_loglikelihood(IPDL_theta, y, x, Psi_stack, Nest_count).sum() - logit.logit_loglikehood(logit_beta, y, x).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scstat.chi2.sf(LR, df = G)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We find the corresponding choice probabilities implied by the MLE $\\hat \\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_hat = IPDL_ccp(IPDL_theta, x, Psi_stack, Nest_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market $t=1$ the choice probabilites $\\hat q_t$ are: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(q_hat[0]).transpose().rename_axis(columns = 'products', index = 'market')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also find the IPDL price elasticities $\\mathcal{E}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_hat = IPDL_elasticity(q_hat, x, IPDL_theta, Psi_stack, Nest_count)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market $t=1$ the price elasticities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(E_hat[0]).rename_axis(columns = 'wrt. product', index = 'elasticity of product')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios for the IPDL model\n",
    "\n",
    "The diversion ratio to product j from product k is the fraction of consumers leaving product k and switching to product j following a one percent increase in the price of product k. Hence we have:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{jk}^i = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial x_{ik\\ell}}{\\partial P_k(u_i|\\lambda) / \\partial x_{ik\\ell}} = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial u_{ik}}{\\partial P_k(u_i|\\lambda) / \\partial u_{ik}}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{D}^i = \\left( \\mathcal{D}_{jk}^i \\right)_{j,k \\in \\{0,1,\\ldots ,5\\}}$ is the matrix of diversion ratios for individual i. This can be written more compactly as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}^i = -100 \\cdot  (\\nabla_u P(u|\\lambda) \\circ I_J)^{-1}\\nabla_u P(u|\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_diversion_ratio(q, x, Theta, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates diversion ratios from the IPDL model\n",
    "\n",
    "    Args.\n",
    "        q: an (N,J) array of choice probabilities\n",
    "        x: an (N,J,K) array of covariates\n",
    "        Theta: an (K+G,) array of parameters\n",
    "        Psi: a (J + sum(C_g),J) numpy array of the JxJ identity stacked on top of the \\Psi^g matrices\n",
    "        nest_indices: a (G+1,) array of indices locating the matrices \\Psi^g\n",
    "\n",
    "    Returns\n",
    "        Diversion_ratio: an (N,J,J) array of diversion ratios from product j to product k for each individual i\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack, nest_count)\n",
    "    inv_diaggrad = {t: np.divide(1, np.diag(Grad[t]), out = np.zeros_like(np.diag(Grad[t])), where = (np.diag(Grad[t]) != 0)) for t in np.arange(T)}\n",
    "    DR = {t: np.multiply(-100, np.multiply(inv_diaggrad[t][:,None], Grad[t])) for t in np.arange(T)}\n",
    "\n",
    "    \n",
    "    return DR "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the implied diversion ratios $\\mathcal{ D}^i$ from our estimates $\\hat \\theta^{\\text{IPDL}}$, we find for market $t=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_hat = IPDL_diversion_ratio(q_hat, x, IPDL_theta, Psi_stack, Nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(DR_hat[0]).rename_axis(index = 'DR of products', columns = 'DR wrt. products')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DR_hat[0].sum(axis = 1).round(decimals = 8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation of elasticities and diversion ratios\n",
    "\n",
    "We now compare the elasticities and the diversion ratios of the Logit and IPDL model. To clarify the interpretation of our results we will aggregate these according to the categorical variable `cla` describing the class or segment code of each vehicle. This variable takes values 'subcompact', 'compact', 'intermediate', 'standard', and 'luxury' encoded as the integers $1,\\ldots, 5$. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all classes/segments $c,\\ell \\in \\{1,\\ldots, 5\\}$ we calculate the change in the probability of class $c$, given as $q_c = \\sum_j 1_{\\{j\\in c\\}} q_j$, for a one unit increase in each of the utilities $u_j$ for products $j\\in\\ell$ i.e. we calculate the directional derivatives $\\frac{\\partial q_c}{\\partial u_{\\ell}}$. Then the price-to-income semi-elasticity of class $c$ wrt. class $\\ell$ is given as $\\bar E_{c\\ell} = \\frac{\\partial q_c}{\\partial u_\\ell} \\frac{1}{q_c} \\beta_{\\text{princ}}$. We use the fact that the directional derivative is calculated as $\\frac{\\partial q_c}{\\partial u_{\\ell}} = \\sum_{j\\in c} \\sum_{k\\in \\ell} \\frac{\\partial q_j}{\\partial u_k}$. In matrix notation this may be calulated as $\\bar E = \\psi^{\\text{class}} \\mathcal{E} {\\psi^{\\text{class}}}'$, where $\\bar E = (\\bar E_{c\\ell})_{c,\\ell = 1,\\ldots,5}$ is the matrix of semi-elasticities between vehicle classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi_clafull, cla_descr, cla_count = Create_nests(dat[['cla', 'market', 'co']], 'market', 'co', ['cla'], outside_option=OO)\n",
    "Psi_cla = {t: Psi_clafull[t][J[t]+2:, :] for t in np.arange(T)}\n",
    "T_agg = Psi_cla[0].shape[0]\n",
    "\n",
    "q_Logit_agg = {t: np.einsum('cj,j->c', Psi_cla[t], logit_q[t]) for t in np.arange(T)}\n",
    "q_IPDL_agg = {t: np.einsum('cj,j->c', Psi_cla[t], q_hat[t]) for t in np.arange(T)}\n",
    "\n",
    "Grad_Logit = {t: (np.diag(logit_q[t]) - np.einsum('j,k->jk', logit_q[t], logit_q[t])) for t in np.arange(T)}\n",
    "Grad_IPDL = ccp_gradient(q_hat, x, IPDL_theta, Psi_stack, Nest_count)\n",
    "\n",
    "dq_dp_Logit_agg = {t: np.einsum('cj,jk,lk->cl', Psi_cla[t], Grad_Logit[t], Psi_cla[t])*logit_beta[K-1] for t in np.arange(T)}\n",
    "dq_dp_IPDL_agg = {t: np.einsum('cj,jk,lk->cl', Psi_cla[t], Grad_IPDL[t], Psi_cla[t])*IPDL_theta[K-1] for t in np.arange(T)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logit_E_agg = {t:  np.einsum('c,cl->cl', 1./ q_Logit_agg[t], dq_dp_Logit_agg[t]) for t in np.arange(T)}\n",
    "IPDL_E_agg = {t: np.einsum('c,cl->cl', 1./q_IPDL_agg[t], dq_dp_IPDL_agg[t]) for t in np.arange(T)}\n",
    "\n",
    "E0, E1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    E0[t,:,:] = Logit_E_agg[t]\n",
    "    E1[t,:,:] = IPDL_E_agg[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logit_E_agg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi_cla[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot histograms of our results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E0p = {j : (E0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E0p[j], num_bins, range = (np.quantile(E0p[j], 0.10), np.quantile(E0p[j], 0.90)), color = 'r', alpha = 1) # Logit is blue\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of weigthed sums of Logit (red) and IPDL (blue) price elasticities by class')\n",
    "fig.supxlabel('Weigthed sum of elasticities wrt. classes')\n",
    "fig.supylabel('Weigthed sum of elasticities of classes')\n",
    "fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(E0[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E1p = {j : (E1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig1, axes1 = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes1[p].hist(E1p[j], num_bins, range = (np.quantile(E1p[j], 0.10), np.quantile(E1p[j], 0.90)), color = 'b', alpha = 1) # IPDL is blue\n",
    "    axes1[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes1[p].get_xaxis().set_visible(False)\n",
    "    axes1[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig1.suptitle('Histograms of weigthed sums of Logit (red) and IPDL (blue) price elasticities by class')\n",
    "fig1.supxlabel('Weigthed sum of elasticities wrt. classes')\n",
    "fig1.supylabel('Weigthed sum of elasticities of classes')\n",
    "fig1.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "fig1.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "fig1.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "fig1.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "fig1.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "fig1.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "fig1.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "fig1.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "fig1.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "fig1.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPDL_theta[K]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios\n",
    "\n",
    "We now visualize the implied diversion ratios $\\mathcal{D}$. If $\\bar D_{c\\ell}$ denotes the sum of choice probability weigthed diversion ratios, then we have as above that $\\bar D_{c\\ell} = \\sum_{j}\\sum_{k} \\mathrm{1}_{\\{j\\in c\\}} \\mathrm{1}_{\\{k\\in \\ell\\}} q_j q_k \\mathcal{D}_{jk}$ i.e. more generally $\\bar D = (\\psi^{\\text{class}} \\circ q) \\mathcal{D} (\\psi^{\\text{class}} \\circ q).'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Logit_D_agg = {t: -100*np.einsum('c,cl->cl', 1./np.diag(dq_dp_Logit_agg[t]), dq_dp_Logit_agg[t]) for t in np.arange(T)}\n",
    "IPDL_D_agg = {t: -100*np.einsum('c,cl->cl', 1./np.diag(dq_dp_IPDL_agg[t]), dq_dp_IPDL_agg[t]) for t in np.arange(T)}\n",
    "\n",
    "D0, D1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    D0[t,:,:] = Logit_D_agg[t]\n",
    "    D1[t,:,:] = IPDL_D_agg[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(Logit_D_agg[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0p = {j : (D0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D0p[j], num_bins, range = (np.quantile(D0p[j], 0.10), np.quantile(D0p[j], 0.90)), color = 'r', alpha = 1) # Logit is red\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of weigthed sums of Logit (red) and IPDL (blue) price diversion ratios by class')\n",
    "fig.supxlabel('Weigthed sum of diversion ratios wrt. classes')\n",
    "fig.supylabel('Weigthed sum of diversion ratios of classes')\n",
    "fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D1p = {j : (D1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg, sharex=False, sharey=False)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D1p[j], num_bins, range = (np.quantile(D1p[j], 0.10), np.quantile(D1p[j], 0.90)), color = 'b', alpha = 1) # IPDL is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of weigthed sums of Logit (red) and IPDL (blue) price diversion ratios by class')\n",
    "fig.supxlabel('Weigthed sum of diversion ratios wrt. classes')\n",
    "fig.supylabel('Weigthed sum of diversion ratios of classes')\n",
    "fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
