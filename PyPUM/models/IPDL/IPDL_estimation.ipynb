{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation of the IPDL Model\n",
    "\n",
    "In this notebook, we present and implement two consistent estimation methods for the IPDL Model: the MLE and the FKN-estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   variable names                                        description\n",
      "0              cy            cylinder volume or displacement (in cc)\n",
      "1              hp                                 horsepower (in kW)\n",
      "2              we                                     weight (in kg)\n",
      "3              le                                     length (in cm)\n",
      "4              wi                                      width (in cm)\n",
      "5              he                                     height (in cm)\n",
      "6              li          average of li1, li2, li3 (used in papers)\n",
      "7              sp                            maximum speed (km/hour)\n",
      "8              ac  time to acceleration (in seconds from 0 to 100...\n",
      "9              pr   price (in destination currency including V.A.T.)\n",
      "10          brand                                      name of brand\n",
      "11           home  domestic car dummy (appropriate interaction of...\n",
      "12            cla                              class or segment code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has full rank\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = \"warn\"\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "import sys\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Files\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utilities.Logit_file import estimate_logit, logit_se, logit_t_p, q_logit, logit_score, logit_score_unweighted, logit_ccp, LogitBLP_estimator\n",
    "from data.Eurocarsdata_file import Eurocars_cleandata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and variable names\n",
    "# os.chdir('../GREENCAR_notebooks/') # Assigns work directory\n",
    "\n",
    "input_path = os.getcwd() # Assigns input path as current working directory (cwd)\n",
    "descr = (pd.read_stata('../data/eurocars.dta', iterator = True)).variable_labels() # Obtain variable descriptions\n",
    "dat_file = pd.read_csv('../data/eurocars.csv') # reads in the data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              cy            cylinder volume or displacement (in cc)\n",
       "1              hp                                 horsepower (in kW)\n",
       "2              we                                     weight (in kg)\n",
       "3              le                                     length (in cm)\n",
       "4              wi                                      width (in cm)\n",
       "5              he                                     height (in cm)\n",
       "6              li          average of li1, li2, li3 (used in papers)\n",
       "7              sp                            maximum speed (km/hour)\n",
       "8              ac  time to acceleration (in seconds from 0 to 100...\n",
       "9              pr   price (in destination currency including V.A.T.)\n",
       "10          brand                                      name of brand\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            cla                              class or segment code"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Outside option is included if OO == True, otherwise analysis is done on the inside options only.\n",
    "OO = True\n",
    "\n",
    "# Choose which variables to include in the analysis, and assign them either as discrete variables or continuous.\n",
    "\n",
    "x_discretevars = [ 'brand', 'home', 'cla']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "z_IV_contvars = ['xexr']\n",
    "z_IV_discretevars = []\n",
    "x_allvars =  [*x_contvars, *x_discretevars]\n",
    "z_allvars = [*z_IV_contvars, *z_IV_discretevars]\n",
    "\n",
    "if OO:\n",
    "    nest_vars = [var for var in ['in_out', *x_allvars] if (var != 'pr')] # We nest over all variables other than price, but an alternative list can be specified here if desired.\n",
    "else:\n",
    "    nest_vars = [var for var in x_allvars if (var != 'pr')] # See above\n",
    "\n",
    "nest_cont_vars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac'] # The list of continuous variables, from which nests will be created according to the deciles of the distribution.\n",
    "\n",
    "G = len(nest_vars)\n",
    "\n",
    "# Print list of chosen variables as a dataframe\n",
    "pd.DataFrame(descr, index=['description'])[x_allvars].transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, dat_org, x_vars, z_vars, N, pop_share, T, J, K = Eurocars_cleandata(dat_file, x_contvars, x_discretevars, z_IV_contvars, z_IV_discretevars, outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of numpy arrays for each market. This allows the size of the data set to vary over markets.\n",
    "\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)} # Dict of explanatory variables\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)} # Dict of market shares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of IPDL\n",
    "\n",
    "The log-likelihood contribution is\n",
    "$$\n",
    "\\ell_t(\\theta)=y_t'\\ln p(\\mathbf{X}_t,\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_t$ and $\\theta$ - calculates $u_t=\\mathbf{X}_t\\beta$ and constructs $\\Gamma$, and then calls the fixed point routine described in 'IPDL_ccp.ipynb'. That routine will return $p(\\mathbf{X}_t,\\theta)$, and we can then evaluate $\\ell_t(\\theta)$. Using our above defined functions we now construct precisely such an estimation procedure.\n",
    "\n",
    "For maximizing the likelihood, we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_t=p(\\mathbf{X}_t,\\theta)$, then we have\n",
    "$$\n",
    "\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)=\\mathrm{diag}(q_t)^{-1}\\left(\\nabla_{qq}^2\\Omega(q_t|\\lambda)^{-1}-q_tq_t' \\right)\\left[\\mathbf{X}_t,-\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P(u|\\lambda)$ and the last term is a block matrix of size $J\\times dim(\\theta)$. Note that the latter cross derivative $\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)$ is given by $\\nabla_{q,\\lambda} \\Omega(q_t|\\lambda)_g = - \\ln(q) + (\\Psi^g)' \\ln(\\Psi^g q)$ for each row $g=1,\\ldots,G$. The derivative of the log-likelihood function can be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_t(\\theta)=\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)' y_t \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (T,) of IPDL loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "    ccp_hat = IPDL_ccp(Theta, x, psi_stack, nest_count)\n",
    "    sum_lambdaplus = np.array([theta for theta in Theta[K:] if theta >0]).sum()\n",
    "\n",
    "    '''if sum_lambdaplus >= 1:\n",
    "        ll = np.NINF*np.ones((T,))'''\n",
    "\n",
    "    \n",
    "    ll=np.empty((T,))\n",
    "    for t in np.arange(T):\n",
    "        ll[t] = sample_share[t]*(y[t]@np.log(ccp_hat[t])) # np.einsum('j,j', y[t], np.log(ccp_hat[t], out = -np.inf*np.ones_like(ccp_hat[t]), where = (ccp_hat[t] > 0)))\n",
    "\n",
    "    print([sum_lambdaplus, -ll.mean()])\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' The negative loglikelihood criterion to minimize\n",
    "    '''\n",
    "    Q = -IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the derivative of the loglikehood wrt. parameters $\\nabla_\\theta \\ell_t(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation(q, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Z = {}\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        G = len(nest_count[t])\n",
    "        indices = np.int64(np.cumsum(nest_count[t])) # Find the indices of the categories g used in the psi_stack matrices\n",
    "        J = q[t].shape[0] # Find the number of alternatives\n",
    "\n",
    "        log_q = np.log(q[t])\n",
    "        Z_t = np.empty((J,G)) # Initialize a J[t] by G numpy matrix for market t\n",
    "\n",
    "        for g in np.arange(G):\n",
    "\n",
    "            # Find the \\psi^g matrix for category g\n",
    "            if g == 0:\n",
    "                Psi = psi_stack[t][J:(J+indices[g]),:] \n",
    "            else:\n",
    "                Psi = psi_stack[t][(J+indices[g-1]):(J+indices[g]),:]\n",
    "\n",
    "            Psi_q = Psi @ q[t] # Compute a matrix product\n",
    "            log_Psiq = np.log(Psi_q) # Determine log of Psi_q, and set entries equal to minus infinity if entry <= 0.\n",
    "            Psi_logPsiq = Psi.T @ log_Psiq # Compute matrix product\n",
    "\n",
    "            Z_t[:,g] = - log_q + Psi_logPsiq # Compute cross differential\n",
    "        \n",
    "        Z[t] = Z_t\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_theta_grad_log_ccp(Theta, x, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the derivative of the IPDL log ccp's wrt. parameters theta\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K+G) of derivatives of the IPDL log ccp's wrt. parameters theta for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "\n",
    "    q = IPDL_ccp(Theta, x, psi_stack, nest_count) # Find choice probabilities\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack, nest_count) # Find cross differentials of the pertubation function\n",
    "    u_grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack, nest_count)  # Find the gradient of log ccp's wrt. utilities\n",
    "    Grad={}\n",
    "\n",
    "    for t in range(T):\n",
    "        G=np.concatenate((x[t], -Z[t]), axis=1)\n",
    "        Grad[t]=u_grad[t]@G\n",
    "   \n",
    "   # G = [np.concatenate((x[t], Z[t]), axis=1) for t in np.arange(T)] # Construct the block matrix of the covariates and the cross differentials as block matrices\n",
    "    #Grad = {t: np.einsum('jk,kd->jd', u_grad[t], G[t]) for t in np.arange(T)} # Compute the derivative by matrix multiplication.\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the score of the IPDL loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of IPDL scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = IPDL_theta_grad_log_ccp(Theta, x, psi_stack, nest_count) # Find derivatives of the IPDL log ccp's wrt. parameters theta\n",
    "    D = log_ccp_grad[0].shape[1] # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] =sample_share[t]*(log_ccp_grad[t].T@y[t]) #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' The derivative of the negative loglikelihood criterion\n",
    "    '''\n",
    "    return -IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analyticgrad(y, x, theta, sample_share, Psi, Nest_count, delta = 1.0e-8):\n",
    "\n",
    "    numgrad = np.empty((T, K+G))\n",
    "\n",
    "    for i in np.arange(K+G):\n",
    "        vec = np.zeros((K+G,))\n",
    "        vec[i] = 1\n",
    "        numgrad[:,i] = (IPDL_loglikelihood(theta + delta*vec, y, x, sample_share, Psi, Nest_count) - IPDL_loglikelihood(theta, y, x, sample_share, Psi, Nest_count)) / delta\n",
    "\n",
    "    angrad = IPDL_score(theta, y, x, sample_share, Psi, Nest_count)\n",
    "\n",
    "    normdiff = la.norm(angrad - numgrad)\n",
    "    \n",
    "    return normdiff, angrad, numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.025328025815875677]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802577337812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802577421719]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802578551036]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802580486519]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025808139996]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025807728423]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025781598762]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025793703513]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802577227401]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575444403]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575261861]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025756880854]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025754301448]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025755661687]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025756371494]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575295324]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575289476]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025753035364]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025758435027]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025757042287]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025753734194]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025753156968]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575297905]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575285652]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025755651164]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.0253280257540525]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575444159]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025753409943]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025753585757]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025757636492]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025756768135]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025755825892]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575742855]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575386931]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575396422]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025752612275]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025752864646]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575263091]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.0253280257528272]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575266442]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575311253]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025752617934]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025753972066]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802575380528]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025754641378]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025752762516]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025769148475]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025767461827]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.02532802576623689]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025764632837]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.2, 0.025328025758722]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.025328025495378503]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.02532802563978794]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.025328025637386464]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.0253280256404344]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.025328025640711845]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.025328025638921562]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.02532802563381742]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.02532802564019459]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.025328025637427264]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.025328025640190606]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.02532802567436993]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.02532802552933927]\n",
      "[0.2, 0.025328025752599303]\n",
      "[0.20000001, 0.02532802559345194]\n",
      "[0.2, 0.025328025752599303]\n"
     ]
    }
   ],
   "source": [
    "theta0 = np.ones((K+G,))/(K+G)\n",
    "diff, an, num = test_analyticgrad(y, x, theta0, pop_share, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors in Maximum Likelihood estimation\n",
    "\n",
    "As usual we may consistently estimate the Covariance Matrix  of the IPDL maximum likelihood estimator for some estimate $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'\\in \\mathbb{R}^{K+G}$ as:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\left( \\sum_{i=1}^N \\nabla_\\theta \\ell_i (\\hat \\theta) \\nabla_\\theta \\ell_i (\\hat \\theta)' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Thereby we may find the estimated standard error of parameter $d$ as the squareroot of the d'th diagonal entry of $\\hat \\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_d = \\sqrt{\\hat \\Sigma_{dd}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_se(score, N):\n",
    "    '''\n",
    "    This function computes the asymptotic standard errors of the MLE.\n",
    "\n",
    "    Args.\n",
    "        score: a numpy array (T,K+G) of IPDL scores\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns\n",
    "        SE: a numpy array (K+G,) of asymptotic IPDL MLE standard errors\n",
    "    '''\n",
    "\n",
    "    SE = np.sqrt(np.diag(la.inv(np.einsum('td,tm->dm', score, score))) / N)\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        SE: a numpy array (K+G,) of asymptotic IPDL MLE standard errors\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        N: an integer giving the number of observations\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated asymptotic p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE\n",
    "    p = 2*scstat.t.sf(T, df = N-1)\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_IPDL(f, Theta0, y, x, sample_share, psi_stack, nest_count, N, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given starting values and variables necessary in the IPDL model specification.\n",
    "\n",
    "    Args:\n",
    "        f: a function to minimize,\n",
    "        Theta0 : a numpy array (K+G,) of initial guess parameters (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests', \n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t,\n",
    "        N: an integer giving the number of observations,\n",
    "        Analytic_jac: a boolean. Default value is 'True'. If 'True' the analytic jacobian of the IPDL loglikelihood function is used in estimation. Else the numerical jacobian is used.\n",
    "        options: dictionary with options for the optimizer (e.g. disp=True which tells it to display information at termination.)\n",
    "    \n",
    "    Returns:\n",
    "        res: a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(f(Theta, y, x, sample_share, psi_stack, nest_count))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_IPDL_score(Theta, y, x, sample_share, psi_stack, nest_count), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Theta0.tolist(), options=options, jac=Grad, **kwargs) # optimize.minimize takes a list of parameters Theta0 (not a numpy array) as initial guess.\n",
    "    se = IPDL_se(IPDL_score(result.x, y, x, sample_share, psi_stack, nest_count), N)\n",
    "    T,p = IPDL_t_p(se, result.x, N)\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'theta': result.x,\n",
    "        'se': se,\n",
    "        't': T,\n",
    "        'p': p,\n",
    "        'success':  result.success, # bool, whether convergence was succesful 1\n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001526\n",
      "         Iterations: 25\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 29\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.zeros((K,))\n",
    "\n",
    "# Estimate the model\n",
    "Logit_beta = estimate_logit(q_logit, beta_0, y, x, sample_share=pop_share, Analytic_jac=True)['beta']\n",
    "Logit_SE = logit_se(logit_score(Logit_beta, y, x, pop_share), pop_share, N)\n",
    "Logit_t, Logit_p = logit_t_p(Logit_beta, logit_score(Logit_beta, y, x, pop_share), pop_share, N)\n",
    "\n",
    "# Initialize \\theta^0\n",
    "theta0 = np.append(Logit_beta,lambda0)\n",
    "q0 = IPDL_ccp(theta0, x, Psi, Nest_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.0015899709273228062]\n",
      "[0.46090876162018596, 0.0015889713847807645]\n",
      "[0.4474009509580727, 0.0015850515424856092]\n",
      "[0.3933697083096197, 0.0015705950490930536]\n",
      "[0.23671077826873116, 0.0015395686298264781]\n",
      "[0.06493464753797744, 0.0015274591822855104]\n",
      "[0.09885323842764626, 0.001526556776446271]\n",
      "[0.09350628851001848, 0.0015265286295808205]\n",
      "[0.09314635294486558, 0.001526527551818973]\n",
      "[0.09277560631463176, 0.0015265254847205024]\n",
      "[0.09229333534239086, 0.0015265214861262715]\n",
      "[0.0903642514534273, 0.0015265090375558383]\n",
      "[0.08723931532582233, 0.0015264827038595846]\n",
      "[0.08374217623589737, 0.0015264371722768877]\n",
      "[0.07958619829060203, 0.0015263546256758743]\n",
      "[0.07479387512040013, 0.0015262026165299041]\n",
      "[0.07230591432952654, 0.0015259236511221328]\n",
      "[0.08108249841372164, 0.0015254217386624547]\n",
      "[0.111069882894378, 0.001524554949365476]\n",
      "[0.17700369913547914, 0.0015231642147646566]\n",
      "[0.3063977259472085, 0.0015212948069764096]\n",
      "[0.428279371543753, 0.001519808498143949]\n",
      "[0.5254814593618558, 0.0015188730817765597]\n",
      "[0.5440629302128863, 0.0015186821824164165]\n",
      "[0.5325144822928136, 0.0015186486369110118]\n",
      "[0.5199199830862458, 0.0015186418905030535]\n",
      "[0.5160771391021937, 0.0015186408008959179]\n",
      "[0.5123927308130474, 0.0015186391422557317]\n",
      "[0.5088525329069138, 0.0015186361822383987]\n",
      "[0.50504023009852, 0.0015186307096877302]\n",
      "[0.5004895361039188, 0.0015186204889989046]\n",
      "[0.49473083732713863, 0.0015186014576802056]\n",
      "[0.48732179953798127, 0.001518566483739109]\n",
      "[0.4780980313341608, 0.0015185037610670692]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001519\n",
      "         Iterations: 30\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n"
     ]
    }
   ],
   "source": [
    "resbla2 = estimate_IPDL(q_IPDL, theta0, y, x, pop_share, Psi, Nest_count, N, Analytic_jac=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.0015899709273228062]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0015899709273228062"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "-IPDL_loglikelihood(theta0, y, x, pop_share, Psi, Nest_count).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_table(theta,se,N,x_vars,nest_vars):\n",
    "    IPDL_t, IPDL_p = IPDL_t_p(se, theta, N)\n",
    "\n",
    "    if OO:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "    else:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "\n",
    "    table  = pd.DataFrame({'theta': [ str(np.round(theta[i], decimals = 4)) + '***' if IPDL_p[i] <0.01 else str(np.round(theta[i], decimals = 3)) + '**' if IPDL_p[i] <0.05 else str(np.round(theta[i], decimals = 3)) + '*' if IPDL_p[i] <0.1 else np.round(theta[i], decimals = 3) for i in range(len(theta))], \n",
    "                'se' : np.round(se, decimals = 5),\n",
    "                't (theta == 0)': np.round(IPDL_t, decimals = 3),\n",
    "                'p': np.round(IPDL_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.119</td>\n",
       "      <td>4.42060</td>\n",
       "      <td>0.479</td>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.547</td>\n",
       "      <td>3.08893</td>\n",
       "      <td>0.177</td>\n",
       "      <td>0.860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.591</td>\n",
       "      <td>3.02535</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.855</td>\n",
       "      <td>2.77277</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.503</td>\n",
       "      <td>4.13013</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-1.686</td>\n",
       "      <td>5.19260</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0.745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-1.819</td>\n",
       "      <td>4.05489</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.874</td>\n",
       "      <td>1.68868</td>\n",
       "      <td>0.518</td>\n",
       "      <td>0.605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.232</td>\n",
       "      <td>3.33815</td>\n",
       "      <td>0.369</td>\n",
       "      <td>0.712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>-0.779</td>\n",
       "      <td>1.29178</td>\n",
       "      <td>0.603</td>\n",
       "      <td>0.547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.052</td>\n",
       "      <td>1.18934</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.009</td>\n",
       "      <td>2.12215</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>0.192</td>\n",
       "      <td>1.09925</td>\n",
       "      <td>0.175</td>\n",
       "      <td>0.861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.115</td>\n",
       "      <td>1.11696</td>\n",
       "      <td>0.103</td>\n",
       "      <td>0.918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.05</td>\n",
       "      <td>1.29807</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.243</td>\n",
       "      <td>1.12500</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.037</td>\n",
       "      <td>2.34163</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.048</td>\n",
       "      <td>3.34526</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-0.126</td>\n",
       "      <td>1.46832</td>\n",
       "      <td>0.086</td>\n",
       "      <td>0.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>-0.099</td>\n",
       "      <td>1.03561</td>\n",
       "      <td>0.096</td>\n",
       "      <td>0.924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>0.182</td>\n",
       "      <td>0.96571</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.081</td>\n",
       "      <td>1.94522</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-0.06</td>\n",
       "      <td>3.08255</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-0.121</td>\n",
       "      <td>0.99457</td>\n",
       "      <td>0.122</td>\n",
       "      <td>0.903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-0.045</td>\n",
       "      <td>3.51266</td>\n",
       "      <td>0.013</td>\n",
       "      <td>0.990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>1.12619</td>\n",
       "      <td>0.266</td>\n",
       "      <td>0.790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.14</td>\n",
       "      <td>1.90482</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.159</td>\n",
       "      <td>1.19239</td>\n",
       "      <td>0.134</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-0.082</td>\n",
       "      <td>1.94184</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.074</td>\n",
       "      <td>1.82080</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>0.147</td>\n",
       "      <td>0.94947</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>-0.031</td>\n",
       "      <td>0.98888</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>0.119</td>\n",
       "      <td>0.96666</td>\n",
       "      <td>0.123</td>\n",
       "      <td>0.902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.43</td>\n",
       "      <td>1.06765</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.08</td>\n",
       "      <td>4.23414</td>\n",
       "      <td>0.019</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.212</td>\n",
       "      <td>1.54702</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-0.001</td>\n",
       "      <td>13.66064</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.039</td>\n",
       "      <td>5.29165</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-0.015</td>\n",
       "      <td>2.85271</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-0.072</td>\n",
       "      <td>1.54258</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-0.006</td>\n",
       "      <td>3.51479</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.043</td>\n",
       "      <td>1.55806</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-0.004</td>\n",
       "      <td>4.63996</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.112</td>\n",
       "      <td>1.08198</td>\n",
       "      <td>0.104</td>\n",
       "      <td>0.917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.107</td>\n",
       "      <td>1.58523</td>\n",
       "      <td>0.068</td>\n",
       "      <td>0.946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.097</td>\n",
       "      <td>1.79266</td>\n",
       "      <td>0.054</td>\n",
       "      <td>0.957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-0.04</td>\n",
       "      <td>3.49391</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.486***</td>\n",
       "      <td>0.53949</td>\n",
       "      <td>2.754</td>\n",
       "      <td>0.006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.039</td>\n",
       "      <td>0.40658</td>\n",
       "      <td>0.097</td>\n",
       "      <td>0.923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.023</td>\n",
       "      <td>0.73442</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>-0.25</td>\n",
       "      <td>1.18297</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>-0.038</td>\n",
       "      <td>1.76949</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.76329</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.027</td>\n",
       "      <td>0.16322</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.15671</td>\n",
       "      <td>0.474</td>\n",
       "      <td>0.636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.043</td>\n",
       "      <td>0.18702</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>0.037</td>\n",
       "      <td>0.22810</td>\n",
       "      <td>0.163</td>\n",
       "      <td>0.871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>0.127</td>\n",
       "      <td>0.23205</td>\n",
       "      <td>0.547</td>\n",
       "      <td>0.584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.13532</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.17205</td>\n",
       "      <td>0.485</td>\n",
       "      <td>0.628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.035</td>\n",
       "      <td>0.16651</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.193</td>\n",
       "      <td>0.14974</td>\n",
       "      <td>1.290</td>\n",
       "      <td>0.197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>-0.026</td>\n",
       "      <td>0.19667</td>\n",
       "      <td>0.133</td>\n",
       "      <td>0.894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.127</td>\n",
       "      <td>0.26026</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>0.271</td>\n",
       "      <td>0.66489</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0.683</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables        theta        se  t (theta == 0)      p\n",
       "in_out          -2.119   4.42060           0.479  0.632\n",
       "cy              -0.547   3.08893           0.177  0.860\n",
       "hp              -0.591   3.02535           0.195  0.845\n",
       "we              -0.855   2.77277           0.309  0.758\n",
       "le              -1.503   4.13013           0.364  0.716\n",
       "wi              -1.686   5.19260           0.325  0.745\n",
       "he              -1.819   4.05489           0.449  0.654\n",
       "li              -0.874   1.68868           0.518  0.605\n",
       "sp              -1.232   3.33815           0.369  0.712\n",
       "ac              -0.779   1.29178           0.603  0.547\n",
       "pr              -0.052   1.18934           0.044  0.965\n",
       "brand_2         -0.009   2.12215           0.004  0.996\n",
       "brand_3          0.192   1.09925           0.175  0.861\n",
       "brand_4         -0.115   1.11696           0.103  0.918\n",
       "brand_5          -0.05   1.29807           0.038  0.969\n",
       "brand_6         -0.243   1.12500           0.216  0.829\n",
       "brand_7         -0.037   2.34163           0.016  0.988\n",
       "brand_8         -0.048   3.34526           0.014  0.989\n",
       "brand_9         -0.126   1.46832           0.086  0.932\n",
       "brand_10        -0.099   1.03561           0.096  0.924\n",
       "brand_11         0.182   0.96571           0.188  0.851\n",
       "brand_12        -0.081   1.94522           0.042  0.967\n",
       "brand_13         -0.06   3.08255           0.019  0.985\n",
       "brand_14        -0.121   0.99457           0.122  0.903\n",
       "brand_15        -0.045   3.51266           0.013  0.990\n",
       "brand_16          -0.3   1.12619           0.266  0.790\n",
       "brand_17         -0.14   1.90482           0.074  0.941\n",
       "brand_18         0.159   1.19239           0.134  0.894\n",
       "brand_19        -0.082   1.94184           0.042  0.966\n",
       "brand_20        -0.074   1.82080           0.040  0.968\n",
       "brand_21         0.147   0.94947           0.155  0.877\n",
       "brand_22        -0.031   0.98888           0.031  0.975\n",
       "brand_23         0.119   0.96666           0.123  0.902\n",
       "brand_24         -0.43   1.06765           0.403  0.687\n",
       "brand_25         -0.08   4.23414           0.019  0.985\n",
       "brand_26        -0.212   1.54702           0.137  0.891\n",
       "brand_27        -0.001  13.66064           0.000  1.000\n",
       "brand_28        -0.039   5.29165           0.007  0.994\n",
       "brand_29        -0.015   2.85271           0.005  0.996\n",
       "brand_30        -0.072   1.54258           0.047  0.963\n",
       "brand_31        -0.006   3.51479           0.002  0.999\n",
       "brand_32        -0.043   1.55806           0.028  0.978\n",
       "brand_33        -0.004   4.63996           0.001  0.999\n",
       "brand_34        -0.112   1.08198           0.104  0.917\n",
       "brand_35        -0.107   1.58523           0.068  0.946\n",
       "brand_36        -0.097   1.79266           0.054  0.957\n",
       "brand_37         -0.04   3.49391           0.012  0.991\n",
       "home_2        1.486***   0.53949           2.754  0.006\n",
       "cla_2            0.039   0.40658           0.097  0.923\n",
       "cla_3            0.023   0.73442           0.031  0.976\n",
       "cla_4            -0.25   1.18297           0.211  0.833\n",
       "cla_5           -0.038   1.76949           0.021  0.983\n",
       "group_in_out    -0.014   0.76329           0.018  0.985\n",
       "group_cy        -0.027   0.16322           0.167  0.867\n",
       "group_hp        -0.074   0.15671           0.474  0.636\n",
       "group_we         0.043   0.18702           0.229  0.819\n",
       "group_le         0.037   0.22810           0.163  0.871\n",
       "group_wi         0.127   0.23205           0.547  0.584\n",
       "group_he        -0.073   0.13532           0.542  0.588\n",
       "group_li        -0.083   0.17205           0.485  0.628\n",
       "group_sp        -0.035   0.16651           0.210  0.834\n",
       "group_ac        -0.193   0.14974           1.290  0.197\n",
       "group_brand     -0.026   0.19667           0.133  0.894\n",
       "group_home      -0.127   0.26026           0.487  0.626\n",
       "group_cla        0.271   0.66489           0.408  0.683"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "IPDL_theta = resbla2['theta']\n",
    "IPDL_SE = resbla2['se']\n",
    "IPDL_t, IPDL_p = IPDL_t_p(IPDL_SE, IPDL_theta, N)\n",
    "reg_table(IPDL_theta, IPDL_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4780980313341608"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array([p for p in IPDL_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that some of the estimated nesting parameters $\\lambda$ are negative. Fosgerau & Nielsen (2023) show that the model is still a PUM in this case if the condition $\\sum_{g: \\lambda_g \\geq 0} \\lambda_g < 1$ is satisfied such that the IPDL Model utility maximization problem still has a unique solution in this case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative approach\n",
    "\n",
    "The log-likelihood function is not globally concave, and finding the global optimum can be difficult. Using the estimation procedure of Fosgerau, Kristensen & Nielsen (2023 working paper), we can instead fit the parameters using the first-order conditions for optimality. The estimator takes the form\n",
    "\n",
    "$$\n",
    "\\hat \\theta^0=\\arg \\min_{\\theta} \\sum_t s_t \\hat \\varepsilon^0_t(\\theta)'\\hat W^0_t\\hat \\varepsilon^0 _t(\\theta),\n",
    "$$\n",
    "where $\\hat W^0_t$ is a positive semidefinite weight matrix, $s_t$ is market $t$'s share of the total population and \n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t(u(X_t,\\beta)- \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)) ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat D^0_t=\\textrm{diag}(\\hat q^0_t)-\\hat q^0_t (\\hat q^0_t)'.\n",
    "$$\n",
    "Using equation (...) above, we have that $\\hat \\epsilon_t$ is a linear function of $\\theta$,\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t \\left(\\hat G^0_t\\theta- \\ln \\hat q^0_t\\right)\\equiv \\hat A^0_t\\theta-\\hat r^0_t.\n",
    "$$\n",
    "Using linearity, the weighted least squares criterion has a unique closed form solution,\n",
    "$$\n",
    "\\hat \\theta^0 =\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat A^0_t \\right)^{-1}\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat r_t^0 \\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_array(q, x, psi_stack, nest_count):\n",
    "    ''' \n",
    "    This function calculates the G block matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        G: a dictionary  of T numpy arrays (J[t],K+G): a G matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack, nest_count) # Find the cross derivative of the pertubation function \\Omega wrt. lambda and ccp's q\n",
    "    G = {t: np.concatenate((x[t], -Z[t]), axis=1) for t in np.arange(T)} # Join block matrices along 2nd dimensions  s.t. last dimension is K+G (same dimension as theta)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_array(q):\n",
    "    '''\n",
    "    This function calculates the D matrix - the logit derivative of ccp's wrt. utilities\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "\n",
    "    Returns\n",
    "        D: a dictionary of T numpy arrays (J[t],J[t]) of logit derivatives of ccp's wrt. utilities for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = {t: np.diag(q[t]) - np.einsum('j,k->jk', q[t], q[t]) for t in np.arange(T)}\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_array(q, x, psi_stack, nest_count):\n",
    "    '''\n",
    "    This function calculates the A matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        A: a dictionary  of T numpy arrays (J[t],K+G): an A matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    D = D_array(q)\n",
    "    G = G_array(q, x, psi_stack, nest_count)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_array(q):\n",
    "    '''\n",
    "    This function calculates 'r'; the logarithm of observed or nonparametrically estimated market shares\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "    \n",
    "    Returns\n",
    "        r: a dictionary of T numpy arrays (J[t],) of the log of ccp's for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = D_array(q) \n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) for t in np.arange(T)}\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS_init(q, x, sample_share, psi_stack, nest_count, N):\n",
    "    ''' \n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the initial FKN parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "\n",
    "    #W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "    A = A_array(q, x, psi_stack, nest_count)\n",
    "    r = r_array(q)\n",
    "\n",
    "    d = A[0].shape[1]\n",
    "    \n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1/q[t], A[t], optimize = True) # Fast product using that the weights are diagonal.\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1/q[t], r[t], optimize = True)\n",
    "    \n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    #se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "    \n",
    "    return theta_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the observed market shares we may thus find initial parameter estimates $\\hat \\theta^0$ as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFKN0 = WLS_init(y, x, pop_share, Psi, Nest_count, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.038008441930889"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array([p for p in thetaFKN0[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for parameter bounds\n",
    "\n",
    "As we see above, the least squares estimator is not guaranteed to respect the parameter bounds $\\sum_g \\hat \\lambda_g<1$. We can use that if we replace $\\hat q^0_t$ with the choice probabilities from the maximum likelihood estimator of the logit model, $\\hat q^{logit}_t\\propto \\exp\\{X_t\\hat \\beta^{logit}\\}$, and plug these choice probabilities into the WLS estimator described above, it will return $\\hat \\theta=(\\hat \\beta^{logit},0,\\ldots,0)$ as the parameter estimate. Let $\\hat q_t(\\alpha)$ denote the weighted average of the logit probabilites and the market shares,\n",
    "$$\n",
    "\\hat q_t(\\alpha) =(1-\\alpha) \\hat q^{logit}_t+\\alpha \\hat q^0_t.\n",
    "$$\n",
    " Let $\\hat \\theta^0(\\alpha)$ denote the resulting parameter vector. We perform a line search for values of $\\alpha$, $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\ldots)$ until $\\hat \\theta^0(\\alpha)$ yields a feasible parameter vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogL(Theta, y, x, sample_share, psi_stack, nest_count):\n",
    "    ''' A function giving the mean IPDL loglikehood evaluated at data and an array of parameters 'Theta'\n",
    "    '''\n",
    "    return np.mean(IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack, nest_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, nest_count, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta0.shape[0]\n",
    "    K = x[0].shape[1]\n",
    "    G = d-K\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    #alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    alpha0=0.5\n",
    "    #LogL_alpha = np.empty((num_alpha,))\n",
    "    #theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in range(1,100):\n",
    "\n",
    "        alpha = alpha0**k\n",
    "\n",
    "      \n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha = WLS_init(q_alpha, x, sample_share, psi_stack, nest_count, N)[0]\n",
    "\n",
    "        lambda_alpha = theta_alpha[K:]\n",
    "        \n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() <1:\n",
    "            break\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, nest_count, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = len(Theta0)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    LogL_alpha = np.empty((num_alpha,))\n",
    "    theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in np.arange(len(alpha_line)):\n",
    "\n",
    "        alpha = alpha_line[k]\n",
    "\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha[k,:] = WLS_init(q_alpha, x, sample_share, psi_stack, nest_count, N)\n",
    "\n",
    "        lambda_alpha = theta_alpha[k,K:]\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() >= 1:\n",
    "            LogL_alpha[k] = np.NINF\n",
    "        else:\n",
    "            LogL_alpha[k] = LogL(theta_alpha[k,:], y, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "    alpha_star = np.argmax(LogL_alpha)\n",
    "    theta_hat_star = theta_alpha[alpha_star,:]\n",
    "\n",
    "    return theta_hat_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the grid search method we find corressponding parameters $\\hat \\theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.351103169530736e-16, 0.02722153305673005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9048287653890591, 0.009636926685372499]\n",
      "[0.9331100364359847, 0.005436328885898605]\n",
      "[0.9510042620683576, 0.002990924562664173]\n"
     ]
    }
   ],
   "source": [
    "theta_alpha = GridSearch(thetaFKN0, beta_0, y, x, pop_share, Psi, Nest_count, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9510042620683576"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array([p for p in theta_alpha[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated FKN estimator\n",
    "\n",
    "The iterated estimator is as the initial one, except there is an additional term on $\\hat \\varepsilon$. First, we update the choice probabilities,\n",
    "$$\n",
    "\\hat q^k_i=p(\\mathbf X_i,\\hat \\theta^{k-1})\\\\\n",
    "$$\n",
    "Then we assign\n",
    "$$\n",
    "\\hat D^k_i=\\nabla^2_{qq}\\Omega(\\hat q_i^k|\\hat \\lambda^{k-1})^{-1}-(\\hat q^k_i \\hat q^k_i)'\n",
    "$$\n",
    "and then construct the residual\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)=\\hat D^k_i\\left( u(x_i,\\beta)-\\nabla_q \\Omega(\\hat q_i^k|\\lambda)\\right) -y_i+\\hat q_i^k,\n",
    "$$\n",
    "Which can once again be simplified as\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)= \\hat A_i^k \\theta-\\hat r^k_i,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat A^k_i=\\hat D_i^k\\hat G^k_i, \\hat r_i^k =\\hat D^k_i\\ln \\hat q_i^k-y_i\n",
    "$$\n",
    "and where $\\hat G^k_i$ is constructed as in the initial estimator. Using the weighted least squares estimator with weights $\\hat W_i^k=\\textrm{diag}(\\hat q^k_i)^{-1}$, we get the estimator\n",
    "$$\n",
    "\\hat \\theta^k = \\arg \\min_{\\theta}\\frac{1}{n}\\sum_i \\hat \\varepsilon^k_i(\\theta)'\\hat W_i^k \\hat \\varepsilon^k_i(\\theta).\n",
    "$$\n",
    "We can once again solve it in closed form as\n",
    "$$\n",
    "\\hat \\theta^k =\\left( \\frac{1}{n}\\sum_i \\hat (A^k_i)'\\hat W_i^k \\hat A^k_i)\\right)^{-1}\\left( \\frac{1}{n}\\sum_i (\\hat A_i^k)'\\hat W_i^k \\hat r_i^k\\right)\n",
    "$$\n",
    "Now we implement this procedure and iterate starting from our initial guess $\\hat \\theta^{*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS(Theta, y, x, sample_share, psi_stack, nest_count, N):\n",
    "    '''\n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the iterated parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta.shape[0]\n",
    "    \n",
    "    # Get ccp's\n",
    "    q = IPDL_ccp(Theta, x, psi_stack, nest_count)\n",
    "\n",
    "    # Construct A\n",
    "    D = ccp_gradient(q, x, Theta, psi_stack, nest_count) # A is here constructed using the IPDL derivative of ccp's wrt. utilities instead of teh Logit derivative\n",
    "    G = G_array(q, x, psi_stack, nest_count)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "\n",
    "    # Construct r\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where=(q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) + y[t] for t in np.arange(T)}\n",
    "\n",
    "    # Estimate parameters\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1./q[t], A[t], optimize = True)\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1./q[t], r[t], optimize = True)\n",
    "\n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "\n",
    "    return theta_hat,se_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKN_estimator(logit_beta, q_obs, x, sample_share, psi_stack, nest_count, N, tol = 1.0e-15, max_iters = 1000):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    theta_init = WLS_init(q_obs, x, sample_share, psi_stack, nest_count, N) #WLS_init(q_obs, x, sample_share, psi_stack, nest_count,  N)\n",
    "    \n",
    "    if np.array([p for p in theta_init[K:] if p>0]).sum() >= 1:\n",
    "        theta_hat_star = GridSearch(theta_init, logit_beta, q_obs, x, sample_share, psi_stack, nest_count, N)\n",
    "        theta0 = theta_hat_star\n",
    "    else:\n",
    "        theta0 = theta_init\n",
    "\n",
    "    #logl0 = LogL(theta0, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    for k in np.arange(max_iters):\n",
    "        theta1, se1 = WLS(theta0, q_obs, x, sample_share, psi_stack, nest_count, N)\n",
    "\n",
    "        '''logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "        \n",
    "        for m in range(10):\n",
    "            if logl1<logl0:\n",
    "                theta1=0.5*theta0+0.5*theta1\n",
    "                logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "            else:\n",
    "                break'''\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.abs(theta1 - theta0))\n",
    "\n",
    "        if dist<tol:\n",
    "            succes = True\n",
    "            iter = k\n",
    "            break\n",
    "        elif k==max_iters:\n",
    "            succes = False\n",
    "            iter = max_iters\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        theta0 = theta1\n",
    "\n",
    "    res = {'theta': theta1,\n",
    "           'se': se1,\n",
    "           'fun': -LogL(theta1, y, x, sample_share, psi_stack, nest_count),\n",
    "           'iter': iter,\n",
    "           'succes': succes}\n",
    "    \n",
    "    return res \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.351103169530736e-16, 0.02722153305673005]\n",
      "[0.9048287653890591, 0.009636926685372499]\n",
      "[0.9331100364359847, 0.005436328885898605]\n",
      "[0.9510042620683576, 0.002990924562664173]\n",
      "[0.9263335667528774, 0.0014941817336802585]\n"
     ]
    }
   ],
   "source": [
    "res = FKN_estimator(beta_0, y, x, pop_share, Psi, Nest_count, N, tol=1.0e-8, max_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-10.0484***</td>\n",
       "      <td>0.00308</td>\n",
       "      <td>3266.600</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.9218***</td>\n",
       "      <td>0.00150</td>\n",
       "      <td>612.937</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-2.8092***</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>1455.524</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.0536***</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>39.014</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.5624***</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>1089.602</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.5314***</td>\n",
       "      <td>0.00299</td>\n",
       "      <td>1850.329</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-0.0604***</td>\n",
       "      <td>0.00179</td>\n",
       "      <td>33.676</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.8833***</td>\n",
       "      <td>0.00103</td>\n",
       "      <td>859.290</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>2.8631***</td>\n",
       "      <td>0.00202</td>\n",
       "      <td>1414.538</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.3022***</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>475.615</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.3285***</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>324.331</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-1.6152***</td>\n",
       "      <td>0.00357</td>\n",
       "      <td>451.927</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>0.1247***</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>268.432</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.6278***</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>997.982</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.3345***</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>752.529</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.2971***</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>650.402</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.7184***</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>524.172</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.5536***</td>\n",
       "      <td>0.00156</td>\n",
       "      <td>355.475</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7631***</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>800.315</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.2176***</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>410.921</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.0279***</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>62.687</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.4581***</td>\n",
       "      <td>0.00076</td>\n",
       "      <td>606.226</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-0.9733***</td>\n",
       "      <td>0.00132</td>\n",
       "      <td>738.765</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-1.6554***</td>\n",
       "      <td>0.00133</td>\n",
       "      <td>1241.480</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.7403***</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>675.572</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-0.7117***</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>1336.209</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.624***</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>868.573</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.2706***</td>\n",
       "      <td>0.00042</td>\n",
       "      <td>638.220</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-0.9988***</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>913.331</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.1378***</td>\n",
       "      <td>0.00061</td>\n",
       "      <td>226.123</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.0482***</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>111.969</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.0334***</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>73.917</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>0.102***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>215.753</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.3409***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>732.432</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.7656***</td>\n",
       "      <td>0.00120</td>\n",
       "      <td>639.770</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.6629***</td>\n",
       "      <td>0.00071</td>\n",
       "      <td>929.061</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.9641***</td>\n",
       "      <td>0.03498</td>\n",
       "      <td>84.728</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.7208***</td>\n",
       "      <td>0.00121</td>\n",
       "      <td>593.423</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.8435***</td>\n",
       "      <td>0.01335</td>\n",
       "      <td>212.978</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.4481***</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>585.819</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-0.9625***</td>\n",
       "      <td>0.00221</td>\n",
       "      <td>436.070</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.38***</td>\n",
       "      <td>0.00077</td>\n",
       "      <td>492.138</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-2.6995***</td>\n",
       "      <td>0.01131</td>\n",
       "      <td>238.604</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.6818***</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>1094.758</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.3467***</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>517.141</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1918***</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>308.472</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.6284***</td>\n",
       "      <td>0.00290</td>\n",
       "      <td>561.773</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.0783***</td>\n",
       "      <td>0.00043</td>\n",
       "      <td>2482.336</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>-0.0053***</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>41.631</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.0693***</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>346.504</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>-0.0125***</td>\n",
       "      <td>0.00031</td>\n",
       "      <td>39.977</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.0545***</td>\n",
       "      <td>0.00044</td>\n",
       "      <td>122.802</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.6457***</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>2726.033</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.0269***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>295.169</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>-0.0042***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>48.400</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>0.735</td>\n",
       "      <td>0.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0191***</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>200.720</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0205***</td>\n",
       "      <td>0.00008</td>\n",
       "      <td>245.343</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>0.0006***</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>7.850</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.0206***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>234.071</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0199***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>230.656</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0242***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>276.550</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.28***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>1230.063</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.2158***</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>986.270</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.0773***</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>633.540</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)      p\n",
       "in_out        -10.0484***  0.00308        3266.600  0.000\n",
       "cy             -0.9218***  0.00150         612.937  0.000\n",
       "hp             -2.8092***  0.00193        1455.524  0.000\n",
       "we             -0.0536***  0.00137          39.014  0.000\n",
       "le             -1.5624***  0.00143        1089.602  0.000\n",
       "wi              5.5314***  0.00299        1850.329  0.000\n",
       "he             -0.0604***  0.00179          33.676  0.000\n",
       "li             -0.8833***  0.00103         859.290  0.000\n",
       "sp              2.8631***  0.00202        1414.538  0.000\n",
       "ac              0.3022***  0.00064         475.615  0.000\n",
       "pr             -0.3285***  0.00101         324.331  0.000\n",
       "brand_2        -1.6152***  0.00357         451.927  0.000\n",
       "brand_3         0.1247***  0.00046         268.432  0.000\n",
       "brand_4        -0.6278***  0.00063         997.982  0.000\n",
       "brand_5        -0.3345***  0.00044         752.529  0.000\n",
       "brand_6        -0.2971***  0.00046         650.402  0.000\n",
       "brand_7        -0.7184***  0.00137         524.172  0.000\n",
       "brand_8        -0.5536***  0.00156         355.475  0.000\n",
       "brand_9        -1.7631***  0.00220         800.315  0.000\n",
       "brand_10        0.2176***  0.00053         410.921  0.000\n",
       "brand_11       -0.0279***  0.00044          62.687  0.000\n",
       "brand_12       -0.4581***  0.00076         606.226  0.000\n",
       "brand_13       -0.9733***  0.00132         738.765  0.000\n",
       "brand_14       -1.6554***  0.00133        1241.480  0.000\n",
       "brand_15       -1.7403***  0.00258         675.572  0.000\n",
       "brand_16       -0.7117***  0.00053        1336.209  0.000\n",
       "brand_17        -0.624***  0.00072         868.573  0.000\n",
       "brand_18        0.2706***  0.00042         638.220  0.000\n",
       "brand_19       -0.9988***  0.00109         913.331  0.000\n",
       "brand_20       -0.1378***  0.00061         226.123  0.000\n",
       "brand_21       -0.0482***  0.00043         111.969  0.000\n",
       "brand_22        0.0334***  0.00045          73.917  0.000\n",
       "brand_23         0.102***  0.00047         215.753  0.000\n",
       "brand_24       -0.3409***  0.00047         732.432  0.000\n",
       "brand_25       -0.7656***  0.00120         639.770  0.000\n",
       "brand_26       -0.6629***  0.00071         929.061  0.000\n",
       "brand_27       -2.9641***  0.03498          84.728  0.000\n",
       "brand_28       -0.7208***  0.00121         593.423  0.000\n",
       "brand_29       -2.8435***  0.01335         212.978  0.000\n",
       "brand_30       -1.4481***  0.00247         585.819  0.000\n",
       "brand_31       -0.9625***  0.00221         436.070  0.000\n",
       "brand_32         -0.38***  0.00077         492.138  0.000\n",
       "brand_33       -2.6995***  0.01131         238.604  0.000\n",
       "brand_34       -0.6818***  0.00062        1094.758  0.000\n",
       "brand_35       -0.3467***  0.00067         517.141  0.000\n",
       "brand_36       -0.1918***  0.00062         308.472  0.000\n",
       "brand_37       -1.6284***  0.00290         561.773  0.000\n",
       "home_2          1.0783***  0.00043        2482.336  0.000\n",
       "cla_2          -0.0053***  0.00013          41.631  0.000\n",
       "cla_3           0.0693***  0.00020         346.504  0.000\n",
       "cla_4          -0.0125***  0.00031          39.977  0.000\n",
       "cla_5           0.0545***  0.00044         122.802  0.000\n",
       "group_in_out    0.6457***  0.00024        2726.033  0.000\n",
       "group_cy       -0.0269***  0.00009         295.169  0.000\n",
       "group_hp       -0.0042***  0.00009          48.400  0.000\n",
       "group_we              0.0  0.00010           0.735  0.462\n",
       "group_le       -0.0191***  0.00010         200.720  0.000\n",
       "group_wi       -0.0205***  0.00008         245.343  0.000\n",
       "group_he        0.0006***  0.00007           7.850  0.000\n",
       "group_li       -0.0206***  0.00009         234.071  0.000\n",
       "group_sp       -0.0199***  0.00009         230.656  0.000\n",
       "group_ac       -0.0242***  0.00009         276.550  0.000\n",
       "group_brand       0.28***  0.00023        1230.063  0.000\n",
       "group_home     -0.2158***  0.00022         986.270  0.000\n",
       "group_cla      -0.0773***  0.00012         633.540  0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FKN_theta = res['theta']\n",
    "FKN_SE = res['se']\n",
    "FKN_t, FKN_p = IPDL_t_p(FKN_SE, FKN_theta, N)\n",
    "reg_table(FKN_theta, FKN_SE, N, x_vars, nest_vars)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
