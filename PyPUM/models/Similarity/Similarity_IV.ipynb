{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV estimation of the Similarity model\n",
    "\n",
    "This notebook introduces a BLP estimator of Similarity demand parameters. Since standard instruments often result in biased parameter estimates and higher standard errors, we also implement optimal instruments (See e.g. Reynaert & Verboven, 2014) in the Similarity Model setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/car_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\models\\Similarity\\Similarity_book.ipynb Cell 4\u001b[0m line \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m module_path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mpath:\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(module_path)\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mLogit_file\u001b[39;00m \u001b[39mimport\u001b[39;00m estimate_logit, logit_se, logit_t_p, q_logit, logit_score, logit_score_unweighted, logit_ccp, LogitBLP_estimator, LogitBLP_se\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mEurocarsdata_file\u001b[39;00m \u001b[39mimport\u001b[39;00m Eurocars_cleandata\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\utilities\\Logit_file.py:67\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39miter\u001b[39;00m\n",
      "\u001b[0;32m     21\u001b[0m \u001b[39m# %% [markdown]\u001b[39;00m\n",
      "\u001b[0;32m     22\u001b[0m \u001b[39m# Data\u001b[39;00m\n",
      "\u001b[0;32m     23\u001b[0m \u001b[39m# ====\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     65\u001b[0m \u001b[39m# %%\u001b[39;00m\n",
      "\u001b[0;32m     66\u001b[0m \u001b[39m# Load dataset and variable names\u001b[39;00m\n",
      "\u001b[1;32m---> 67\u001b[0m dat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m../data/car_data.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;32m     68\u001b[0m lab \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/car_labels.csv\u001b[39m\u001b[39m'\u001b[39m, index_col \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvariable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m     70\u001b[0m \u001b[39m# %%\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n",
      "\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n",
      "\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n",
      "\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n",
      "\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n",
      "\u001b[0;32m    310\u001b[0m     )\n",
      "\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n",
      "\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n",
      "\u001b[0;32m    666\u001b[0m     dialect,\n",
      "\u001b[0;32m    667\u001b[0m     delimiter,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n",
      "\u001b[0;32m    677\u001b[0m )\n",
      "\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n",
      "\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n",
      "\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n",
      "\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n",
      "\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n",
      "\u001b[0;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n",
      "\u001b[0;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n",
      "\u001b[0;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n",
      "\u001b[0;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n",
      "\u001b[1;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n",
      "\u001b[0;32m   1219\u001b[0m     f,\n",
      "\u001b[0;32m   1220\u001b[0m     mode,\n",
      "\u001b[0;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n",
      "\u001b[0;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n",
      "\u001b[0;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n",
      "\u001b[0;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n",
      "\u001b[0;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n",
      "\u001b[0;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n",
      "\u001b[0;32m   1227\u001b[0m )\n",
      "\u001b[0;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n",
      "\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n",
      "\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n",
      "\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n",
      "\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n",
      "\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n",
      "\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n",
      "\u001b[0;32m    787\u001b[0m             handle,\n",
      "\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n",
      "\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n",
      "\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n",
      "\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m    792\u001b[0m         )\n",
      "\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n",
      "\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/car_data.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "import sys\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Files\n",
    "module_path = os.path.abspath(os.path.join('....'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utilities.Logit_file import estimate_logit, logit_se, logit_t_p, q_logit, logit_score, logit_score_unweighted, logit_ccp, LogitBLP_estimator, LogitBLP_se\n",
    "from data.Eurocarsdata_file import Eurocars_cleandata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/eurocars.dta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\models\\Similarity\\Similarity_book.ipynb Cell 5\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load dataset and variable names\u001b[39;00m\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m descr \u001b[39m=\u001b[39m (pd\u001b[39m.\u001b[39;49mread_stata(\u001b[39m'\u001b[39;49m\u001b[39m../data/eurocars.dta\u001b[39;49m\u001b[39m'\u001b[39;49m, iterator \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m))\u001b[39m.\u001b[39mvariable_labels() \u001b[39m# Obtain variable descriptions\u001b[39;00m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dat_file \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/eurocars.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\stata.py:2004\u001b[0m, in \u001b[0;36mread_stata\u001b[1;34m(filepath_or_buffer, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals, chunksize, iterator, compression, storage_options)\u001b[0m\n",
      "\u001b[0;32m   1988\u001b[0m \u001b[39m@Appender\u001b[39m(_read_stata_doc)\n",
      "\u001b[0;32m   1989\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_stata\u001b[39m(\n",
      "\u001b[0;32m   1990\u001b[0m     filepath_or_buffer: FilePath \u001b[39m|\u001b[39m ReadBuffer[\u001b[39mbytes\u001b[39m],\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   2001\u001b[0m     storage_options: StorageOptions \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n",
      "\u001b[0;32m   2002\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m StataReader:\n",
      "\u001b[1;32m-> 2004\u001b[0m     reader \u001b[39m=\u001b[39m StataReader(\n",
      "\u001b[0;32m   2005\u001b[0m         filepath_or_buffer,\n",
      "\u001b[0;32m   2006\u001b[0m         convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n",
      "\u001b[0;32m   2007\u001b[0m         convert_categoricals\u001b[39m=\u001b[39;49mconvert_categoricals,\n",
      "\u001b[0;32m   2008\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n",
      "\u001b[0;32m   2009\u001b[0m         convert_missing\u001b[39m=\u001b[39;49mconvert_missing,\n",
      "\u001b[0;32m   2010\u001b[0m         preserve_dtypes\u001b[39m=\u001b[39;49mpreserve_dtypes,\n",
      "\u001b[0;32m   2011\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n",
      "\u001b[0;32m   2012\u001b[0m         order_categoricals\u001b[39m=\u001b[39;49morder_categoricals,\n",
      "\u001b[0;32m   2013\u001b[0m         chunksize\u001b[39m=\u001b[39;49mchunksize,\n",
      "\u001b[0;32m   2014\u001b[0m         storage_options\u001b[39m=\u001b[39;49mstorage_options,\n",
      "\u001b[0;32m   2015\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n",
      "\u001b[0;32m   2016\u001b[0m     )\n",
      "\u001b[0;32m   2018\u001b[0m     \u001b[39mif\u001b[39;00m iterator \u001b[39mor\u001b[39;00m chunksize:\n",
      "\u001b[0;32m   2019\u001b[0m         \u001b[39mreturn\u001b[39;00m reader\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\stata.py:1151\u001b[0m, in \u001b[0;36mStataReader.__init__\u001b[1;34m(self, path_or_buf, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals, chunksize, compression, storage_options)\u001b[0m\n",
      "\u001b[0;32m   1148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lines_read \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;32m   1150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_native_byteorder \u001b[39m=\u001b[39m _set_endianness(sys\u001b[39m.\u001b[39mbyteorder)\n",
      "\u001b[1;32m-> 1151\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n",
      "\u001b[0;32m   1152\u001b[0m     path_or_buf,\n",
      "\u001b[0;32m   1153\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m   1154\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n",
      "\u001b[0;32m   1155\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m   1156\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n",
      "\u001b[0;32m   1157\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n",
      "\u001b[0;32m   1158\u001b[0m     \u001b[39m# Copy to BytesIO, and ensure no encoding\u001b[39;00m\n",
      "\u001b[0;32m   1159\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_or_buf \u001b[39m=\u001b[39m BytesIO(handles\u001b[39m.\u001b[39mhandle\u001b[39m.\u001b[39mread())\n",
      "\u001b[0;32m   1161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_header()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:795\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n",
      "\u001b[0;32m    786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n",
      "\u001b[0;32m    787\u001b[0m             handle,\n",
      "\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[0;32m    792\u001b[0m         )\n",
      "\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n",
      "\u001b[1;32m--> 795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n",
      "\u001b[0;32m    796\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n",
      "\u001b[0;32m    798\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/eurocars.dta'"
     ]
    }
   ],
   "source": [
    "# Load dataset and variable names\n",
    "descr = (pd.read_stata('../data/eurocars.dta', iterator = True)).variable_labels() # Obtain variable descriptions\n",
    "dat_file = pd.read_csv('../data/eurocars.csv') # reads in the data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              cy            cylinder volume or displacement (in cc)\n",
       "1              hp                                 horsepower (in kW)\n",
       "2              we                                     weight (in kg)\n",
       "3              le                                     length (in cm)\n",
       "4              wi                                      width (in cm)\n",
       "5              he                                     height (in cm)\n",
       "6              li          average of li1, li2, li3 (used in papers)\n",
       "7              sp                            maximum speed (km/hour)\n",
       "8              ac  time to acceleration (in seconds from 0 to 100...\n",
       "9              pr   price (in destination currency including V.A.T.)\n",
       "10          brand                                      name of brand\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            cla                              class or segment code"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Outside option is included if OO == True, otherwise analysis is done on the inside options only.\n",
    "OO = True\n",
    "\n",
    "# Choose which variables to include in the analysis, and assign them either as discrete variables or continuous.\n",
    "\n",
    "x_discretevars = [ 'brand', 'home', 'cla']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "z_IV_contvars = ['xexr']\n",
    "z_IV_discretevars = []\n",
    "x_allvars =  [*x_contvars, *x_discretevars]\n",
    "z_allvars = [*z_IV_contvars, *z_IV_discretevars]\n",
    "\n",
    "if OO:\n",
    "    nest_contvars = [var for var in x_contvars if var != 'pr'] # We nest over all variables other than price, but an alternative list can be specified here if desired.\n",
    "    nest_discvars = ['in_out', *x_discretevars]\n",
    "    nest_vars = ['in_out', *nest_contvars, *x_discretevars]\n",
    "else:\n",
    "    nest_contvars = [var for var in x_contvars if (var != 'pr')]\n",
    "    nest_discvars = x_discretevars # See above\n",
    "    nest_vars = [*nest_contvars, *nest_discvars]\n",
    "\n",
    "G = len(nest_vars)\n",
    "\n",
    "# Print list of chosen variables as a dataframe\n",
    "pd.DataFrame(descr, index=['description'])[x_allvars].transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, dat_org, x_vars, z_vars, N, pop_share, T, J, K = Eurocars_cleandata(dat_file, x_contvars, x_discretevars, z_IV_contvars, z_IV_discretevars, outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of numpy arrays for each market. This allows the size of the data set to vary over markets.\n",
    "\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)} # Dict of explanatory variables\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)} # Dict of market shares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP Estimation and instruments\n",
    "\n",
    "The setting is now a bit different. Instead of the noise coming from random sampling of individuals, we now have an additional source of uncertainty, stemming frm the random sampling of the fixed effects $\\xi_{tj}$ for each market and each product. The number of ”observations” is therefore\n",
    "\n",
    "$$\n",
    "S = T \\cdot \\sum_t J_t\n",
    "$$\n",
    "\n",
    "Note that while random sampling of individuals choices (number of observations\n",
    "in the hundreds of millions) still has an effect on the estimated parameters in\n",
    "principle, this effect is completely drowned out by the sampling variance of the\n",
    "fixed effects (number of observations $S \\approx 150^2 \\cdot 50$), so we choose to ignore it\n",
    "here. When estimating random coefficients Models, there is also a third source\n",
    "of uncertainty stemming from approximation of numerical integrals. This is not\n",
    "an issue in Similarity, as we have the inverse demand in closed form.\n",
    "\n",
    "The principles are pretty similar to what we have been doing already. When\n",
    "applicable, we will use the same notation as in the FKN section. Define the\n",
    "residual,\n",
    "\n",
    "$$\\xi_t(\\theta) = u(X_t, \\beta) − \\nabla_q \\Omega(q_t^0|\\lambda)$$\n",
    "\n",
    "In the Similarity Model, this residual is a linear function of $\\theta$ which has the form\n",
    "\n",
    "$$\\xi_t(\\theta) =  G^0_t \\theta − r_t^0$$\n",
    "\n",
    "where $ G^0_t=[X_t, -\\nabla_{q,\\lambda}\\Omega(q_t^0|\\lambda)]$ and $r^0_t = \\ln q^0_t$ as in the FKN section with $q^0_t$ being e.g. the observed market shares in market $t = 1, \\ldots, T$. For the BLP estimator, we set this residual orthogonal to a matrix of instruments $ Z_t$ of size $J_t \\times (K+G)$, and find the estimator $ \\hat \\theta^{IV}$ which solves the moment conditions\n",
    "\n",
    "$$\\frac{1}{T} \\sum_t  Z_t' \\xi(\\hat \\theta^{IV}) = 0$$\n",
    "\n",
    "Since $\\hat \\xi$ is linear, the moment equations have a unique solution,\n",
    "\n",
    "$$\\hat \\theta^{IV} = \\left(\\frac{1}{T}\\sum_t  Z_t' G^0_t \\right)^{-1}\\left(\\frac{1}{T}\\sum_t  Z_t' r^0_t \\right)$$\n",
    "\n",
    "We require an instrument for the price of the goods. This is something which is correlated with the price, but uncorrelated with the error term $\\xi_t$ (in the BLP Model, $\\xi_{tj}$ represents unobserved components of car quality). A standard instrument in this case would be a measure of marginal cost (or something which is correlated with marginal cost, like a production price index). For everything other than price, we can simply use the regressor itself as the instrument i.e. $  Z^{tjd} = G^0_{tjd}$, for all other dimensions than price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct our instruments $ Z$. We'll use the average exchange rate of the destination country relative to average exchange rate of the origin country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = T*np.sum(np.array([x[t].shape[0] for t in np.arange(T)]))\n",
    "\n",
    "xexr = {t: dat[dat['market'] == t][z_vars[0]].values for t in np.arange(T)}\n",
    "G0 = G_array(y, x, Model)\n",
    "pr_index = len(x_contvars)\n",
    "for t in np.arange(T):\n",
    "    G0[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z = G0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the moment estimator $\\hat \\theta^{IV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_estimator(q_obs, z, x, sample_share, model):\n",
    "    '''\n",
    "    Args.\n",
    "        q_obs: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        z: a dictionary of T numpy arrays (J[t],K+G) of instruments for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a numpy array (K+G,) of BLP parameter estimates\n",
    "    '''\n",
    "    T = len(z)\n",
    "\n",
    "    G = G_array(q_obs, x, model)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(q_obs[t], out = np.NINF*np.ones_like((q_obs[t])), where = (q_obs[t] > 0)) for t in np.arange(T)}\n",
    "    \n",
    "    sZG = np.empty((T,d,d))\n",
    "    sZr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sZG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', z[t], G[t])\n",
    "        sZr[t,:] = sample_share[t]*np.einsum('jd,j->d', z[t], r[t])\n",
    "\n",
    "    theta_hat = la.solve(sZG.sum(axis=0), sZr.sum(axis=0))\n",
    "    \n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLP_theta = BLP_estimator(y, z, x, np.ones((T,)), Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Logit Model we get the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_logit = x\n",
    "for t in np.arange(T):\n",
    "    G_logit[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z_logit = G_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.92920752,  -2.3589754 ,  -6.76421995,   0.02963003,\n",
       "        -2.05176127,  10.84731336,  -1.04140126,  -0.58331478,\n",
       "         5.15289118,   0.51808091,  -0.17336342,  -2.037768  ,\n",
       "        -0.81720168,  -1.44357757,  -1.04059281,  -1.16245013,\n",
       "        -1.74530433,  -0.85123531,  -2.72300281,  -1.08758839,\n",
       "        -0.68958989,  -0.95909482,  -2.11727698,  -2.93039275,\n",
       "        -2.90655875,  -2.05527142,  -1.82107985,   0.51974428,\n",
       "        -2.02980519,  -0.79701277,  -0.86356478,  -0.86816254,\n",
       "        -0.81661044,  -1.48858878,  -0.9378501 ,  -1.87621854,\n",
       "        -3.7657435 ,  -1.52567201,  -3.14936663,  -2.07998398,\n",
       "        -1.85954898,  -0.7631942 ,  -1.94891051,  -1.60837966,\n",
       "        -1.15784827,  -0.48973547,  -2.57588437,   1.56903974,\n",
       "         0.0275757 ,   0.04982496,  -0.30342661,  -0.3829885 ])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "LogitBLP_beta = LogitBLP_estimator(y, z_logit, x, np.ones((T,)))\n",
    "LogitBLP_SE = LogitBLP_se(LogitBLP_beta, y, z_logit, x)\n",
    "LogitBLP_t,LogitBLP_p = logit_t_p(LogitBLP_beta, logit_score_unweighted(LogitBLP_beta, y, x), np.ones((T,)), S)\n",
    "LogitBLP_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLP approximation to optimal instruments\n",
    "\n",
    "BLP propose an algorithm for constructing an approximation to the optimal instruments. It is described in simple terms in Reynaert & Verboven (2014), and it has the following steps.\n",
    "It requires a consistent initial parameter estimate $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'$; here we can just use the MLE or the FKN estimates we have already computed. Let $Z_t$ denote the matrix of instruments (this is the matrix $X_t$ with the price replaced by the exchange rate). The steps are then as follows:\n",
    "\n",
    "First we form the regression equation of the covariates on the instruments:\n",
    "$$\n",
    "X_t = Z_t \\Pi + \\Epsilon_t\n",
    "$$\n",
    "\n",
    "The OLS estimate is then given as:\n",
    "$$\n",
    "\\hat \\Pi = \\left( \\frac{1}{T}\\sum_t Z_t' Z_t \\right)^{-1}\\left( \\frac{1}{T}\\sum_t Z_t' X_t\\right)\n",
    "$$\n",
    "\n",
    "Thus the predicted covariates given the instruments $W$ are:\n",
    "$$\n",
    "\\hat X_t = Z_t \\hat \\Pi\n",
    "$$\n",
    "\n",
    "Having constructed $\\hat X_t$ (which consists of the exogenous regressors, and the predicted price given $Z_t$), we compute the predicted mean utility:\n",
    "\n",
    "$$\n",
    "\\hat u_t = \\hat X_t \\hat \\beta\n",
    "$$\n",
    "\n",
    "and then the predicted market shares at the mean utility:\n",
    "\n",
    "$$\n",
    "\\hat q_t^{*} = P(\\hat u_t | \\hat \\lambda)\n",
    "$$\n",
    "\n",
    "Computationally, here we just use $\\hat X_t$ in place of $X_t$ in the CCP function.\n",
    "Given the predicted market shares, we compute\n",
    "\n",
    "$$\n",
    "\\hat G_t^{*} = \\left[\\hat X_t, -\\nabla_{q,\\lambda} \\Omega (\\hat q_t^{*} | \\hat \\lambda)\\right]\n",
    "$$\n",
    "\n",
    "which is the same as the function $\\hat G_t^0$ we already have constructed, except we evaluate it at the\n",
    "predictions $\\hat X_t$ and $\\hat q_t^{*}$ instead of at $X_t$ and $\\hat q_t^0$.\n",
    "\n",
    "The procedure above gives an approximation to the optimal instruments. We also require a weight matrix. The optimal weight matrix is the (generalized) inverse of the conditional (on the instruments) covariance of the fixed effects. Assuming $\\xi_{tj}$ is independently and identically distributed over markets t and products j, the conditional covariance simplifies to a scalar $\\sigma^2$ times an identity matrix (of size $J_t$).\n",
    "This means that all fixed effects are weighted equally, and the weights therefore drop out of the IV regression. The optimal IV estimator is therefore\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} = \\left(\\frac{1}{T}\\sum_t (\\hat G_t^*)'\\hat G_t^0\\right)^{-1}\\left( \\frac{1}{T}\\sum_t (\\hat G_t^*)'\\hat r_t^0 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat \\xi^*$ denote the estimated residual evaluated at the new parameter estimates,\n",
    "\n",
    "$$\n",
    "\\hat \\xi_{tj}^* = \\hat \\xi_{tj}(\\hat \\theta^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "We may estimate the constant $\\sigma^2$ by\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{S}\\sum_{t}\\sum_{j = 1}^{J_t} \\left(\\hat \\xi_{tj}^*\\right)^2 \n",
    "$$\n",
    "\n",
    "The distribution of the estimator $\\hat \\theta^{\\text{IV}}$ is then\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} \\sim \\mathcal{N}(\\theta_0, \\Sigma^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "which can be consistently estimated by\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma^{\\text{IV}} = \\hat \\sigma^2 \\left( \\sum_t (\\hat G_t^*)'\\hat G_t^0 \\right)^{-1}\n",
    "$$\n",
    "\n",
    "and the standard errors are then the square root of the diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_x(x, w, sample_share):\n",
    "    ''' \n",
    "    This function computes the predicted covariates from a regression on the instruments\n",
    "\n",
    "    Args:\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        w: a dictionary of T numpy arrays (J[t],K) of instruments for each covariate for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "\n",
    "    Returns.\n",
    "        X_hat: a dictionary of T numpy arrays (J[t],K) of predicted covariates for each market t\n",
    "    '''\n",
    "    \n",
    "    T = len(w)\n",
    "    K = w[0].shape[1]\n",
    "\n",
    "    sWW = np.empty((T,K,K))\n",
    "    sWX = np.empty((T,K,K))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sWW[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], w[t])\n",
    "        sWX[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], x[t])\n",
    "\n",
    "    Pi_hat = la.solve(sWW.sum(axis=0), sWX.sum(axis=0))\n",
    "    X_hat = {t: np.einsum('jl,lk->jk', w[t], Pi_hat) for t in np.arange(T)}\n",
    "\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_se(Theta, y, x, model):\n",
    "    '''\n",
    "    This function computes BLP standard errors which are consistent when using optimal instruments\n",
    "\n",
    "    Args:\n",
    "        Theta: a numpy array (K+G,) of BLP estimated \n",
    "        y: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns.\n",
    "        SE: a numpy array (K+G,) of estimated BLP standard errors using optimal instruments\n",
    "    '''\n",
    "    T = len(x)\n",
    "    S = T * np.array([x[t].shape[0] for t in np.arange(T)]).sum()\n",
    "\n",
    "    G = G_array(y, x, model)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t]) for t in np.arange(T)}\n",
    "    \n",
    "    # We calculate \\sigma^2\n",
    "    xi = {t: np.einsum('jd,d->j', G[t], Theta) - r[t] for t in np.arange(T)}\n",
    "    sum_xij2 = np.empty((T,))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sum_xij2[t] = (xi[t]**2).sum()\n",
    "    \n",
    "    sigma2 = np.sum(sum_xij2) / S\n",
    "\n",
    "    # We calculate GG for each market t\n",
    "    GG = np.empty((T,d,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        GG[t,:,:] = np.einsum('jd,jp->dp', G[t], G[t])\n",
    "\n",
    "    # Finally we compute \\Sigma and the standard errors\n",
    "    Sigma = sigma2*la.inv(GG.sum(axis=0))\n",
    "    SE = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalBLP_estimator(Theta0, q_obs, w, x, sample_share, model):\n",
    "    '''\n",
    "    This function estimates the Similarity demand model using optimal instruments in the BLP setting\n",
    "    \n",
    "    Args:\n",
    "        Theta0: a numpy array (K+G,) of consistent parameter estimates from estimation using the covariates ('first-stage parameters')\n",
    "        q_obs: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        w: a dictionary of T numpy arrays (J[t],K+G) of instruments for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: a (T,) numpy array of the fraction of observations in each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns.\n",
    "        Theta_IV: a numpy array (K+G,) of BLP parameter estimates in the Similarity Model using optimal instruments\n",
    "        SE_IV: a numpy array (K+G,) of estimated BLP standard errors using optimal instruments\n",
    "    '''\n",
    "    \n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "    \n",
    "    X_hat = predict_x(x, w, sample_share)\n",
    "    q0 = Similarity_ccp(Theta0, X_hat, model)\n",
    "    G_star = G_array(q0, X_hat, model)\n",
    "    G0 = G_array(q_obs, x, model)\n",
    "    \n",
    "    r = {t: np.log(q_obs[t]) for t in np.arange(T)}\n",
    "\n",
    "    d = G0[0].shape[1]\n",
    "\n",
    "    sGG = np.empty((T,d,d))\n",
    "    sGr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sGG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', G_star[t], G0[t])\n",
    "        sGr[t,:] = sample_share[t]*np.einsum('jd,j->d', G_star[t], r[t])\n",
    "\n",
    "    Theta_IV = la.solve(sGG.sum(axis=0), sGr.sum(axis=0))\n",
    "    SE_IV = BLP_se(Theta_IV, q_obs, x, model)\n",
    "\n",
    "    return Theta_IV, SE_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThetaOptBLP, SEOptBLP = OptimalBLP_estimator(FKN_theta, y, z_logit, x, np.ones((T,)), Model)\n",
    "OptBLP_t, OptBLP_p = Similarity_t_p(SEOptBLP, ThetaOptBLP, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.259433210474408"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array([p for p in ThetaOptBLP[K:]  if p > 0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-11.7985***</td>\n",
       "      <td>0.03458</td>\n",
       "      <td>341.164</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.7545***</td>\n",
       "      <td>0.02002</td>\n",
       "      <td>37.691</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-5.587***</td>\n",
       "      <td>0.02601</td>\n",
       "      <td>214.837</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.2574***</td>\n",
       "      <td>0.02089</td>\n",
       "      <td>12.326</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.4046***</td>\n",
       "      <td>0.02323</td>\n",
       "      <td>103.500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.9706***</td>\n",
       "      <td>0.03394</td>\n",
       "      <td>175.927</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.8705***</td>\n",
       "      <td>0.02738</td>\n",
       "      <td>31.794</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.77***</td>\n",
       "      <td>0.01218</td>\n",
       "      <td>63.199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>5.02***</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>188.901</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>1.1385***</td>\n",
       "      <td>0.01299</td>\n",
       "      <td>87.662</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.1493***</td>\n",
       "      <td>0.00237</td>\n",
       "      <td>63.042</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.8311***</td>\n",
       "      <td>0.03193</td>\n",
       "      <td>26.030</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>-0.1506***</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>33.578</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.9499***</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>197.945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.4631***</td>\n",
       "      <td>0.00446</td>\n",
       "      <td>103.938</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.3605***</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>82.317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.9956***</td>\n",
       "      <td>0.00762</td>\n",
       "      <td>130.612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.3995***</td>\n",
       "      <td>0.01094</td>\n",
       "      <td>36.520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7608***</td>\n",
       "      <td>0.00720</td>\n",
       "      <td>244.600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>-0.2598***</td>\n",
       "      <td>0.00429</td>\n",
       "      <td>60.520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.0442***</td>\n",
       "      <td>0.00439</td>\n",
       "      <td>10.061</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.49***</td>\n",
       "      <td>0.00527</td>\n",
       "      <td>92.979</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-1.2869***</td>\n",
       "      <td>0.00660</td>\n",
       "      <td>194.956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-2.1215***</td>\n",
       "      <td>0.00919</td>\n",
       "      <td>230.955</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.9771***</td>\n",
       "      <td>0.00876</td>\n",
       "      <td>225.685</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-1.2687***</td>\n",
       "      <td>0.00426</td>\n",
       "      <td>297.597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.9739***</td>\n",
       "      <td>0.00508</td>\n",
       "      <td>191.567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.4086***</td>\n",
       "      <td>0.00471</td>\n",
       "      <td>86.702</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-1.2959***</td>\n",
       "      <td>0.00586</td>\n",
       "      <td>221.279</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.2857***</td>\n",
       "      <td>0.00533</td>\n",
       "      <td>53.653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.252***</td>\n",
       "      <td>0.00413</td>\n",
       "      <td>61.072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>-0.2039***</td>\n",
       "      <td>0.00422</td>\n",
       "      <td>48.275</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>-0.1584***</td>\n",
       "      <td>0.00458</td>\n",
       "      <td>34.615</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.6969***</td>\n",
       "      <td>0.00431</td>\n",
       "      <td>161.524</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.8286***</td>\n",
       "      <td>0.00512</td>\n",
       "      <td>161.965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-1.053***</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>206.553</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.4134***</td>\n",
       "      <td>0.05259</td>\n",
       "      <td>45.894</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-1.0134***</td>\n",
       "      <td>0.00865</td>\n",
       "      <td>117.111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.001***</td>\n",
       "      <td>0.01765</td>\n",
       "      <td>113.381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.2162***</td>\n",
       "      <td>0.00894</td>\n",
       "      <td>136.105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-1.59***</td>\n",
       "      <td>0.01807</td>\n",
       "      <td>88.011</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.4178***</td>\n",
       "      <td>0.00812</td>\n",
       "      <td>51.450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-1.689***</td>\n",
       "      <td>0.03057</td>\n",
       "      <td>55.247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.8166***</td>\n",
       "      <td>0.00547</td>\n",
       "      <td>149.207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.5157***</td>\n",
       "      <td>0.00514</td>\n",
       "      <td>100.390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.148***</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>32.947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.845***</td>\n",
       "      <td>0.01093</td>\n",
       "      <td>168.873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.0623***</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>549.310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.0278***</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>11.317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.133***</td>\n",
       "      <td>0.00330</td>\n",
       "      <td>40.263</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.1386***</td>\n",
       "      <td>0.00461</td>\n",
       "      <td>30.081</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.1537***</td>\n",
       "      <td>0.00585</td>\n",
       "      <td>26.258</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.8427***</td>\n",
       "      <td>0.00209</td>\n",
       "      <td>404.174</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.0928***</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>64.999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.2064***</td>\n",
       "      <td>0.00219</td>\n",
       "      <td>94.099</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>-0.0069***</td>\n",
       "      <td>0.00189</td>\n",
       "      <td>3.647</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0488***</td>\n",
       "      <td>0.00172</td>\n",
       "      <td>28.375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0669***</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>66.415</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0613***</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>90.549</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.0221***</td>\n",
       "      <td>0.00092</td>\n",
       "      <td>24.009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0571***</td>\n",
       "      <td>0.00163</td>\n",
       "      <td>35.132</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0402***</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>37.025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.1882***</td>\n",
       "      <td>0.00097</td>\n",
       "      <td>193.976</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.3915***</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>329.078</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.0971***</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>68.498</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)    p\n",
       "in_out        -11.7985***  0.03458         341.164  0.0\n",
       "cy             -0.7545***  0.02002          37.691  0.0\n",
       "hp              -5.587***  0.02601         214.837  0.0\n",
       "we              0.2574***  0.02089          12.326  0.0\n",
       "le             -2.4046***  0.02323         103.500  0.0\n",
       "wi              5.9706***  0.03394         175.927  0.0\n",
       "he              0.8705***  0.02738          31.794  0.0\n",
       "li               -0.77***  0.01218          63.199  0.0\n",
       "sp                5.02***  0.02657         188.901  0.0\n",
       "ac              1.1385***  0.01299          87.662  0.0\n",
       "pr             -0.1493***  0.00237          63.042  0.0\n",
       "brand_2        -0.8311***  0.03193          26.030  0.0\n",
       "brand_3        -0.1506***  0.00449          33.578  0.0\n",
       "brand_4        -0.9499***  0.00480         197.945  0.0\n",
       "brand_5        -0.4631***  0.00446         103.938  0.0\n",
       "brand_6        -0.3605***  0.00438          82.317  0.0\n",
       "brand_7        -0.9956***  0.00762         130.612  0.0\n",
       "brand_8        -0.3995***  0.01094          36.520  0.0\n",
       "brand_9        -1.7608***  0.00720         244.600  0.0\n",
       "brand_10       -0.2598***  0.00429          60.520  0.0\n",
       "brand_11       -0.0442***  0.00439          10.061  0.0\n",
       "brand_12         -0.49***  0.00527          92.979  0.0\n",
       "brand_13       -1.2869***  0.00660         194.956  0.0\n",
       "brand_14       -2.1215***  0.00919         230.955  0.0\n",
       "brand_15       -1.9771***  0.00876         225.685  0.0\n",
       "brand_16       -1.2687***  0.00426         297.597  0.0\n",
       "brand_17       -0.9739***  0.00508         191.567  0.0\n",
       "brand_18        0.4086***  0.00471          86.702  0.0\n",
       "brand_19       -1.2959***  0.00586         221.279  0.0\n",
       "brand_20       -0.2857***  0.00533          53.653  0.0\n",
       "brand_21        -0.252***  0.00413          61.072  0.0\n",
       "brand_22       -0.2039***  0.00422          48.275  0.0\n",
       "brand_23       -0.1584***  0.00458          34.615  0.0\n",
       "brand_24       -0.6969***  0.00431         161.524  0.0\n",
       "brand_25       -0.8286***  0.00512         161.965  0.0\n",
       "brand_26        -1.053***  0.00510         206.553  0.0\n",
       "brand_27       -2.4134***  0.05259          45.894  0.0\n",
       "brand_28       -1.0134***  0.00865         117.111  0.0\n",
       "brand_29        -2.001***  0.01765         113.381  0.0\n",
       "brand_30       -1.2162***  0.00894         136.105  0.0\n",
       "brand_31         -1.59***  0.01807          88.011  0.0\n",
       "brand_32       -0.4178***  0.00812          51.450  0.0\n",
       "brand_33        -1.689***  0.03057          55.247  0.0\n",
       "brand_34       -0.8166***  0.00547         149.207  0.0\n",
       "brand_35       -0.5157***  0.00514         100.390  0.0\n",
       "brand_36        -0.148***  0.00449          32.947  0.0\n",
       "brand_37        -1.845***  0.01093         168.873  0.0\n",
       "home_2          1.0623***  0.00193         549.310  0.0\n",
       "cla_2           0.0278***  0.00245          11.317  0.0\n",
       "cla_3            0.133***  0.00330          40.263  0.0\n",
       "cla_4           0.1386***  0.00461          30.081  0.0\n",
       "cla_5           0.1537***  0.00585          26.258  0.0\n",
       "group_in_out    0.8427***  0.00209         404.174  0.0\n",
       "group_cy       -0.0928***  0.00143          64.999  0.0\n",
       "group_hp        0.2064***  0.00219          94.099  0.0\n",
       "group_we       -0.0069***  0.00189           3.647  0.0\n",
       "group_le       -0.0488***  0.00172          28.375  0.0\n",
       "group_wi       -0.0669***  0.00101          66.415  0.0\n",
       "group_he       -0.0613***  0.00068          90.549  0.0\n",
       "group_li        0.0221***  0.00092          24.009  0.0\n",
       "group_sp       -0.0571***  0.00163          35.132  0.0\n",
       "group_ac       -0.0402***  0.00109          37.025  0.0\n",
       "group_brand     0.1882***  0.00097         193.976  0.0\n",
       "group_home     -0.3915***  0.00119         329.078  0.0\n",
       "group_cla      -0.0971***  0.00142          68.498  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "reg_table(ThetaOptBLP, SEOptBLP, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qOpt = Similarity_ccp(ThetaOptBLP, z_logit, Model)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
