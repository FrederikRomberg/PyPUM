{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Demand for Cars with the Similarity Model\n",
    "\n",
    "In this notebook, we will introduce and estimate the Similarity Model of Fosgerau & Nielsen (2023) using publically available data on the European car market from Frank Verboven's website at https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market. We begin by introducing the data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "The dataset consists of approximately 110 vehicle makes per year in the period 1970-1999 in five European markets (Belgium, France, Germany, Italy, and the United Kingdom). The data set includes 47 variables in total. The first four columns are market and product codes for the year, country, and make as well as quantity sold (No. of new registrations) which will be used in computing observed market shares. The remaining variables consist of car characteristics such as prices, horse power, weight and other physical car characteristics as well as macroeconomic variables such as GDP per capita which have been used to construct estimates of the average wage income and purchasing power.\n",
    "\n",
    "We have in total 30 years and 5 countries, totalling $T=150$ year-country combinations, indexed by $t$, and we refer to each simply as market $t$. In market $t$, the choice set is $\\mathcal{J}_t$ which includes the set of available makes as well as an outside option. Let $\\mathcal{J} := \\bigcup_{t=1}^T \\mathcal{J}_t$ be the full choice set and \n",
    " $J:=\\#\\mathcal{J}$ the number of choices which were available in at least one market, for this data set there are $J=357$ choices.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the dataset `eurocars.csv` we thus have a dataframe of $\\sum_{t=1}^T \\#\\mathcal{J}_t = 11459$ rows and $47$ columns. The `ye` column runs through $y=70,\\ldots,99$, the `ma` column runs through $m=1,\\ldots,M$, and the ``co`` column takes values $j\\in \\mathcal{J}$. \n",
    "\n",
    "Because we consider a country-year pair as the level of observation, we construct a `market` column taking values $t=1,\\ldots,T$. In Python, this variable will take values $t=0,\\ldots,T-1$. We construct an outside option $j=0$ in each market $t$ by letting the 'sales' of $j=0$ be determined as \n",
    "\n",
    "$$\\mathrm{sales}_{0t} = \\mathrm{pop}_t - \\sum_{j=1}^J \\mathrm{sales}_{jt}$$\n",
    "\n",
    "where $\\mathrm{pop}_t$ is the total population in market $t$, and the car characteristics of the outside option is set to zero. The market shares of each product in market $t$ can then be found as\n",
    "$$\n",
    "\\textrm{market share}_{jt}=\\frac{\\mathrm{sales_{jt}}}{\\mathrm{pop}_t}.\n",
    "$$\n",
    "We also read in the variable description of the dataset contained in `eurocars.dta`. We will use the list `x_vars` throughout to work with our explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "import sys\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Files\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "data_path = os.path.join(module_path, 'data')\n",
    "\n",
    "from utilities.Logit_file import estimate_logit, logit_se, logit_t_p, q_logit, logit_score, logit_score_unweighted, logit_ccp, LogitBLP_estimator, LogitBLP_se\n",
    "from data.Eurocarsdata_file import Eurocars_cleandata, rank_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mgq977\\\\OneDrive - University of Copenhagen\\\\Desktop\\\\Practice\\\\GREENCAR_notebooks\\\\PyPUM'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\mgq977\\\\OneDrive - University of Copenhagen\\\\Desktop\\\\Practice\\\\GREENCAR_notebooks\\\\PyPUM\\\\data'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and variable names\n",
    "descr = (pd.read_stata(os.path.join(data_path,'eurocars.dta'), iterator = True)).variable_labels() # Obtain variable descriptions\n",
    "dat_file = pd.read_csv(os.path.join(data_path, 'eurocars.csv')) # reads in the data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ye</td>\n",
       "      <td>year (=first dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ma</td>\n",
       "      <td>market (=second dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>co</td>\n",
       "      <td>model code (=third dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zcode</td>\n",
       "      <td>alternative model code (predecessors and succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brd</td>\n",
       "      <td>brand code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type</td>\n",
       "      <td>name of brand and model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>name of model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>org</td>\n",
       "      <td>origin code (demand side, country with which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>loc</td>\n",
       "      <td>location code (production side, country where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frm</td>\n",
       "      <td>firm code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qu</td>\n",
       "      <td>sales (number of new car registrations)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pl</td>\n",
       "      <td>places (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>doors (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>li1</td>\n",
       "      <td>measure 1 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>li2</td>\n",
       "      <td>measure 2 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>li3</td>\n",
       "      <td>measure 3 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>princ</td>\n",
       "      <td>=pr/(ngdp/pop): price relative to per capita i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eurpr</td>\n",
       "      <td>=pr/avdexr: price in common currency (in SDR t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>exppr</td>\n",
       "      <td>=pr/avexr: price in exporter currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>avexr</td>\n",
       "      <td>av. exchange rate of exporter country (exporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>avdexr</td>\n",
       "      <td>av. exchange rate of destination country (dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>avcpr</td>\n",
       "      <td>av. consumer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>avppr</td>\n",
       "      <td>av. producer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>avdcpr</td>\n",
       "      <td>av. consumer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>avdppr</td>\n",
       "      <td>av. producer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xexr</td>\n",
       "      <td>avdexr/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tax</td>\n",
       "      <td>percentage VAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pop</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ngdp</td>\n",
       "      <td>nominal gross domestic product of destination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rgdp</td>\n",
       "      <td>real gross domestic product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>engdp</td>\n",
       "      <td>=ngdp/avdexr: nominal gdp in common currency (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ergdp</td>\n",
       "      <td>=rgdp/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>engdpc</td>\n",
       "      <td>=engdp/pop: nominal gdp per capita in common c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ergdpc</td>\n",
       "      <td>=ergdp/pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              ye                   year (=first dimension of panel)\n",
       "1              ma                market (=second dimension of panel)\n",
       "2              co             model code (=third dimension of panel)\n",
       "3           zcode  alternative model code (predecessors and succe...\n",
       "4             brd                                         brand code\n",
       "5            type                            name of brand and model\n",
       "6           brand                                      name of brand\n",
       "7           model                                      name of model\n",
       "8             org  origin code (demand side, country with which c...\n",
       "9             loc  location code (production side, country where ...\n",
       "10            cla                              class or segment code\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            frm                                          firm code\n",
       "13             qu            sales (number of new car registrations)\n",
       "14             cy            cylinder volume or displacement (in cc)\n",
       "15             hp                                 horsepower (in kW)\n",
       "16             we                                     weight (in kg)\n",
       "17             pl             places (number, not reliable variable)\n",
       "18             do              doors (number, not reliable variable)\n",
       "19             le                                     length (in cm)\n",
       "20             wi                                      width (in cm)\n",
       "21             he                                     height (in cm)\n",
       "22            li1  measure 1 for fuel efficiency (liter per km, a...\n",
       "23            li2  measure 2 for fuel efficiency (liter per km, a...\n",
       "24            li3  measure 3 for fuel efficiency (liter per km, a...\n",
       "25             li          average of li1, li2, li3 (used in papers)\n",
       "26             sp                            maximum speed (km/hour)\n",
       "27             ac  time to acceleration (in seconds from 0 to 100...\n",
       "28             pr   price (in destination currency including V.A.T.)\n",
       "29          princ  =pr/(ngdp/pop): price relative to per capita i...\n",
       "30          eurpr  =pr/avdexr: price in common currency (in SDR t...\n",
       "31          exppr              =pr/avexr: price in exporter currency\n",
       "32          avexr  av. exchange rate of exporter country (exporte...\n",
       "33         avdexr  av. exchange rate of destination country (dest...\n",
       "34          avcpr       av. consumer price index of exporter country\n",
       "35          avppr       av. producer price index of exporter country\n",
       "36         avdcpr    av. consumer price index of destination country\n",
       "37         avdppr    av. producer price index of destination country\n",
       "38           xexr                                       avdexr/avexr\n",
       "39            tax                                     percentage VAT\n",
       "40            pop                                         population\n",
       "41           ngdp  nominal gross domestic product of destination ...\n",
       "42           rgdp                        real gross domestic product\n",
       "43          engdp  =ngdp/avdexr: nominal gdp in common currency (...\n",
       "44          ergdp                                        =rgdp/avexr\n",
       "45         engdpc  =engdp/pop: nominal gdp per capita in common c...\n",
       "46         ergdpc                                         =ergdp/pop"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(descr, index=['description']).transpose().reset_index().rename(columns={'index' : 'variable names'}) # Prints data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              cy            cylinder volume or displacement (in cc)\n",
       "1              hp                                 horsepower (in kW)\n",
       "2              we                                     weight (in kg)\n",
       "3              le                                     length (in cm)\n",
       "4              wi                                      width (in cm)\n",
       "5              he                                     height (in cm)\n",
       "6              li          average of li1, li2, li3 (used in papers)\n",
       "7              sp                            maximum speed (km/hour)\n",
       "8              ac  time to acceleration (in seconds from 0 to 100...\n",
       "9              pr   price (in destination currency including V.A.T.)\n",
       "10          brand                                      name of brand\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            cla                              class or segment code"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside option is included if OO == True, otherwise analysis is done on the inside options only.\n",
    "OO = True\n",
    "\n",
    "# Choose which variables to include in the analysis, and assign them either as discrete variables or continuous.\n",
    "\n",
    "x_discretevars = [ 'brand', 'home', 'cla']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "z_IV_contvars = ['xexr']\n",
    "z_IV_discretevars = []\n",
    "x_allvars =  [*x_contvars, *x_discretevars]\n",
    "z_allvars = [*z_IV_contvars, *z_IV_discretevars]\n",
    "\n",
    "if OO:\n",
    "    nest_contvars = [var for var in x_contvars if var != 'pr'] # We nest over all variables other than price, but an alternative list can be specified here if desired.\n",
    "    nest_discvars = ['in_out', *x_discretevars]\n",
    "    nest_vars = ['in_out', *nest_contvars, *x_discretevars]\n",
    "else:\n",
    "    nest_contvars = [var for var in x_contvars if (var != 'pr')]\n",
    "    nest_discvars = x_discretevars # See above\n",
    "    nest_vars = [*nest_contvars, *nest_discvars]\n",
    "\n",
    "G = len(nest_vars)\n",
    "\n",
    "# Print list of chosen variables as a dataframe\n",
    "pd.DataFrame(descr, index=['description'])[x_allvars].transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the data to fit our setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, dat_org, x_vars, z_vars, N, pop_share, T, J, K = Eurocars_cleandata(dat_file, x_contvars, x_discretevars, z_IV_contvars, z_IV_discretevars, outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of numpy arrays for each market. This allows the size of the data set to vary over markets.\n",
    "\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)} # Dict of explanatory variables\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)} # Dict of market shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'T' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\models\\Similarity\\Similarity_book.ipynb Cell 13\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rank_test(x)\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\data\\Eurocarsdata_file.py:141\u001b[0m, in \u001b[0;36mrank_test\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrank_test\u001b[39m(x):\n\u001b[1;32m--> 141\u001b[0m     x_stacked \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate([x[t] \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m np\u001b[39m.\u001b[39marange(T)], axis \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m)\n\u001b[0;32m    142\u001b[0m     eigs\u001b[39m=\u001b[39mla\u001b[39m.\u001b[39meig(x_stacked\u001b[39m.\u001b[39mT\u001b[39m@x_stacked\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    144\u001b[0m     \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mmin(eigs)\u001b[39m<\u001b[39m\u001b[39m1.0e-8\u001b[39m:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'T' is not defined"
     ]
    }
   ],
   "source": [
    "rank_test(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed utility, logit and nested logit\n",
    "\n",
    "In the following, a vector $z\\in \\mathbb R^d$ is always a column vector. The Similarity Model is a discrete choice model, where the probability vector over the alternatives is given by the solution to a utility maximization problem of the form\n",
    "$$\n",
    "P(u|\\theta)=\\arg\\max_{q\\in \\Delta} q'u(\\theta)-\\Omega(q|\\theta)\n",
    "$$\n",
    "where $\\Delta$ is the probability simplex over the set of discrete choices, $u$ is a vector of payoffs for each option, $\\Omega$ is a convex function and $q'$ denotes the transpose of $q$, and $\\theta$ is a vector of parameters. All Additive Random Utility Models can be represented in this way (Fosgerau and SÃ¸rensen (2021)). For example, the logit choice probabilities result from the perturbation function $\\Omega(q)=q'\\ln q$ where $\\ln q$ is the elementwise logarithm.\n",
    "\n",
    "In the Nested Logit Model, the choice set is divided into a partition $\\mathcal C=\\left\\{C_1,\\ldots,C_L\\right\\}$, and the perturbation function is given by\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\sum_{\\ell =1}^L \\left( \\sum_{j\\in C_\\ell}q_j\\right)\\ln \\left( \\sum_{j\\in C}q_j\\right),\n",
    "$$\n",
    "where $\\lambda\\in [0,1)$ is a parameter. This function can be written equivalently as\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\left(\\psi q\\right)'\\ln \\left( \\psi q\\right),\n",
    "$$\n",
    "where $\\psi$ is a $J \\times L$ matrix, where $\\psi_{j\\ell}=1$ if option $j$ belongs to nest $C_\\ell$ and zero otherwise.\n",
    " This specification generates nested logit choice probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Similarity Model\n",
    "\n",
    "Similarity Model:\n",
    "- Choice of $\\Omega$ \n",
    "- relation to entropy\n",
    "- Interpretation of $\\psi^g q$ for discrete and continuous variables $g$\n",
    "- Normalisation via $q'\\delta = q'\\varphi \\lambda$\n",
    "- Familiar special cases of the Similarity Model\n",
    "- Relation to IPDL Model\n",
    "- Kernel density + Silverman's rule of thumb\n",
    "- The stacked block matrix $\\Psi$\n",
    "- The $\\Gamma$ matrix\n",
    "\n",
    "The Similarity Model generalizes the Nested Logit Model. It allows for multiple nesting structures, and it also allows for 'continuous' nesting by measuring similarity of products in the space of characteristics. Let $g = 1,\\ldots, G$ index a set of distinct nesting structures, and let $\\mathcal{C}_{t} = \\{C_{t1}, \\ldots, C_{tG}\\}$ denote the set containing the spaces of characteristics. To indicate which alternatives $j$ are close to each other in the space of characteristics $C_{tg} \\in \\mathcal{C}_{t}$, we introduce the square matrix $\\psi^g \\in \\mathbb{R}^{J_t \\times J_t}$ constructed in the following way. \n",
    "\n",
    "Let $W \\in \\mathbb{R}^{G \\times J_{t}}$ be a matrix of characteristics for vehicles $j = 1, \\ldots, J_t$, and suppose we partition the choice set $\\mathcal{J}_t$ according to some discrete variable $w_{(g)}$. For alternatives $j$ and $k$, let the kernel function $K_g(w_{gj} - w_{gk}) = 1(w_{gj} - w_{gk} = 0)$ be the dummy variable on whether choices $j$ and $k$ take the same values in $w_{(g)}$. For continuous variables $w_{(g)}$, we choose the Gaussian kernel function $K_g(w_{gj} - w_{gk}) = e^{-\\frac{(w_{gj} - w_{gk})^2}{2 h_g}}$, where $h_g$ is a bandwidth for nesting structure $g$ determined by Silverman's rule of thumb\n",
    "\n",
    "$$\n",
    "h_g = 0.9\\cdot \\min \\left \\{ \\hat \\sigma_g, \\frac{\\text{IQR}_g}{1.34} \\right \\} \\cdot J_t^{-\\frac{1}{5}}\n",
    "$$\n",
    "\n",
    "where $ \\hat \\sigma_g$ and $\\text{IQR}_g$ are the empirical standard deviation and the interquartile range of $w_{(g)}$, respectively. Finally, we set\n",
    "\n",
    "$$\n",
    "\\psi^g_{jk} = \\frac{K_g(w_{gj} - w_{gk})}{\\sum_{\\ell = 1}^{J_t} K_g(w_{g\\ell} - w_{gk})}\n",
    "$$\n",
    "\n",
    "for all discrete aswell as continuous nesting structures $g = 1, \\ldots, G$, such that the rows $\\psi^g_{(j)}$ correspond to a probability distribution over the choice set. A row $\\psi^g_{(j)}$ may therefore also be interpreted as the similarity of products $k$ to $j$ in the space of the $g$'th characteristic of $W$. Having specified a nesting structure, we define the Similarity pertubation function $\\Omega$ as:\n",
    "\n",
    "$$\n",
    "\\Omega(q|\\lambda) = \\left( 1 - \\sum_{g = 1}^G \\lambda_g\\right) q' \\ln (q) + \\sum_{g = 1}^G \\lambda_g (\\psi^g q)' \\ln( \\psi^g q) - q' \\delta\n",
    "$$\n",
    "\n",
    "where $\\lambda \\in \\mathbb{R}^G$ is a vector of nesting parameters and $\\delta \\in \\mathbb{R}^{J_t}$ is a normalizing constant vector. If the sum of the positive nesting parameters $\\sum_{g : \\lambda_g > 0} \\lambda_g $ is strictly less than $1$, then the pertubation function $\\Omega(\\cdot|\\lambda)$ is strictly convex, such that the Similarity Model is a perturbed utility Model.\n",
    "\n",
    "Note that $q$ and $\\psi^g q$ are probability distributions, wherefore the terms $q'\\ln(q)$ and $(\\psi^g q)' \\ln(\\psi^g q)$ is interpreted as the negative entropy of $q$ and of the probability distribution over nests in $C_g$, respectively.\n",
    "\n",
    "When choosing the normalizing factor $\\delta$, we want to normalize the pertubation function such that $\\Omega(q|\\lambda) = 0$ at the corners of the probability simplex $\\Delta$, i.e. when the vector of choice probabilities $q$ contains a probability equal to $0$ or $1$. If $e_j$ is the $j$'th standard basis vector in $R^{J_t}$, then $0 = \\Omega(e_j | \\lambda) = \\left( 1 - \\sum_{g = 1}^G \\lambda_g\\right) \\cdot 0 + \\sum_{g = 1}^G \\lambda_g (\\psi_{[j]}^g)' \\ln( \\psi_{[j]}^g) - \\delta_j$ implies that we must choose $\\delta_j = \\sum_{g = 1}^G \\lambda_g (\\psi_{[j]}^g)' \\ln( \\psi_{[j]}^g)$ to achieve this normalization, where $\\psi_{[j]}^g$ here denotes the $j$'th row of $\\psi^g$.\n",
    "\n",
    "Furthermore, if $\\lambda = 0$ then the Similarity Model reduces to the Multinomial Logit Model, since $\\Omega(q|0) = q' \\ln (q)$ is the negative Shannon-entropy, and the Nested Logit Model, as described above, may be obtained if $G = 1$ and $\\delta = 0$. Furthermore, the IPDL Model by Fosgerau et. al (2022) may obtained by setting $\\delta = 0$. Hence the Similarity Model allows for greater flexibility than many workhorse models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In implementions of the Similarity Model, it will be useful to define a few matrices to use in computations. First we define the matrix $\\Psi \\in \\mathbb{R}^{(G + 1)J_t \\times J_t}$ as the matrix stacking the Identity matrix $I_{J_t}$ in $R^{J_t \\times J_t}$ on top of the $\\psi^g$ matrices:\n",
    "\n",
    "$$\n",
    "\\Psi = \\left(\n",
    "    \\begin{array}{c}\n",
    "        I_{J_t} \\\\\n",
    "        \\psi^1 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\psi^G\n",
    "    \\end{array}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "Another useful matrix for carrying out computations is the matrix $\\Gamma \\in \\mathbb{R}^{(G + 1)J_t \\times J_t}$ defined by:\n",
    "\n",
    "$$\n",
    "\\Gamma = \\left(\n",
    "    \\begin{array}{c}\n",
    "        \\left(1 - \\sum_{g = 1}^G \\lambda_g\\right) I_{J_t} \\\\\n",
    "        \\lambda_1 \\psi^1 \\\\\n",
    "        \\vdots \\\\\n",
    "        \\lambda_G \\psi^G\n",
    "    \\end{array}\n",
    "    \\right)\n",
    "$$\n",
    "\n",
    "Finally, since the nomarlizing vector $\\delta$ is linear-in-parameters $\\lambda$, we wish to construct a matrix $\\varphi \\in \\mathbb{R}^{J_t \\times G}$ such that $\\delta = \\varphi \\lambda$. Hence for any nesting structure $g$, set $\\varphi_{[g]} = (\\psi^g \\circ \\ln (\\psi^g))'\\iota_{J_t}$, where $\\iota_{J_t} = (1, \\ldots, 1)' \\in R^{J_t}$ is the all-ones vector; then \n",
    "\n",
    "$$\n",
    "\\varphi = \\left(\\varphi_{[1]} \\ldots \\varphi_{[G]}\\right)$$ \n",
    "\n",
    "has the desired property. Using the above matrices, the Similarity pertubation function may be computed by: $\\Omega(q|\\lambda) = (\\Gamma q)' \\ln (\\Psi q) - q' \\varphi \\lambda$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_nests(data, markets_id, products_id, in_out_id, cont_var, disc_var, outside_option = True):\n",
    "    '''\n",
    "    This function creates the nest matrices \\Psi^{gt}, and stack them over groups g for each market t.\n",
    "\n",
    "    Args.\n",
    "        data: a pandas DataFrame\n",
    "        markets_id: a string denoting the column of 'data' containing an enumeration t=0,1,...,T-1 of markets\n",
    "        products_id: a string denoting the column of 'data' containing product codes which uniquely identifies products\n",
    "        in_out_id: a string denoting the column of 'data' containing the dummy for being an inside or outside option. If 'outside_option = True' then this may be set to e.g. the empty string ''.\n",
    "        cont_var: a list of the continuous variables among the covariates\n",
    "        disc_var: a list of the discrete variables among the covariates\n",
    "        outside_option: a boolean indicating whether the model is estimated with or without an outside option. Default is set to 'True' i.e. with an outside option.\n",
    "\n",
    "    Returns\n",
    "        Psi: a dictionary of length T of numpy arrays ((G+1)*J[t], J[t]) the J[t] by J[t] identity stacked on top of the Psi_g matrices for each market t and each gropuing g\n",
    "        Psi_dim: a dictionary of length T of (G+1,J[t],J[t]) numpy arrays with the top most array being the J[t] by J[t] identity matrix and the following G matrices being the \\psi^g matrices \n",
    "    '''\n",
    "\n",
    "    T = data[markets_id].nunique()\n",
    "    J = np.array([data[data[markets_id] == t][products_id].nunique() for t in np.arange(T)])\n",
    "    \n",
    "    # We include nest on outside vs. inside options. The amount of categories varies if the outside option is included in the analysis.\n",
    "    dat = data.sort_values(by = [markets_id, products_id]) # We sort the data in ascending, first according to market and then according to the product id\n",
    "    \n",
    "    Psi = {}\n",
    "    Psi_dim = {}\n",
    "\n",
    "    if OO:\n",
    "        in_out_index = [n for n in np.arange(len(disc_var)) if disc_var[n] == in_out_id][0]\n",
    "        non_in_out_indices = np.array([n for n in np.arange(len(disc_var)) if disc_var[n] != in_out_id])\n",
    "\n",
    "    # Assign nests for products in each market t\n",
    "    for t in np.arange(T):\n",
    "        data_t = dat[dat[markets_id] == t] # Subset data on market t\n",
    "\n",
    "        # Estimate discrete kernels\n",
    "        D_disc = len(disc_var)\n",
    "        K_disc = np.empty((D_disc, J[t], J[t]))\n",
    "        C = np.array(data_t[disc_var].nunique())\n",
    "\n",
    "        for d in np.arange(D_disc):\n",
    "            Indicator = pd.get_dummies(data_t[disc_var[d]]).values.reshape((J[t], C[d]))\n",
    "            K_disc[d,:,:] = Indicator@(Indicator.T) # Get the indicator kernel function for the discrete variables\n",
    "\n",
    "        Psidisc_t = np.einsum('djk,dk->djk', K_disc, 1./(K_disc.sum(axis=1)))\n",
    "            \n",
    "        # Estimate continuous kernels\n",
    "        D_cont = len(cont_var)\n",
    "        IQR = scstat.iqr(data_t[cont_var].values, axis = 0) # Compute interquartile range of each continuous variable\n",
    "        sd = np.std(data_t[cont_var].values, axis = 0) # Compute empirical standard deviation of each continuous variable\n",
    "        h = 0.9*np.fmin(sd, IQR/1.34)/(J[t]**(1/5)) # Use Silverman's rule of thumb for bandwidth estimation for each continuous variable\n",
    "        w = data_t[cont_var].values.transpose()\n",
    "        diff = w[:,:,None]*np.ones((D_cont, J[t], J[t])) - w[:,None,:] # calculates the differences w_j - w_k for all continuous g = 1, ... , G , and for all alternatives j,k.\n",
    "        \n",
    "        # Compute continuous kernel functions\n",
    "        if outside_option:\n",
    "            K_cont = np.exp(-(diff**2)/(2*(h[:,None,None]**2)))[:,1:,1:] # Compute continuous kernel function for inside options\n",
    "            Psicontinner_t = np.einsum('djk,dk->djk', K_cont, 1./K_cont.sum(axis=1))\n",
    "            Psicont_t = np.zeros((D_cont, J[t], J[t]))\n",
    "            Psicont_t[:,0,0] = 1 # The outside option is only similar to itself\n",
    "            Psicont_t[:,1:,1:] = Psicontinner_t # The inside option are only similar to each other\n",
    "        else:\n",
    "            K_cont = np.exp(-(diff**2)/(2*(h[:,None,None]**2))) # -=-\n",
    "            Psicont_t = np.einsum('djk,dk->djk', K_cont, 1./K_cont.sum(axis=1))\n",
    "\n",
    "        # Stack Psi\n",
    "        D = len([*cont_var, *disc_var]) + 1\n",
    "\n",
    "        if outside_option:\n",
    "            Psi_dim[t] = np.concatenate((np.eye(J[t]).reshape((1,J[t],J[t])), Psidisc_t[in_out_index,:,:].reshape((1,J[t],J[t])), Psicont_t, Psidisc_t[non_in_out_indices,:,:]), axis = 0)\n",
    "            Psi[t] = Psi_dim[t].reshape((D*J[t], J[t]))\n",
    "        else:\n",
    "            Psi_dim[t] = np.concatenate((np.eye(J[t]).reshape((1,J[t],J[t])), Psicont_t, Psidisc_t), axis = 0)\n",
    "            Psi[t] = Psi_dim[t].reshape((D*J[t], J[t]))\n",
    "\n",
    "    return Psi, Psi_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_matrix(psi):\n",
    "    '''\n",
    "    This function computes the \\varphi matrix used in e.g. calculating \\delta. \n",
    "\n",
    "    Args:\n",
    "        psi: a dictionary of length T of numpy arrays ((G+1)*J[t], J[t]) the J[t] by J[t] identity stacked on top of the Psi_g matrices for each market t and each gropuing g as outputted by 'Create_nests'-function\n",
    "\n",
    "    Returns.\n",
    "        phi: a dictionary of length T of numpy arrays (J[t],G) of the \\varphi^g matrices\n",
    "    '''\n",
    "    T = len(psi)\n",
    "    J = np.array([psi[t].shape[1] for t in np.arange(T)])\n",
    "    G = np.int32(psi[0].shape[0] / J[0] - 1)\n",
    "\n",
    "    phi = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        phi_t = np.empty((J[t], G))\n",
    "        psi_t = psi[t]\n",
    "\n",
    "        # Compute phi_g = (psi^g \\circ log(psi^g))^T %o% \\iota \n",
    "        for g in np.arange(1,G+1):\n",
    "            psi_g = psi_t[g*J[t]:(g+1)*J[t],:]\n",
    "            phi_t[:,g-1] = (psi_g*np.log(psi_g, out = np.zeros_like(psi_g), where = (psi_g > 0))).sum(axis=0)\n",
    "        \n",
    "        phi[t] = phi_t\n",
    "\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_specification(data, markets_id, products_id, in_out_id, cont_var, disc_var, outside_option = True):\n",
    "    '''\n",
    "    This function returns the Similarity Model specification as given by the covariates and the nesting structure\n",
    "\n",
    "    Args:\n",
    "        data: a pandas DataFrame\n",
    "        markets_id: a string denoting the column of 'data' containing an enumeration t=0,1,...,T-1 of markets\n",
    "        products_id: a string denoting the column of 'data' containing product codes which uniquely identifies products\n",
    "        in_out_id: a string denoting the column of 'data' containing the dummy for being an inside or outside option. If 'outside_option = True' then this may be set to e.g. the empty string ''.\n",
    "        cont_var: a list of the continuous variables among the covariates\n",
    "        disc_var: a list of the discrete variables among the covariates\n",
    "        outside_option: a boolean indicating whether the model is estimated with or without an outside option. Default is set to 'True' i.e. with an outside option.\n",
    "\n",
    "    Returns.\n",
    "        Model: a dictionary of length 3, containing the stacked Psi, the 3-dimensional Psi, and the Phi matrix as outputted by 'Create_nests' and 'phi_matrix', respectively.\n",
    "    '''\n",
    "\n",
    "    Psi, Psi_3d = Create_nests(data, markets_id, products_id, in_out_id, cont_var, disc_var, outside_option)\n",
    "    Phi = phi_matrix(Psi)\n",
    "    Model = {'psi' : Psi, 'psi_3d' : Psi_3d, 'phi' : Phi}\n",
    "\n",
    "    return Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dat' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\models\\Similarity\\Similarity_book.ipynb Cell 19\u001b[0m line \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m Model \u001b[39m=\u001b[39m Similarity_specification(dat, \u001b[39m'\u001b[39m\u001b[39mmarket\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mco\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39min_out\u001b[39m\u001b[39m'\u001b[39m, nest_contvars, nest_discvars, outside_option \u001b[39m=\u001b[39m OO)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#X24sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m Psi \u001b[39m=\u001b[39m Model[\u001b[39m'\u001b[39m\u001b[39mpsi\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dat' is not defined"
     ]
    }
   ],
   "source": [
    "Model = Similarity_specification(dat, 'market', 'co', 'in_out', nest_contvars, nest_discvars, outside_option = OO)\n",
    "Psi = Model['psi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions of the continuous explanatory variables\n",
    "\n",
    "We now plot the empirical distributions of the continuous variables against the kernel approximations of their densities. For any nesting structure $g = 1, \\ldots, G$ with Kernel function $K_g$ and bandwidth $h_g$ the Kernel estimator of the density $f_g$ of any variable $w_g \\in \\mathbb{R}^{J_t}$ is given as \n",
    "\n",
    "$$\n",
    "\\hat f_g(z) = \\frac{1}{J}\\sum_{j=1}^J K_{g}\\left(\\frac{z-w_{gj}}{h_g}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_kernel_estimate(x_cont, cont_vars, x_min, x_max, n_points, J, outside_option: bool = True):\n",
    "    '''\n",
    "    This function calculates kernel density estimates of each of the densities governing the distribution of the continuous covariates using a Gaussian kernel function.\n",
    "\n",
    "    Args:\n",
    "        x_cont: a numpy array (J[t],D_cont) of continuous covariates for a given market t\n",
    "        cont_vars: a list of the labels of the continuous covariates\n",
    "        x_min: a numpy array (D_cont,) of the minimum value within each characteristic of x_cont. Should be compatible with the value of outside_option (i.e. if outside_option = True, x_min should be the lowest value in x_cont when disregarding the outside_option)\n",
    "        x_max: a numpy array (D_cont,) of the maximum value within each characteristic of x_cont. Should be compatible with the value of outside_option\n",
    "        n_points_ an integer of the amount of points to evaluate the kernel estimator in.\n",
    "        J: an integer describing the amount of alternatives in the given market t (i.e. J[t]). Should be compatible with the value of outside_option.\n",
    "        outside_option: a boolean of whether outside is included in x_cont or not. If 'True' outside option is included in x_cont.\n",
    "    \n",
    "    Returns.\n",
    "        f_hat: a (G,n_points) array of the kernel estimated density for each continuous variable in cont_vars \n",
    "    '''\n",
    "\n",
    "    D_cont = len(cont_vars)\n",
    "    \n",
    "    if outside_option:\n",
    "        z_cont = np.linspace(x_min, x_max, n_points).transpose()\n",
    "        diff = z_cont[:,:,None]*np.ones((D_cont, n_points, J - 1)) - x_cont.transpose()[:,None,1:]\n",
    "        IQR = scstat.iqr(x_cont[1:,:], axis = 0) # Compute interquartile range of each continuous variable\n",
    "        sd = np.std(x_cont[1:,:], axis = 0) # Compute empirical standard deviation of each continuous variable\n",
    "        h = 0.9*np.fmin(sd, IQR/1.34)/((J-1)**(1/5)) # Silverman's rule of thumb\n",
    "    else:\n",
    "        z_cont = np.linspace(x_min, x_max, n_points).transpose()\n",
    "        diff = z_cont[:,:,None]*np.ones((D_cont, n_points, J)) - x_cont.transpose()[:,None,:]\n",
    "        IQR = scstat.iqr(x_cont, axis = 0) # Compute interquartile range of each continuous variable\n",
    "        sd = np.std(x_cont, axis = 0) # Compute empirical standard deviation of each continuous variable\n",
    "        h = 0.9*np.fmin(sd, IQR/1.34)/(J**(1/5)) # Silverman's rule of thumb\n",
    "\n",
    "    K = np.exp(-(diff**2)/(2*(h[:,None,None]**2))) / (np.sqrt(2*np.pi)*h[:,None,None]) # Use a gaussian kernel function\n",
    "\n",
    "    f_hat = K.mean(axis=2)\n",
    "\n",
    "    return f_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the observed x's for each explanatory variable\n",
    "t0 = 0\n",
    "x_cont0 = dat[dat['market'] == t0][nest_contvars].values.reshape((J[t0],len(nest_contvars)))\n",
    "x_min0 = x_cont0[1:,:].min(axis = 0) # min of x's different from outside option\n",
    "x_max0 = x_cont0[1:,:].max(axis = 0) # max -=-\n",
    "n_points = 200\n",
    "K0 = plot_kernel_estimate(x_cont0, nest_contvars, x_min0, x_max0, n_points, J[t0], outside_option = OO)\n",
    "\n",
    "x_pairs = iter.product(np.arange(3), np.arange(3))\n",
    "num_bins = 20\n",
    "\n",
    "fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "for p,d in zip(x_pairs, np.arange(len(nest_contvars))):\n",
    "    axes[p].hist(x_cont0[1:,d], num_bins, color = 'r', alpha = 1, density = True)\n",
    "    axes[p].plot(np.linspace(x_min0[d], x_max0[d], n_points), K0[d,:], color = 'black', label = 'f^hat')\n",
    "    axes[p].legend(framealpha = 0.3)\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "    axes[p].set_title(nest_contvars[d])\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The space of characteristics\n",
    "\n",
    "To get a sense of how we have chosen to model the similarity of products $j$ in the spaces of continuous characteristics $g$, we construct a scatterplot with the vectors $\\psi^g_{(j)}$ against the characteristics $w_{(g)} = X_{t,k(g)}$ of products for market $t = 1$. To reduce overplotting we only plot the rows $\\psi^g_{(j)}$ for a median product $j^g = \\mathrm{med}(X_{t,k(g)})$ of each nesting structure $g$. Clearly, by construction, the distribution of characteristics will be Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_medindex0 = np.empty((len(nest_contvars),), dtype = 'int64')\n",
    "\n",
    "for d in np.arange(len(nest_contvars)):\n",
    "    x_medindex0[d] = np.int64(np.argwhere(x_cont0[1:,d] == np.percentile(x_cont0[1:,d], 50, interpolation = 'higher'))[0])\n",
    "\n",
    "x_pairs = iter.product(np.arange(3), np.arange(3))\n",
    "num_bins = 20\n",
    "\n",
    "fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "for p,d in zip(x_pairs, np.arange(len(nest_contvars))):\n",
    "    if OO:\n",
    "        axes[p].scatter(x_cont0[1:,d], Psi[0][(np.int64((d+2)*J[0]) + 1):(np.int64((d+3)*J[0])), x_medindex0[d] + 1], s = 10)\n",
    "    else:\n",
    "        axes[p].scatter(x_cont0[1:,d], Psi[0][np.int64((d+1)*J[0]):np.int64((d+2)*J[0]), x_medindex0[d]], s = 10)\n",
    "    axes[p].axvline(np.median(x_cont0[1:,d]), color = 'r', label = 'median')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "    axes[p].set_title(nest_contvars[d])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "Suppose we are evaluating the choice probability function $P_t(u|\\theta)$ at some parameter vector $\\theta$. While it is possible to solve for the choice probabilities explicitly by numerical maximization, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Let $u_t = X_t\\beta$ for some parameter vector $\\beta \\in \\mathbb{R}^{K}$, such that $\\theta = (\\beta', \\lambda')'$, and let $q_t^0$ be an initial guess of the choice probabilities, e.g. $q_t^0\\propto \\exp(X_t\\beta)$. Define further\n",
    "$$\n",
    "a=\\sum_{g:\\lambda_g\\geq 0} \\lambda_g   \\qquad b=\\sum_{g:\\lambda_g<0} |\\lambda_g|.\n",
    "$$\n",
    "\n",
    "The choice probabilities are then updated iteratively as\n",
    "$$\n",
    "q_t^{r} = \\frac{e^{v_t^{r}}}{\\sum_{j\\in \\mathcal J_t} e^{v_{tj}^{r}}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "v_t^{r} =\\ln q_t^{r-1}+\\left(u_t-\\nabla_q \\Omega_t(q^{r-1}_t|\\lambda)\\right)/(1+b).\n",
    "$$\n",
    "The gradient $\\nabla_q \\Omega_t(q_t|\\lambda)$ is easily computed using the formula\n",
    "$$\n",
    "\\nabla_q \\Omega_t(q_t|\\lambda) = \\Gamma' \\ln (\\Psi q_t) - \\delta + \\iota_{J_t}\n",
    "$$\n",
    "For numerical stability, it can be a good idea to also do max-rescaling of $v^r_t$ at every iteration. The Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p_t(\\theta)||q_t^{r})\\leq \\frac{a+b}{1+b}D_{KL}(p_t(\\theta)||q^{r-1}_t).\n",
    "$$\n",
    "This is implemented in the function \"Similarity_ccp\" below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Gamma(Lambda, model):\n",
    "    '''\n",
    "    This function computes the Gamma matrix\n",
    "\n",
    "    Args:\n",
    "        Lambda: a (G,) numpy array of grouping parameters \\lambda_g\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns.\n",
    "        Gamma: a dictionary of length T containing the ((G+1)*J[t],J[t]) numpy arrays of the \\Gamma matrices for each market t.\n",
    "    '''\n",
    "\n",
    "    Psi = model['psi']\n",
    "    T = len(Psi)\n",
    "    J = np.array([Psi[t].shape[1] for t in np.arange(T)])\n",
    "    \n",
    "    Gamma = {}\n",
    "    lambda0 = np.array([1 - sum(Lambda)])\n",
    "    Lambda_full = np.concatenate((lambda0, Lambda)) # create vector (1- sum(lambda), lambda_1, ..., lambda_G)\n",
    "    D = len(Lambda_full)\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Lambda_long =(Lambda_full[:,None]*np.ones((D,J[t]))).reshape((D*J[t],))\n",
    "        Gamma[t] = Lambda_long[:,None]*Psi[t]\n",
    "\n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_ccp(Theta, x, model, tol = 1.0e-15, maximum_iterations = 1000):\n",
    "    '''\n",
    "    This function finds approximations to the true conditional choice probabilities given parameters.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        tol: tolerated approximation error\n",
    "        maximum_iterations: a no. of maximum iterations which if reached will stop the algorithm\n",
    "\n",
    "    Output\n",
    "        q_1: a dictionary of T numpy arrays (J[t],) of Similarity choice probabilities for each market t\n",
    "    '''\n",
    "\n",
    "    # Objects in model specification\n",
    "    psi = model['psi']\n",
    "    phi = model['phi']\n",
    "\n",
    "    T = len(x) # Number of markets\n",
    "    K = x[0].shape[1] # Number of car characteristics\n",
    "\n",
    "    # Parameters\n",
    "    Beta = Theta[:K]\n",
    "    Lambda = Theta[K:]\n",
    "    G = len(Lambda)  # Number of groups\n",
    "\n",
    "    # Calculate small b\n",
    "    C_minus = np.array([True if Lambda[g] < 0 else False for g in np.arange(G)])\n",
    "    if C_minus.all() == False:\n",
    "        b = 0\n",
    "    else:    \n",
    "        b = np.abs(Lambda[C_minus]).sum() # sum of absolute value of negative lambda parameters.\n",
    "\n",
    "    # Find the Gamma matrix\n",
    "    Gamma = Create_Gamma(Lambda, model)\n",
    "\n",
    "    u = {t: np.einsum('jk,k->j', x[t], Beta) for t in np.arange(T)} # Calculate linear utilities\n",
    "    q = {t: np.exp(u[t] - u[t].max()) / np.exp(u[t] - u[t].max()).sum() for t in np.arange(T)}\n",
    "    q0 = q\n",
    "    Epsilon = 1.0e-10\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        q1 = {}\n",
    "        for t in np.arange(T):\n",
    "            # Calculate v\n",
    "            psi_q = psi[t] @ q0[t] # Compute matrix product\n",
    "            log_psiq =  np.log(np.abs(psi_q) + Epsilon) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "            delta = phi[t]@Lambda\n",
    "            Grad = (Gamma[t].T @ log_psiq) - delta # Compute matrix product\n",
    "            v = np.log(q0[t] + Epsilon) + (u[t] - Grad)/(1 + b) # Calculate v = log(q) + (u - Gamma^T %o% log(Gamma %o% q) - delta)/(1 + b)\n",
    "            v -= v.max(keepdims = True) # Do max rescaling wrt. alternatives\n",
    "\n",
    "            # Calculate iterated ccp q^k\n",
    "            numerator = np.exp(v)\n",
    "            denom = numerator.sum()\n",
    "            q1[t] = numerator/denom\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.array([np.sum((q1[t]-q0[t])**2/q[t].std()) for t in np.arange(T)])) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "        \n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001526\n",
      "         Iterations: 25\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 29\n"
     ]
    }
   ],
   "source": [
    "beta0 = estimate_logit(q_logit, np.zeros((K,)), y, x, pop_share)['beta']\n",
    "lambda0 = np.zeros((G,))\n",
    "theta0 = np.append(beta0, lambda0)\n",
    "q0 = Similarity_ccp(theta0, x, Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand derivatives and price Elasticity\n",
    "\n",
    "While the demand derivatives in the Similarity Model are not quite as simple as in the Logit Model, they are still easy to compute. \n",
    "Let $q=P(u|\\theta)$, then\n",
    "$$\n",
    "\\nabla_u P(u|\\theta)=\\left(\\nabla^2_{qq}\\Omega(q|\\lambda)\\right)^{-1}-qq'\n",
    "$$\n",
    "where the $()^{-1}$ denotes the matrix inverse, and $\\nabla^2_{qq}\\Omega(q|\\lambda) = \\Gamma' \\mathrm{diag}(\\Psi q)^{-1} \\Psi$ is the hessian of $\\Omega$. The derivatives with respect to any $x_{tk\\ell}$ can now easily be computed by the chain rule,\n",
    "$$\n",
    "    \\frac{\\partial P_j(u_t|\\theta)}{\\partial x_{tk\\ell}}=\\frac{\\partial P_j(u_t|\\theta)}{\\partial u_{tk}}\\frac{\\partial u_{tk}}{\\partial x_{tk\\ell}}=\\frac{\\partial P_j(u_t|\\theta)}{\\partial u_{tk}}\\beta_\\ell,\n",
    "$$\n",
    "\n",
    "Finally, moving to price elasticity is the same as in the logit Model, if $x_{tk\\ell}$ is the price of product $k$ in market $t$, then\n",
    "$$\n",
    "    \\mathcal{E}_{jk}= \\frac{\\partial P_j(u_t|\\theta)}{\\partial x_{tk\\ell}}\\frac{1}{P_j(u_t|\\theta)}=\\frac{\\partial P_j(u_t|\\theta)}{\\partial u_{tk}}\\frac{1}{P_j(u_t|\\theta)}\\beta_\\ell=\\frac{\\partial \\ln P_j(u_t|\\theta)}{\\partial u_{tk}}\\beta_\\ell$$\n",
    "we can also write this compactly as\n",
    "$$\n",
    "\\nabla_u \\ln P(u|\\theta)=\\mathrm{diag}(P(u|\\theta))^{-1}\\nabla_u P(u|\\theta) = \\mathrm{diag}(q)^{-1}\\left[\\left(\\nabla^2_{qq}\\Omega(q|\\lambda)\\right)^{-1}-qq'\\right].\n",
    "$$\n",
    "Note that these elasticities may deviate significantly from the Logit elasticities. In particular, the IIA property will not generally apply to the Similarity Model. Additionally, the Similarity Model may detect both substitution and complementarity between products, contrasting it with well-known Nested Logit and Additive Random Utility Models according to which all products can only be substitutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pertubation_hessian(q, x, Theta, model):\n",
    "    '''\n",
    "    This function calucates the hessian of the pertubation function \\Omega\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    \n",
    "    Returns\n",
    "        Hess: a dictionary of T numpy arrays (J[t],J[t]) of second partial derivatives of the pertubation function \\Omega for each market t\n",
    "    '''\n",
    "    psi = model['psi']\n",
    "    T = len(q.keys())\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    Gamma = Create_Gamma(Theta[K:], model) # Find the \\Gamma matrices \n",
    "    \n",
    "    Hess={}\n",
    "    for t in np.arange(T):\n",
    "        psi_q = np.einsum('cj,j->c', psi[t], q[t]) # Compute a matrix product\n",
    "        Hess[t] = np.einsum('cj,c,cl->jl', Gamma[t], 1/psi_q, psi[t], optimize=True) # Computes the product \\Gamma' diag(\\psi q)^{-1} \\psi (but faster)\n",
    "        \n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_gradient(q, x, Theta, model):\n",
    "    \n",
    "    '''\n",
    "    This function calucates the gradient of the choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    \n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K) of partial derivatives of the choice proabilities wrt. utilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Grad = {}\n",
    "    Hess = compute_pertubation_hessian(q, x, Theta, model) # Compute the hessian of the pertubation function\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        inv_omega_hess = la.inv(Hess[t]) # (J,J) for each t=1,...,T , computes the inverse of the Hessian\n",
    "        qqT = q[t][:,None]*q[t][None,:] # (J,J) outerproduct of ccp's for each market t\n",
    "        Grad[t] = inv_omega_hess - qqT  # Compute Similarity gradient of ccp's wrt. utilities\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_u_grad_Log_ccp(q, x, Theta, model):\n",
    "    '''\n",
    "    This function calucates the gradient of the log choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    \n",
    "    Returns\n",
    "        Epsilon: a dictionary of T numpy arrays (J[t],J[t]) of partial derivatives of the log choice proabilities of products j wrt. utilites of products k for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = ccp_gradient(q, x, Theta, model) # Find the gradient of ccp's wrt. utilities\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = Grad[t]/q[t][:,None] # Computes diag(q)^{-1}Grad[t]\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_elasticity(q, x, Theta, model, char_number = K-1):\n",
    "    ''' \n",
    "    This function calculates the elasticity of choice probabilities wrt. any characteristic or nest grouping of products\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        char_number: an integer which is an index of the parameter in theta wrt. which we wish calculate the elasticity. Default is the index for the parameter of 'pr'.\n",
    "\n",
    "    Returns\n",
    "        a dictionary of T numpy arrays (J[t],J[t]) of choice probability semi-elasticities for each market t\n",
    "    '''\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = Similarity_u_grad_Log_ccp(q, x, Theta, model) # Find the gradient of log ccp's wrt. utilities\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = Grad[t]*Theta[char_number] # Calculate semi-elasticities\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of Similarity\n",
    "\n",
    "The log-likelihood contribution of market t is\n",
    "$$\n",
    "\\ell_t(\\theta)=y_t'\\ln P_t(u_t|\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_t$ and $\\theta$ - calculates $u_t=\\mathbf{X}_t\\beta$ and then calls the fixed point routine described above. That routine will return the choice probabilities $P_t(u_t|\\theta)$, and we can then evaluate $\\ell_t(\\theta)$. Using our above defined functions we now construct precisely such an estimation procedure.\n",
    "\n",
    "In addition, when maximizing the likelihood we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_t= P_t(u_t|\\theta)$, then we have\n",
    "$$\n",
    "\\nabla_\\theta \\ln P_t(u_t|\\theta)=\\mathrm{diag}(q_t)^{-1}\\left(\\nabla_{qq}^2\\Omega_t(q_t|\\lambda)^{-1}-q_tq_t' \\right)\\left[\\mathbf{X}_t,-\\nabla_{q,\\lambda}^2 \\Omega_t(q_t|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P_t(u_t|\\theta)$ and the last term is a block matrix of size $J\\times (K+G)$. The latter cross derivative $\\nabla_{q,\\lambda}^2 \\Omega_t(q_t|\\lambda)$ may be computed using the identity $\\nabla_{q,\\lambda}^2 \\Omega_t(q_t|\\lambda)_g = -\\ln(q) + (\\Psi^g)' \\ln(\\Psi^g q) - \\varphi^g$ for each row $g=1,\\ldots,G$. The derivative of the log-likelihood function can then be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_t(\\theta)= y_t' \\left(\\nabla_\\theta \\ln P_t(u_t|\\theta)\\right) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_loglikelihood(Theta, y, x, sample_share, model):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (T,) of Similarity loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "    ccp_hat = Similarity_ccp(Theta, x, model) # Find Similarity choice probabilities\n",
    "    sum_lambdaplus = np.array([theta for theta in Theta[K:] if theta >0]).sum() # Get sum of positive Lambda's\n",
    "    \n",
    "    ll=np.empty((T,))\n",
    "    for t in np.arange(T):\n",
    "        ll[t] = sample_share[t]*(y[t].T@np.log(ccp_hat[t])) # np.einsum('j,j', y[t], np.log(ccp_hat[t], out = -np.inf*np.ones_like(ccp_hat[t]), where = (ccp_hat[t] > 0)))\n",
    "\n",
    "    print([sum_lambdaplus, -ll.mean()])\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_Similarity(Theta, y, x, sample_share, model):\n",
    "    ''' The negative loglikelihood criterion to minimize\n",
    "    '''\n",
    "    Q = -Similarity_loglikelihood(Theta, y, x, sample_share, model)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the derivative of the loglikehood wrt. parameters $\\nabla_\\theta \\ell_t(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation(q, model):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    # Get psidim object from the model specification\n",
    "    psidim = model['psi_3d']\n",
    "    phi = model['phi']\n",
    "\n",
    "    # Get the amount of markets\n",
    "    T = len(q.keys())\n",
    "\n",
    "    # Initialize Z; the cross differential of the pertubation function\n",
    "    Z = {}\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        log_q = log_q = np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0))\n",
    "        psidim_t = psidim[t][1:,:,:] # Get each of the Psi^g nesting matrices\n",
    "        psiq = psidim_t @ q[t] # Computes a matrix product\n",
    "        log_psiq = np.log(psiq, out = np.NINF*np.ones_like(psiq), where = (psiq > 0))\n",
    "        Z[t] = - log_q[:,None] + np.einsum('dkj,dk->jd', psidim_t, log_psiq) - phi[t] # Compute cross differential\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation_old(q, psi, phi):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        psi: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    J = np.array([q[t].shape[0] for t in np.arange(T)])\n",
    "    G = np.int32((psi[0].shape[0] / J[0]) - 1)\n",
    "    \n",
    "    Z = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        log_q = np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0))\n",
    "        Psi_t = psi[t]\n",
    "        Z_t = np.empty((J[t], G))\n",
    "\n",
    "        for g in np.arange(1,G+1):\n",
    "            Psi_d = Psi_t[g*J[t]:(g+1)*J[t],:]\n",
    "            Psiq = np.einsum('cj,j->c', Psi_d, q[t])\n",
    "            log_psiq = np.log(Psiq, out = -np.inf*np.ones_like(Psiq), where = (Psiq > 0))\n",
    "            Z_t[:,g-1] = -log_q + np.einsum('cj,c->j', Psi_d, log_psiq) - phi[t][:,g-1]\n",
    "\n",
    "        Z[t] = Z_t\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_theta_grad_log_ccp(Theta, x, model):\n",
    "    '''\n",
    "    This function calculates the derivative of the Similarity log ccp's wrt. parameters theta\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K+G) of derivatives of the Similarity log ccp's wrt. parameters theta for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "\n",
    "    q = Similarity_ccp(Theta, x, model) # Find choice probabilities\n",
    "\n",
    "    Z = cross_grad_pertubation(q, model) # Find cross differentials of the pertubation function\n",
    "    u_grad = Similarity_u_grad_Log_ccp(q, x, Theta, model)  # Find the gradient of log ccp's wrt. utilities\n",
    "    Grad={}\n",
    "\n",
    "    for t in range(T):\n",
    "        G = np.concatenate((x[t], -Z[t]), axis = 1) # Compute the block matrix\n",
    "        Grad[t] = u_grad[t] @ G # Compute the derivative wrt. parameters\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_score(Theta, y, x, sample_share, model):\n",
    "    '''\n",
    "    This function calculates the score of the Similarity loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of Similarity scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = Similarity_theta_grad_log_ccp(Theta, x, model) # Find derivatives of Similarity log ccp's wrt. parameters theta\n",
    "    D = len(Theta) # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] =sample_share[t]*(log_ccp_grad[t].T@y[t]) #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_score_unweighted(Theta, y, x, model):\n",
    "    '''\n",
    "    This function calculates the score of the Similarity loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of Similarity scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = Similarity_theta_grad_log_ccp(Theta, x, model) # Find derivatives of Similarity log ccp's wrt. parameters theta\n",
    "    D = log_ccp_grad[0].shape[1] # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] = log_ccp_grad[t].T@y[t] #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_Similarity_score(Theta, y, x, sample_share, model):\n",
    "    ''' The derivative of the negative loglikelihood criterion\n",
    "    '''\n",
    "    return -Similarity_score(Theta, y, x, sample_share, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the score function\n",
    "\n",
    "To make sure that our optimization procedure works as intended and is precise, we may calculate the normed difference of the numerical and our analytical gradients of the likelihood function $\\ell_t(\\theta)$ at some parameter $\\hat \\theta^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analyticgrad(y, x, theta, sample_share, model, delta = 1.0e-8):\n",
    "    ''' \n",
    "    This function calculates the numerical and the analytical score functions at a given parameter \\theta aswell the norm of their difference\n",
    "\n",
    "    Args:\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        delta: the incremental change in the argument, a float, used in calculating numerical gradients\n",
    "\n",
    "    Returns.\n",
    "        normdiff: a float of the euclidean norm of the difference between the numerical and analytical score functions at \\theta\n",
    "        angrad: a numpy array (T,K+G) of analytical Similarity scores\n",
    "        numgrad: a numpy array (T,K+G) of numerical Similarity scores\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "    G = len(theta[K:])\n",
    "\n",
    "    numgrad = np.empty((T, K+G))\n",
    "\n",
    "    for i in np.arange(K+G):\n",
    "        vec = np.zeros((K+G,))\n",
    "        vec[i] = 1\n",
    "        numgrad[:,i] = (Similarity_loglikelihood(theta + delta*vec, y, x, sample_share, model) - Similarity_loglikelihood(theta, y, x, sample_share, model)) / delta\n",
    "\n",
    "    angrad = Similarity_score(theta, y, x, sample_share, model)\n",
    "\n",
    "    normdiff = la.norm(angrad - numgrad)\n",
    "    \n",
    "    return normdiff, angrad, normdiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diff, an, num = test_analyticgrad(y, x, theta0, pop_share, Model)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors in Maximum Likelihood estimation\n",
    "\n",
    "As usual we may consistently estimate the covariance matrix of the Similarity maximum likelihood estimator $\\hat \\theta^{\\text{MLE}}$ by the inverse information matrix:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\left( \\sum_{t=1}^T s_t\\nabla_\\theta \\ell_t \\left(\\hat \\theta^{\\text{MLE}}\\right) \\nabla_\\theta \\ell_t \\left(\\hat \\theta^{\\text{MLE}}\\right)' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Accordingly we may find the estimated standard error of parameter $d = 1,\\ldots,K+G$ as the squareroot of the $d$'th diagonal entry of $\\hat \\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_d = \\sqrt{\\hat \\Sigma_{dd}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_se(score, sample_share, N):\n",
    "    '''\n",
    "    This function computes the asymptotic standard errors of the MLE.\n",
    "\n",
    "    Args.\n",
    "        score: a numpy array (T,K+G) of Similarity scores as outputted by the function 'Similarity_score_unweighted'.\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns\n",
    "        SE: a numpy array (K+G,) of asymptotic Similarity MLE standard errors\n",
    "    '''\n",
    "\n",
    "    SE = np.sqrt(np.diag(la.inv(np.einsum('td,tm->dm', sample_share[:,None]*score, score))) / N) # Compute standard errors by taking the squareroot of the diagonal elements of the variance estimate\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        SE: a numpy array (K+G,) of asymptotic Similarity MLE standard errors\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        N: an integer giving the number of observations\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated asymptotic p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE # Compute two-sided t-tests\n",
    "    p = 2*scstat.t.sf(T, df = N-1) # Compute p-values\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_Similarity(f, Theta0, y, x, sample_share, model, N, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given starting values and variables necessary in the Similarity Model specification.\n",
    "\n",
    "    Args:\n",
    "        f: a function to minimize,\n",
    "        Theta0 : a numpy array (K+G,) of initial guess parameters (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations,\n",
    "        Analytic_jac: a boolean. Default value is 'True'. If 'True' the analytic jacobian of the Similarity loglikelihood function is used in estimation. Else the numerical jacobian is used.\n",
    "        options: dictionary with options for the optimizer (e.g. disp=True which tells it to display information at termination.)\n",
    "    \n",
    "    Returns:\n",
    "        res: a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(f(Theta, y, x, sample_share, model))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_Similarity_score(Theta, y, x, sample_share, model), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Theta0.tolist(), options=options, jac=Grad, **kwargs) # optimize.minimize takes a list of parameters Theta0 (not a numpy array) as initial guess.\n",
    "    \n",
    "    # Find estimated standard errors, t-tests, and p-values\n",
    "    se = Similarity_se(Similarity_score_unweighted(result.x, y, x, model), sample_share, N) # Calculate standard errors using the unweighted score contributions from each market\n",
    "    T,p = Similarity_t_p(se, result.x, N)\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'theta': result.x,\n",
    "        'se': se,\n",
    "        't': T,\n",
    "        'p': p,\n",
    "        'success':  result.success, # bool, whether convergence was succesful 1\n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001529\n",
      "         Iterations: 26\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.ones((K,))\n",
    "\n",
    "# Estimate the model\n",
    "Logit_beta = estimate_logit(q_logit, beta_0, y, x, sample_share=pop_share, Analytic_jac=True)['beta']\n",
    "Logit_SE = logit_se(logit_score_unweighted(Logit_beta, y, x), pop_share, N)\n",
    "Logit_t, Logit_p = logit_t_p(Logit_beta, logit_score_unweighted(Logit_beta, y, x), pop_share, N)\n",
    "\n",
    "# Initialize \\theta^0\n",
    "theta0 = np.append(Logit_beta,lambda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.001528535134161618]\n",
      "[3.7176505867959405e-06, 0.001528530844629761]\n",
      "[1.8588252933979702e-05, 0.001528514281678381]\n",
      "[7.807066232271476e-05, 0.0015284575876433064]\n",
      "[0.0002888968393993265, 0.0015283779712649036]\n",
      "[0.0003156820978933802, 0.0015283756589398688]\n",
      "[0.0004228231318695951, 0.001528366650756558]\n",
      "[0.0008514977339305401, 0.0015283344814752554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002552430433766674, 0.0015282508819820435]\n",
      "[0.011407411931886541, 0.001528002344870776]\n",
      "[0.0336266372510129, 0.001527432169794866]\n",
      "[0.07652390229927372, 0.0015264248802469536]\n",
      "[0.1598061804339339, 0.0015247252520192492]\n",
      "[0.3261793453534204, 0.0015224054228309036]\n",
      "[0.41714512626147443, 0.0015215190856618535]\n",
      "[0.43667551144630096, 0.0015212756051922887]\n",
      "[0.4294172691651903, 0.0015212519744114967]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001521\n",
      "         Iterations: 11\n",
      "         Function evaluations: 17\n",
      "         Gradient evaluations: 17\n"
     ]
    }
   ],
   "source": [
    "resMLE = estimate_Similarity(q_Similarity, theta0, y, x, pop_share, Model, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_table(theta, se, N, x_vars, nest_vars):\n",
    "    '''\n",
    "    This function constructs a regression table based on Similarity parameter standard error estimates\n",
    "\n",
    "    Args:\n",
    "        theta: a (K+G,) numpy array of estimated parameters\n",
    "        se: a (K+G,) numpy array of estimated standard errors\n",
    "        N: an integer; the number of observations\n",
    "        x_vars: a list containing the names of the covariates\n",
    "        nest_vars: a list containing the names of the nesting groups\n",
    "\n",
    "    Returns.\n",
    "        table: a pandas dataframe structured as a regression table w. parameter estiamtes, standard errors, t-tests, and p-values  \n",
    "    '''\n",
    "    Similarity_t, Similarity_p = Similarity_t_p(se, theta, N) # Get t-test values and p values\n",
    "\n",
    "    if OO:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]] # Set the names of the covariates and the nesting groups as the index\n",
    "    else:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]] # -=-\n",
    "\n",
    "    table  = pd.DataFrame({'theta': [ str(np.round(theta[i], decimals = 4)) + '***' if Similarity_p[i] <0.01 else str(np.round(theta[i], decimals = 3)) + '**' if Similarity_p[i] <0.05 else str(np.round(theta[i], decimals = 3)) + '*' if Similarity_p[i] <0.1 else np.round(theta[i], decimals = 3) for i in range(len(theta))], # Give stars to parameter estimates according to t-tests at levels of significance 0.1, 0.05, and 0.01\n",
    "                'se' : np.round(se, decimals = 5),\n",
    "                't (theta == 0)': np.round(Similarity_t, decimals = 3),\n",
    "                'p': np.round(Similarity_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.5498***</td>\n",
       "      <td>0.43873</td>\n",
       "      <td>5.812</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.22618</td>\n",
       "      <td>1.408</td>\n",
       "      <td>0.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.457*</td>\n",
       "      <td>0.27293</td>\n",
       "      <td>1.676</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.9464***</td>\n",
       "      <td>0.22234</td>\n",
       "      <td>4.256</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.9569***</td>\n",
       "      <td>0.30143</td>\n",
       "      <td>6.492</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.0975***</td>\n",
       "      <td>0.42346</td>\n",
       "      <td>4.953</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.0849***</td>\n",
       "      <td>0.31568</td>\n",
       "      <td>6.604</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.7439***</td>\n",
       "      <td>0.11864</td>\n",
       "      <td>6.270</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.3924***</td>\n",
       "      <td>0.27032</td>\n",
       "      <td>5.151</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.10572</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.8436***</td>\n",
       "      <td>0.09099</td>\n",
       "      <td>9.272</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>0.9939***</td>\n",
       "      <td>0.23615</td>\n",
       "      <td>4.209</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>1.0494***</td>\n",
       "      <td>0.12909</td>\n",
       "      <td>8.130</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>0.7984***</td>\n",
       "      <td>0.11243</td>\n",
       "      <td>7.101</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>0.8222***</td>\n",
       "      <td>0.13417</td>\n",
       "      <td>6.128</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>0.7241***</td>\n",
       "      <td>0.12674</td>\n",
       "      <td>5.713</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>0.9519***</td>\n",
       "      <td>0.18761</td>\n",
       "      <td>5.074</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>0.971***</td>\n",
       "      <td>0.24506</td>\n",
       "      <td>3.962</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>0.909***</td>\n",
       "      <td>0.15489</td>\n",
       "      <td>5.868</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.8222***</td>\n",
       "      <td>0.12488</td>\n",
       "      <td>6.584</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>1.0521***</td>\n",
       "      <td>0.11617</td>\n",
       "      <td>9.056</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>0.8661***</td>\n",
       "      <td>0.16136</td>\n",
       "      <td>5.368</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>0.9185***</td>\n",
       "      <td>0.25339</td>\n",
       "      <td>3.625</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>0.9261***</td>\n",
       "      <td>0.12006</td>\n",
       "      <td>7.714</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>0.9596***</td>\n",
       "      <td>0.23619</td>\n",
       "      <td>4.063</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>0.6353***</td>\n",
       "      <td>0.12252</td>\n",
       "      <td>5.185</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>0.8342***</td>\n",
       "      <td>0.19716</td>\n",
       "      <td>4.231</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.9485***</td>\n",
       "      <td>0.15793</td>\n",
       "      <td>6.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>0.8852***</td>\n",
       "      <td>0.14316</td>\n",
       "      <td>6.183</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>0.906***</td>\n",
       "      <td>0.19149</td>\n",
       "      <td>4.731</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>0.9336***</td>\n",
       "      <td>0.12241</td>\n",
       "      <td>7.627</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.8577***</td>\n",
       "      <td>0.11582</td>\n",
       "      <td>7.406</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>1.0311***</td>\n",
       "      <td>0.11766</td>\n",
       "      <td>8.763</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>0.5224***</td>\n",
       "      <td>0.12264</td>\n",
       "      <td>4.260</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>0.8725***</td>\n",
       "      <td>0.23902</td>\n",
       "      <td>3.650</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>0.8056***</td>\n",
       "      <td>0.15549</td>\n",
       "      <td>5.181</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.98588</td>\n",
       "      <td>1.013</td>\n",
       "      <td>0.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>0.972**</td>\n",
       "      <td>0.41495</td>\n",
       "      <td>2.342</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>0.9887***</td>\n",
       "      <td>0.25101</td>\n",
       "      <td>3.939</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>0.9473***</td>\n",
       "      <td>0.15076</td>\n",
       "      <td>6.283</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>0.9933***</td>\n",
       "      <td>0.18372</td>\n",
       "      <td>5.406</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>0.9673***</td>\n",
       "      <td>0.13301</td>\n",
       "      <td>7.272</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>0.9978***</td>\n",
       "      <td>0.29060</td>\n",
       "      <td>3.433</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>0.8721***</td>\n",
       "      <td>0.10862</td>\n",
       "      <td>8.029</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>0.8744***</td>\n",
       "      <td>0.15332</td>\n",
       "      <td>5.703</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>0.8032***</td>\n",
       "      <td>0.14780</td>\n",
       "      <td>5.435</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>0.971***</td>\n",
       "      <td>0.25528</td>\n",
       "      <td>3.804</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.6015***</td>\n",
       "      <td>0.03366</td>\n",
       "      <td>47.574</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.3074***</td>\n",
       "      <td>0.02881</td>\n",
       "      <td>10.670</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.3473***</td>\n",
       "      <td>0.05590</td>\n",
       "      <td>6.213</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.07304</td>\n",
       "      <td>1.284</td>\n",
       "      <td>0.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.4534***</td>\n",
       "      <td>0.13370</td>\n",
       "      <td>3.391</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.1073***</td>\n",
       "      <td>0.03976</td>\n",
       "      <td>2.698</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02550</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.03359</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.061*</td>\n",
       "      <td>0.03187</td>\n",
       "      <td>1.902</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0774***</td>\n",
       "      <td>0.02724</td>\n",
       "      <td>2.842</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.1292***</td>\n",
       "      <td>0.01933</td>\n",
       "      <td>6.685</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0528***</td>\n",
       "      <td>0.01413</td>\n",
       "      <td>3.734</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.01348</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0728***</td>\n",
       "      <td>0.02374</td>\n",
       "      <td>3.065</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.1813***</td>\n",
       "      <td>0.01269</td>\n",
       "      <td>14.288</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.044**</td>\n",
       "      <td>0.01847</td>\n",
       "      <td>2.361</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.01933</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>0.1534***</td>\n",
       "      <td>0.03315</td>\n",
       "      <td>4.628</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables          theta       se  t (theta == 0)      p\n",
       "in_out        -2.5498***  0.43873           5.812  0.000\n",
       "cy                -0.318  0.22618           1.408  0.159\n",
       "hp               -0.457*  0.27293           1.676  0.094\n",
       "we            -0.9464***  0.22234           4.256  0.000\n",
       "le            -1.9569***  0.30143           6.492  0.000\n",
       "wi            -2.0975***  0.42346           4.953  0.000\n",
       "he            -2.0849***  0.31568           6.604  0.000\n",
       "li            -0.7439***  0.11864           6.270  0.000\n",
       "sp            -1.3924***  0.27032           5.151  0.000\n",
       "ac                -0.055  0.10572           0.522  0.601\n",
       "pr             0.8436***  0.09099           9.272  0.000\n",
       "brand_2        0.9939***  0.23615           4.209  0.000\n",
       "brand_3        1.0494***  0.12909           8.130  0.000\n",
       "brand_4        0.7984***  0.11243           7.101  0.000\n",
       "brand_5        0.8222***  0.13417           6.128  0.000\n",
       "brand_6        0.7241***  0.12674           5.713  0.000\n",
       "brand_7        0.9519***  0.18761           5.074  0.000\n",
       "brand_8         0.971***  0.24506           3.962  0.000\n",
       "brand_9         0.909***  0.15489           5.868  0.000\n",
       "brand_10       0.8222***  0.12488           6.584  0.000\n",
       "brand_11       1.0521***  0.11617           9.056  0.000\n",
       "brand_12       0.8661***  0.16136           5.368  0.000\n",
       "brand_13       0.9185***  0.25339           3.625  0.000\n",
       "brand_14       0.9261***  0.12006           7.714  0.000\n",
       "brand_15       0.9596***  0.23619           4.063  0.000\n",
       "brand_16       0.6353***  0.12252           5.185  0.000\n",
       "brand_17       0.8342***  0.19716           4.231  0.000\n",
       "brand_18       0.9485***  0.15793           6.006  0.000\n",
       "brand_19       0.8852***  0.14316           6.183  0.000\n",
       "brand_20        0.906***  0.19149           4.731  0.000\n",
       "brand_21       0.9336***  0.12241           7.627  0.000\n",
       "brand_22       0.8577***  0.11582           7.406  0.000\n",
       "brand_23       1.0311***  0.11766           8.763  0.000\n",
       "brand_24       0.5224***  0.12264           4.260  0.000\n",
       "brand_25       0.8725***  0.23902           3.650  0.000\n",
       "brand_26       0.8056***  0.15549           5.181  0.000\n",
       "brand_27           0.999  0.98588           1.013  0.311\n",
       "brand_28         0.972**  0.41495           2.342  0.019\n",
       "brand_29       0.9887***  0.25101           3.939  0.000\n",
       "brand_30       0.9473***  0.15076           6.283  0.000\n",
       "brand_31       0.9933***  0.18372           5.406  0.000\n",
       "brand_32       0.9673***  0.13301           7.272  0.000\n",
       "brand_33       0.9978***  0.29060           3.433  0.001\n",
       "brand_34       0.8721***  0.10862           8.029  0.000\n",
       "brand_35       0.8744***  0.15332           5.703  0.000\n",
       "brand_36       0.8032***  0.14780           5.435  0.000\n",
       "brand_37        0.971***  0.25528           3.804  0.000\n",
       "home_2         1.6015***  0.03366          47.574  0.000\n",
       "cla_2          0.3074***  0.02881          10.670  0.000\n",
       "cla_3          0.3473***  0.05590           6.213  0.000\n",
       "cla_4              0.094  0.07304           1.284  0.199\n",
       "cla_5          0.4534***  0.13370           3.391  0.001\n",
       "group_in_out   0.1073***  0.03976           2.698  0.007\n",
       "group_cy            0.01  0.02550           0.396  0.692\n",
       "group_hp           0.025  0.03359           0.741  0.458\n",
       "group_we          0.061*  0.03187           1.902  0.057\n",
       "group_le      -0.0774***  0.02724           2.842  0.004\n",
       "group_wi      -0.1292***  0.01933           6.685  0.000\n",
       "group_he      -0.0528***  0.01413           3.734  0.000\n",
       "group_li           0.011  0.01348           0.794  0.427\n",
       "group_sp      -0.0728***  0.02374           3.065  0.002\n",
       "group_ac      -0.1813***  0.01269          14.288  0.000\n",
       "group_brand      0.044**  0.01847           2.361  0.018\n",
       "group_home         0.019  0.01933           0.972  0.331\n",
       "group_cla      0.1534***  0.03315           4.628  0.000"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Similarity_theta = resMLE['theta']\n",
    "Similarity_SE = resMLE['se']\n",
    "Similarity_t, Similarity_p = Similarity_t_p(Similarity_SE, Similarity_theta, N)\n",
    "reg_table(Similarity_theta, Similarity_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4294172691651903"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in Similarity_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative approach\n",
    "\n",
    "The log-likelihood function is not globally concave, and finding the global optimum can be difficult. Using the estimation procedure of Fosgerau et. al. (2023 working paper), we can instead fit the parameters using the first-order conditions  for optimality $0=u(X_t,\\beta) - \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)$ and the observed market shares $\\hat q^0_t$ or some non-parametric estimate of the CCP's. The estimator takes the form\n",
    "\n",
    "$$\n",
    "\\hat \\theta^0=\\arg \\min_{\\theta} \\sum_t s_t \\hat \\varepsilon^0_t(\\theta)'\\hat W^0_t\\hat \\varepsilon^0 _t(\\theta),\n",
    "$$\n",
    "where $\\hat W^0_t$ is a positive semidefinite weight matrix, $s_t$ is market $t$'s share of the total population\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta) = \\hat D^0_t(u(X_t,\\beta)- \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)) ,\n",
    "$$\n",
    "is the residual of the first-order conditions utilizing the logit derivatives of CCP's wrt. utilities\n",
    "$$\n",
    "\\hat D^0_t=\\textrm{diag}(\\hat q^0_t)-\\hat q^0_t (\\hat q^0_t)'.\n",
    "$$\n",
    "Using equation (...) above, we have that $\\hat \\epsilon_t$ is a linear function of $\\theta$,\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t \\left(\\hat G^0_t\\theta- \\ln \\hat q^0_t\\right)\\equiv \\hat A^0_t\\theta-\\hat r^0_t.\n",
    "$$\n",
    "With $\\hat G^0_t = [X_t, -\\nabla^2_{q,\\lambda} \\Omega (q^0_t|\\lambda)]$, $\\hat A^0_t = \\hat D^0_t \\hat G^0_t$ and $\\hat r^0_t = \\hat D^0_t \\ln \\hat q^0_t $. Using linearity, the weighted least squares criterion has a unique closed form solution,\n",
    "$$\n",
    "\\hat \\theta^0 =\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat A^0_t \\right)^{-1}\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat r_t^0 \\right)\n",
    "$$\n",
    "\n",
    "In our estimation procedure we will use the inverse of the matrix with the observed market shares CCP's $\\hat q^0_t$ along its main diagonal $\\hat W^0_t = \\mathrm{diag}(\\hat q^0_t)^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_array(q, x, model):\n",
    "    ''' \n",
    "    This function calculates the G block matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        G: a dictionary  of T numpy arrays (J[t],K+G): a G matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    Z = cross_grad_pertubation(q, model) # Find the cross derivative of the pertubation function \\Omega wrt. lambda and ccp's q\n",
    "    G = {t: np.concatenate((x[t],-Z[t]), axis=1) for t in np.arange(T)} # Join block matrices along 2nd dimensions  s.t. last dimension is K+G (same dimension as theta)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_array(q):\n",
    "    '''\n",
    "    This function calculates the D matrix - the logit derivative of ccp's wrt. utilities\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "\n",
    "    Returns\n",
    "        D: a dictionary of T numpy arrays (J[t],J[t]) of logit derivatives of ccp's wrt. utilities for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = {t: np.diag(q[t]) - np.einsum('j,k->jk', q[t], q[t]) for t in np.arange(T)} # Compute logit derivatives of ccp's wrt. utilities\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_array(q, x, model):\n",
    "    '''\n",
    "    This function calculates the A matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        A: a dictionary  of T numpy arrays (J[t],K+G): an A matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    D = D_array(q) # Compute the derivatives of logit ccp's\n",
    "    G = G_array(q, x, model) # Get the G block matrix\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)} # Compute a matrix product for each market t\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_array(q):\n",
    "    '''\n",
    "    This function calculates 'r'; the logarithm of observed or nonparametrically estimated market shares\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "    \n",
    "    Returns\n",
    "        r: a dictionary of T numpy arrays (J[t],) of the log of ccp's for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = D_array(q) \n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)} # Take logs of ccp-s\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) for t in np.arange(T)} # Compute matrix product\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS_init(q, x, sample_share, model, N):\n",
    "    ''' \n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the initial FKN parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "\n",
    "    A = A_array(q, x, model)\n",
    "    r = r_array(q)\n",
    "\n",
    "    d = A[0].shape[1] # Get the total number of parameters; this is equal to K+G\n",
    "    \n",
    "    # Initialize AWA and AWr matrices\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1/q[t], A[t], optimize = True) # Fast product using that the weights 'W' are diagonal.\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1/q[t], r[t], optimize = True)\n",
    "    \n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0)) # Solve system of equations AWA.sum()*theta = AWr.sum() for parameter estimates theta\n",
    "    #se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "    \n",
    "    return theta_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the observed market shares we may thus find initial parameter estimates $\\hat \\theta^0$ as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFKN0 = WLS_init(y, x, pop_share, Model, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.218988344960131"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in thetaFKN0[K:] if p > 0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for parameter bounds\n",
    "\n",
    "As we see above, the least squares estimator is not guaranteed to respect the parameter bounds $\\sum_g \\hat \\lambda_g<1$. We can use that if we replace $\\hat q^0_t$ with the choice probabilities from the maximum likelihood estimator of the Logit Model, $\\hat q^{logit}_t\\propto \\exp\\{X_t\\hat \\beta^{logit}\\}$, and plug these choice probabilities into the WLS estimator described above, it will return $\\hat \\theta=(\\hat \\beta^{logit},0,\\ldots,0)$ as the parameter estimate. Let $\\hat q_t(\\alpha)$ denote the weighted average of the logit probabilites and the market shares,\n",
    "$$\n",
    "\\hat q_t(\\alpha) =(1-\\alpha) \\hat q^{logit}_t+\\alpha \\hat q^0_t.\n",
    "$$\n",
    " Let $\\hat \\theta^0(\\alpha)$ denote the resulting parameter vector. We perform a line search for values of $\\alpha$, $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\ldots)$ until $\\hat \\theta^0(\\alpha)$ yields a feasible parameter vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogL(Theta, y, x, sample_share, model):\n",
    "    ''' \n",
    "    A function giving the mean Similarity loglikehood evaluated at data and an array of parameters 'Theta'\n",
    "    '''\n",
    "    return np.mean(Similarity_loglikelihood(Theta, y, x, sample_share, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineSearch(Logit_Beta, q_obs, x, sample_share, model, N):\n",
    "    '''\n",
    "    This function performs a line search to find feasible lambda parameters\n",
    "\n",
    "    Args:\n",
    "        Logit_beta: a (K,) numpy array of estimated beta parameters from a corresponding Logit Model\n",
    "        q_obs: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns.\n",
    "        theta_alpha: a (K+G,) numpy array of feasible parameters found by line search\n",
    "    '''\n",
    "\n",
    "    # Get dimensions of data\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit_ccp(Logit_Beta, x)\n",
    "\n",
    "    # Search over alphas s.t. alpha = (1/2)^{k} for some positive integer k\n",
    "    alpha0 = 0.5\n",
    "\n",
    "    for k in np.arange(1,100):\n",
    "\n",
    "        # Set alpha\n",
    "        alpha = alpha0**k \n",
    "        \n",
    "        # Compute convex combination of ccp's\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha = WLS_init(q_alpha, x, sample_share, model, N) # Compute initial FKN parameters but using q_alpha ccp's \n",
    "\n",
    "        lambda_alpha = theta_alpha[K:] # Find lambda parameters\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0]) # Find positive lambda parameters\n",
    "\n",
    "        if pos_pars.sum() <1:\n",
    "            break # Break if positive parameters sum to less than 1\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(Logit_Beta, y, x, sample_share, model, N, num_alpha = 5):\n",
    "    '''\n",
    "    This function performs a grid search on the unit interval to find feasible parameters \\theta\n",
    "\n",
    "    Args:\n",
    "        Logit_beta: a (K,) numpy array of estimated beta parameters from a corresponding Logit Model\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations,\n",
    "        num_alpha: an integer of the number of alphas for which the search is to be performed\n",
    "\n",
    "    Returns.\n",
    "        theta_star: a (K+G,) numpy array of feasible parameters found by grid search\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "    J0 = x[0].shape[0]\n",
    "    psi_3d0 = model['psi_3d'][0]\n",
    "    G = np.int64(psi_3d0.shape[0] - 1)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    LogL_alpha = np.empty((num_alpha,))\n",
    "    theta_alpha = np.empty((num_alpha,K+G))\n",
    "\n",
    "    for k in np.arange(len(alpha_line)):\n",
    "\n",
    "        alpha = alpha_line[k]\n",
    "\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha[k,:] = WLS_init(q_alpha, x, sample_share, model, N)\n",
    "\n",
    "        #lambda_inout = theta_alpha[k,K]\n",
    "        lambda_alpha = theta_alpha[k,K:] # theta_alpha[k,K+1:]\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if (pos_pars.sum() >= 1): #|(lambda_inout >= 1)\n",
    "            LogL_alpha[k] = np.NINF\n",
    "        else:\n",
    "            LogL_alpha[k] = LogL(theta_alpha[k,:], y, x, sample_share, model)\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "    alpha_star = np.argmax(LogL_alpha)\n",
    "    theta_star = theta_alpha[alpha_star,:]\n",
    "\n",
    "    return theta_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the line search method we find corressponding parameters $\\hat \\theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_alpha = LineSearch(Logit_beta, y, x, pop_share, Model, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7549534173977517, 0.0015132588931061905]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0015132588931061905"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_Similarity(theta_alpha, y, x, pop_share, Model).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated FKN estimator\n",
    "\n",
    "The initial estimator $\\hat \\theta^0$ is usually biased. However this can be accommodated for via iterating a similar estimator on the estimator $\\hat \\theta^0$. The iterated estimator is as the initial one, except there is an additional term on $\\hat \\varepsilon$. First, we update the choice probabilities,\n",
    "$$\n",
    "\\hat q^k_t=p(\\mathbf X_t,\\hat \\theta^{k-1})\\\\\n",
    "$$\n",
    "Then we assign\n",
    "$$\n",
    "\\hat D^k_t=\\nabla^2_{qq}\\Omega(\\hat q_t^k|\\hat \\lambda^{k-1})^{-1}-(\\hat q^k_t \\hat q^k_t)'\n",
    "$$\n",
    "and then construct the residual\n",
    "$$\n",
    "\\hat \\varepsilon^k_t(\\theta)=\\hat D^k_t\\left( u(x_t,\\beta)-\\nabla_q \\Omega(\\hat q_t^k|\\lambda)\\right) -y_t+\\hat q_t^k,\n",
    "$$\n",
    "Which can once again be simplified as\n",
    "$$\n",
    "\\hat \\varepsilon^k_t(\\theta)= \\hat A_t^k \\theta-\\hat r^k_t,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat A^k_t=\\hat D_t^k\\hat G^k_t, \\hat r_t^k =\\hat D^k_t\\ln \\hat q_t^k-y_t\n",
    "$$\n",
    "and where $\\hat G^k_t$ is constructed as in the initial estimator. Using the weighted least squares estimator with weights $\\hat W_t^k=\\textrm{diag}(\\hat q^k_t)^{-1}$, we get the estimator\n",
    "$$\n",
    "\\hat \\theta^k = \\arg \\min_{\\theta}\\frac{1}{T}\\sum_t \\hat \\varepsilon^k_t(\\theta)'\\hat W_t^k \\hat \\varepsilon^k_t(\\theta).\n",
    "$$\n",
    "We can once again solve it in closed form as\n",
    "$$\n",
    "\\hat \\theta^k =\\left( \\frac{1}{T}\\sum_t  ( \\hat A^k_t)'\\hat W_t^k \\hat A^k_t\\right)^{-1}\\left( \\frac{1}{T}\\sum_t (\\hat A_t^k)'\\hat W_t^k \\hat r_t^k\\right)\n",
    "$$\n",
    "\n",
    "FKN (2021) show that the estimator $\\hat \\theta^k$ converges to the (true) MLE $\\hat \\theta^{\\text{MLE}}$ as the number of iterations approaches infinity, avoiding thereby \n",
    "\n",
    "Now we implement this procedure and iterate starting from our initial guess $\\hat \\theta^{*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS(Theta, y, x, sample_share, model, N):\n",
    "    '''\n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the iterated parameter estimates.\n",
    "\n",
    "    Args:\n",
    "        Theta: a (K+G,) numpy array of previously estimated \\theta parameters\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations,\n",
    "\n",
    "    Returns.\n",
    "        theta_hat: a (K+G,) numpy array of iterated FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for iterated FKN parameter estimates\n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta.shape[0]\n",
    "    \n",
    "    # Get ccp's\n",
    "    q = Similarity_ccp(Theta, x, model)\n",
    "\n",
    "    # Construct A\n",
    "    D = ccp_gradient(q, x, Theta, model) # A is here constructed using the Similarity derivative of ccp's wrt. utilities instead of the Logit derivative\n",
    "    G = G_array(q, x, model)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "\n",
    "    # Construct r\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where=(q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) + y[t] for t in np.arange(T)} # r = D %o% log(q) + y\n",
    "\n",
    "    # Estimate parameters\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1./q[t], A[t], optimize = True)\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1./q[t], r[t], optimize = True)\n",
    "\n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "\n",
    "    return theta_hat,se_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKN_estimator(logit_beta, q_obs, x, sample_share, model, N, tol = 1.0e-15, max_iters = 1000):\n",
    "    '''\n",
    "    This function estimates the Similarity Model via the FKN estimator\n",
    "\n",
    "    Args:\n",
    "        logit_beta: a (K,) numpy array of estimated beta parameters from a corresponding Logit Model\n",
    "        q_obs: a dictionary of T numpy arrays (J[t],) of observed choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns.\n",
    "        res: a dictionary containing FKN parameter estimates, standard errors, iterations to convergence, etc.\n",
    "    '''\n",
    "\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Get initial FKN parameters using observed market shares\n",
    "    theta_init = WLS_init(q_obs, x, sample_share, model, N) \n",
    "    \n",
    "    # If the positive nesting paramters sum to more than 1, then perform a linesearch.\n",
    "    if (np.array([p for p in theta_init[K:] if p>0]).sum() >= 1):\n",
    "        theta_star = LineSearch(logit_beta, q_obs, x, sample_share, model, N)\n",
    "        theta0 = theta_star\n",
    "    else:\n",
    "        theta0 = theta_init\n",
    "\n",
    "    '''logl0 = LogL(theta0, q_obs, x, sample_share, psi, nest_count)'''\n",
    "    \n",
    "    # Debiasing iterations\n",
    "    for k in np.arange(max_iters):\n",
    "        # Compute iterated FKN parameters and standard errors\n",
    "        theta1, se1 = WLS(theta0, q_obs, x, sample_share, model, N)\n",
    "\n",
    "        '''logl1=LogL(theta1, q_obs, x, sample_share, psi, nest_count)\n",
    "        \n",
    "        for m in range(10):\n",
    "            if logl1<logl0:\n",
    "                theta1=0.5*theta0+0.5*theta1\n",
    "                logl1=LogL(theta1, q_obs, x, sample_share, psi, nest_count)\n",
    "            else:\n",
    "                break'''\n",
    "\n",
    "        # Check convergence in an appropriate norm\n",
    "        dist = la.norm(theta1 - theta0)\n",
    "\n",
    "        if dist<tol:\n",
    "            succes = True\n",
    "            iter = k\n",
    "            break\n",
    "        elif k==max_iters:\n",
    "            succes = False\n",
    "            iter = max_iters\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        theta0 = theta1\n",
    "\n",
    "    res = {'theta': theta1,\n",
    "           'se': se1,\n",
    "           'fun': -LogL(theta1, y, x, sample_share, model),\n",
    "           'iter': iter,\n",
    "           'succes': succes}\n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1675046701290483, 0.0014929418072264493]\n"
     ]
    }
   ],
   "source": [
    "res = FKN_estimator(Logit_beta, y, x, pop_share, Model, N, tol=1.0e-8, max_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-10.4593***</td>\n",
       "      <td>0.00342</td>\n",
       "      <td>3057.771</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.5007***</td>\n",
       "      <td>0.00170</td>\n",
       "      <td>294.137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-3.5089***</td>\n",
       "      <td>0.00255</td>\n",
       "      <td>1378.193</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.0686***</td>\n",
       "      <td>0.00162</td>\n",
       "      <td>42.379</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.2163***</td>\n",
       "      <td>0.00171</td>\n",
       "      <td>1299.239</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.4956***</td>\n",
       "      <td>0.00318</td>\n",
       "      <td>1727.421</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.342***</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>173.217</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-1.0238***</td>\n",
       "      <td>0.00111</td>\n",
       "      <td>918.509</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>3.2825***</td>\n",
       "      <td>0.00229</td>\n",
       "      <td>1433.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.7804***</td>\n",
       "      <td>0.00081</td>\n",
       "      <td>958.274</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.2116***</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>203.542</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.6157***</td>\n",
       "      <td>0.00256</td>\n",
       "      <td>240.561</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>0.1901***</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>379.236</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.6004***</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>911.291</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.2588***</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>541.947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.1982***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>408.478</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.6187***</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>439.537</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.5005***</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>299.205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.6751***</td>\n",
       "      <td>0.00224</td>\n",
       "      <td>749.046</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.2893***</td>\n",
       "      <td>0.00057</td>\n",
       "      <td>506.358</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>0.0527***</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>109.716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.3133***</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>401.886</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-0.8757***</td>\n",
       "      <td>0.00134</td>\n",
       "      <td>652.582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-1.5846***</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>1160.229</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.6567***</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>631.724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-0.6894***</td>\n",
       "      <td>0.00056</td>\n",
       "      <td>1231.625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.5225***</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>710.004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.3154***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>673.229</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-0.9024***</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>806.989</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.0478***</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>74.398</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>0.0098***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>21.159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.1067***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>217.936</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>0.1704***</td>\n",
       "      <td>0.00052</td>\n",
       "      <td>329.639</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.2503***</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>505.247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.8084***</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>648.105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.5776***</td>\n",
       "      <td>0.00073</td>\n",
       "      <td>794.027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.706***</td>\n",
       "      <td>0.03484</td>\n",
       "      <td>77.674</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.6662***</td>\n",
       "      <td>0.00124</td>\n",
       "      <td>536.215</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.3179***</td>\n",
       "      <td>0.01393</td>\n",
       "      <td>166.350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.3017***</td>\n",
       "      <td>0.00249</td>\n",
       "      <td>522.873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-0.9614***</td>\n",
       "      <td>0.00236</td>\n",
       "      <td>407.022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.3562***</td>\n",
       "      <td>0.00083</td>\n",
       "      <td>429.612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-2.5192***</td>\n",
       "      <td>0.01106</td>\n",
       "      <td>227.763</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.667***</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>1010.355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.2463***</td>\n",
       "      <td>0.00069</td>\n",
       "      <td>355.353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1235***</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>188.905</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.5635***</td>\n",
       "      <td>0.00293</td>\n",
       "      <td>534.335</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.115***</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>2454.987</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>-0.0496***</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>326.455</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.045***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>195.954</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.0481***</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>134.471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.1448***</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>284.256</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.7171***</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>2871.216</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.1345***</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>801.206</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.1228***</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>466.355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.0057***</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>29.340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.1108***</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>674.315</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0325***</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>292.058</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0439***</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>621.558</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.0556***</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>516.183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0543***</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>311.878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0329***</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>315.408</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.2663***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>1139.753</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.2293***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>1010.956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.06***</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>467.573</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)    p\n",
       "in_out        -10.4593***  0.00342        3057.771  0.0\n",
       "cy             -0.5007***  0.00170         294.137  0.0\n",
       "hp             -3.5089***  0.00255        1378.193  0.0\n",
       "we              0.0686***  0.00162          42.379  0.0\n",
       "le             -2.2163***  0.00171        1299.239  0.0\n",
       "wi              5.4956***  0.00318        1727.421  0.0\n",
       "he               0.342***  0.00197         173.217  0.0\n",
       "li             -1.0238***  0.00111         918.509  0.0\n",
       "sp              3.2825***  0.00229        1433.185  0.0\n",
       "ac              0.7804***  0.00081         958.274  0.0\n",
       "pr             -0.2116***  0.00104         203.542  0.0\n",
       "brand_2        -0.6157***  0.00256         240.561  0.0\n",
       "brand_3         0.1901***  0.00050         379.236  0.0\n",
       "brand_4        -0.6004***  0.00066         911.291  0.0\n",
       "brand_5        -0.2588***  0.00048         541.947  0.0\n",
       "brand_6        -0.1982***  0.00049         408.478  0.0\n",
       "brand_7        -0.6187***  0.00141         439.537  0.0\n",
       "brand_8        -0.5005***  0.00167         299.205  0.0\n",
       "brand_9        -1.6751***  0.00224         749.046  0.0\n",
       "brand_10        0.2893***  0.00057         506.358  0.0\n",
       "brand_11        0.0527***  0.00048         109.716  0.0\n",
       "brand_12       -0.3133***  0.00078         401.886  0.0\n",
       "brand_13       -0.8757***  0.00134         652.582  0.0\n",
       "brand_14       -1.5846***  0.00137        1160.229  0.0\n",
       "brand_15       -1.6567***  0.00262         631.724  0.0\n",
       "brand_16       -0.6894***  0.00056        1231.625  0.0\n",
       "brand_17       -0.5225***  0.00074         710.004  0.0\n",
       "brand_18        0.3154***  0.00047         673.229  0.0\n",
       "brand_19       -0.9024***  0.00112         806.989  0.0\n",
       "brand_20       -0.0478***  0.00064          74.398  0.0\n",
       "brand_21        0.0098***  0.00047          21.159  0.0\n",
       "brand_22        0.1067***  0.00049         217.936  0.0\n",
       "brand_23        0.1704***  0.00052         329.639  0.0\n",
       "brand_24       -0.2503***  0.00050         505.247  0.0\n",
       "brand_25       -0.8084***  0.00125         648.105  0.0\n",
       "brand_26       -0.5776***  0.00073         794.027  0.0\n",
       "brand_27        -2.706***  0.03484          77.674  0.0\n",
       "brand_28       -0.6662***  0.00124         536.215  0.0\n",
       "brand_29       -2.3179***  0.01393         166.350  0.0\n",
       "brand_30       -1.3017***  0.00249         522.873  0.0\n",
       "brand_31       -0.9614***  0.00236         407.022  0.0\n",
       "brand_32       -0.3562***  0.00083         429.612  0.0\n",
       "brand_33       -2.5192***  0.01106         227.763  0.0\n",
       "brand_34        -0.667***  0.00066        1010.355  0.0\n",
       "brand_35       -0.2463***  0.00069         355.353  0.0\n",
       "brand_36       -0.1235***  0.00065         188.905  0.0\n",
       "brand_37       -1.5635***  0.00293         534.335  0.0\n",
       "home_2           1.115***  0.00045        2454.987  0.0\n",
       "cla_2          -0.0496***  0.00015         326.455  0.0\n",
       "cla_3            0.045***  0.00023         195.954  0.0\n",
       "cla_4           0.0481***  0.00036         134.471  0.0\n",
       "cla_5           0.1448***  0.00051         284.256  0.0\n",
       "group_in_out    0.7171***  0.00025        2871.216  0.0\n",
       "group_cy       -0.1345***  0.00017         801.206  0.0\n",
       "group_hp        0.1228***  0.00026         466.355  0.0\n",
       "group_we        0.0057***  0.00020          29.340  0.0\n",
       "group_le       -0.1108***  0.00016         674.315  0.0\n",
       "group_wi       -0.0325***  0.00011         292.058  0.0\n",
       "group_he       -0.0439***  0.00007         621.558  0.0\n",
       "group_li        0.0556***  0.00011         516.183  0.0\n",
       "group_sp       -0.0543***  0.00017         311.878  0.0\n",
       "group_ac       -0.0329***  0.00010         315.408  0.0\n",
       "group_brand     0.2663***  0.00023        1139.753  0.0\n",
       "group_home     -0.2293***  0.00023        1010.956  0.0\n",
       "group_cla        -0.06***  0.00013         467.573  0.0"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKN_theta = res['theta']\n",
    "FKN_SE = res['se']\n",
    "FKN_t, FKN_p = Similarity_t_p(FKN_SE, FKN_theta, N)\n",
    "reg_table(FKN_theta, FKN_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that $\\sum_{g: \\lambda_g \\geq 0} \\lambda_g > 1 $, but that this is not the case if we disregard the 'inside/outisde option' nesting parameter $\\lambda_1 \\approx 0.7$, where the outside option is nested only with itself. Furthermore, as we check below, the Hessian of $\\Omega$ is still positive definite such that the utility maximization problem has a unique solution. (To be done: explain why this case retains convexity of $\\Omega$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ccp's and Hessian from FKN_parameters\n",
    "q_FKN = Similarity_ccp(FKN_theta, x, Model)\n",
    "FKN_Hess = compute_pertubation_hessian(q_FKN, x, FKN_theta, Model)\n",
    "\n",
    "# Assert that all eigenvalues are strictly positive (and in fact not even close to 0):\n",
    "for t in np.arange(T):\n",
    "    assert la.eigvals(FKN_Hess[t]).all() > 1.0e-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP Estimation and instruments\n",
    "\n",
    "The setting is now a bit different. Instead of the noise coming from random sampling of individuals, we now have an additional source of uncertainty, stemming frm the random sampling of the fixed effects $\\xi_{tj}$ for each market and each product. The number of âobservationsâ is therefore\n",
    "\n",
    "$$\n",
    "S = T \\cdot \\sum_t J_t\n",
    "$$\n",
    "\n",
    "Note that while random sampling of individuals choices (number of observations\n",
    "in the hundreds of millions) still has an effect on the estimated parameters in\n",
    "principle, this effect is completely drowned out by the sampling variance of the\n",
    "fixed effects (number of observations $S \\approx 150^2 \\cdot 50$), so we choose to ignore it\n",
    "here. When estimating random coefficients Models, there is also a third source\n",
    "of uncertainty stemming from approximation of numerical integrals. This is not\n",
    "an issue in Similarity, as we have the inverse demand in closed form.\n",
    "\n",
    "The principles are pretty similar to what we have been doing already. When\n",
    "applicable, we will use the same notation as in the FKN section. Define the\n",
    "residual,\n",
    "\n",
    "$$\\xi_t(\\theta) = u(X_t, \\beta) â \\nabla_q \\Omega(q_t^0|\\lambda)$$\n",
    "\n",
    "In the Similarity Model, this residual is a linear function of $\\theta$ which has the form\n",
    "\n",
    "$$\\xi_t(\\theta) =  G^0_t \\theta â r_t^0$$\n",
    "\n",
    "where $ G^0_t=[X_t, -\\nabla_{q,\\lambda}\\Omega(q_t^0|\\lambda)]$ and $r^0_t = \\ln q^0_t$ as in the FKN section with $q^0_t$ being e.g. the observed market shares in market $t = 1, \\ldots, T$. For the BLP estimator, we set this residual orthogonal to a matrix of instruments $ Z_t$ of size $J_t \\times (K+G)$, and find the estimator $ \\hat \\theta^{IV}$ which solves the moment conditions\n",
    "\n",
    "$$\\frac{1}{T} \\sum_t  Z_t' \\xi(\\hat \\theta^{IV}) = 0$$\n",
    "\n",
    "Since $\\hat \\xi$ is linear, the moment equations have a unique solution,\n",
    "\n",
    "$$\\hat \\theta^{IV} = \\left(\\frac{1}{T}\\sum_t  Z_t' G^0_t \\right)^{-1}\\left(\\frac{1}{T}\\sum_t  Z_t' r^0_t \\right)$$\n",
    "\n",
    "We require an instrument for the price of the goods. This is something which is correlated with the price, but uncorrelated with the error term $\\xi_t$ (in the BLP Model, $\\xi_{tj}$ represents unobserved components of car quality). A standard instrument in this case would be a measure of marginal cost (or something which is correlated with marginal cost, like a production price index). For everything other than price, we can simply use the regressor itself as the instrument i.e. $  Z^{tjd} = G^0_{tjd}$, for all other dimensions than price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct our instruments $ Z$. We'll use the average exchange rate of the destination country relative to average exchange rate of the origin country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = T*np.sum(np.array([x[t].shape[0] for t in np.arange(T)]))\n",
    "\n",
    "xexr = {t: dat[dat['market'] == t][z_vars[0]].values for t in np.arange(T)}\n",
    "G0 = G_array(y, x, Model)\n",
    "pr_index = len(x_contvars)\n",
    "for t in np.arange(T):\n",
    "    G0[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z = G0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the moment estimator $\\hat \\theta^{IV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_estimator(q_obs, z, x, sample_share, model):\n",
    "    '''\n",
    "    Args.\n",
    "        q_obs: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        z: a dictionary of T numpy arrays (J[t],K+G) of instruments for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a numpy array (K+G,) of BLP parameter estimates\n",
    "    '''\n",
    "    T = len(z)\n",
    "\n",
    "    G = G_array(q_obs, x, model)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(q_obs[t], out = np.NINF*np.ones_like((q_obs[t])), where = (q_obs[t] > 0)) for t in np.arange(T)}\n",
    "    \n",
    "    sZG = np.empty((T,d,d))\n",
    "    sZr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sZG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', z[t], G[t])\n",
    "        sZr[t,:] = sample_share[t]*np.einsum('jd,j->d', z[t], r[t])\n",
    "\n",
    "    theta_hat = la.solve(sZG.sum(axis=0), sZr.sum(axis=0))\n",
    "    \n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLP_theta = BLP_estimator(y, z, x, np.ones((T,)), Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Logit Model we get the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_logit = x\n",
    "for t in np.arange(T):\n",
    "    G_logit[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z_logit = G_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.92920752,  -2.3589754 ,  -6.76421995,   0.02963003,\n",
       "        -2.05176127,  10.84731336,  -1.04140126,  -0.58331478,\n",
       "         5.15289118,   0.51808091,  -0.17336342,  -2.037768  ,\n",
       "        -0.81720168,  -1.44357757,  -1.04059281,  -1.16245013,\n",
       "        -1.74530433,  -0.85123531,  -2.72300281,  -1.08758839,\n",
       "        -0.68958989,  -0.95909482,  -2.11727698,  -2.93039275,\n",
       "        -2.90655875,  -2.05527142,  -1.82107985,   0.51974428,\n",
       "        -2.02980519,  -0.79701277,  -0.86356478,  -0.86816254,\n",
       "        -0.81661044,  -1.48858878,  -0.9378501 ,  -1.87621854,\n",
       "        -3.7657435 ,  -1.52567201,  -3.14936663,  -2.07998398,\n",
       "        -1.85954898,  -0.7631942 ,  -1.94891051,  -1.60837966,\n",
       "        -1.15784827,  -0.48973547,  -2.57588437,   1.56903974,\n",
       "         0.0275757 ,   0.04982496,  -0.30342661,  -0.3829885 ])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta = LogitBLP_estimator(y, z_logit, x, np.ones((T,)))\n",
    "LogitBLP_SE = LogitBLP_se(LogitBLP_beta, y, z_logit, x)\n",
    "LogitBLP_t,LogitBLP_p = logit_t_p(LogitBLP_beta, logit_score_unweighted(LogitBLP_beta, y, x), np.ones((T,)), S)\n",
    "LogitBLP_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLP approximation to optimal instruments\n",
    "\n",
    "BLP propose an algorithm for constructing an approximation to the optimal instruments. It is described in simple terms in Reynaert & Verboven (2014), and it has the following steps.\n",
    "It requires a consistent initial parameter estimate $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'$; here we can just use the MLE or the FKN estimates we have already computed. Let $Z_t$ denote the matrix of instruments (this is the matrix $X_t$ with the price replaced by the exchange rate). The steps are then as follows:\n",
    "\n",
    "First we form the regression equation of the covariates on the instruments:\n",
    "$$\n",
    "X_t = Z_t \\Pi + \\Epsilon_t\n",
    "$$\n",
    "\n",
    "The OLS estimate is then given as:\n",
    "$$\n",
    "\\hat \\Pi = \\left( \\frac{1}{T}\\sum_t Z_t' Z_t \\right)^{-1}\\left( \\frac{1}{T}\\sum_t Z_t' X_t\\right)\n",
    "$$\n",
    "\n",
    "Thus the predicted covariates given the instruments $W$ are:\n",
    "$$\n",
    "\\hat X_t = Z_t \\hat \\Pi\n",
    "$$\n",
    "\n",
    "Having constructed $\\hat X_t$ (which consists of the exogenous regressors, and the predicted price given $Z_t$), we compute the predicted mean utility:\n",
    "\n",
    "$$\n",
    "\\hat u_t = \\hat X_t \\hat \\beta\n",
    "$$\n",
    "\n",
    "and then the predicted market shares at the mean utility:\n",
    "\n",
    "$$\n",
    "\\hat q_t^{*} = P(\\hat u_t | \\hat \\lambda)\n",
    "$$\n",
    "\n",
    "Computationally, here we just use $\\hat X_t$ in place of $X_t$ in the CCP function.\n",
    "Given the predicted market shares, we compute\n",
    "\n",
    "$$\n",
    "\\hat G_t^{*} = \\left[\\hat X_t, -\\nabla_{q,\\lambda} \\Omega (\\hat q_t^{*} | \\hat \\lambda)\\right]\n",
    "$$\n",
    "\n",
    "which is the same as the function $\\hat G_t^0$ we already have constructed, except we evaluate it at the\n",
    "predictions $\\hat X_t$ and $\\hat q_t^{*}$ instead of at $X_t$ and $\\hat q_t^0$.\n",
    "\n",
    "The procedure above gives an approximation to the optimal instruments. We also require a weight matrix. The optimal weight matrix is the (generalized) inverse of the conditional (on the instruments) covariance of the fixed effects. Assuming $\\xi_{tj}$ is independently and identically distributed over markets t and products j, the conditional covariance simplifies to a scalar $\\sigma^2$ times an identity matrix (of size $J_t$).\n",
    "This means that all fixed effects are weighted equally, and the weights therefore drop out of the IV regression. The optimal IV estimator is therefore\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} = \\left(\\frac{1}{T}\\sum_t (\\hat G_t^*)'\\hat G_t^0\\right)^{-1}\\left( \\frac{1}{T}\\sum_t (\\hat G_t^*)'\\hat r_t^0 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat \\xi^*$ denote the estimated residual evaluated at the new parameter estimates,\n",
    "\n",
    "$$\n",
    "\\hat \\xi_{tj}^* = \\hat \\xi_{tj}(\\hat \\theta^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "We may estimate the constant $\\sigma^2$ by\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{S}\\sum_{t}\\sum_{j = 1}^{J_t} \\left(\\hat \\xi_{tj}^*\\right)^2 \n",
    "$$\n",
    "\n",
    "The distribution of the estimator $\\hat \\theta^{\\text{IV}}$ is then\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} \\sim \\mathcal{N}(\\theta_0, \\Sigma^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "which can be consistently estimated by\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma^{\\text{IV}} = \\hat \\sigma^2 \\left( \\sum_t (\\hat G_t^*)'\\hat G_t^0 \\right)^{-1}\n",
    "$$\n",
    "\n",
    "and the standard errors are then the square root of the diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_x(x, w, sample_share):\n",
    "    ''' \n",
    "    This function computes the predicted covariates from a regression on the instruments\n",
    "\n",
    "    Args:\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        w: a dictionary of T numpy arrays (J[t],K) of instruments for each covariate for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "\n",
    "    Returns.\n",
    "        X_hat: a dictionary of T numpy arrays (J[t],K) of predicted covariates for each market t\n",
    "    '''\n",
    "    \n",
    "    T = len(w)\n",
    "    K = w[0].shape[1]\n",
    "\n",
    "    sWW = np.empty((T,K,K))\n",
    "    sWX = np.empty((T,K,K))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sWW[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], w[t])\n",
    "        sWX[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], x[t])\n",
    "\n",
    "    Pi_hat = la.solve(sWW.sum(axis=0), sWX.sum(axis=0))\n",
    "    X_hat = {t: np.einsum('jl,lk->jk', w[t], Pi_hat) for t in np.arange(T)}\n",
    "\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_se(Theta, y, x, model):\n",
    "    '''\n",
    "    This function computes BLP standard errors which are consistent when using optimal instruments\n",
    "\n",
    "    Args:\n",
    "        Theta: a numpy array (K+G,) of BLP estimated \n",
    "        y: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns.\n",
    "        SE: a numpy array (K+G,) of estimated BLP standard errors using optimal instruments\n",
    "    '''\n",
    "    T = len(x)\n",
    "    S = T * np.array([x[t].shape[0] for t in np.arange(T)]).sum()\n",
    "\n",
    "    G = G_array(y, x, model)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t]) for t in np.arange(T)}\n",
    "    \n",
    "    # We calculate \\sigma^2\n",
    "    xi = {t: np.einsum('jd,d->j', G[t], Theta) - r[t] for t in np.arange(T)}\n",
    "    sum_xij2 = np.empty((T,))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sum_xij2[t] = (xi[t]**2).sum()\n",
    "    \n",
    "    sigma2 = np.sum(sum_xij2) / S\n",
    "\n",
    "    # We calculate GG for each market t\n",
    "    GG = np.empty((T,d,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        GG[t,:,:] = np.einsum('jd,jp->dp', G[t], G[t])\n",
    "\n",
    "    # Finally we compute \\Sigma and the standard errors\n",
    "    Sigma = sigma2*la.inv(GG.sum(axis=0))\n",
    "    SE = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalBLP_estimator(Theta0, q_obs, w, x, sample_share, model):\n",
    "    '''\n",
    "    This function estimates the Similarity demand model using optimal instruments in the BLP setting\n",
    "    \n",
    "    Args:\n",
    "        Theta0: a numpy array (K+G,) of consistent parameter estimates from estimation using the covariates ('first-stage parameters')\n",
    "        q_obs: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        w: a dictionary of T numpy arrays (J[t],K+G) of instruments for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: a (T,) numpy array of the fraction of observations in each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns.\n",
    "        Theta_IV: a numpy array (K+G,) of BLP parameter estimates in the Similarity Model using optimal instruments\n",
    "        SE_IV: a numpy array (K+G,) of estimated BLP standard errors using optimal instruments\n",
    "    '''\n",
    "    \n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "    \n",
    "    X_hat = predict_x(x, w, sample_share)\n",
    "    q0 = Similarity_ccp(Theta0, X_hat, model)\n",
    "    G_star = G_array(q0, X_hat, model)\n",
    "    G0 = G_array(q_obs, x, model)\n",
    "    \n",
    "    r = {t: np.log(q_obs[t]) for t in np.arange(T)}\n",
    "\n",
    "    d = G0[0].shape[1]\n",
    "\n",
    "    sGG = np.empty((T,d,d))\n",
    "    sGr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sGG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', G_star[t], G0[t])\n",
    "        sGr[t,:] = sample_share[t]*np.einsum('jd,j->d', G_star[t], r[t])\n",
    "\n",
    "    Theta_IV = la.solve(sGG.sum(axis=0), sGr.sum(axis=0))\n",
    "    SE_IV = BLP_se(Theta_IV, q_obs, x, model)\n",
    "\n",
    "    return Theta_IV, SE_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThetaOptBLP, SEOptBLP = OptimalBLP_estimator(FKN_theta, y, z_logit, x, np.ones((T,)), Model)\n",
    "OptBLP_t, OptBLP_p = Similarity_t_p(SEOptBLP, ThetaOptBLP, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.259433210474408"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in ThetaOptBLP[K:]  if p > 0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-11.7985***</td>\n",
       "      <td>0.03458</td>\n",
       "      <td>341.164</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.7545***</td>\n",
       "      <td>0.02002</td>\n",
       "      <td>37.691</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-5.587***</td>\n",
       "      <td>0.02601</td>\n",
       "      <td>214.837</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.2574***</td>\n",
       "      <td>0.02089</td>\n",
       "      <td>12.326</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.4046***</td>\n",
       "      <td>0.02323</td>\n",
       "      <td>103.500</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.9706***</td>\n",
       "      <td>0.03394</td>\n",
       "      <td>175.927</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.8705***</td>\n",
       "      <td>0.02738</td>\n",
       "      <td>31.794</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.77***</td>\n",
       "      <td>0.01218</td>\n",
       "      <td>63.199</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>5.02***</td>\n",
       "      <td>0.02657</td>\n",
       "      <td>188.901</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>1.1385***</td>\n",
       "      <td>0.01299</td>\n",
       "      <td>87.662</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.1493***</td>\n",
       "      <td>0.00237</td>\n",
       "      <td>63.042</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.8311***</td>\n",
       "      <td>0.03193</td>\n",
       "      <td>26.030</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>-0.1506***</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>33.578</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.9499***</td>\n",
       "      <td>0.00480</td>\n",
       "      <td>197.945</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.4631***</td>\n",
       "      <td>0.00446</td>\n",
       "      <td>103.938</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.3605***</td>\n",
       "      <td>0.00438</td>\n",
       "      <td>82.317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.9956***</td>\n",
       "      <td>0.00762</td>\n",
       "      <td>130.612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.3995***</td>\n",
       "      <td>0.01094</td>\n",
       "      <td>36.520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7608***</td>\n",
       "      <td>0.00720</td>\n",
       "      <td>244.600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>-0.2598***</td>\n",
       "      <td>0.00429</td>\n",
       "      <td>60.520</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.0442***</td>\n",
       "      <td>0.00439</td>\n",
       "      <td>10.061</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.49***</td>\n",
       "      <td>0.00527</td>\n",
       "      <td>92.979</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-1.2869***</td>\n",
       "      <td>0.00660</td>\n",
       "      <td>194.956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-2.1215***</td>\n",
       "      <td>0.00919</td>\n",
       "      <td>230.955</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.9771***</td>\n",
       "      <td>0.00876</td>\n",
       "      <td>225.685</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-1.2687***</td>\n",
       "      <td>0.00426</td>\n",
       "      <td>297.597</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.9739***</td>\n",
       "      <td>0.00508</td>\n",
       "      <td>191.567</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.4086***</td>\n",
       "      <td>0.00471</td>\n",
       "      <td>86.702</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-1.2959***</td>\n",
       "      <td>0.00586</td>\n",
       "      <td>221.279</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.2857***</td>\n",
       "      <td>0.00533</td>\n",
       "      <td>53.653</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.252***</td>\n",
       "      <td>0.00413</td>\n",
       "      <td>61.072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>-0.2039***</td>\n",
       "      <td>0.00422</td>\n",
       "      <td>48.275</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>-0.1584***</td>\n",
       "      <td>0.00458</td>\n",
       "      <td>34.615</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.6969***</td>\n",
       "      <td>0.00431</td>\n",
       "      <td>161.524</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.8286***</td>\n",
       "      <td>0.00512</td>\n",
       "      <td>161.965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-1.053***</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>206.553</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.4134***</td>\n",
       "      <td>0.05259</td>\n",
       "      <td>45.894</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-1.0134***</td>\n",
       "      <td>0.00865</td>\n",
       "      <td>117.111</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.001***</td>\n",
       "      <td>0.01765</td>\n",
       "      <td>113.381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.2162***</td>\n",
       "      <td>0.00894</td>\n",
       "      <td>136.105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-1.59***</td>\n",
       "      <td>0.01807</td>\n",
       "      <td>88.011</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.4178***</td>\n",
       "      <td>0.00812</td>\n",
       "      <td>51.450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-1.689***</td>\n",
       "      <td>0.03057</td>\n",
       "      <td>55.247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.8166***</td>\n",
       "      <td>0.00547</td>\n",
       "      <td>149.207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.5157***</td>\n",
       "      <td>0.00514</td>\n",
       "      <td>100.390</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.148***</td>\n",
       "      <td>0.00449</td>\n",
       "      <td>32.947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.845***</td>\n",
       "      <td>0.01093</td>\n",
       "      <td>168.873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.0623***</td>\n",
       "      <td>0.00193</td>\n",
       "      <td>549.310</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.0278***</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>11.317</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.133***</td>\n",
       "      <td>0.00330</td>\n",
       "      <td>40.263</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.1386***</td>\n",
       "      <td>0.00461</td>\n",
       "      <td>30.081</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.1537***</td>\n",
       "      <td>0.00585</td>\n",
       "      <td>26.258</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.8427***</td>\n",
       "      <td>0.00209</td>\n",
       "      <td>404.174</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.0928***</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>64.999</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.2064***</td>\n",
       "      <td>0.00219</td>\n",
       "      <td>94.099</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>-0.0069***</td>\n",
       "      <td>0.00189</td>\n",
       "      <td>3.647</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0488***</td>\n",
       "      <td>0.00172</td>\n",
       "      <td>28.375</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0669***</td>\n",
       "      <td>0.00101</td>\n",
       "      <td>66.415</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0613***</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>90.549</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.0221***</td>\n",
       "      <td>0.00092</td>\n",
       "      <td>24.009</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0571***</td>\n",
       "      <td>0.00163</td>\n",
       "      <td>35.132</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0402***</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>37.025</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.1882***</td>\n",
       "      <td>0.00097</td>\n",
       "      <td>193.976</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.3915***</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>329.078</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.0971***</td>\n",
       "      <td>0.00142</td>\n",
       "      <td>68.498</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)    p\n",
       "in_out        -11.7985***  0.03458         341.164  0.0\n",
       "cy             -0.7545***  0.02002          37.691  0.0\n",
       "hp              -5.587***  0.02601         214.837  0.0\n",
       "we              0.2574***  0.02089          12.326  0.0\n",
       "le             -2.4046***  0.02323         103.500  0.0\n",
       "wi              5.9706***  0.03394         175.927  0.0\n",
       "he              0.8705***  0.02738          31.794  0.0\n",
       "li               -0.77***  0.01218          63.199  0.0\n",
       "sp                5.02***  0.02657         188.901  0.0\n",
       "ac              1.1385***  0.01299          87.662  0.0\n",
       "pr             -0.1493***  0.00237          63.042  0.0\n",
       "brand_2        -0.8311***  0.03193          26.030  0.0\n",
       "brand_3        -0.1506***  0.00449          33.578  0.0\n",
       "brand_4        -0.9499***  0.00480         197.945  0.0\n",
       "brand_5        -0.4631***  0.00446         103.938  0.0\n",
       "brand_6        -0.3605***  0.00438          82.317  0.0\n",
       "brand_7        -0.9956***  0.00762         130.612  0.0\n",
       "brand_8        -0.3995***  0.01094          36.520  0.0\n",
       "brand_9        -1.7608***  0.00720         244.600  0.0\n",
       "brand_10       -0.2598***  0.00429          60.520  0.0\n",
       "brand_11       -0.0442***  0.00439          10.061  0.0\n",
       "brand_12         -0.49***  0.00527          92.979  0.0\n",
       "brand_13       -1.2869***  0.00660         194.956  0.0\n",
       "brand_14       -2.1215***  0.00919         230.955  0.0\n",
       "brand_15       -1.9771***  0.00876         225.685  0.0\n",
       "brand_16       -1.2687***  0.00426         297.597  0.0\n",
       "brand_17       -0.9739***  0.00508         191.567  0.0\n",
       "brand_18        0.4086***  0.00471          86.702  0.0\n",
       "brand_19       -1.2959***  0.00586         221.279  0.0\n",
       "brand_20       -0.2857***  0.00533          53.653  0.0\n",
       "brand_21        -0.252***  0.00413          61.072  0.0\n",
       "brand_22       -0.2039***  0.00422          48.275  0.0\n",
       "brand_23       -0.1584***  0.00458          34.615  0.0\n",
       "brand_24       -0.6969***  0.00431         161.524  0.0\n",
       "brand_25       -0.8286***  0.00512         161.965  0.0\n",
       "brand_26        -1.053***  0.00510         206.553  0.0\n",
       "brand_27       -2.4134***  0.05259          45.894  0.0\n",
       "brand_28       -1.0134***  0.00865         117.111  0.0\n",
       "brand_29        -2.001***  0.01765         113.381  0.0\n",
       "brand_30       -1.2162***  0.00894         136.105  0.0\n",
       "brand_31         -1.59***  0.01807          88.011  0.0\n",
       "brand_32       -0.4178***  0.00812          51.450  0.0\n",
       "brand_33        -1.689***  0.03057          55.247  0.0\n",
       "brand_34       -0.8166***  0.00547         149.207  0.0\n",
       "brand_35       -0.5157***  0.00514         100.390  0.0\n",
       "brand_36        -0.148***  0.00449          32.947  0.0\n",
       "brand_37        -1.845***  0.01093         168.873  0.0\n",
       "home_2          1.0623***  0.00193         549.310  0.0\n",
       "cla_2           0.0278***  0.00245          11.317  0.0\n",
       "cla_3            0.133***  0.00330          40.263  0.0\n",
       "cla_4           0.1386***  0.00461          30.081  0.0\n",
       "cla_5           0.1537***  0.00585          26.258  0.0\n",
       "group_in_out    0.8427***  0.00209         404.174  0.0\n",
       "group_cy       -0.0928***  0.00143          64.999  0.0\n",
       "group_hp        0.2064***  0.00219          94.099  0.0\n",
       "group_we       -0.0069***  0.00189           3.647  0.0\n",
       "group_le       -0.0488***  0.00172          28.375  0.0\n",
       "group_wi       -0.0669***  0.00101          66.415  0.0\n",
       "group_he       -0.0613***  0.00068          90.549  0.0\n",
       "group_li        0.0221***  0.00092          24.009  0.0\n",
       "group_sp       -0.0571***  0.00163          35.132  0.0\n",
       "group_ac       -0.0402***  0.00109          37.025  0.0\n",
       "group_brand     0.1882***  0.00097         193.976  0.0\n",
       "group_home     -0.3915***  0.00119         329.078  0.0\n",
       "group_cla      -0.0971***  0.00142          68.498  0.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_table(ThetaOptBLP, SEOptBLP, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "qOpt = Similarity_ccp(ThetaOptBLP, z_logit, Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market $t=1$ the price elasticities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Semi-elasticity wrt. product</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semi-elasticity of product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001968</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000081</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>1.225436e-06</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.243637</td>\n",
       "      <td>0.003726</td>\n",
       "      <td>0.009419</td>\n",
       "      <td>0.004263</td>\n",
       "      <td>-0.002899</td>\n",
       "      <td>0.005185</td>\n",
       "      <td>-0.029609</td>\n",
       "      <td>-0.002225</td>\n",
       "      <td>-0.005826</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003913</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.002161</td>\n",
       "      <td>0.001780</td>\n",
       "      <td>-2.280778e-04</td>\n",
       "      <td>0.004698</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>-0.010173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>-0.238959</td>\n",
       "      <td>-0.000207</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>0.003627</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>0.012833</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000391</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>-0.003856</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>7.368991e-05</td>\n",
       "      <td>-0.003176</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.005999</td>\n",
       "      <td>0.005874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002857</td>\n",
       "      <td>-0.000120</td>\n",
       "      <td>-0.245924</td>\n",
       "      <td>0.014464</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>-0.006796</td>\n",
       "      <td>0.024542</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000629</td>\n",
       "      <td>-0.003296</td>\n",
       "      <td>0.000336</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>-0.000169</td>\n",
       "      <td>9.679006e-05</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.002281</td>\n",
       "      <td>-0.001942</td>\n",
       "      <td>0.020213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.002426</td>\n",
       "      <td>0.019944</td>\n",
       "      <td>-0.242792</td>\n",
       "      <td>0.004951</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.002569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002789</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>1.437861e-06</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>-0.002248</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.018398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.000978</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.007222</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>-0.216636</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>-0.008817</td>\n",
       "      <td>-0.000233</td>\n",
       "      <td>-0.001192</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0.002291</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.001330</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>1.105184e-04</td>\n",
       "      <td>0.003478</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>-0.004025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003165</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.013680</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>-0.245631</td>\n",
       "      <td>0.059487</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.006172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>-0.001157</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>8.184730e-05</td>\n",
       "      <td>-0.000743</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>-0.002467</td>\n",
       "      <td>0.010462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.004964</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.013568</td>\n",
       "      <td>0.004379</td>\n",
       "      <td>-0.004381</td>\n",
       "      <td>0.016339</td>\n",
       "      <td>-0.240614</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>-0.007101</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010768</td>\n",
       "      <td>0.008886</td>\n",
       "      <td>0.006484</td>\n",
       "      <td>0.002139</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>-1.552227e-04</td>\n",
       "      <td>0.005997</td>\n",
       "      <td>-0.000863</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>-0.012832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.004998</td>\n",
       "      <td>0.003034</td>\n",
       "      <td>0.008728</td>\n",
       "      <td>0.004265</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>0.014270</td>\n",
       "      <td>0.012274</td>\n",
       "      <td>-0.244461</td>\n",
       "      <td>-0.004562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009970</td>\n",
       "      <td>0.008487</td>\n",
       "      <td>0.004414</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>2.716150e-04</td>\n",
       "      <td>0.004016</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>-0.008927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.003284</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.010060</td>\n",
       "      <td>0.003462</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>-0.023871</td>\n",
       "      <td>-0.001145</td>\n",
       "      <td>-0.227049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>-1.495743e-04</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>-0.000546</td>\n",
       "      <td>-0.008387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>-0.007397</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.003828</td>\n",
       "      <td>-0.003700</td>\n",
       "      <td>0.015404</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.031054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>-0.001419</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>4.004515e-05</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.001274</td>\n",
       "      <td>-0.001036</td>\n",
       "      <td>0.008286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.003931</td>\n",
       "      <td>-0.001492</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>-0.000315</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>-0.000651</td>\n",
       "      <td>0.023174</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>-1.028693e-05</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>-0.000043</td>\n",
       "      <td>-0.006669</td>\n",
       "      <td>0.002035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>0.005415</td>\n",
       "      <td>0.001215</td>\n",
       "      <td>0.001690</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000935</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.017415</td>\n",
       "      <td>0.000266</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>1.213743e-05</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>0.000354</td>\n",
       "      <td>0.003585</td>\n",
       "      <td>0.003649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003026</td>\n",
       "      <td>-0.000229</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.004266</td>\n",
       "      <td>0.005595</td>\n",
       "      <td>0.000158</td>\n",
       "      <td>0.024374</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.003796</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>-0.001573</td>\n",
       "      <td>-0.002741</td>\n",
       "      <td>0.021873</td>\n",
       "      <td>0.016492</td>\n",
       "      <td>1.811638e-03</td>\n",
       "      <td>-0.004339</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.006119</td>\n",
       "      <td>0.006028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.001539</td>\n",
       "      <td>0.002931</td>\n",
       "      <td>0.006860</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>-0.017319</td>\n",
       "      <td>-0.000950</td>\n",
       "      <td>-0.004267</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002860</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>-1.569236e-04</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>-0.001166</td>\n",
       "      <td>-0.004851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002261</td>\n",
       "      <td>-0.002639</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.005119</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.019141</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.003175</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>0.000204</td>\n",
       "      <td>-0.001632</td>\n",
       "      <td>-0.001013</td>\n",
       "      <td>-0.000958</td>\n",
       "      <td>1.110465e-04</td>\n",
       "      <td>-0.000789</td>\n",
       "      <td>0.010035</td>\n",
       "      <td>0.028981</td>\n",
       "      <td>0.002228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003418</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>-0.006741</td>\n",
       "      <td>0.001531</td>\n",
       "      <td>0.007686</td>\n",
       "      <td>-0.003363</td>\n",
       "      <td>0.008219</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.006035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>-0.001507</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.000268</td>\n",
       "      <td>0.000312</td>\n",
       "      <td>4.709231e-05</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.003440</td>\n",
       "      <td>-0.002915</td>\n",
       "      <td>0.005511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.010003</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>-0.008641</td>\n",
       "      <td>0.003167</td>\n",
       "      <td>0.003849</td>\n",
       "      <td>-0.004030</td>\n",
       "      <td>0.015738</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>-0.001258</td>\n",
       "      <td>-0.001697</td>\n",
       "      <td>0.000643</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>5.518338e-05</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.000237</td>\n",
       "      <td>0.008149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.008461</td>\n",
       "      <td>0.004602</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>-0.005152</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.004098</td>\n",
       "      <td>-0.002986</td>\n",
       "      <td>-0.000219</td>\n",
       "      <td>-0.000740</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001291</td>\n",
       "      <td>0.002276</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>-1.851759e-05</td>\n",
       "      <td>0.006226</td>\n",
       "      <td>-0.004222</td>\n",
       "      <td>-0.000713</td>\n",
       "      <td>0.002266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.011508</td>\n",
       "      <td>-0.004009</td>\n",
       "      <td>0.000238</td>\n",
       "      <td>0.002613</td>\n",
       "      <td>0.005560</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.022384</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.004477</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>-0.002221</td>\n",
       "      <td>-0.006494</td>\n",
       "      <td>-0.003374</td>\n",
       "      <td>-0.001830</td>\n",
       "      <td>1.915490e-04</td>\n",
       "      <td>-0.006651</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.008215</td>\n",
       "      <td>0.006245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.012574</td>\n",
       "      <td>-0.001776</td>\n",
       "      <td>-0.001273</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.030178</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>-0.001632</td>\n",
       "      <td>-0.003390</td>\n",
       "      <td>-0.003136</td>\n",
       "      <td>-0.001398</td>\n",
       "      <td>2.193575e-04</td>\n",
       "      <td>-0.005635</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.007940</td>\n",
       "      <td>0.007260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.002618</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.001265</td>\n",
       "      <td>-0.018656</td>\n",
       "      <td>0.011271</td>\n",
       "      <td>-0.003039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>-5.173188e-04</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>-0.009073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>-0.008118</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.011350</td>\n",
       "      <td>-0.004253</td>\n",
       "      <td>0.009662</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.006070</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000525</td>\n",
       "      <td>-0.001855</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>-1.246739e-04</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>-0.005499</td>\n",
       "      <td>0.005651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.000330</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>-0.003984</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.004069</td>\n",
       "      <td>-0.007165</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>-0.001504</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000901</td>\n",
       "      <td>0.002472</td>\n",
       "      <td>0.004143</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>-1.890894e-04</td>\n",
       "      <td>0.003784</td>\n",
       "      <td>-0.001836</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>-0.000974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.002030</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.002562</td>\n",
       "      <td>0.004548</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>-0.006042</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>-0.003642</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>-3.922955e-04</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>-0.001929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.002153</td>\n",
       "      <td>0.002923</td>\n",
       "      <td>0.008171</td>\n",
       "      <td>0.005070</td>\n",
       "      <td>-0.032424</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>-0.007145</td>\n",
       "      <td>-0.017928</td>\n",
       "      <td>-0.001880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.000549</td>\n",
       "      <td>6.885890e-03</td>\n",
       "      <td>0.003786</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.002712</td>\n",
       "      <td>-0.005435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002464</td>\n",
       "      <td>-0.002092</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.022738</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000137</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>-0.002911</td>\n",
       "      <td>-0.001534</td>\n",
       "      <td>-0.000347</td>\n",
       "      <td>1.296588e-04</td>\n",
       "      <td>-0.005247</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.007360</td>\n",
       "      <td>0.005077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.014383</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>-0.005280</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>0.000868</td>\n",
       "      <td>0.007478</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.003114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002643</td>\n",
       "      <td>0.000829</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>2.603260e-07</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>-0.001765</td>\n",
       "      <td>0.003163</td>\n",
       "      <td>0.006411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.015194</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>-0.005076</td>\n",
       "      <td>0.004506</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.002796</td>\n",
       "      <td>0.000504</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002149</td>\n",
       "      <td>0.001385</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>-4.481235e-06</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>-0.001341</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.005136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002240</td>\n",
       "      <td>0.014330</td>\n",
       "      <td>-0.005727</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.011042</td>\n",
       "      <td>-0.002744</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>-0.001670</td>\n",
       "      <td>0.004093</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>5.841003e-06</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.002471</td>\n",
       "      <td>-0.008515</td>\n",
       "      <td>0.002247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.010423</td>\n",
       "      <td>0.002412</td>\n",
       "      <td>0.003874</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.026973</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000386</td>\n",
       "      <td>-0.004060</td>\n",
       "      <td>-0.001774</td>\n",
       "      <td>-0.000247</td>\n",
       "      <td>1.611079e-04</td>\n",
       "      <td>-0.006614</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.009613</td>\n",
       "      <td>0.005917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.019469</td>\n",
       "      <td>0.005654</td>\n",
       "      <td>0.004801</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.010433</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003222</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>1.765248e-05</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>-0.002231</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.018171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.003671</td>\n",
       "      <td>0.003623</td>\n",
       "      <td>0.028348</td>\n",
       "      <td>0.018988</td>\n",
       "      <td>-0.007343</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>-0.030654</td>\n",
       "      <td>-0.001808</td>\n",
       "      <td>-0.007199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.005039</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>-1.623947e-04</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>-0.002922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003189</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>-0.006306</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.006499</td>\n",
       "      <td>-0.001727</td>\n",
       "      <td>0.008347</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>-0.003806</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>-0.000904</td>\n",
       "      <td>-0.001212</td>\n",
       "      <td>6.130039e-05</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.002362</td>\n",
       "      <td>-0.003103</td>\n",
       "      <td>0.005568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003806</td>\n",
       "      <td>-0.000384</td>\n",
       "      <td>-0.008239</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.011030</td>\n",
       "      <td>-0.004377</td>\n",
       "      <td>0.010594</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000699</td>\n",
       "      <td>-0.001484</td>\n",
       "      <td>-0.000557</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.000368</td>\n",
       "      <td>4.304762e-05</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>-0.002353</td>\n",
       "      <td>0.005665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003237</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>-0.001716</td>\n",
       "      <td>-0.005517</td>\n",
       "      <td>0.005963</td>\n",
       "      <td>0.008231</td>\n",
       "      <td>0.053127</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245080</td>\n",
       "      <td>0.005993</td>\n",
       "      <td>-0.001402</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000534</td>\n",
       "      <td>5.849767e-05</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.001162</td>\n",
       "      <td>0.005849</td>\n",
       "      <td>0.010776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>-0.011144</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>0.004999</td>\n",
       "      <td>0.054349</td>\n",
       "      <td>0.003875</td>\n",
       "      <td>0.006293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007429</td>\n",
       "      <td>-0.247228</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>-0.001467</td>\n",
       "      <td>-0.001416</td>\n",
       "      <td>1.351826e-04</td>\n",
       "      <td>0.001424</td>\n",
       "      <td>0.002337</td>\n",
       "      <td>-0.001496</td>\n",
       "      <td>0.009679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>-0.003400</td>\n",
       "      <td>0.000509</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.005080</td>\n",
       "      <td>-0.000872</td>\n",
       "      <td>0.017783</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000779</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>-0.237666</td>\n",
       "      <td>-0.001370</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>1.018665e-04</td>\n",
       "      <td>-0.006143</td>\n",
       "      <td>0.002249</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.005757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>-0.002941</td>\n",
       "      <td>-0.001443</td>\n",
       "      <td>0.003100</td>\n",
       "      <td>0.005268</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.017049</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.004099</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>-0.001911</td>\n",
       "      <td>-0.003981</td>\n",
       "      <td>-0.228974</td>\n",
       "      <td>0.014422</td>\n",
       "      <td>1.654167e-03</td>\n",
       "      <td>-0.003996</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.006290</td>\n",
       "      <td>0.006171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.003208</td>\n",
       "      <td>-0.002301</td>\n",
       "      <td>-0.001007</td>\n",
       "      <td>0.002760</td>\n",
       "      <td>0.005535</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.010469</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.004715</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>-0.002489</td>\n",
       "      <td>-0.002592</td>\n",
       "      <td>0.019450</td>\n",
       "      <td>-0.234529</td>\n",
       "      <td>1.644134e-03</td>\n",
       "      <td>-0.001277</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.005952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.005083</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.007113</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.007299</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>-0.020633</td>\n",
       "      <td>0.002695</td>\n",
       "      <td>-0.005915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.002938</td>\n",
       "      <td>0.004937</td>\n",
       "      <td>0.027584</td>\n",
       "      <td>0.020330</td>\n",
       "      <td>-2.818249e-01</td>\n",
       "      <td>0.004802</td>\n",
       "      <td>-0.000373</td>\n",
       "      <td>-0.000671</td>\n",
       "      <td>-0.007688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>-0.002852</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.002697</td>\n",
       "      <td>0.004826</td>\n",
       "      <td>-0.000570</td>\n",
       "      <td>0.016746</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.003821</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000075</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>-0.006256</td>\n",
       "      <td>-0.001400</td>\n",
       "      <td>-0.000332</td>\n",
       "      <td>1.008877e-04</td>\n",
       "      <td>-0.195033</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.005842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>-0.004965</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>-0.004752</td>\n",
       "      <td>0.000100</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001297</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>-1.545820e-05</td>\n",
       "      <td>0.004082</td>\n",
       "      <td>-0.241191</td>\n",
       "      <td>0.020364</td>\n",
       "      <td>-0.000774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.004403</td>\n",
       "      <td>-0.002451</td>\n",
       "      <td>0.002358</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>-0.001547</td>\n",
       "      <td>0.000959</td>\n",
       "      <td>0.000299</td>\n",
       "      <td>-0.000371</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>-0.000558</td>\n",
       "      <td>0.005712</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>-1.152046e-05</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>-0.234533</td>\n",
       "      <td>0.001056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.147326</td>\n",
       "      <td>-0.004447</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>0.019233</td>\n",
       "      <td>-0.005215</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>-0.033457</td>\n",
       "      <td>-0.001737</td>\n",
       "      <td>-0.006506</td>\n",
       "      <td>...</td>\n",
       "      <td>0.005695</td>\n",
       "      <td>0.004126</td>\n",
       "      <td>0.005473</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.001444</td>\n",
       "      <td>-1.508066e-04</td>\n",
       "      <td>0.005455</td>\n",
       "      <td>-0.000366</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>-0.251119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows Ã 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Semi-elasticity wrt. product        0         1         2         3   \\\n",
       "Semi-elasticity of product                                             \n",
       "0                            -0.001968  0.000027  0.000052  0.000090   \n",
       "1                             0.147326 -0.243637  0.003726  0.009419   \n",
       "2                             0.147326  0.001943 -0.238959 -0.000207   \n",
       "3                             0.147326  0.002857 -0.000120 -0.245924   \n",
       "4                             0.147326  0.001782  0.002426  0.019944   \n",
       "5                             0.147326 -0.000978  0.002347  0.007222   \n",
       "6                             0.147326  0.003165 -0.000077 -0.013680   \n",
       "7                             0.147326 -0.004964  0.004126  0.013568   \n",
       "8                             0.147326 -0.004998  0.003034  0.008728   \n",
       "9                             0.147326 -0.003284  0.003854  0.010060   \n",
       "10                            0.147326  0.001557  0.001303 -0.007397   \n",
       "11                            0.147326  0.000495  0.003931 -0.001492   \n",
       "12                            0.147326  0.000891 -0.001207  0.005415   \n",
       "13                            0.147326  0.003026 -0.000229  0.001438   \n",
       "14                            0.147326 -0.001539  0.002931  0.006860   \n",
       "15                            0.147326  0.002261 -0.002639  0.003144   \n",
       "16                            0.147326  0.003418  0.000017 -0.006741   \n",
       "17                            0.147326  0.010003  0.001982 -0.008641   \n",
       "18                            0.147326  0.008461  0.004602  0.008455   \n",
       "19                            0.147326  0.011508 -0.004009  0.000238   \n",
       "20                            0.147326  0.012574 -0.001776 -0.001273   \n",
       "21                            0.147326  0.003847  0.000859  0.003714   \n",
       "22                            0.147326  0.003140  0.000530 -0.008118   \n",
       "23                            0.147326 -0.000330  0.003445  0.008271   \n",
       "24                            0.147326 -0.002030  0.000699  0.002895   \n",
       "25                            0.147326 -0.002153  0.002923  0.008171   \n",
       "26                            0.147326  0.002464 -0.002092  0.003191   \n",
       "27                            0.147326  0.001750  0.014383  0.002145   \n",
       "28                            0.147326  0.002132  0.015194  0.002947   \n",
       "29                            0.147326  0.002240  0.014330 -0.005727   \n",
       "30                            0.147326  0.002750  0.010423  0.002412   \n",
       "31                            0.147326  0.001616  0.002352  0.019469   \n",
       "32                            0.147326 -0.003671  0.003623  0.028348   \n",
       "33                            0.147326  0.003189  0.002010 -0.006306   \n",
       "34                            0.147326  0.003806 -0.000384 -0.008239   \n",
       "35                            0.147326  0.003237  0.000621 -0.001716   \n",
       "36                            0.147326  0.004007  0.000872 -0.011144   \n",
       "37                            0.147326  0.002160 -0.003400  0.000509   \n",
       "38                            0.147326  0.002888 -0.002941 -0.001443   \n",
       "39                            0.147326  0.003208 -0.002301 -0.001007   \n",
       "40                            0.147326 -0.005083  0.003149  0.007113   \n",
       "41                            0.147326  0.002200 -0.002852  0.000131   \n",
       "42                            0.147326  0.001410  0.003615  0.006944   \n",
       "43                            0.147326  0.000324  0.004403 -0.002451   \n",
       "44                            0.147326 -0.004447  0.004924  0.029137   \n",
       "\n",
       "Semi-elasticity wrt. product        4         5         6         7   \\\n",
       "Semi-elasticity of product                                             \n",
       "0                             0.000065  0.000081  0.000045  0.000163   \n",
       "1                             0.004263 -0.002899  0.005185 -0.029609   \n",
       "2                             0.003026  0.003627 -0.000066  0.012833   \n",
       "3                             0.014464  0.006491 -0.006796  0.024542   \n",
       "4                            -0.242792  0.004951  0.001319  0.010922   \n",
       "5                             0.003995 -0.216636  0.003746 -0.008817   \n",
       "6                             0.001926  0.006777 -0.245631  0.059487   \n",
       "7                             0.004379 -0.004381  0.016339 -0.240614   \n",
       "8                             0.004265 -0.001549  0.014270  0.012274   \n",
       "9                             0.003462 -0.001991  0.005698 -0.023871   \n",
       "10                            0.002595  0.003828 -0.003700  0.015404   \n",
       "11                            0.002157  0.002499 -0.000315 -0.000436   \n",
       "12                            0.001215  0.001690  0.002651  0.005229   \n",
       "13                            0.004266  0.005595  0.000158  0.024374   \n",
       "14                            0.002182  0.002769  0.003967 -0.017319   \n",
       "15                            0.002878  0.005119  0.000466  0.019141   \n",
       "16                            0.001531  0.007686 -0.003363  0.008219   \n",
       "17                            0.003167  0.003849 -0.004030  0.015738   \n",
       "18                           -0.005152  0.000888  0.004098 -0.002986   \n",
       "19                            0.002613  0.005560  0.000473  0.022384   \n",
       "20                            0.004437  0.006394  0.000653  0.030178   \n",
       "21                            0.002618  0.003497  0.001265 -0.018656   \n",
       "22                            0.001213  0.011350 -0.004253  0.009662   \n",
       "23                           -0.003984  0.003847  0.004069 -0.007165   \n",
       "24                            0.002562  0.004548  0.001253 -0.006042   \n",
       "25                            0.005070 -0.032424  0.004989 -0.007145   \n",
       "26                            0.002269  0.004551  0.000858  0.022738   \n",
       "27                           -0.005280  0.003768  0.000868  0.007478   \n",
       "28                           -0.005076  0.004506  0.001394  0.002796   \n",
       "29                            0.001792  0.011042 -0.002744  0.001039   \n",
       "30                            0.003874  0.005489  0.000543  0.026973   \n",
       "31                            0.005654  0.004801  0.000150  0.010433   \n",
       "32                            0.018988 -0.007343  0.006611 -0.030654   \n",
       "33                            0.002335  0.006499 -0.001727  0.008347   \n",
       "34                            0.000092  0.011030 -0.004377  0.010594   \n",
       "35                           -0.005517  0.005963  0.008231  0.053127   \n",
       "36                            0.001762  0.006963  0.004999  0.054349   \n",
       "37                            0.001748  0.005080 -0.000872  0.017783   \n",
       "38                            0.003100  0.005268  0.000012  0.017049   \n",
       "39                            0.002760  0.005535  0.000979  0.010469   \n",
       "40                            0.000077  0.007299  0.002988 -0.020633   \n",
       "41                            0.002697  0.004826 -0.000570  0.016746   \n",
       "42                           -0.004965  0.005835  0.003549 -0.004752   \n",
       "43                            0.002358  0.002389 -0.001547  0.000959   \n",
       "44                            0.019233 -0.005215  0.007492 -0.033457   \n",
       "\n",
       "Semi-elasticity wrt. product        8         9   ...        35        36  \\\n",
       "Semi-elasticity of product                        ...                       \n",
       "0                             0.000012  0.000048  ...  0.000033  0.000027   \n",
       "1                            -0.002225 -0.005826  ...  0.003913  0.003908   \n",
       "2                             0.000704  0.003566  ...  0.000391  0.000444   \n",
       "3                             0.001178  0.005413  ... -0.000629 -0.003296   \n",
       "4                             0.000794  0.002569  ... -0.002789  0.000719   \n",
       "5                            -0.000233 -0.001192  ...  0.002432  0.002291   \n",
       "6                             0.003878  0.006172  ...  0.006074  0.002975   \n",
       "7                             0.000916 -0.007101  ...  0.010768  0.008886   \n",
       "8                            -0.244461 -0.004562  ...  0.009970  0.008487   \n",
       "9                            -0.001145 -0.227049  ...  0.003887  0.003459   \n",
       "10                            0.000766  0.031054  ...  0.000147 -0.001419   \n",
       "11                            0.000277  0.000191  ...  0.002612 -0.000651   \n",
       "12                            0.000226  0.000848  ...  0.000935  0.001367   \n",
       "13                            0.000881  0.003796  ...  0.000955 -0.001573   \n",
       "14                           -0.000950 -0.004267  ...  0.002860  0.002340   \n",
       "15                            0.000777  0.003175  ... -0.000342  0.000204   \n",
       "16                            0.000600  0.006035  ...  0.000327 -0.001507   \n",
       "17                            0.000408  0.004501  ...  0.000156 -0.001258   \n",
       "18                           -0.000219 -0.000740  ... -0.001291  0.002276   \n",
       "19                            0.000679  0.004477  ... -0.000187 -0.002221   \n",
       "20                            0.000645  0.004812  ...  0.000770 -0.001632   \n",
       "21                            0.011271 -0.003039  ...  0.000772  0.001580   \n",
       "22                            0.001020  0.006070  ...  0.000525 -0.001855   \n",
       "23                           -0.000019 -0.001504  ... -0.000901  0.002472   \n",
       "24                            0.000810 -0.003642  ...  0.001024  0.001257   \n",
       "25                           -0.017928 -0.001880  ...  0.003463  0.002886   \n",
       "26                            0.000735  0.003350  ... -0.000137  0.000138   \n",
       "27                            0.000656  0.003114  ... -0.002643  0.000829   \n",
       "28                            0.000504  0.003902  ... -0.002149  0.001385   \n",
       "29                            0.000520  0.002884  ...  0.001551 -0.001670   \n",
       "30                            0.000900  0.003994  ...  0.000313  0.000386   \n",
       "31                            0.000832  0.003265  ... -0.003222  0.001063   \n",
       "32                           -0.001808 -0.007199  ...  0.004600  0.003685   \n",
       "33                            0.000528  0.005061  ...  0.001793 -0.003806   \n",
       "34                            0.000757  0.006723  ... -0.000699 -0.001484   \n",
       "35                            0.003672  0.005705  ... -0.245080  0.005993   \n",
       "36                            0.003875  0.006293  ...  0.007429 -0.247228   \n",
       "37                            0.000904  0.004282  ... -0.000779  0.000444   \n",
       "38                            0.000881  0.004099  ...  0.000499 -0.001911   \n",
       "39                            0.000816  0.004715  ...  0.001164 -0.002489   \n",
       "40                            0.002695 -0.005915  ...  0.001576  0.002938   \n",
       "41                            0.000837  0.003821  ... -0.000075  0.000650   \n",
       "42                            0.000100  0.001201  ... -0.001297  0.002104   \n",
       "43                            0.000299 -0.000371  ...  0.002707 -0.000558   \n",
       "44                           -0.001737 -0.006506  ...  0.005695  0.004126   \n",
       "\n",
       "Semi-elasticity wrt. product        37        38        39            40  \\\n",
       "Semi-elasticity of product                                                 \n",
       "0                             0.000059  0.000020  0.000015  1.225436e-06   \n",
       "1                             0.004697  0.002161  0.001780 -2.280778e-04   \n",
       "2                            -0.003856 -0.001148 -0.000666  7.368991e-05   \n",
       "3                             0.000336 -0.000327 -0.000169  9.679006e-05   \n",
       "4                             0.001590  0.000970  0.000640  1.437861e-06   \n",
       "5                             0.003728  0.001330  0.001036  1.105184e-04   \n",
       "6                            -0.001157  0.000005  0.000332  8.184730e-05   \n",
       "7                             0.006484  0.002139  0.000974 -1.552227e-04   \n",
       "8                             0.004414  0.001481  0.001016  2.716150e-04   \n",
       "9                             0.005248  0.001729  0.001475 -1.495743e-04   \n",
       "10                            0.001181  0.000820  0.000400  4.004515e-05   \n",
       "11                            0.023174  0.001861  0.000937 -1.028693e-05   \n",
       "12                            0.017415  0.000266 -0.000202  1.213743e-05   \n",
       "13                           -0.002741  0.021873  0.016492  1.811638e-03   \n",
       "14                            0.004271  0.001483  0.001156 -1.569236e-04   \n",
       "15                           -0.001632 -0.001013 -0.000958  1.110465e-04   \n",
       "16                            0.000623  0.000268  0.000312  4.709231e-05   \n",
       "17                           -0.001697  0.000643  0.000619  5.518338e-05   \n",
       "18                            0.006068  0.002342  0.001110 -1.851759e-05   \n",
       "19                           -0.006494 -0.003374 -0.001830  1.915490e-04   \n",
       "20                           -0.003390 -0.003136 -0.001398  2.193575e-04   \n",
       "21                            0.001185  0.001025  0.000925 -5.173188e-04   \n",
       "22                            0.001437  0.000612  0.000379 -1.246739e-04   \n",
       "23                            0.004143  0.001456  0.001146 -1.890894e-04   \n",
       "24                            0.001525  0.000853  0.000558 -3.922955e-04   \n",
       "25                            0.004105  0.000786  0.000549  6.885890e-03   \n",
       "26                           -0.002911 -0.001534 -0.000347  1.296588e-04   \n",
       "27                            0.001041  0.001034  0.000679  2.603260e-07   \n",
       "28                            0.002350  0.001329  0.000781 -4.481235e-06   \n",
       "29                            0.004093  0.001148  0.000534  5.841003e-06   \n",
       "30                           -0.004060 -0.001774 -0.000247  1.611079e-04   \n",
       "31                            0.001403  0.001001  0.000529  1.765248e-05   \n",
       "32                            0.005039  0.001661  0.001428 -1.623947e-04   \n",
       "33                            0.002697 -0.000904 -0.001212  6.130039e-05   \n",
       "34                           -0.000557  0.000249  0.000368  4.304762e-05   \n",
       "35                           -0.001402  0.000309  0.000534  5.849767e-05   \n",
       "36                            0.000990 -0.001467 -0.001416  1.351826e-04   \n",
       "37                           -0.237666 -0.001370 -0.000661  1.018665e-04   \n",
       "38                           -0.003981 -0.228974  0.014422  1.654167e-03   \n",
       "39                           -0.002592  0.019450 -0.234529  1.644134e-03   \n",
       "40                            0.004937  0.027584  0.020330 -2.818249e-01   \n",
       "41                           -0.006256 -0.001400 -0.000332  1.008877e-04   \n",
       "42                            0.004515  0.001492  0.001164 -1.545820e-05   \n",
       "43                            0.005712  0.001801  0.000998 -1.152046e-05   \n",
       "44                            0.005473  0.002018  0.001444 -1.508066e-04   \n",
       "\n",
       "Semi-elasticity wrt. product        41        42        43        44  \n",
       "Semi-elasticity of product                                            \n",
       "0                             0.000058  0.000030  0.000071  0.000062  \n",
       "1                             0.004698  0.001527  0.000847 -0.010173  \n",
       "2                            -0.003176  0.002042  0.005999  0.005874  \n",
       "3                             0.000085  0.002281 -0.001942  0.020213  \n",
       "4                             0.002408 -0.002248  0.002576  0.018398  \n",
       "5                             0.003478  0.002133  0.002106 -0.004025  \n",
       "6                            -0.000743  0.002347 -0.002467  0.010462  \n",
       "7                             0.005997 -0.000863  0.000420 -0.012832  \n",
       "8                             0.004016  0.000243  0.001754 -0.008927  \n",
       "9                             0.004600  0.000733 -0.000546 -0.008387  \n",
       "10                            0.002003  0.001274 -0.001036  0.008286  \n",
       "11                            0.005346 -0.000043 -0.006669  0.002035  \n",
       "12                            0.000657  0.000354  0.003585  0.003649  \n",
       "13                           -0.004339  0.002267  0.006119  0.006028  \n",
       "14                            0.003878  0.001015 -0.001166 -0.004851  \n",
       "15                           -0.000789  0.010035  0.028981  0.002228  \n",
       "16                            0.001173  0.003440 -0.002915  0.005511  \n",
       "17                           -0.000536  0.001805  0.000237  0.008149  \n",
       "18                            0.006226 -0.004222 -0.000713  0.002266  \n",
       "19                           -0.006651  0.002252  0.008215  0.006245  \n",
       "20                           -0.005635  0.002586  0.007940  0.007260  \n",
       "21                            0.001132  0.001522  0.002991 -0.009073  \n",
       "22                            0.001489  0.003135 -0.005499  0.005651  \n",
       "23                            0.003784 -0.001836  0.000279 -0.000974  \n",
       "24                            0.001924  0.002178  0.002441 -0.001929  \n",
       "25                            0.003786  0.000736  0.002712 -0.005435  \n",
       "26                           -0.005247  0.001740  0.007360  0.005077  \n",
       "27                            0.002237 -0.001765  0.003163  0.006411  \n",
       "28                            0.003204 -0.001341  0.001199  0.005136  \n",
       "29                            0.003665  0.002471 -0.008515  0.002247  \n",
       "30                           -0.006614  0.002907  0.009613  0.005917  \n",
       "31                            0.002311 -0.002231  0.002065  0.018171  \n",
       "32                            0.004530  0.001508  0.001768 -0.002922  \n",
       "33                            0.002540  0.002362 -0.003103  0.005568  \n",
       "34                            0.000137  0.003720 -0.002353  0.005665  \n",
       "35                           -0.000133 -0.001162  0.005849  0.010776  \n",
       "36                            0.001424  0.002337 -0.001496  0.009679  \n",
       "37                           -0.006143  0.002249  0.006861  0.005757  \n",
       "38                           -0.003996  0.002159  0.006290  0.006171  \n",
       "39                           -0.001277  0.002272  0.004700  0.005952  \n",
       "40                            0.004802 -0.000373 -0.000671 -0.007688  \n",
       "41                           -0.195033  0.002070  0.006089  0.005842  \n",
       "42                            0.004082 -0.241191  0.020364 -0.000774  \n",
       "43                            0.004978  0.008443 -0.234533  0.001056  \n",
       "44                            0.005455 -0.000366  0.001206 -0.251119  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_hat = Similarity_elasticity(qOpt, z_logit, ThetaOptBLP, Model, char_number = pr_index)\n",
    "pd.DataFrame(E_hat[0]).rename_axis(index = 'Semi-elasticity of product', columns = 'Semi-elasticity wrt. product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios for the Similarity Model\n",
    "\n",
    "The diversion ratio to product j from product k is the fraction of consumers leaving product k and switching to product j following a one percent increase in the price of product k. Hence we have:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{tjk} = -100 \\cdot \\frac{\\partial P_j(u_t|\\lambda) / \\partial x_{tk\\ell}}{\\partial P_k(u_t|\\lambda) / \\partial x_{tk\\ell}} = -100 \\cdot \\frac{\\partial P_j(u_t|\\lambda) / \\partial u_{tk}}{\\partial P_k(u_t|\\lambda) / \\partial u_{tk}}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{D}_{t} = \\left( \\mathcal{D}_{tjk} \\right)_{j,k \\in \\{0,1,\\ldots,J_t\\}}$ is the matrix of diversion ratios for market $t$. This can be written more compactly as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_t = -100 \\cdot  (\\nabla_u P(u|\\lambda) \\circ I_J)^{-1}\\nabla_u P(u|\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_diversion_ratio(q, x, Theta, model):\n",
    "    '''\n",
    "    This function calculates diversion ratios from the Similarity Model\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Diversion_ratio: a dictionary of T numpy arrays (J,J) of diversion ratios from product j to product k for each individual i\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "\n",
    "    Grad = ccp_gradient(q, x, Theta, model) # Find the derivatives of ccp's wrt. utilities\n",
    "    inv_diaggrad = {t: np.divide(1, np.diag(Grad[t]), out = np.zeros_like(np.diag(Grad[t])), where = (np.diag(Grad[t]) != 0)) for t in np.arange(T)}  # Compute the inverse of the 'own'-derivatives of ccp's\n",
    "    DR = {t: np.multiply(-100, np.einsum('j,jk->jk', inv_diaggrad[t], Grad[t])) for t in np.arange(T)} # Compute diversion ratios as a hadamard product.\n",
    "    \n",
    "    return DR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the implied diversion ratios $\\mathcal{ D}_t$ from our estimates $\\hat \\theta^{\\text{Similarity}}$, we find for market $t=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Diversion ratio wrt. product</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diversion ratio of product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.387480</td>\n",
       "      <td>2.660693</td>\n",
       "      <td>4.575202</td>\n",
       "      <td>3.318107</td>\n",
       "      <td>4.111900</td>\n",
       "      <td>2.272980</td>\n",
       "      <td>8.275516</td>\n",
       "      <td>0.617707</td>\n",
       "      <td>2.461866</td>\n",
       "      <td>...</td>\n",
       "      <td>1.677369</td>\n",
       "      <td>1.352992</td>\n",
       "      <td>3.017614</td>\n",
       "      <td>1.038177</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.062257</td>\n",
       "      <td>2.963479</td>\n",
       "      <td>1.502768</td>\n",
       "      <td>3.624784</td>\n",
       "      <td>3.173936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60.469500</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.529167</td>\n",
       "      <td>3.866165</td>\n",
       "      <td>1.749631</td>\n",
       "      <td>-1.189705</td>\n",
       "      <td>2.128258</td>\n",
       "      <td>-12.153008</td>\n",
       "      <td>-0.913291</td>\n",
       "      <td>-2.391402</td>\n",
       "      <td>...</td>\n",
       "      <td>1.606052</td>\n",
       "      <td>1.603856</td>\n",
       "      <td>1.928039</td>\n",
       "      <td>0.887003</td>\n",
       "      <td>0.730656</td>\n",
       "      <td>-0.093614</td>\n",
       "      <td>1.928391</td>\n",
       "      <td>0.626792</td>\n",
       "      <td>0.347782</td>\n",
       "      <td>-4.175364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.653282</td>\n",
       "      <td>0.813030</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-0.086513</td>\n",
       "      <td>1.266147</td>\n",
       "      <td>1.517779</td>\n",
       "      <td>-0.027602</td>\n",
       "      <td>5.370454</td>\n",
       "      <td>0.294729</td>\n",
       "      <td>1.492123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.163766</td>\n",
       "      <td>0.185656</td>\n",
       "      <td>-1.613793</td>\n",
       "      <td>-0.480251</td>\n",
       "      <td>-0.278623</td>\n",
       "      <td>0.030838</td>\n",
       "      <td>-1.329252</td>\n",
       "      <td>0.854338</td>\n",
       "      <td>2.510353</td>\n",
       "      <td>2.458086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.907182</td>\n",
       "      <td>1.161554</td>\n",
       "      <td>-0.048886</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>5.881460</td>\n",
       "      <td>2.639372</td>\n",
       "      <td>-2.763588</td>\n",
       "      <td>9.979497</td>\n",
       "      <td>0.479174</td>\n",
       "      <td>2.201255</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.255751</td>\n",
       "      <td>-1.340111</td>\n",
       "      <td>0.136624</td>\n",
       "      <td>-0.133162</td>\n",
       "      <td>-0.068884</td>\n",
       "      <td>0.039358</td>\n",
       "      <td>0.034480</td>\n",
       "      <td>0.927468</td>\n",
       "      <td>-0.789652</td>\n",
       "      <td>8.219180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.680062</td>\n",
       "      <td>0.734163</td>\n",
       "      <td>0.999260</td>\n",
       "      <td>8.214330</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>2.039063</td>\n",
       "      <td>0.543389</td>\n",
       "      <td>4.498427</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>1.057969</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.148790</td>\n",
       "      <td>0.295994</td>\n",
       "      <td>0.654812</td>\n",
       "      <td>0.399442</td>\n",
       "      <td>0.263775</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.991939</td>\n",
       "      <td>-0.926073</td>\n",
       "      <td>1.060851</td>\n",
       "      <td>7.577503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.006506</td>\n",
       "      <td>-0.451479</td>\n",
       "      <td>1.083316</td>\n",
       "      <td>3.333804</td>\n",
       "      <td>1.844093</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.729358</td>\n",
       "      <td>-4.069884</td>\n",
       "      <td>-0.107392</td>\n",
       "      <td>-0.550378</td>\n",
       "      <td>...</td>\n",
       "      <td>1.122771</td>\n",
       "      <td>1.057597</td>\n",
       "      <td>1.720739</td>\n",
       "      <td>0.614010</td>\n",
       "      <td>0.478332</td>\n",
       "      <td>0.051016</td>\n",
       "      <td>1.605609</td>\n",
       "      <td>0.984403</td>\n",
       "      <td>0.972293</td>\n",
       "      <td>-1.858056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59.978718</td>\n",
       "      <td>1.288594</td>\n",
       "      <td>-0.031433</td>\n",
       "      <td>-5.569371</td>\n",
       "      <td>0.784073</td>\n",
       "      <td>2.759170</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>24.218127</td>\n",
       "      <td>1.578768</td>\n",
       "      <td>2.512724</td>\n",
       "      <td>...</td>\n",
       "      <td>2.472756</td>\n",
       "      <td>1.211340</td>\n",
       "      <td>-0.471091</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.135048</td>\n",
       "      <td>0.033321</td>\n",
       "      <td>-0.302582</td>\n",
       "      <td>0.955377</td>\n",
       "      <td>-1.004276</td>\n",
       "      <td>4.259038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61.229248</td>\n",
       "      <td>-2.063185</td>\n",
       "      <td>1.714800</td>\n",
       "      <td>5.639023</td>\n",
       "      <td>1.819990</td>\n",
       "      <td>-1.820698</td>\n",
       "      <td>6.790518</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.380763</td>\n",
       "      <td>-2.951378</td>\n",
       "      <td>...</td>\n",
       "      <td>4.475344</td>\n",
       "      <td>3.692894</td>\n",
       "      <td>2.694916</td>\n",
       "      <td>0.888912</td>\n",
       "      <td>0.404741</td>\n",
       "      <td>-0.064511</td>\n",
       "      <td>2.492233</td>\n",
       "      <td>-0.358617</td>\n",
       "      <td>0.174512</td>\n",
       "      <td>-5.332898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60.265815</td>\n",
       "      <td>-2.044505</td>\n",
       "      <td>1.240938</td>\n",
       "      <td>3.570366</td>\n",
       "      <td>1.744541</td>\n",
       "      <td>-0.633506</td>\n",
       "      <td>5.837210</td>\n",
       "      <td>5.020870</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-1.865979</td>\n",
       "      <td>...</td>\n",
       "      <td>4.078448</td>\n",
       "      <td>3.471582</td>\n",
       "      <td>1.805713</td>\n",
       "      <td>0.605888</td>\n",
       "      <td>0.415755</td>\n",
       "      <td>0.111108</td>\n",
       "      <td>1.642932</td>\n",
       "      <td>0.099422</td>\n",
       "      <td>0.717657</td>\n",
       "      <td>-3.651505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64.887494</td>\n",
       "      <td>-1.446238</td>\n",
       "      <td>1.697227</td>\n",
       "      <td>4.430966</td>\n",
       "      <td>1.524804</td>\n",
       "      <td>-0.877101</td>\n",
       "      <td>2.509804</td>\n",
       "      <td>-10.513748</td>\n",
       "      <td>-0.504098</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.712118</td>\n",
       "      <td>1.523249</td>\n",
       "      <td>2.311494</td>\n",
       "      <td>0.761393</td>\n",
       "      <td>0.649434</td>\n",
       "      <td>-0.065878</td>\n",
       "      <td>2.026014</td>\n",
       "      <td>0.322942</td>\n",
       "      <td>-0.240353</td>\n",
       "      <td>-3.694033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>66.979094</td>\n",
       "      <td>0.707718</td>\n",
       "      <td>0.592168</td>\n",
       "      <td>-3.362890</td>\n",
       "      <td>1.179851</td>\n",
       "      <td>1.740138</td>\n",
       "      <td>-1.681983</td>\n",
       "      <td>7.002975</td>\n",
       "      <td>0.348337</td>\n",
       "      <td>14.117882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.066641</td>\n",
       "      <td>-0.645200</td>\n",
       "      <td>0.537014</td>\n",
       "      <td>0.372864</td>\n",
       "      <td>0.181878</td>\n",
       "      <td>0.018206</td>\n",
       "      <td>0.910460</td>\n",
       "      <td>0.579261</td>\n",
       "      <td>-0.471209</td>\n",
       "      <td>3.767280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>63.327033</td>\n",
       "      <td>0.212889</td>\n",
       "      <td>1.689897</td>\n",
       "      <td>-0.641422</td>\n",
       "      <td>0.927215</td>\n",
       "      <td>1.074338</td>\n",
       "      <td>-0.135339</td>\n",
       "      <td>-0.187576</td>\n",
       "      <td>0.119076</td>\n",
       "      <td>0.082043</td>\n",
       "      <td>...</td>\n",
       "      <td>1.122655</td>\n",
       "      <td>-0.279908</td>\n",
       "      <td>9.961093</td>\n",
       "      <td>0.800114</td>\n",
       "      <td>0.402839</td>\n",
       "      <td>-0.004422</td>\n",
       "      <td>2.298076</td>\n",
       "      <td>-0.018331</td>\n",
       "      <td>-2.866578</td>\n",
       "      <td>0.874856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>63.582957</td>\n",
       "      <td>0.384577</td>\n",
       "      <td>-0.521084</td>\n",
       "      <td>2.337138</td>\n",
       "      <td>0.524185</td>\n",
       "      <td>0.729557</td>\n",
       "      <td>1.143910</td>\n",
       "      <td>2.256750</td>\n",
       "      <td>0.097572</td>\n",
       "      <td>0.365997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.403576</td>\n",
       "      <td>0.590107</td>\n",
       "      <td>7.515828</td>\n",
       "      <td>0.114867</td>\n",
       "      <td>-0.087306</td>\n",
       "      <td>0.005238</td>\n",
       "      <td>0.283422</td>\n",
       "      <td>0.152804</td>\n",
       "      <td>1.547027</td>\n",
       "      <td>1.574748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>58.444183</td>\n",
       "      <td>1.200303</td>\n",
       "      <td>-0.090903</td>\n",
       "      <td>0.570410</td>\n",
       "      <td>1.692422</td>\n",
       "      <td>2.219699</td>\n",
       "      <td>0.062557</td>\n",
       "      <td>9.669036</td>\n",
       "      <td>0.349363</td>\n",
       "      <td>1.505882</td>\n",
       "      <td>...</td>\n",
       "      <td>0.378692</td>\n",
       "      <td>-0.623867</td>\n",
       "      <td>-1.087195</td>\n",
       "      <td>8.677156</td>\n",
       "      <td>6.542356</td>\n",
       "      <td>0.718675</td>\n",
       "      <td>-1.721417</td>\n",
       "      <td>0.899327</td>\n",
       "      <td>2.427540</td>\n",
       "      <td>2.391199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>77.454338</td>\n",
       "      <td>-0.808982</td>\n",
       "      <td>1.540877</td>\n",
       "      <td>3.606396</td>\n",
       "      <td>1.147087</td>\n",
       "      <td>1.455618</td>\n",
       "      <td>2.085813</td>\n",
       "      <td>-9.105410</td>\n",
       "      <td>-0.499582</td>\n",
       "      <td>-2.243516</td>\n",
       "      <td>...</td>\n",
       "      <td>1.503818</td>\n",
       "      <td>1.230140</td>\n",
       "      <td>2.245202</td>\n",
       "      <td>0.779906</td>\n",
       "      <td>0.608010</td>\n",
       "      <td>-0.082500</td>\n",
       "      <td>2.038624</td>\n",
       "      <td>0.533706</td>\n",
       "      <td>-0.613027</td>\n",
       "      <td>-2.550531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>61.190369</td>\n",
       "      <td>0.939171</td>\n",
       "      <td>-1.096236</td>\n",
       "      <td>1.305756</td>\n",
       "      <td>1.195475</td>\n",
       "      <td>2.126230</td>\n",
       "      <td>0.193735</td>\n",
       "      <td>7.949890</td>\n",
       "      <td>0.322513</td>\n",
       "      <td>1.318823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.142085</td>\n",
       "      <td>0.084904</td>\n",
       "      <td>-0.677791</td>\n",
       "      <td>-0.420672</td>\n",
       "      <td>-0.397761</td>\n",
       "      <td>0.046122</td>\n",
       "      <td>-0.327762</td>\n",
       "      <td>4.167842</td>\n",
       "      <td>12.037093</td>\n",
       "      <td>0.925189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>77.937041</td>\n",
       "      <td>1.808230</td>\n",
       "      <td>0.008980</td>\n",
       "      <td>-3.566296</td>\n",
       "      <td>0.809796</td>\n",
       "      <td>4.065925</td>\n",
       "      <td>-1.779129</td>\n",
       "      <td>4.347853</td>\n",
       "      <td>0.317612</td>\n",
       "      <td>3.192827</td>\n",
       "      <td>...</td>\n",
       "      <td>0.172943</td>\n",
       "      <td>-0.797198</td>\n",
       "      <td>0.329747</td>\n",
       "      <td>0.141857</td>\n",
       "      <td>0.165257</td>\n",
       "      <td>0.024912</td>\n",
       "      <td>0.620790</td>\n",
       "      <td>1.819775</td>\n",
       "      <td>-1.542094</td>\n",
       "      <td>2.915376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>60.392946</td>\n",
       "      <td>4.100531</td>\n",
       "      <td>0.812656</td>\n",
       "      <td>-3.542207</td>\n",
       "      <td>1.298094</td>\n",
       "      <td>1.577958</td>\n",
       "      <td>-1.651858</td>\n",
       "      <td>6.451580</td>\n",
       "      <td>0.167227</td>\n",
       "      <td>1.845113</td>\n",
       "      <td>...</td>\n",
       "      <td>0.063885</td>\n",
       "      <td>-0.515690</td>\n",
       "      <td>-0.695505</td>\n",
       "      <td>0.263641</td>\n",
       "      <td>0.253674</td>\n",
       "      <td>0.022621</td>\n",
       "      <td>-0.219893</td>\n",
       "      <td>0.739814</td>\n",
       "      <td>0.096954</td>\n",
       "      <td>3.340334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60.836671</td>\n",
       "      <td>3.493927</td>\n",
       "      <td>1.900293</td>\n",
       "      <td>3.491311</td>\n",
       "      <td>-2.127550</td>\n",
       "      <td>0.366832</td>\n",
       "      <td>1.692412</td>\n",
       "      <td>-1.232852</td>\n",
       "      <td>-0.090510</td>\n",
       "      <td>-0.305521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.533151</td>\n",
       "      <td>0.939830</td>\n",
       "      <td>2.505645</td>\n",
       "      <td>0.967252</td>\n",
       "      <td>0.458510</td>\n",
       "      <td>-0.007647</td>\n",
       "      <td>2.571076</td>\n",
       "      <td>-1.743337</td>\n",
       "      <td>-0.294344</td>\n",
       "      <td>0.935594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60.149039</td>\n",
       "      <td>4.698178</td>\n",
       "      <td>-1.636937</td>\n",
       "      <td>0.097366</td>\n",
       "      <td>1.066804</td>\n",
       "      <td>2.270049</td>\n",
       "      <td>0.193044</td>\n",
       "      <td>9.138842</td>\n",
       "      <td>0.277360</td>\n",
       "      <td>1.827967</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.076435</td>\n",
       "      <td>-0.906602</td>\n",
       "      <td>-2.651503</td>\n",
       "      <td>-1.377584</td>\n",
       "      <td>-0.747189</td>\n",
       "      <td>0.078204</td>\n",
       "      <td>-2.715524</td>\n",
       "      <td>0.919305</td>\n",
       "      <td>3.353924</td>\n",
       "      <td>2.549784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>55.321262</td>\n",
       "      <td>4.721505</td>\n",
       "      <td>-0.666721</td>\n",
       "      <td>-0.478200</td>\n",
       "      <td>1.666189</td>\n",
       "      <td>2.401066</td>\n",
       "      <td>0.245383</td>\n",
       "      <td>11.331813</td>\n",
       "      <td>0.242211</td>\n",
       "      <td>1.806889</td>\n",
       "      <td>...</td>\n",
       "      <td>0.289014</td>\n",
       "      <td>-0.612788</td>\n",
       "      <td>-1.272993</td>\n",
       "      <td>-1.177457</td>\n",
       "      <td>-0.525082</td>\n",
       "      <td>0.082369</td>\n",
       "      <td>-2.115983</td>\n",
       "      <td>0.971218</td>\n",
       "      <td>2.981476</td>\n",
       "      <td>2.726325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>64.955145</td>\n",
       "      <td>1.696266</td>\n",
       "      <td>0.378859</td>\n",
       "      <td>1.637520</td>\n",
       "      <td>1.154458</td>\n",
       "      <td>1.541739</td>\n",
       "      <td>0.557842</td>\n",
       "      <td>-8.225238</td>\n",
       "      <td>4.969385</td>\n",
       "      <td>-1.339751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.340445</td>\n",
       "      <td>0.696640</td>\n",
       "      <td>0.522253</td>\n",
       "      <td>0.451927</td>\n",
       "      <td>0.407918</td>\n",
       "      <td>-0.228082</td>\n",
       "      <td>0.499075</td>\n",
       "      <td>0.671201</td>\n",
       "      <td>1.318835</td>\n",
       "      <td>-4.000276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>63.642470</td>\n",
       "      <td>1.356499</td>\n",
       "      <td>0.228874</td>\n",
       "      <td>-3.506816</td>\n",
       "      <td>0.524194</td>\n",
       "      <td>4.902846</td>\n",
       "      <td>-1.837075</td>\n",
       "      <td>4.173987</td>\n",
       "      <td>0.440665</td>\n",
       "      <td>2.622094</td>\n",
       "      <td>...</td>\n",
       "      <td>0.226863</td>\n",
       "      <td>-0.801488</td>\n",
       "      <td>0.620670</td>\n",
       "      <td>0.264463</td>\n",
       "      <td>0.163821</td>\n",
       "      <td>-0.053857</td>\n",
       "      <td>0.643306</td>\n",
       "      <td>1.354382</td>\n",
       "      <td>-2.375547</td>\n",
       "      <td>2.441160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>64.309252</td>\n",
       "      <td>-0.143830</td>\n",
       "      <td>1.503926</td>\n",
       "      <td>3.610438</td>\n",
       "      <td>-1.739213</td>\n",
       "      <td>1.679331</td>\n",
       "      <td>1.776339</td>\n",
       "      <td>-3.127421</td>\n",
       "      <td>-0.008232</td>\n",
       "      <td>-0.656399</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.393350</td>\n",
       "      <td>1.079149</td>\n",
       "      <td>1.808402</td>\n",
       "      <td>0.635587</td>\n",
       "      <td>0.500128</td>\n",
       "      <td>-0.082539</td>\n",
       "      <td>1.651580</td>\n",
       "      <td>-0.801612</td>\n",
       "      <td>0.121932</td>\n",
       "      <td>-0.425076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>61.557200</td>\n",
       "      <td>-0.848153</td>\n",
       "      <td>0.291875</td>\n",
       "      <td>1.209435</td>\n",
       "      <td>1.070327</td>\n",
       "      <td>1.900255</td>\n",
       "      <td>0.523544</td>\n",
       "      <td>-2.524506</td>\n",
       "      <td>0.338435</td>\n",
       "      <td>-1.521526</td>\n",
       "      <td>...</td>\n",
       "      <td>0.427808</td>\n",
       "      <td>0.525302</td>\n",
       "      <td>0.637026</td>\n",
       "      <td>0.356362</td>\n",
       "      <td>0.233193</td>\n",
       "      <td>-0.163912</td>\n",
       "      <td>0.803774</td>\n",
       "      <td>0.909937</td>\n",
       "      <td>1.020078</td>\n",
       "      <td>-0.806133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>69.523181</td>\n",
       "      <td>-1.015781</td>\n",
       "      <td>1.379396</td>\n",
       "      <td>3.855826</td>\n",
       "      <td>2.392729</td>\n",
       "      <td>-15.301063</td>\n",
       "      <td>2.354170</td>\n",
       "      <td>-3.371867</td>\n",
       "      <td>-8.460292</td>\n",
       "      <td>-0.887293</td>\n",
       "      <td>...</td>\n",
       "      <td>1.634017</td>\n",
       "      <td>1.361980</td>\n",
       "      <td>1.937074</td>\n",
       "      <td>0.370877</td>\n",
       "      <td>0.258996</td>\n",
       "      <td>3.249446</td>\n",
       "      <td>1.786759</td>\n",
       "      <td>0.347150</td>\n",
       "      <td>1.279791</td>\n",
       "      <td>-2.564794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>74.778292</td>\n",
       "      <td>1.250633</td>\n",
       "      <td>-1.061780</td>\n",
       "      <td>1.619472</td>\n",
       "      <td>1.151447</td>\n",
       "      <td>2.309941</td>\n",
       "      <td>0.435552</td>\n",
       "      <td>11.540877</td>\n",
       "      <td>0.373273</td>\n",
       "      <td>1.700425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069441</td>\n",
       "      <td>0.070249</td>\n",
       "      <td>-1.477670</td>\n",
       "      <td>-0.778710</td>\n",
       "      <td>-0.176219</td>\n",
       "      <td>0.065811</td>\n",
       "      <td>-2.663460</td>\n",
       "      <td>0.883163</td>\n",
       "      <td>3.735883</td>\n",
       "      <td>2.577140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>61.936025</td>\n",
       "      <td>0.735564</td>\n",
       "      <td>6.046506</td>\n",
       "      <td>0.901813</td>\n",
       "      <td>-2.219771</td>\n",
       "      <td>1.583975</td>\n",
       "      <td>0.364760</td>\n",
       "      <td>3.143683</td>\n",
       "      <td>0.275844</td>\n",
       "      <td>1.309120</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.111009</td>\n",
       "      <td>0.348670</td>\n",
       "      <td>0.437666</td>\n",
       "      <td>0.434658</td>\n",
       "      <td>0.285330</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.940490</td>\n",
       "      <td>-0.741928</td>\n",
       "      <td>1.329521</td>\n",
       "      <td>2.695191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>61.501625</td>\n",
       "      <td>0.889931</td>\n",
       "      <td>6.342602</td>\n",
       "      <td>1.230168</td>\n",
       "      <td>-2.119050</td>\n",
       "      <td>1.881146</td>\n",
       "      <td>0.581758</td>\n",
       "      <td>1.167276</td>\n",
       "      <td>0.210371</td>\n",
       "      <td>1.628975</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.897001</td>\n",
       "      <td>0.578170</td>\n",
       "      <td>0.980863</td>\n",
       "      <td>0.554869</td>\n",
       "      <td>0.325866</td>\n",
       "      <td>-0.001871</td>\n",
       "      <td>1.337660</td>\n",
       "      <td>-0.559823</td>\n",
       "      <td>0.500701</td>\n",
       "      <td>2.143942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>60.737783</td>\n",
       "      <td>0.923454</td>\n",
       "      <td>5.907957</td>\n",
       "      <td>-2.361190</td>\n",
       "      <td>0.738597</td>\n",
       "      <td>4.552436</td>\n",
       "      <td>-1.131413</td>\n",
       "      <td>0.428507</td>\n",
       "      <td>0.214377</td>\n",
       "      <td>1.189144</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639274</td>\n",
       "      <td>-0.688364</td>\n",
       "      <td>1.687584</td>\n",
       "      <td>0.473165</td>\n",
       "      <td>0.220306</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>1.511107</td>\n",
       "      <td>1.018692</td>\n",
       "      <td>-3.510441</td>\n",
       "      <td>0.926456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>57.249370</td>\n",
       "      <td>1.068514</td>\n",
       "      <td>4.050268</td>\n",
       "      <td>0.937332</td>\n",
       "      <td>1.505403</td>\n",
       "      <td>2.133127</td>\n",
       "      <td>0.210899</td>\n",
       "      <td>10.481250</td>\n",
       "      <td>0.349698</td>\n",
       "      <td>1.552051</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121481</td>\n",
       "      <td>0.149936</td>\n",
       "      <td>-1.577507</td>\n",
       "      <td>-0.689232</td>\n",
       "      <td>-0.095865</td>\n",
       "      <td>0.062605</td>\n",
       "      <td>-2.570185</td>\n",
       "      <td>1.129724</td>\n",
       "      <td>3.735646</td>\n",
       "      <td>2.299098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>60.738675</td>\n",
       "      <td>0.666366</td>\n",
       "      <td>0.969771</td>\n",
       "      <td>8.026549</td>\n",
       "      <td>2.331043</td>\n",
       "      <td>1.979331</td>\n",
       "      <td>0.061989</td>\n",
       "      <td>4.301059</td>\n",
       "      <td>0.342975</td>\n",
       "      <td>1.345995</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.328462</td>\n",
       "      <td>0.438069</td>\n",
       "      <td>0.578256</td>\n",
       "      <td>0.412845</td>\n",
       "      <td>0.218051</td>\n",
       "      <td>0.007278</td>\n",
       "      <td>0.952591</td>\n",
       "      <td>-0.919785</td>\n",
       "      <td>0.851383</td>\n",
       "      <td>7.491537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>58.794900</td>\n",
       "      <td>-1.465002</td>\n",
       "      <td>1.445828</td>\n",
       "      <td>11.313034</td>\n",
       "      <td>7.577525</td>\n",
       "      <td>-2.930264</td>\n",
       "      <td>2.638354</td>\n",
       "      <td>-12.233369</td>\n",
       "      <td>-0.721597</td>\n",
       "      <td>-2.872841</td>\n",
       "      <td>...</td>\n",
       "      <td>1.835744</td>\n",
       "      <td>1.470637</td>\n",
       "      <td>2.011046</td>\n",
       "      <td>0.663030</td>\n",
       "      <td>0.569800</td>\n",
       "      <td>-0.064808</td>\n",
       "      <td>1.808024</td>\n",
       "      <td>0.601748</td>\n",
       "      <td>0.705698</td>\n",
       "      <td>-1.166142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>67.382476</td>\n",
       "      <td>1.458343</td>\n",
       "      <td>0.919446</td>\n",
       "      <td>-2.884178</td>\n",
       "      <td>1.067752</td>\n",
       "      <td>2.972394</td>\n",
       "      <td>-0.789724</td>\n",
       "      <td>3.817858</td>\n",
       "      <td>0.241431</td>\n",
       "      <td>2.314578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820011</td>\n",
       "      <td>-1.740678</td>\n",
       "      <td>1.233408</td>\n",
       "      <td>-0.413248</td>\n",
       "      <td>-0.554238</td>\n",
       "      <td>0.028037</td>\n",
       "      <td>1.161733</td>\n",
       "      <td>1.080379</td>\n",
       "      <td>-1.419048</td>\n",
       "      <td>2.546845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>67.166837</td>\n",
       "      <td>1.735013</td>\n",
       "      <td>-0.175068</td>\n",
       "      <td>-3.756026</td>\n",
       "      <td>0.042014</td>\n",
       "      <td>5.028758</td>\n",
       "      <td>-1.995422</td>\n",
       "      <td>4.829770</td>\n",
       "      <td>0.345154</td>\n",
       "      <td>3.064892</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.318573</td>\n",
       "      <td>-0.676590</td>\n",
       "      <td>-0.254025</td>\n",
       "      <td>0.113535</td>\n",
       "      <td>0.167673</td>\n",
       "      <td>0.019626</td>\n",
       "      <td>0.062353</td>\n",
       "      <td>1.696155</td>\n",
       "      <td>-1.072807</td>\n",
       "      <td>2.582548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>60.113489</td>\n",
       "      <td>1.320667</td>\n",
       "      <td>0.253284</td>\n",
       "      <td>-0.699990</td>\n",
       "      <td>-2.251274</td>\n",
       "      <td>2.432913</td>\n",
       "      <td>3.358328</td>\n",
       "      <td>21.677341</td>\n",
       "      <td>1.498131</td>\n",
       "      <td>2.327987</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>2.445127</td>\n",
       "      <td>-0.572080</td>\n",
       "      <td>0.125898</td>\n",
       "      <td>0.218073</td>\n",
       "      <td>0.023869</td>\n",
       "      <td>-0.054377</td>\n",
       "      <td>-0.474031</td>\n",
       "      <td>2.386695</td>\n",
       "      <td>4.397057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>59.591284</td>\n",
       "      <td>1.620852</td>\n",
       "      <td>0.352886</td>\n",
       "      <td>-4.507747</td>\n",
       "      <td>0.712876</td>\n",
       "      <td>2.816435</td>\n",
       "      <td>2.021864</td>\n",
       "      <td>21.983182</td>\n",
       "      <td>1.567207</td>\n",
       "      <td>2.545433</td>\n",
       "      <td>...</td>\n",
       "      <td>3.005007</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.400590</td>\n",
       "      <td>-0.593181</td>\n",
       "      <td>-0.572780</td>\n",
       "      <td>0.054679</td>\n",
       "      <td>0.576155</td>\n",
       "      <td>0.945234</td>\n",
       "      <td>-0.605162</td>\n",
       "      <td>3.914825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>61.988718</td>\n",
       "      <td>0.908773</td>\n",
       "      <td>-1.430656</td>\n",
       "      <td>0.214342</td>\n",
       "      <td>0.735547</td>\n",
       "      <td>2.137253</td>\n",
       "      <td>-0.366735</td>\n",
       "      <td>7.482216</td>\n",
       "      <td>0.380197</td>\n",
       "      <td>1.801545</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.327916</td>\n",
       "      <td>0.186836</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-0.576313</td>\n",
       "      <td>-0.278226</td>\n",
       "      <td>0.042861</td>\n",
       "      <td>-2.584841</td>\n",
       "      <td>0.946118</td>\n",
       "      <td>2.886768</td>\n",
       "      <td>2.422184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>64.341843</td>\n",
       "      <td>1.261356</td>\n",
       "      <td>-1.284485</td>\n",
       "      <td>-0.630283</td>\n",
       "      <td>1.353692</td>\n",
       "      <td>2.300859</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>7.445899</td>\n",
       "      <td>0.384880</td>\n",
       "      <td>1.790337</td>\n",
       "      <td>...</td>\n",
       "      <td>0.217719</td>\n",
       "      <td>-0.834684</td>\n",
       "      <td>-1.738728</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>6.298724</td>\n",
       "      <td>0.722424</td>\n",
       "      <td>-1.745172</td>\n",
       "      <td>0.943025</td>\n",
       "      <td>2.746905</td>\n",
       "      <td>2.694997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>62.818063</td>\n",
       "      <td>1.368003</td>\n",
       "      <td>-0.981159</td>\n",
       "      <td>-0.429270</td>\n",
       "      <td>1.176961</td>\n",
       "      <td>2.359964</td>\n",
       "      <td>0.417609</td>\n",
       "      <td>4.463719</td>\n",
       "      <td>0.347722</td>\n",
       "      <td>2.010583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496526</td>\n",
       "      <td>-1.061167</td>\n",
       "      <td>-1.105178</td>\n",
       "      <td>8.293041</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.701038</td>\n",
       "      <td>-0.544475</td>\n",
       "      <td>0.968775</td>\n",
       "      <td>2.003851</td>\n",
       "      <td>2.537817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>52.275854</td>\n",
       "      <td>-1.803600</td>\n",
       "      <td>1.117463</td>\n",
       "      <td>2.523895</td>\n",
       "      <td>0.027192</td>\n",
       "      <td>2.590046</td>\n",
       "      <td>1.060304</td>\n",
       "      <td>-7.321174</td>\n",
       "      <td>0.956239</td>\n",
       "      <td>-2.098707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.559239</td>\n",
       "      <td>1.042428</td>\n",
       "      <td>1.751964</td>\n",
       "      <td>9.787706</td>\n",
       "      <td>7.213873</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.704004</td>\n",
       "      <td>-0.132398</td>\n",
       "      <td>-0.238003</td>\n",
       "      <td>-2.728028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>75.539197</td>\n",
       "      <td>1.127862</td>\n",
       "      <td>-1.462232</td>\n",
       "      <td>0.067123</td>\n",
       "      <td>1.382611</td>\n",
       "      <td>2.474585</td>\n",
       "      <td>-0.292289</td>\n",
       "      <td>8.586085</td>\n",
       "      <td>0.429241</td>\n",
       "      <td>1.959369</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.038676</td>\n",
       "      <td>0.333444</td>\n",
       "      <td>-3.207416</td>\n",
       "      <td>-0.717772</td>\n",
       "      <td>-0.170084</td>\n",
       "      <td>0.051729</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.061269</td>\n",
       "      <td>3.122245</td>\n",
       "      <td>2.995548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>61.082846</td>\n",
       "      <td>0.584576</td>\n",
       "      <td>1.498634</td>\n",
       "      <td>2.879105</td>\n",
       "      <td>-2.058339</td>\n",
       "      <td>2.419313</td>\n",
       "      <td>1.471637</td>\n",
       "      <td>-1.970127</td>\n",
       "      <td>0.041421</td>\n",
       "      <td>0.498029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.537639</td>\n",
       "      <td>0.872326</td>\n",
       "      <td>1.872076</td>\n",
       "      <td>0.618484</td>\n",
       "      <td>0.482577</td>\n",
       "      <td>-0.006409</td>\n",
       "      <td>1.692320</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>8.443175</td>\n",
       "      <td>-0.320889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>62.816931</td>\n",
       "      <td>0.138290</td>\n",
       "      <td>1.877448</td>\n",
       "      <td>-1.045110</td>\n",
       "      <td>1.005295</td>\n",
       "      <td>1.018788</td>\n",
       "      <td>-0.659548</td>\n",
       "      <td>0.408748</td>\n",
       "      <td>0.127474</td>\n",
       "      <td>-0.158033</td>\n",
       "      <td>...</td>\n",
       "      <td>1.154112</td>\n",
       "      <td>-0.238111</td>\n",
       "      <td>2.435329</td>\n",
       "      <td>0.768097</td>\n",
       "      <td>0.425575</td>\n",
       "      <td>-0.004912</td>\n",
       "      <td>2.122712</td>\n",
       "      <td>3.599757</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.450425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>58.667970</td>\n",
       "      <td>-1.770874</td>\n",
       "      <td>1.960824</td>\n",
       "      <td>11.602799</td>\n",
       "      <td>7.659024</td>\n",
       "      <td>-2.076604</td>\n",
       "      <td>2.983410</td>\n",
       "      <td>-13.323011</td>\n",
       "      <td>-0.691809</td>\n",
       "      <td>-2.590641</td>\n",
       "      <td>...</td>\n",
       "      <td>2.267888</td>\n",
       "      <td>1.642963</td>\n",
       "      <td>2.179521</td>\n",
       "      <td>0.803783</td>\n",
       "      <td>0.574883</td>\n",
       "      <td>-0.060054</td>\n",
       "      <td>2.172245</td>\n",
       "      <td>-0.145925</td>\n",
       "      <td>0.480430</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows Ã 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Diversion ratio wrt. product          0           1           2           3   \\\n",
       "Diversion ratio of product                                                     \n",
       "0                            -100.000000    1.387480    2.660693    4.575202   \n",
       "1                              60.469500 -100.000000    1.529167    3.866165   \n",
       "2                              61.653282    0.813030 -100.000000   -0.086513   \n",
       "3                              59.907182    1.161554   -0.048886 -100.000000   \n",
       "4                              60.680062    0.734163    0.999260    8.214330   \n",
       "5                              68.006506   -0.451479    1.083316    3.333804   \n",
       "6                              59.978718    1.288594   -0.031433   -5.569371   \n",
       "7                              61.229248   -2.063185    1.714800    5.639023   \n",
       "8                              60.265815   -2.044505    1.240938    3.570366   \n",
       "9                              64.887494   -1.446238    1.697227    4.430966   \n",
       "10                             66.979094    0.707718    0.592168   -3.362890   \n",
       "11                             63.327033    0.212889    1.689897   -0.641422   \n",
       "12                             63.582957    0.384577   -0.521084    2.337138   \n",
       "13                             58.444183    1.200303   -0.090903    0.570410   \n",
       "14                             77.454338   -0.808982    1.540877    3.606396   \n",
       "15                             61.190369    0.939171   -1.096236    1.305756   \n",
       "16                             77.937041    1.808230    0.008980   -3.566296   \n",
       "17                             60.392946    4.100531    0.812656   -3.542207   \n",
       "18                             60.836671    3.493927    1.900293    3.491311   \n",
       "19                             60.149039    4.698178   -1.636937    0.097366   \n",
       "20                             55.321262    4.721505   -0.666721   -0.478200   \n",
       "21                             64.955145    1.696266    0.378859    1.637520   \n",
       "22                             63.642470    1.356499    0.228874   -3.506816   \n",
       "23                             64.309252   -0.143830    1.503926    3.610438   \n",
       "24                             61.557200   -0.848153    0.291875    1.209435   \n",
       "25                             69.523181   -1.015781    1.379396    3.855826   \n",
       "26                             74.778292    1.250633   -1.061780    1.619472   \n",
       "27                             61.936025    0.735564    6.046506    0.901813   \n",
       "28                             61.501625    0.889931    6.342602    1.230168   \n",
       "29                             60.737783    0.923454    5.907957   -2.361190   \n",
       "30                             57.249370    1.068514    4.050268    0.937332   \n",
       "31                             60.738675    0.666366    0.969771    8.026549   \n",
       "32                             58.794900   -1.465002    1.445828   11.313034   \n",
       "33                             67.382476    1.458343    0.919446   -2.884178   \n",
       "34                             67.166837    1.735013   -0.175068   -3.756026   \n",
       "35                             60.113489    1.320667    0.253284   -0.699990   \n",
       "36                             59.591284    1.620852    0.352886   -4.507747   \n",
       "37                             61.988718    0.908773   -1.430656    0.214342   \n",
       "38                             64.341843    1.261356   -1.284485   -0.630283   \n",
       "39                             62.818063    1.368003   -0.981159   -0.429270   \n",
       "40                             52.275854   -1.803600    1.117463    2.523895   \n",
       "41                             75.539197    1.127862   -1.462232    0.067123   \n",
       "42                             61.082846    0.584576    1.498634    2.879105   \n",
       "43                             62.816931    0.138290    1.877448   -1.045110   \n",
       "44                             58.667970   -1.770874    1.960824   11.602799   \n",
       "\n",
       "Diversion ratio wrt. product          4           5           6           7   \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               3.318107    4.111900    2.272980    8.275516   \n",
       "1                               1.749631   -1.189705    2.128258  -12.153008   \n",
       "2                               1.266147    1.517779   -0.027602    5.370454   \n",
       "3                               5.881460    2.639372   -2.763588    9.979497   \n",
       "4                            -100.000000    2.039063    0.543389    4.498427   \n",
       "5                               1.844093 -100.000000    1.729358   -4.069884   \n",
       "6                               0.784073    2.759170 -100.000000   24.218127   \n",
       "7                               1.819990   -1.820698    6.790518 -100.000000   \n",
       "8                               1.744541   -0.633506    5.837210    5.020870   \n",
       "9                               1.524804   -0.877101    2.509804  -10.513748   \n",
       "10                              1.179851    1.740138   -1.681983    7.002975   \n",
       "11                              0.927215    1.074338   -0.135339   -0.187576   \n",
       "12                              0.524185    0.729557    1.143910    2.256750   \n",
       "13                              1.692422    2.219699    0.062557    9.669036   \n",
       "14                              1.147087    1.455618    2.085813   -9.105410   \n",
       "15                              1.195475    2.126230    0.193735    7.949890   \n",
       "16                              0.809796    4.065925   -1.779129    4.347853   \n",
       "17                              1.298094    1.577958   -1.651858    6.451580   \n",
       "18                             -2.127550    0.366832    1.692412   -1.232852   \n",
       "19                              1.066804    2.270049    0.193044    9.138842   \n",
       "20                              1.666189    2.401066    0.245383   11.331813   \n",
       "21                              1.154458    1.541739    0.557842   -8.225238   \n",
       "22                              0.524194    4.902846   -1.837075    4.173987   \n",
       "23                             -1.739213    1.679331    1.776339   -3.127421   \n",
       "24                              1.070327    1.900255    0.523544   -2.524506   \n",
       "25                              2.392729  -15.301063    2.354170   -3.371867   \n",
       "26                              1.151447    2.309941    0.435552   11.540877   \n",
       "27                             -2.219771    1.583975    0.364760    3.143683   \n",
       "28                             -2.119050    1.881146    0.581758    1.167276   \n",
       "29                              0.738597    4.552436   -1.131413    0.428507   \n",
       "30                              1.505403    2.133127    0.210899   10.481250   \n",
       "31                              2.331043    1.979331    0.061989    4.301059   \n",
       "32                              7.577525   -2.930264    2.638354  -12.233369   \n",
       "33                              1.067752    2.972394   -0.789724    3.817858   \n",
       "34                              0.042014    5.028758   -1.995422    4.829770   \n",
       "35                             -2.251274    2.432913    3.358328   21.677341   \n",
       "36                              0.712876    2.816435    2.021864   21.983182   \n",
       "37                              0.735547    2.137253   -0.366735    7.482216   \n",
       "38                              1.353692    2.300859    0.005060    7.445899   \n",
       "39                              1.176961    2.359964    0.417609    4.463719   \n",
       "40                              0.027192    2.590046    1.060304   -7.321174   \n",
       "41                              1.382611    2.474585   -0.292289    8.586085   \n",
       "42                             -2.058339    2.419313    1.471637   -1.970127   \n",
       "43                              1.005295    1.018788   -0.659548    0.408748   \n",
       "44                              7.659024   -2.076604    2.983410  -13.323011   \n",
       "\n",
       "Diversion ratio wrt. product          8           9   ...          35  \\\n",
       "Diversion ratio of product                            ...               \n",
       "0                               0.617707    2.461866  ...    1.677369   \n",
       "1                              -0.913291   -2.391402  ...    1.606052   \n",
       "2                               0.294729    1.492123  ...    0.163766   \n",
       "3                               0.479174    2.201255  ...   -0.255751   \n",
       "4                               0.327000    1.057969  ...   -1.148790   \n",
       "5                              -0.107392   -0.550378  ...    1.122771   \n",
       "6                               1.578768    2.512724  ...    2.472756   \n",
       "7                               0.380763   -2.951378  ...    4.475344   \n",
       "8                            -100.000000   -1.865979  ...    4.078448   \n",
       "9                              -0.504098 -100.000000  ...    1.712118   \n",
       "10                              0.348337   14.117882  ...    0.066641   \n",
       "11                              0.119076    0.082043  ...    1.122655   \n",
       "12                              0.097572    0.365997  ...    0.403576   \n",
       "13                              0.349363    1.505882  ...    0.378692   \n",
       "14                             -0.499582   -2.243516  ...    1.503818   \n",
       "15                              0.322513    1.318823  ...   -0.142085   \n",
       "16                              0.317612    3.192827  ...    0.172943   \n",
       "17                              0.167227    1.845113  ...    0.063885   \n",
       "18                             -0.090510   -0.305521  ...   -0.533151   \n",
       "19                              0.277360    1.827967  ...   -0.076435   \n",
       "20                              0.242211    1.806889  ...    0.289014   \n",
       "21                              4.969385   -1.339751  ...    0.340445   \n",
       "22                              0.440665    2.622094  ...    0.226863   \n",
       "23                             -0.008232   -0.656399  ...   -0.393350   \n",
       "24                              0.338435   -1.521526  ...    0.427808   \n",
       "25                             -8.460292   -0.887293  ...    1.634017   \n",
       "26                              0.373273    1.700425  ...   -0.069441   \n",
       "27                              0.275844    1.309120  ...   -1.111009   \n",
       "28                              0.210371    1.628975  ...   -0.897001   \n",
       "29                              0.214377    1.189144  ...    0.639274   \n",
       "30                              0.349698    1.552051  ...    0.121481   \n",
       "31                              0.342975    1.345995  ...   -1.328462   \n",
       "32                             -0.721597   -2.872841  ...    1.835744   \n",
       "33                              0.241431    2.314578  ...    0.820011   \n",
       "34                              0.345154    3.064892  ...   -0.318573   \n",
       "35                              1.498131    2.327987  ... -100.000000   \n",
       "36                              1.567207    2.545433  ...    3.005007   \n",
       "37                              0.380197    1.801545  ...   -0.327916   \n",
       "38                              0.384880    1.790337  ...    0.217719   \n",
       "39                              0.347722    2.010583  ...    0.496526   \n",
       "40                              0.956239   -2.098707  ...    0.559239   \n",
       "41                              0.429241    1.959369  ...   -0.038676   \n",
       "42                              0.041421    0.498029  ...   -0.537639   \n",
       "43                              0.127474   -0.158033  ...    1.154112   \n",
       "44                             -0.691809   -2.590641  ...    2.267888   \n",
       "\n",
       "Diversion ratio wrt. product          36          37          38          39  \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               1.352992    3.017614    1.038177    0.769841   \n",
       "1                               1.603856    1.928039    0.887003    0.730656   \n",
       "2                               0.185656   -1.613793   -0.480251   -0.278623   \n",
       "3                              -1.340111    0.136624   -0.133162   -0.068884   \n",
       "4                               0.295994    0.654812    0.399442    0.263775   \n",
       "5                               1.057597    1.720739    0.614010    0.478332   \n",
       "6                               1.211340   -0.471091    0.002155    0.135048   \n",
       "7                               3.692894    2.694916    0.888912    0.404741   \n",
       "8                               3.471582    1.805713    0.605888    0.415755   \n",
       "9                               1.523249    2.311494    0.761393    0.649434   \n",
       "10                             -0.645200    0.537014    0.372864    0.181878   \n",
       "11                             -0.279908    9.961093    0.800114    0.402839   \n",
       "12                              0.590107    7.515828    0.114867   -0.087306   \n",
       "13                             -0.623867   -1.087195    8.677156    6.542356   \n",
       "14                              1.230140    2.245202    0.779906    0.608010   \n",
       "15                              0.084904   -0.677791   -0.420672   -0.397761   \n",
       "16                             -0.797198    0.329747    0.141857    0.165257   \n",
       "17                             -0.515690   -0.695505    0.263641    0.253674   \n",
       "18                              0.939830    2.505645    0.967252    0.458510   \n",
       "19                             -0.906602   -2.651503   -1.377584   -0.747189   \n",
       "20                             -0.612788   -1.272993   -1.177457   -0.525082   \n",
       "21                              0.696640    0.522253    0.451927    0.407918   \n",
       "22                             -0.801488    0.620670    0.264463    0.163821   \n",
       "23                              1.079149    1.808402    0.635587    0.500128   \n",
       "24                              0.525302    0.637026    0.356362    0.233193   \n",
       "25                              1.361980    1.937074    0.370877    0.258996   \n",
       "26                              0.070249   -1.477670   -0.778710   -0.176219   \n",
       "27                              0.348670    0.437666    0.434658    0.285330   \n",
       "28                              0.578170    0.980863    0.554869    0.325866   \n",
       "29                             -0.688364    1.687584    0.473165    0.220306   \n",
       "30                              0.149936   -1.577507   -0.689232   -0.095865   \n",
       "31                              0.438069    0.578256    0.412845    0.218051   \n",
       "32                              1.470637    2.011046    0.663030    0.569800   \n",
       "33                             -1.740678    1.233408   -0.413248   -0.554238   \n",
       "34                             -0.676590   -0.254025    0.113535    0.167673   \n",
       "35                              2.445127   -0.572080    0.125898    0.218073   \n",
       "36                           -100.000000    0.400590   -0.593181   -0.572780   \n",
       "37                              0.186836 -100.000000   -0.576313   -0.278226   \n",
       "38                             -0.834684   -1.738728 -100.000000    6.298724   \n",
       "39                             -1.061167   -1.105178    8.293041 -100.000000   \n",
       "40                              1.042428    1.751964    9.787706    7.213873   \n",
       "41                              0.333444   -3.207416   -0.717772   -0.170084   \n",
       "42                              0.872326    1.872076    0.618484    0.482577   \n",
       "43                             -0.238111    2.435329    0.768097    0.425575   \n",
       "44                              1.642963    2.179521    0.803783    0.574883   \n",
       "\n",
       "Diversion ratio wrt. product          40          41          42          43  \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               0.062257    2.963479    1.502768    3.624784   \n",
       "1                              -0.093614    1.928391    0.626792    0.347782   \n",
       "2                               0.030838   -1.329252    0.854338    2.510353   \n",
       "3                               0.039358    0.034480    0.927468   -0.789652   \n",
       "4                               0.000592    0.991939   -0.926073    1.060851   \n",
       "5                               0.051016    1.605609    0.984403    0.972293   \n",
       "6                               0.033321   -0.302582    0.955377   -1.004276   \n",
       "7                              -0.064511    2.492233   -0.358617    0.174512   \n",
       "8                               0.111108    1.642932    0.099422    0.717657   \n",
       "9                              -0.065878    2.026014    0.322942   -0.240353   \n",
       "10                              0.018206    0.910460    0.579261   -0.471209   \n",
       "11                             -0.004422    2.298076   -0.018331   -2.866578   \n",
       "12                              0.005238    0.283422    0.152804    1.547027   \n",
       "13                              0.718675   -1.721417    0.899327    2.427540   \n",
       "14                             -0.082500    2.038624    0.533706   -0.613027   \n",
       "15                              0.046122   -0.327762    4.167842   12.037093   \n",
       "16                              0.024912    0.620790    1.819775   -1.542094   \n",
       "17                              0.022621   -0.219893    0.739814    0.096954   \n",
       "18                             -0.007647    2.571076   -1.743337   -0.294344   \n",
       "19                              0.078204   -2.715524    0.919305    3.353924   \n",
       "20                              0.082369   -2.115983    0.971218    2.981476   \n",
       "21                             -0.228082    0.499075    0.671201    1.318835   \n",
       "22                             -0.053857    0.643306    1.354382   -2.375547   \n",
       "23                             -0.082539    1.651580   -0.801612    0.121932   \n",
       "24                             -0.163912    0.803774    0.909937    1.020078   \n",
       "25                              3.249446    1.786759    0.347150    1.279791   \n",
       "26                              0.065811   -2.663460    0.883163    3.735883   \n",
       "27                              0.000109    0.940490   -0.741928    1.329521   \n",
       "28                             -0.001871    1.337660   -0.559823    0.500701   \n",
       "29                              0.002408    1.511107    1.018692   -3.510441   \n",
       "30                              0.062605   -2.570185    1.129724    3.735646   \n",
       "31                              0.007278    0.952591   -0.919785    0.851383   \n",
       "32                             -0.064808    1.808024    0.601748    0.705698   \n",
       "33                              0.028037    1.161733    1.080379   -1.419048   \n",
       "34                              0.019626    0.062353    1.696155   -1.072807   \n",
       "35                              0.023869   -0.054377   -0.474031    2.386695   \n",
       "36                              0.054679    0.576155    0.945234   -0.605162   \n",
       "37                              0.042861   -2.584841    0.946118    2.886768   \n",
       "38                              0.722424   -1.745172    0.943025    2.746905   \n",
       "39                              0.701038   -0.544475    0.968775    2.003851   \n",
       "40                           -100.000000    1.704004   -0.132398   -0.238003   \n",
       "41                              0.051729 -100.000000    1.061269    3.122245   \n",
       "42                             -0.006409    1.692320 -100.000000    8.443175   \n",
       "43                             -0.004912    2.122712    3.599757 -100.000000   \n",
       "44                             -0.060054    2.172245   -0.145925    0.480430   \n",
       "\n",
       "Diversion ratio wrt. product          44  \n",
       "Diversion ratio of product                \n",
       "0                               3.173936  \n",
       "1                              -4.175364  \n",
       "2                               2.458086  \n",
       "3                               8.219180  \n",
       "4                               7.577503  \n",
       "5                              -1.858056  \n",
       "6                               4.259038  \n",
       "7                              -5.332898  \n",
       "8                              -3.651505  \n",
       "9                              -3.694033  \n",
       "10                              3.767280  \n",
       "11                              0.874856  \n",
       "12                              1.574748  \n",
       "13                              2.391199  \n",
       "14                             -2.550531  \n",
       "15                              0.925189  \n",
       "16                              2.915376  \n",
       "17                              3.340334  \n",
       "18                              0.935594  \n",
       "19                              2.549784  \n",
       "20                              2.726325  \n",
       "21                             -4.000276  \n",
       "22                              2.441160  \n",
       "23                             -0.425076  \n",
       "24                             -0.806133  \n",
       "25                             -2.564794  \n",
       "26                              2.577140  \n",
       "27                              2.695191  \n",
       "28                              2.143942  \n",
       "29                              0.926456  \n",
       "30                              2.299098  \n",
       "31                              7.491537  \n",
       "32                             -1.166142  \n",
       "33                              2.546845  \n",
       "34                              2.582548  \n",
       "35                              4.397057  \n",
       "36                              3.914825  \n",
       "37                              2.422184  \n",
       "38                              2.694997  \n",
       "39                              2.537817  \n",
       "40                             -2.728028  \n",
       "41                              2.995548  \n",
       "42                             -0.320889  \n",
       "43                              0.450425  \n",
       "44                           -100.000000  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DR_hat = Similarity_diversion_ratio(qOpt, z_logit, ThetaOptBLP, Model)\n",
    "pd.DataFrame(DR_hat[0]).rename_axis(index = 'Diversion ratio of product', columns = 'Diversion ratio wrt. product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticities and Diversion Ratios visualization\n",
    "\n",
    "Finally we advance our exposition towards visualing the Similarity semi-elasticities $\\mathcal{E}$ and diversion ratios $\\mathcal{D}$ compared to those implied by a multinomial Logit Model. Since the number of products varies across markets $t$ we aggregate our results according to the categorical variable `cla` describing the class or segment of each vehicle $j$. This variable takes values 'subcompact', 'compact', 'intermediate', 'standard', and 'luxury' encoded as the integers $0,1,\\ldots, 5$ in our dataset. To this end we consider the 'pooled' elasticities and diversion ratios calculated using the directional derivative $\\frac{\\partial q_c}{\\partial u_{\\ell}} = \\sum_{j: x_{j,\\text{cla}} = c} \\sum_{k: x_{k,\\text{cla}} = \\ell} \\frac{\\partial q_j}{\\partial u_k}$ of class $c$ wrt. the utility of class $\\ell$, where $q_c = \\sum_{j: x_{j,\\text{cla}} = c} q_j$ denotes the within-group choice proabbilitity of choosing a car of class $c$. \n",
    "\n",
    "The pooled choice probability semi-elasticity $\\mathcal{E}_{c\\ell}$ of class $c$ wrt. the prices of cars of class $\\ell$ can then be computed as $\\mathcal{E}_{c\\ell} = \\frac{\\partial q_c}{\\partial u_{\\ell}}\\frac{1}{q_c}\\theta^{\\text{price}}$. \n",
    "\n",
    "Similarly, we may compute the pooled diversion ratio $\\mathcal{D}_{c\\ell}$ following a unit increase in the price of cars of class $\\ell$ from class $\\ell$ to cars of class $c$ as $\\mathcal{D}_{c\\ell} = -100\\cdot\\frac{\\partial q_c / \\partial u_{\\ell}}{\\partial q_{\\ell} / \\partial u_{\\ell}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_directionalgrad(data, Theta, q, x, model, direction_var, market_id = 'market', product_id = 'co', mode = 'Similarity', outside_option = True):\n",
    "    '''\n",
    "    '''\n",
    "    T = len(q)\n",
    "    J = np.array([q[t].shape[0] for t in np.arange(T)])\n",
    "    data = data.sort_values([market_id, product_id])\n",
    "\n",
    "    vec = {}\n",
    "    q_agg = {}\n",
    "    dq_du_agg = {}\n",
    "\n",
    "    if mode == 'Similarity':\n",
    "        Grad = ccp_gradient(q, x, Theta, model)\n",
    "    else:\n",
    "        Grad = {t: (np.diag(q[t]) - q[t][:,None]*q[t][None,:]) for t in np.arange(T)} \n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        G_t = data[data[market_id] == t][direction_var].nunique()\n",
    "        vec[t] = pd.get_dummies(data[data[market_id] == t][direction_var], columns = direction_var).values.reshape((J[t], G_t)).transpose()\n",
    "        \n",
    "        # Calculate the sum of within-group probabilities\n",
    "        q_agg[t] = vec[t]@q[t]\n",
    "\n",
    "        # Calculate directional derivatives\n",
    "        dq_du_agg[t] = np.einsum('cj,jk,lk->cl', vec[t], Grad[t], vec[t])\n",
    "    \n",
    "    return q_agg, dq_du_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elasticity_agg(data, Theta, q, x, model, direction_var, market_id = 'market', product_id = 'co', char_number = K-1, mode = 'Similarity', outside_option = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    q_agg, dq_du_agg = ccp_directionalgrad(data, Theta, q, x, model, direction_var, market_id, product_id, mode, outside_option)\n",
    "    E_agg = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        E_agg[t] = Theta[char_number]*np.einsum('cl,c->cl', dq_du_agg[t], 1./q_agg[t])\n",
    "\n",
    "    return E_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_Similarityagg = Elasticity_agg(dat, ThetaOptBLP, qOpt, z_logit, Model, 'cla', char_number = pr_index)\n",
    "E_Logitagg = Elasticity_agg(dat, LogitBLP_beta, logit_ccp(LogitBLP_beta, z_logit), z_logit, Model, 'cla', char_number = pr_index, mode = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_agg = E_Similarityagg[0].shape[0]\n",
    "\n",
    "E0, E1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    E0[t,:,:] = E_Logitagg[t]\n",
    "    E1[t,:,:] = E_Similarityagg[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot histograms of our results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAHlCAYAAAD/f1iPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB1xElEQVR4nO3dd3hTZf8G8Dsd6W4ZbaGlg4rspYDsVRCxUrQoKkMF5cUBKoiLpQxREPQFXhUUUYaI4GALCChLoVqWKCDiC4UyLWBpKU1pkuf3R9/ml6Qr5+Q5TZren+vq1TQ5OfnmzjfpkzN1QggBIiIiIgIAeLm6ACIiIiJ3wsERERERkRUOjoiIiIiscHBEREREZIWDIyIiIiIrHBwRERERWeHgiIiIiMgKB0dEREREVjg4IiIiIrLCwZFCixcvhk6nw759+0q8PTk5GXXr1rW5rm7duhg6dKiix9mzZw8mT56MrKwsdYVSiSZOnIi4uDj4+PigWrVqri6nREOHDi3WQ87q3r07unfvXu50devWRXJystTHLs+OHTug0+mwY8cOy3UbN27E5MmTK7SO9PR06HQ6LF68uEIf151MnjwZOp1O6jyLPjPT09OlzrfI8uXLMWfOnBJv0+l0ivuopD4o6/PY0feWTK54n5ZGzf+3ysDH1QVUBatXr0ZoaKii++zZswdTpkzB0KFD3fafeGWzdu1avPnmm5gwYQKSkpLg5+fn6pIIQKtWrbB37140adLEct3GjRvxwQcfVOgAKSoqCnv37kW9evUq7DHdzb/+9S/cfffdri5DkeXLl+P333/H6NGji922d+9exMTEKJpfSX1Q1ufxvHnz1JRNbo6Dowpw++23u7oExQoKCqDT6eDj4zkt8vvvvwMAnn/+eURGRrq4GioSGhqK9u3bu+zxTSYTjEYj/Pz8XFqHO4iJiVE8mHBnal5PpX1gPagnz8HVahXAfrGj2WzGtGnT0LBhQwQEBKBatWpo0aIF5s6dC6Bw0fbLL78MAEhISIBOp7NZ7WA2mzFz5kw0atQIfn5+iIyMxGOPPYazZ8/aPK4QAm+99Rbi4+Ph7++PNm3aYOvWrcUWAxet1vjss8/w4osvok6dOvDz88Nff/2FzMxMjBgxAk2aNEFwcDAiIyPRo0cP7N692+axihZFz5o1C2+//Tbq1q2LgIAAdO/eHX/++ScKCgowduxYREdHIywsDP369cPff/9tM48ffvgB3bt3R82aNREQEIC4uDg88MADuHHjRpn5OpJH3bp1MXHiRABArVq1yl3cPnToUAQHB+PIkSPo2bMngoKCEBERgWeffbZYPQaDAePGjUNCQgL0ej3q1KmDkSNHFlsE7+jrVhIhBObNm4fbbrsNAQEBqF69Ovr374+TJ08Wm27mzJmW17xVq1bYtGlTufNXwtHnm5+fjxdffBG1a9dGYGAgunbtiv379xd7P9ivVhs6dCg++OADALD0fnmrZbp3745mzZph9+7daN++PQICAlCnTh289tprMJlMlumK+nTmzJmYNm0aEhIS4Ofnh+3bt5e6Wu2PP/7AwIEDUatWLfj5+SEuLg6PPfYY8vPzLdNcvHgRTz31FGJiYqDX65GQkIApU6bAaDSWm6cjfX/z5k1MmzbN0jsRERF4/PHHkZmZaTOvotUtGzZswO23346AgAA0btwYGzZsAFC4iqtx48YICgpC27Zti20eoHS12rZt29CzZ0+EhoYiMDAQnTp1wvfff1/u/bZu3Yr77rsPMTEx8Pf3x6233oqnnnoKly9ftpkuMzMTTz75JGJjYy3Pu1OnTti2bRuAwtf922+/xenTp216pUhJ7/Nz585Z5qnX6xEdHY3+/fvj0qVLAIqvVivv87ik1WqOvl5qP/OKrF69Gi1atIC/vz9uueUW/Oc//7Hcdv36dVSrVg1PPfVUsfulp6fD29sbs2bNKnP++fn5mDp1Kho3bgx/f3/UrFkTiYmJ2LNnT6n3MRgMePHFF3HbbbchLCwMNWrUQIcOHbB27dpi03711Vdo164dwsLCEBgYiFtuuQVPPPGE5fby/ldqSpAiixYtEgBEamqqKCgoKPZzzz33iPj4eJv7xMfHiyFDhlj+nj59uvD29haTJk0S33//vdi8ebOYM2eOmDx5shBCiIyMDPHcc88JAGLVqlVi7969Yu/eveLatWtCCCGefPJJAUA8++yzYvPmzeLDDz8UERERIjY2VmRmZloeZ9y4cQKAePLJJ8XmzZvFxx9/LOLi4kRUVJTo1q2bZbrt27cLAKJOnTqif//+Yt26dWLDhg3iypUr4o8//hDPPPOMWLFihdixY4fYsGGDGDZsmPDy8hLbt2+3zOPUqVMCgIiPjxd9+/YVGzZsEMuWLRO1atUSDRo0EI8++qh44oknxKZNm8SHH34ogoODRd++fW3u7+/vL3r16iXWrFkjduzYIT7//HPx6KOPin/++afM18SRPA4cOCCGDRsmAIjNmzeLvXv3ioyMjFLnOWTIEKHX60VcXJx48803xZYtW8TkyZOFj4+PSE5OtkxnNptF7969hY+Pj3jttdfEli1bxDvvvCOCgoLE7bffLgwGg6I6ix7bvoeGDx8ufH19xYsvvig2b94sli9fLho1aiRq1aolLl68aJlu0qRJAoAYNmyY2LRpk1iwYIGoU6eOqF27ts1rXpr4+HjRp0+fUm9X8nwHDhwovLy8xNixY8WWLVvEnDlzRGxsrAgLC7N5PxT1X1E//fXXX6J///4CgKX39+7dazNve926dRM1a9YU0dHR4j//+Y/47rvvxPPPPy8AiJEjR1qmK+rTOnXqiMTERPH111+LLVu2iFOnTlluW7RokWX6Q4cOieDgYFG3bl3x4Ycfiu+//14sW7ZMPPTQQyI7O1sIIcSFCxdEbGysiI+PFx999JHYtm2beOONN4Sfn58YOnRomXk70vcmk0ncfffdIigoSEyZMkVs3bpVLFy4UNSpU0c0adJE3Lhxw+b1i4mJEc2aNRNffPGF2Lhxo2jXrp3w9fUVr7/+uujUqZNYtWqVWL16tWjQoIGoVauWzf2L+scRn332mdDpdCIlJUWsWrVKrF+/XiQnJwtvb2+xbds2y3RFn5mnTp2yXDd//nwxffp0sW7dOrFz506xZMkS0bJlS9GwYUNx8+ZNy3S9e/cWERERYsGCBWLHjh1izZo14vXXXxcrVqwQQghx5MgR0alTJ1G7dm2bXikCQEyaNMny99mzZ0VUVJQIDw8X//73v8W2bdvEypUrxRNPPCGOHTtmeU2s+6C8z+Nu3brZvLccfb2c+cyLj48XderUEXFxceLTTz8VGzduFIMHDxYAxKxZsyzTvfDCCyIoKEhkZWXZ3P/ll18W/v7+4vLly6U+RkFBgUhMTBQ+Pj7ipZdeEhs3bhTr1q0T48ePF1988YVNLdbv56ysLDF06FDx2WefiR9++EFs3rxZvPTSS8LLy0ssWbLEMt2ePXuETqcTAwYMEBs3bhQ//PCDWLRokXj00Uct05T3v1JLHBwpVPRGL+unvMFRcnKyuO2228p8nFmzZhX7QBFCiGPHjgkAYsSIETbX//zzzwKAGD9+vBBCiKtXrwo/Pz/x8MMP20y3d+9eAaDEwVHXrl3Lff5Go1EUFBSInj17in79+lmuL/pAadmypTCZTJbr58yZIwCIe++912Y+o0ePFgAsHzBff/21ACAOHTpUbg3WHM1DiP//4LceiJRmyJAhAoCYO3euzfVvvvmmACB+/PFHIYQQmzdvFgDEzJkzbaZbuXKlACAWLFiguE77wVHRa/buu+/a3DcjI0MEBASIV155RQghxD///CP8/f1tXhchhPjpp5+KvealKW9w5OjzPXLkiAAgXn31VZvpvvjiCwGgzMGREEKMHDnS4X/SQhT+gwIg1q5da3P98OHDhZeXlzh9+rQQ4v/7tF69ejb/hK1vsx4c9ejRQ1SrVk38/fffpT72U089JYKDgy2PUeSdd94RAMSRI0dKva8jfV+U2TfffGNzfVpamgAg5s2bZ7kuPj5eBAQEiLNnz1quO3TokAAgoqKiRG5uruX6NWvWCABi3bp1luscHRzl5uaKGjVq2HzBEaJwYNCyZUvRtm1by3UlDY6smc1mUVBQIE6fPl3sNQwODhajR48us5Y+ffoU+8wtYj84euKJJ4Svr684evRoqfMrqQ9K+zwWovjgyNHXS+1nnhCFr7NOpyt23169eonQ0FDL6/zf//5XeHl5idmzZ1umycvLEzVr1hSPP/54mY+xdOlSAUB8/PHH5dZi/X62V/Q/Y9iwYeL222+3XF/0/rAfuFlz5H+lVrhaTaWlS5ciLS2t2E/nzp3LvW/btm3x66+/YsSIEfjuu++QnZ3t8ONu374dAIrtHdC2bVs0btzYskg7NTUV+fn5eOihh2yma9++fal7Qj3wwAMlXv/hhx+iVatW8Pf3h4+PD3x9ffH999/j2LFjxaa955574OX1/23VuHFjAECfPn1spiu6/syZMwCA2267DXq9Hk8++SSWLFlSbHVRaRzNQ63Bgwfb/D1o0CCbx/3hhx9KfPwHH3wQQUFBlsd3ps4NGzZAp9PhkUcegdFotPzUrl0bLVu2tCze37t3LwwGQ7GaO3bsiPj4eMefdBkcfb47d+4EgGL9179/f822YwsJCcG9995rc92gQYNgNpuxa9cum+vvvfde+Pr6ljm/GzduYOfOnXjooYcQERFR6nQbNmxAYmIioqOjbV6fpKQkAP+fRUkc6fsNGzagWrVq6Nu3r838b7vtNtSuXdtmL7+iedapU8fyd9F7rXv37ggMDCx2/enTp0utz2w22zxm0SrKPXv24OrVqxgyZIjN7WazGXfffTfS0tKQm5tb6nz//vtvPP3004iNjbV8phT1qPXnStu2bbF48WJMmzYNqampKCgoKHWejti0aRMSExMtz10Ljr5eaj/zijRt2hQtW7a0uW7QoEHIzs7GgQMHAAC33HILkpOTMW/ePAghABRuwH7lyhU8++yzZc5/06ZN8Pf3t1nN5aivvvoKnTp1QnBwsOX1/eSTT2xe2zvuuANA4WfEl19+iXPnzhWbjzP/K53FwZFKjRs3Rps2bYr9hIWFlXvfcePG4Z133kFqaiqSkpJQs2ZN9OzZs9TDA1i7cuUKgMI9KuxFR0dbbi/6XatWrWLTlXRdafP897//jWeeeQbt2rXDN998g9TUVKSlpeHuu+9GXl5eselr1Khh87dery/zeoPBAACoV68etm3bhsjISIwcORL16tVDvXr1yl237Ggeavj4+KBmzZo219WuXdvmca9cuQIfH59i/zx1Oh1q165d7PVQU+elS5cghECtWrXg6+tr85OammrZTqNoHkU1llS3s5Q+X/teKylTWUrqa/vXq0hJr4O9f/75ByaTqdwNlC9duoT169cXe22aNm0KAMW2o7HmSN9funQJWVlZ0Ov1xR7j4sWLxeav9j1YkqlTp9o8XtEeXEXb5/Tv379YTW+//TaEELh69WqJ8zSbzbjrrruwatUqvPLKK/j+++/xyy+/IDU1FQBsPldWrlyJIUOGYOHChejQoQNq1KiBxx57DBcvXiy15rJkZmZqvsG5o6+X2s+8ImW9z637fdSoUThx4gS2bt0KAPjggw/QoUMHtGrVqsz5Z2ZmIjo62ubLriNWrVqFhx56CHXq1MGyZcuwd+9epKWl4YknnrDpta5du2LNmjUwGo147LHHEBMTg2bNmuGLL76wTOPM/0pnec6uSJWIj48PxowZgzFjxiArKwvbtm3D+PHj0bt3b2RkZNh8u7NX9I/lwoULxd7k58+fR3h4uM10RR9i1i5evFji0qOSNsRctmwZunfvjvnz59tcn5OTU/aTVKFLly7o0qULTCYT9u3bh/feew+jR49GrVq1MGDAgBLv42geahiNRly5csXmn3nRh3LRdTVr1oTRaERmZqbNgEEIgYsXL1q+HTlTZ3h4OHQ6HXbv3l3i4QeKrit6jJL+cZT2miul9PleunTJZilGUaZaKK3Xresp4shGxzVq1IC3t3e5G8yHh4ejRYsWePPNN0u8PTo6usz7l9f34eHhqFmzJjZv3lzi/UNCQsp9Lmo9+eSTNsfTKeq1on597733St2zq7QvYb///jt+/fVXLF68GEOGDLFc/9dffxWbNjw8HHPmzMGcOXNw5swZrFu3DmPHjsXff/9dah5liYiIcGgHCGcoeb3UfOYVKe19Dtj2e48ePdCsWTO8//77CA4OxoEDB7Bs2bJyn0dERAR+/PFHmM1mRQOkZcuWISEhAStXrrR5n1nvwFDkvvvuw3333Yf8/HykpqZi+vTpGDRoEOrWrYsOHTo49b/SWVxy5GLVqlVD//79MXLkSFy9etWyR07Rh5D90pkePXoAQLHmTktLw7Fjx9CzZ08AQLt27eDn54eVK1faTJeamlrmYnR7Op2u2D/kw4cPY+/evQ7PQylvb2+0a9fOssdS0SLikjiah1qff/65zd/Lly8HAMveKUXzt3/8b775Brm5uZbbnakzOTkZQgicO3euxKWVzZs3B1C4ytTf379YzXv27FH0mpfF0efbtWtXACjWf19//bVDe3CV1v9lycnJwbp162yuW758Oby8vCz1KBEQEIBu3brhq6++KnPpT3JyMn7//XfUq1evxNenvMFRkdL6Pjk5GVeuXIHJZCpx/g0bNlT83BwVHR1dYq916tQJ1apVw9GjR0usqU2bNpYlU/aK/mHaf6589NFHZdYSFxeHZ599Fr169bL5TPDz83O4T5KSkrB9+3YcP37coemtHwNwrB/VvF5KPvOKHDlyBL/++qvNdcuXL0dISEixpULPP/88vv32W4wbNw61atXCgw8+WO78k5KSYDAYFB8UVafTQa/X2wyMLl68WOLeakX8/PzQrVs3vP322wCAgwcPFpumtP+VWuGSIxfo27cvmjVrhjZt2iAiIgKnT5/GnDlzEB8fj/r16wOA5UNo7ty5GDJkCHx9fdGwYUM0bNgQTz75JN577z14eXkhKSkJ6enpeO211xAbG4sXXngBQOG33jFjxmD69OmoXr06+vXrh7Nnz2LKlCmIiopy+JtAcnIy3njjDUyaNAndunXD8ePHMXXqVCQkJDj0T85RH374IX744Qf06dMHcXFxMBgM+PTTTwEAd955Z6n3czQPNfR6Pd59911cv34dd9xxB/bs2YNp06YhKSnJsm1Zr1690Lt3b7z66qvIzs5Gp06dcPjwYUyaNAm33347Hn30Uafr7NSpE5588kk8/vjj2LdvH7p27YqgoCBcuHABP/74I5o3b45nnnkG1atXx0svvYRp06bhX//6Fx588EFkZGRg8uTJilarXbx4EV9//XWx6+vWrevw823atCkGDhyId999F97e3ujRoweOHDmCd999F2FhYeX2X1H/v/3220hKSoK3tzdatGhR6j9coPDb8jPPPIMzZ86gQYMG2LhxIz7++GM888wziIuLc/j5W/v3v/+Nzp07o127dhg7dixuvfVWXLp0CevWrcNHH32EkJAQTJ06FVu3bkXHjh3x/PPPo2HDhjAYDEhPT8fGjRvx4Ycflroqx5G+HzBgAD7//HPcc889GDVqFNq2bQtfX1+cPXsW27dvx3333Yd+/fqpen5qBQcH47333sOQIUNw9epV9O/fH5GRkcjMzMSvv/6KzMzMYkubizRq1Aj16tXD2LFjIYRAjRo1sH79estqnyLXrl1DYmIiBg0ahEaNGiEkJARpaWnYvHkz7r//fst0zZs3x6pVqzB//ny0bt0aXl5eaNOmTYmPPXXqVGzatAldu3bF+PHj0bx5c2RlZWHz5s0YM2YMGjVqVOL9Svs8LmmpnaOvl9rPvCLR0dG49957MXnyZERFRWHZsmXYunUr3n777WJLVB555BGMGzcOu3btwsSJE8t8HxUZOHAgFi1ahKeffhrHjx9HYmIizGYzfv75ZzRu3LjUJVvJyclYtWoVRowYgf79+yMjIwNvvPEGoqKicOLECct0r7/+Os6ePYuePXsiJiYGWVlZmDt3Lnx9fdGtWzcAjv2v1IxLNgOvxIr2vEhLSyvx9pL2nLDfmv/dd98VHTt2FOHh4ZbdxYcNGybS09Nt7jdu3DgRHR0tvLy8bPbmMZlM4u233xYNGjQQvr6+Ijw8XDzyyCPFdk03m81i2rRpIiYmRuj1etGiRQuxYcMG0bJlS5s9mor2Fvrqq6+KPZ/8/Hzx0ksviTp16gh/f3/RqlUrsWbNmmJ7VBXt4WG9G2lZ87bPce/evaJfv34iPj5e+Pn5iZo1a4pu3brZ7ElTGkfzULq3WlBQkDh8+LDo3r27CAgIEDVq1BDPPPOMuH79us20eXl54tVXXxXx8fHC19dXREVFiWeeeabY7riO1lnSrvxCCPHpp5+Kdu3aiaCgIBEQECDq1asnHnvsMbFv3z7LNGazWUyfPl3ExsZaXvP169cX26OmNPHx8aXuhVnUw44+X4PBIMaMGSMiIyOFv7+/aN++vdi7d68ICwsTL7zwgmW6kvZWy8/PF//6179ERESE0Ol0Ze7tJEThHkNNmzYVO3bsEG3atBF+fn4iKipKjB8/XhQUFFimK61PrW+z3ktJCCGOHj0qHnzwQVGzZk3L+3Xo0KE2hxbIzMwUzz//vEhISBC+vr6iRo0aonXr1mLChAnF+sWao31fUFAg3nnnHdGyZUvh7+8vgoODRaNGjcRTTz0lTpw4YZmutL0NYXdIg9KyULIrvxBC7Ny5U/Tp00fUqFFD+Pr6ijp16og+ffrYvN9L2lvt6NGjolevXiIkJERUr15dPPjgg+LMmTM2e5cZDAbx9NNPixYtWojQ0FAREBAgGjZsKCZNmmSz193Vq1dF//79RbVq1Sy9Yv28rfdWE6JwL88nnnhC1K5dW/j6+oro6Gjx0EMPiUuXLtnkYt8HpX0el/TecuT1cuYzr+h1/vrrr0XTpk2FXq8XdevWFf/+979Lvc/QoUOFj4+PzZ6M5cnLyxOvv/66qF+/vtDr9aJmzZqiR48eYs+ePTa12O+tNmPGDFG3bl3h5+cnGjduLD7++ONivbVhwwaRlJQk6tSpI/R6vYiMjBT33HOP2L17t2UaR/9XakEnxP82Yacq4dSpU2jUqBEmTZqE8ePHu7octzV06FB8/fXXuH79uqtL8Sh79uxBp06d8Pnnn1v2/JOhe/fuuHz5suUo6ET0/27evIm6deuic+fO+PLLL11dTqXA1Woe7Ndff8UXX3yBjh07IjQ0FMePH8fMmTMRGhqKYcOGubo88nBbt27F3r170bp1awQEBODXX3/FjBkzUL9+fZvVIkSkjczMTBw/fhyLFi3CpUuXMHbsWFeXVGlwcOTBgoKCsG/fPnzyySfIyspCWFgYunfvjjfffLPUPUmIZAkNDcWWLVswZ84c5OTkIDw8HElJSZg+fTr8/f1dXR6Rx/v222/x+OOPIyoqCvPmzSt39336f1ytRkRERGSFu/ITERERWeHgiIiIiMgKB0dEREREVjg4IiIiIrLCwRERERGRFQ6OiIiIiKxwcERERERkhYMjIiIiIiscHBERERFZ4eCIiIiIyAoHR0RERERWODgiIiIissLBEREREZEVDo6IiIiIrHBwRERERGSFgyMiIiIiKxwcVWLz5s1DQkIC/P390bp1a+zevdvVJXmUXbt2oW/fvoiOjoZOp8OaNWtcXZLHmT59Ou644w6EhIQgMjISKSkpOH78uKvLcmvz589HixYtEBoaitDQUHTo0AGbNm1ydVkebfr06dDpdBg9erSrS3FbkydPhk6ns/mpXbu2q8tSjYOjSmrlypUYPXo0JkyYgIMHD6JLly5ISkrCmTNnXF2ax8jNzUXLli3x/vvvu7oUj7Vz506MHDkSqamp2Lp1K4xGI+666y7k5ua6ujS3FRMTgxkzZmDfvn3Yt28fevTogfvuuw9HjhxxdWkeKS0tDQsWLECLFi1cXYrba9q0KS5cuGD5+e2331xdkmo6IYRwdRGkXLt27dCqVSvMnz/fcl3jxo2RkpKC6dOnu7Ayz6TT6bB69WqkpKS4uhSPlpmZicjISOzcuRNdu3Z1dTmVRo0aNTBr1iwMGzbM1aV4lOvXr6NVq1aYN28epk2bhttuuw1z5sxxdVluafLkyVizZg0OHTrk6lKk4JKjSujmzZvYv38/7rrrLpvr77rrLuzZs8dFVRE579q1awAK/9lT+UwmE1asWIHc3Fx06NDB1eV4nJEjR6JPnz648847XV1KpXDixAlER0cjISEBAwYMwMmTJ11dkmo+ri6AlLt8+TJMJhNq1aplc32tWrVw8eJFF1VF5BwhBMaMGYPOnTujWbNmri7Hrf3222/o0KEDDAYDgoODsXr1ajRp0sTVZXmUFStWYP/+/di3b5+rS6kU2rVrh6VLl6JBgwa4dOkSpk2bho4dO+LIkSOoWbOmq8tTjIOjSkyn09n8LYQodh1RZfHss8/i8OHD+PHHH11dittr2LAhDh06hKysLHzzzTcYMmQIdu7cyQGSJBkZGRg1ahS2bNkCf39/V5dTKSQlJVkuN2/eHB06dEC9evWwZMkSjBkzxoWVqcPBUSUUHh4Ob2/vYkuJ/v7772JLk4gqg+eeew7r1q3Drl27EBMT4+py3J5er8ett94KAGjTpg3S0tIwd+5cfPTRRy6uzDPs378ff//9N1q3bm25zmQyYdeuXXj//feRn58Pb29vF1bo/oKCgtC8eXOcOHHC1aWowm2OKiG9Xo/WrVtj69atNtdv3boVHTt2dFFVRMoJIfDss89i1apV+OGHH5CQkODqkiolIQTy8/NdXYbH6NmzJ3777TccOnTI8tOmTRsMHjwYhw4d4sDIAfn5+Th27BiioqJcXYoqXHJUSY0ZMwaPPvoo2rRpgw4dOmDBggU4c+YMnn76aVeX5jGuX7+Ov/76y/L3qVOncOjQIdSoUQNxcXEurMxzjBw5EsuXL8fatWsREhJiWRoaFhaGgIAAF1fnnsaPH4+kpCTExsYiJycHK1aswI4dO7B582ZXl+YxQkJCim33FhQUhJo1a3J7uFK89NJL6Nu3L+Li4vD3339j2rRpyM7OxpAhQ1xdmiocHFVSDz/8MK5cuYKpU6fiwoULaNasGTZu3Ij4+HhXl+Yx9u3bh8TERMvfRevNhwwZgsWLF7uoKs9SdCiK7t2721y/aNEiDB06tOILqgQuXbqERx99FBcuXEBYWBhatGiBzZs3o1evXq4ujaqws2fPYuDAgbh8+TIiIiLQvn17pKamVtr/STzOEREREZEVbnNEREREZIWDIyIiIiIrHBwRERERWeEG2QqZzWacP38eISEhPOCiHSEEcnJyEB0dDS8vx8fdzLR0zFQ+ZiofM5WPmcqnJFMOjhQ6f/48YmNjXV2GW8vIyFB0ID9mWj5mKh8zlY+ZysdM5XMkUw6OFAoJCQFQGG5oaKiLq3Ev2dnZiI2NtWTkKGZaOmYqHzOVj5nKx0zlU5IpB0cKFS2mDA0NZeOVQumiXGZaPmYqHzOVj5nKx0zlcyRTDo5INaPZiA1/bgAAJDdIljK/NX+ssczPx4vt6Sxmqg373meuzmOm8qnKtGjgUMUPgcjuI9V8vHyQ0ijFbedHzFQrzFU+ZiofM1WPu/ITERERWeGSI1LNZDZh95ndAIAucV2kzG9H+g7L/Ly9eOZrZzFTbdj3PnN1HjOVj5mqx8ERqWYwGpC4pPDErNfHXZc+vyB9kNPzrOqYqTaYq3zMVD5mqh4HR6SaTqdDk4gmlsvuNj9iplphrvIxU/mYqXocHJFqgb6BODLiiOXv7LxsqfMj5zFTbTBX+ZipfMxUPW6QTURERGSFgyMiIiIiKxwckWp5BXno9Vkv9PqsF/IK8txufsRMtcJc5WOm8jFT9bjNEalmFmZsO7nNctnd5kfMVCvMVT5mKh8zVY+DI1LNz8cPy/ots1y+gRtS50fOY6baYK7yMVP5mKl6HByRaj5ePhjcYrDbzo+YqVaYq3zMVD7NM/Xg87BxmyMiIiIiK1xyRKqZzCYcuHAAANAqqpWU+aWdS7PMj4e6dx4z1YZ97zNX5zFT+ZipehwckWoGowFtF7YFIO/0Idbz46HuncdMtcFc5WOm8jFT9Tg4ItV0Oh3iw+Itl91tfsRMtcJc5WOm8jFT9Tg4ItUCfQORPjrd8reM04dYz4+cx0y1wVzlY6byMVP1ODgiIiIix1WBpVDcW42IiIjICgdHpJrBaEDKihSkrEiBwWhwu/kRM9UKc5WPmcpXYZnqdB63NImr1f5n3rx5mDVrFi5cuICmTZtizpw56NKli6vLcmsmswlrj6+1XHa3+REz1QpzlY+ZysdM1ePgCMDKlSsxevRozJs3D506dcJHH32EpKQkHD16FHFxca4uz23pvfVYkLzAcjkPzp3Y0H5+5Dxmqg3mKh8zlY+ZqqcTwgOP+61Qu3bt0KpVK8yfP99yXePGjZGSkoLp06fbTJudnY2wsDBcu3YNoaGhFV2qW1ObDTMtHTOVj5nKx0zlc1mmjpwSpLRVaG4+nFCSTZXf5ujmzZvYv38/7rrrLpvr77rrLuzZs8dFVREREZGrVPnVapcvX4bJZEKtWrVsrq9VqxYuXrzooqoqB7Mw41jmMQBA44jGUuZ35O8jlvl56ar82N1pzFQb9r3PXJ3HTOVjpupV+cFREfujhwoheETRcuQV5KHZ/GYA5Jw+xH5+PNS985ipNpirfBWWaXmrjTzoTPPsU/Wq/OAoPDwc3t7exZYS/f3338WWJlFx4YHhbj0/YqZaYa7yMVP5mKk6VX5wpNfr0bp1a2zduhX9+vWzXL9161bcd999LqzM/QXpg5D5cqbl72yDc6cPsZ8fOY+ZaoO5ysdM5ZOaaRVbk1LlB0cAMGbMGDz66KNo06YNOnTogAULFuDMmTN4+umnXV0aERERVTAOjgA8/PDDuHLlCqZOnYoLFy6gWbNm2LhxI+Lj411dGlHl4kHba5CHY69SGbjp+v+MGDEC6enpyM/Px/79+9G1a1dXl+T2DEYDBq8ajMGrBks7fYjM+REz1QpzlY+ZysdM1ePgqCJ52PlnTGYTlv+2HMt/Wy7t9CEy56dI0Wuj5jVy49fVZZmWlokbZ6WES3vVQzFT+ZipelytRqrpvfWY3Xu25bKM04dYz4+cx0y1wVzlY6byMVP1ODiSrQqtx/b19sXo9qMtfzs7OLKfHznP5Zl66PvB5bmWRqertFm7baaVGDNVj6vViIiIiKxwyZGz3OGbsf02HI6cMFBCvWZhxplrZwAAcWFxUuaXnpVumZ9Th7p39Hlqsf2LO/TE/0jNVCYlJ7d0gxzt2fe+lF4FHO9XrTNxQfZSMy2J2ve6G/dheRzO1AO2A5SNgyNSLa8gDwlzEwDIO31Iwgf/Pz8e6t55zFQb9r3PXJ3HTOVjpupxcFSZKRnta/TNINA30K3n5zRntuFQskRAQ05lKutbszP9p7YGjb/xu0Wvarnks6T5F2WpUbZSM9Xq9a9k83WLPq2EODgi1YL0Qcgdn2v5W8bpQ6znR85jptpgrvIxU/mYqXocHLmDkr79afwNrcrSclskZ+5T2V5fLet2dmkIt5+wVYXOQg+g+OtfXj/IWjLpaTlWcW6ydSYRERGRe+DgSBbZR1Yu7wjDjjyWxt+g8435GL5uOIavG458Y77bzU8a+6Nna3kUbclHkHbbTIuoyVVpRhoclVuzXGVmYD9NefdRM0+J3L5XS+NozkpfDwkqbaZugIMjUs1oNmLhwYVYeHAhjGaj282PmKlWmKt8zFQ+ZqoetznSWmnfIGTMR8a0TvD19sW0xGmWyyY4d+4e+/lJpXQ7hIqmUT3SMnXn7SlcUJumvWpNyz51s/eA05m62fNxmIZ1O5WprP9T7viZ4QAOjkg1vbceE7pOsPxtgHNnfbafHzmPmWqDucrHTOVjpupxcKQVd/kW4y51UCF3XgLjCFf2k7sv/XMXju6dRbYqKhfmX6is48CpOYK+5OPKcXBEqgkhcPnGZQBAeGC4lPll5mZa5qfjh4jTmKk27HufuTqPmcrHTNXj4IhUu1FwA5HvRAKQc/qQGwU3ED0n2jI/KYe6d9dtNiroQ0qTTK2504dtBdZi3/senWsF0TxTpTzgeFtulamSpUGOXl/abaUd2V0BDo4UEv8LOTvbuaNBu5yE+nNv5qJoM6Ps7GyYDIUbZAuFjVg0fU5Oju389M5t4F2p/e/1KeozZiqBpEyzs7PhfdObuQJVK9OK+sz3tEztcyspx9KylZW5mkwFKZKRkSEA8KeMn4yMDGbKTN3+h5ky08rww0xdk6lOiMq6ZahrmM1mnD9/HiEhIVx/a0cIgZycHERHR8PLy/FDaDHT0jFT+ZipfMxUPmYqn5JMOTgiIiIissIjZBMRERFZ4eCIiIiIyAoHR0RERERWODgiIiIissLBEREREZEVDo6IiIiIrHBwRERERGSFpw9RiAfYKh0PWiYfM5WPmcrHTOVjpvIpyZSDI4XOnz+P2NhYV5fh1jIyMhATE+Pw9My0fMxUPmYqHzOVj5nK50imHBwpFBISAqAw3NDQUBdX416ys7MRGxtrychRzLR0zFQ+ZiofM5WPmcqnJFMOjhQqWkzp7e+NsNlhAIDr464jSB/kyrLcitJFuUXTh4aGVvk3c+7NXARPDwZQ2FehKMxDbabs09I506fe/t42rxNzLcRM5avwTIsez4PPLOZIphwcqeTn44fVD6+2XCaSwb6vbuCG1PmRHMxVPmYqHzNVj4MjlXy8fJDSKMXVZZCHkd1X7FNtMFf5mKl8zFQ97spPREREZIVLjlQymU3Ykb4DANAlrgu8vbxdWxB5BJPZhN1ndgMo7CsZ82Ofymf/OjFX5zFT+ZipehwcqWQwGpC4JBEANx4keez7Svb82KdyMFf5mKl8zFQ9Do5U0ul0aBLRxHKZSAbZfcU+1QZzlY+ZysdM1ePgSKVA30AcGXHE1WWQh7Hvq+y8bKnzIzmYq3zMVD5mqh43yCYiIiKywsERERERkRUOjlTKK8hDr896oddnvZBXkOfqcshDyO4r9qk2mKt8zFQ+ZqoetzlSySzM2HZym+UykQyy+4p9qg3mKh8zlY+ZqsfBkUp+Pn5Y1m+Z5TKRDPZ9JeP0IexT+ZirfMxUPs0z9eDzsHFwpJKPlw8Gtxjs6jLIw8juK/apNpirfMxUPmaqHrc5IiIiIrLCJUcqmcwmpJ1LAwC0imrFw7KTFCazCQcuHABQ2Fcy5sc+lc/+dWKuzmOm8jFT9Tg4UslgNKDtwrYAeFh2kse+r2TPj30qB3OVj5nKx0zV4+BIJZ1Oh/iweMtlIhlk9xX7VBvMVT5mKh8zVY+DI5UCfQORPjrd1WWQh7HvKxmnD2Gfysdc5WOm8mmWaRUYaHGDbCIiIiIrHBwRERGRejqdxy1N4uBIJYPRgJQVKUhZkQKD0eDqcshDyO4r9qk2mKt8zFQ+ZqqetG2ODAYD/P39Zc3O7ZnMJqw9vtZymUgG2X3FPtUGc5WPmcrHTNVTPDhauXIlrly5ghEjRgAA/vrrL9x77704fvw4OnbsiHXr1qF69erSC3U3em89FiQvsFwmksG+r/Lg3Mki2afaYK7yMVP5mKl6igdH77zzDh566CHL3y+//DL++ecfjBo1Cp999hneeustzJo1S2qR7sjX2xfDWw93dRnkYez7ytnBEftUG8xVPmYqHzNVT/E2RydPnkSzZs0AFK5K++677/D222/j3//+N6ZNm4Y1a9bIrlFzu3btQt++fREdHQ2dTlcpnwMRERHJoXhwdOPGDQQFFR5l8+eff0Z+fj6SkpIAAE2aNMG5c+fkVlgBcnNz0bJlS7z//vsO38cszDjy9xEc+fsIzMKsYXVUlcjuK/apNpirfBWWaXl7VnnQnlfsU/UUr1aLiorCoUOH0LVrV2zevBkNGzZEREQEAOCff/5BYGCg9CK1lpSUZBngOSqvIA/N5hcuQeNh2UkW+76SPT/2qRzMVT5mKh8zVU/x4Oj+++/HhAkTsHPnTmzatAmvvvqq5bbDhw+jXr16Ugt0Z+GB4a4ugTyQ7L5in2qDucrHTOWTlqmHLE1zlOLB0RtvvIHr169jz549GDRoEF555RXLbRs2bMCdd94ptUB3FaQPQubLma4ugzyMfV9lG5w7fQj7VBvMVT5mKh8zVU/x4CggIAAffvhhibelpqY6XRARVWJF3y6FcG0dROVhr1IZpBwhOyMjA5s3b8aVK1dkzI6IiIjIZRQPjiZOnIgXXnjB8ve2bdvQoEED9OnTB/Xr18eRI0ekFuiuDEYDBq8ajMGrBvOw7CSN7L5yWZ+WtsePh+wJxPe/fMxUPmaqnuLB0TfffIMmTZpY/p44cSJatGiB1atXo27dupg2bZrUAivC9evXcejQIRw6dAgAcOrUKRw6dAhnzpwp9T4mswnLf1uO5b8t52HZSRrZfcU+1QZzlY+ZysdM1VO8zdG5c+dw6623AgCuXLmCtLQ0bNy4Eb1794bBYMCLL74ovUit7du3D4mJiZa/x4wZAwAYMmQIFi9eXOJ99N56zO4923KZSAb7vpJx+hCX9qmHbtfh8lxLo9NV2qzdNtNKjJmqp3hwJISA2Vx4MKmffvoJ3t7e6Nq1K4DCYyBdvnxZboUVoHv37hAKP1B8vX0xuv1obQqiKsu+r2ScPoR9Kh9zlY+ZysdM1VO8Wq1evXrYsGEDAGDFihVo27YtAgICAAAXLlyoEiedJSsesP0IuYAj2x55yPZJ5Sp6no4814rKxBOzV/ucPDELe0p6sIpQvOToqaeewsiRI7F06VJkZWXh008/tdz2008/2WyP5MnMwoz0rHQAQFxYHLx0Unb8oyrOLMw4c61wW7e4sDgp82Ofymf/OjFX5zFT+ZipeooHR8888wyqV6+OPXv2oG3btnjkkUcst+Xl5WHo0KEy63NbeQV5SPggAQAPy07y5BXkIWHu//eVlPk506eythly5hup2ho03N7J/nVy2ftfi2/61vO0n39RlhpkKz1TrV7/SjRft+nTSkjx4AgABgwYgAEDBhS7fsGCBU4XVJkE+la+88iR+5PdV+xTbTBX+ZipfMxUHVWDIyo8LHvu+FxXl0Eexr6vZJw+hH0qH3OVj5nKx0zVUzU42rVrF/7zn//g2LFjyMsrvjfNyZMnnS6MiNyYlrvoO7uqiBuV2irvtfK0wy3Yv/6ObPgPOL/a1tNyrOIUb531448/omfPnrh27RqOHTuGRo0aoU6dOjhz5gx8fHzQrVs3LeokIiIiqhCKB0eTJk3C448/js2bNwMApk2bht27d+PAgQO4fv067r//fulFuqN8Yz6GrxuO4euGI9+Y7+pyyEPI7iu371PrXYi12p1dg12UNctVZgb205R3HxcfXsHte7U0juas9PWQoNJm6gYUD45+//139OvXD7r/vagmU+EhyVu0aIHXXnsNU6dOlVuhmzKajVh4cCEWHlwIo9no6nLIQ8juK/apNpirfMxUPmaqnuJtjm7cuIHg4GB4eXnBz8/P5ojYjRo1wtGjR6UW6K58vX0xLXGa5TJJVIlPgeAs+74ywbnzIUnrU3fensIFtVXY+1/LJQtutm2W05m62fNxmIZ1O5Wp2gNm2nPHzwwHKB4cxcXF4dKlSwCAJk2a4Ntvv0VSUhIAYOfOnahZs6bcCt2U3luPCV0nuLoM8jD2fWWAc2fSZp9qg7nKx0zlY6bqKR4cde/eHTt27ED//v0xfPhwjBgxAseOHYOfnx+2bNlSKU88S0QOcuW3c6V7IVVVju6dRbYqKhfmX8g6B/ulS44sDS5tb8Hy7ucgxYOjKVOm4OrVqwCAp59+Gjdu3MDnn38OnU6HiRMnYsKEqjFKFUIgMzcTABAeGG7ZBovIGUIIXL5RuKo6PDBcyvzYp/LZv07M1XnMVD5mqp7iwVF4eDjCw///Q3vMmDEYM2aM1KIqgxsFNxA9JxoAD8tO8twouIHIdyIByDl9iOZ96k4fthVYi/3r5NG5VhDNM1XKA4635VaZKlka5Oj1pd1W2mlvFOARshUS/ws5JycHRZuDZGdnw6R3bsPZSi07+3+/Cn8LhY1YNH3R/a3nWdXk3sy17StDYV+pzZR9akVin3rf9GauQNXKtKI+kzwtU/vcSsqxtGxlZa4iU51wYColu+frdDq89tprDk9f2Zw9exaxsbGuLsOtZWRkICYmxuHpmWn5mKl8zFQ+ZiofM5XPkUwdGhx5eTl+OCSdTmc59pEnMpvNOH/+PEJCQrj+1o4QAjk5OYiOjlbUM8y0dMxUPmYqHzOVj5nKpyRThwZHRERERFWF4iNkExEREXkyxYOjP//8Ezt37izxtp07d+LEiRNOF0VERETkKooHR2PGjMHatWtLvG39+vU8CCQRERFVaooHR2lpaejatWuJt3Xr1g1paWlOF0VERETkKooHR9euXUNwcHCJtwUEBOCff/5xuigiIiIiV1E8OKpTpw5++eWXEm/75ZdfEBUV5XRRRERERK6i+AjZKSkpmDFjBjp06IDExETL9Tt27MDbb7+NYcOGSS3Q3fAYEqXjcTnkY6byMVP5mKl8zFQ+RZkKhbKyskTTpk2Fl5eXaNSokbjzzjtFo0aNhJeXl2jWrJm4du2a0llWKhkZGQIAf8r4ycjIYKbM1O1/mCkzrQw/zNQ1mSpechQWFobU1FTMnj0bmzdvxunTpxEREYEpU6Zg9OjRpW6P5ClCQkIAFB5+PDQ01MXVuJfs7GzExsZaMnIUMy0dM5WPmcrHTOVjpvIpyVTViWeDg4Px2muvefQ51EpTtJgyNDSUjVcKpYtymWn5mKl8zFQ+ZiofM5XPkUxVDY4IMJqNWPPHGgBAcoNk+HgxSmcxU9KC0WzEhj83ACjsKy3myV51HjOVT1WmRQOHKn5mMXafSj5ePkhplOLqMjwKMyUtaNFX7FX5mKl8zFQ9nluNiIiIyAqXHKlkMpuwI30HAKBLXBd4e3m7tiAPwExJCyazCbvP7AZQ2FdazJO96jxmKh8zVc+hwdG6devQrVs3hIWFaV1PpWEwGpC4pPA4T9fHXUeQPsjFFVV+zJS0YN9XWsyTveo8ZiofM1XPocFRv379sHfvXrRt2xa33HILVq9ejZYtW2pdm1vT6XRoEtHEcpmcx0xJC1r0FXtVPmYqHzNVz6HBUUBAAG7cuAEASE9PR35+vqZFVQaBvoE4MuKIq8vwKMyUtGDfV9l52dLnSc5jpvIxU/UcGhw1btwYEyZMQL9+/QAAy5cvx48//ljitDqdDi+88IK8ComIiIgqkEODoxkzZuDhhx/GK6+8Ap1Oh//85z+lTsvBEREREVVmDg2OevbsicuXL+PcuXOIjY3F6tWrcdttt2lcmnvLK8jDA589AABYN2AdAnwDXFxR5cdMSQt5BXm4d8W9AAr7Sot5sledx0zlY6bqKdqVv06dOpg0aRLuuOMOREdHa1VTpWAWZmw7uc1ymZzHTEkLWvQVe1U+ZiofM1VP8XGOJk2aZLn8559/4sqVKwgPD0f9+vWlFubu/Hz8sKzfMstlch4zJS3Y99UN3JA+T3IeM5WPmaqn6iCQX331FV566SWcPXvWcl1MTAzeffdd9O/fX1px7szHyweDWwx2dRkehZmSFrToK/aqfMxUPs0z9eDzsCk+fcjGjRsxYMAAhIWFYcaMGVi6dCmmT5+OsLAwDBgwAJs2bdKiTiIiIqIKoXjJ0Ztvvom77roL3377Lby8/n9s9fLLLyMpKQnTpk1DUlKS1CLdkclsQtq5NABAq6hWPCy7BMyUtGAym3DgwgEAhX2lxTzZq85jpvIxU/UUD44OHTqEFStW2AyMgMJd+EeMGIFBgwZJK86dGYwGtF3YFgAPyy4LMyUt2PeVFvNkrzqPmcrHTNVTPDjy9vbGzZs3S7ytoKCg2KDJU+l0OsSHxVsuk/OYKWlBi75ir8rHTOVjpuopHhzdcccdmDlzJu655x4EBPz/MRPy8/PxzjvvoF27dlILdFeBvoFIH53u6jI8CjMlLdj3lazTh7BX5WKm8jFT9RQPjqZMmYKePXvilltuwYMPPojatWvjwoULWLVqFa5cuYIffvhBizqJiIjIHVSBpVCK14F17twZW7ZsQd26dfHBBx9g4sSJmD9/PurWrYstW7agY8eOWtSpmenTp+OOO+5ASEgIIiMjkZKSguPHj7u6LCIiInIRVRsIdevWDXv37kVOTg4yMjKQnZ2Nn376CV27dpVdn+Z27tyJkSNHIjU1FVu3boXRaMRdd92F3NzcMu9nMBqQsiIFKStSYDAaKqhaz8ZMSQta9BV7VT5mKl+FZarTedzSJFUHgSwSGBiIwMBAWbW4xObNm23+XrRoESIjI7F///4yB3smswlrj6+1XCbnMVPSghZ9xV6Vj5nKx0zVc2pw5ImuXbsGAKhRo0aZ0+m99ViQvMBymZzHTEkL9n2Vhzzp8yTnMVP5mKl6HBxZEUJgzJgx6Ny5M5o1a1bmtL7evhjeengFVVY1MFPSgn1fyRgcsVflY6byMVP1ODiy8uyzz+Lw4cP48ccfXV0KERERuQgHR//z3HPPYd26ddi1axdiYmLKnd4szDjy9xEAQOOIxvDSVY2DX2qJmZIWzMKMY5nHABT2lRbzZK86j5nKx0zVUzw4unjxImrXrq1FLS4hhMBzzz2H1atXY8eOHUhISHDofnkFeWg2v3DVGw/LLgczJS3Y95UW82SvOq/CMi3vTPIedKZ59ql6igdHcXFxeOCBB/Dss8+iU6dOWtRUoUaOHInly5dj7dq1CAkJwcWLFwEAYWFhNkcAL0l4YHhFlFilMFPSghZ9xV6Vj5nKx0zVUTw4mjhxIhYsWIAvv/wSzZs3x3PPPYdBgwaVO5BwV/PnzwcAdO/e3eb6RYsWYejQoaXeL0gfhMyXMzWsrOphpqQF+77KNjh/+hD2qnzMVD6pmXrYcYzKo3gF5Ouvv47Tp0/jiy++QGhoKIYPH46YmBi89NJL+O9//6tFjZoSQpT4U9bAiIiIiDyXqq2zvL298dBDD2HXrl04dOgQHnjgAXz44Ydo2LAhkpOT8d1338muk4iISB4PPKozyeP0puvNmzdHUlISmjVrBrPZjO+//x733HMP2rRpgz///FNGjW7JYDRg8KrBGLxqMA91LwkzJS1o0VfsVfmYqXzMVD3Vg6PLly9j+vTpSEhIQP/+/eHj44OVK1ciOzsba9asQU5OjkevmjKZTVj+23Is/205D8suiUszLfoWqebbJL+BujUt+orvf/mYqXzMVD3FG2T//PPP+OCDD/DVV19BCIGHH34Yo0aNQqtWrSzT9O3bFz4+PkhJSZFZq1vRe+sxu/dsy2VyHjMlLdj3lazTh7BX5WKm8jFT9RQPjjp06IDatWtj7NixeOaZZxAZGVnidHXr1kXHjh2dLtBd+Xr7YnT70a4uw6MwU9KCfV/JOn2IW/aqTldpj8/jtplWYsxUPcWDo6VLl+Lhhx+Gr69vmdM1btwY27dvV10YERERkSso3ubo5MmTyMws+bgJFy5cwNSpU50uqjIwCzPSs9KRnpUOszC7uhzXkbitjdRMHd0OSIvthbgNklvR4r2qSa+6ql9d+ThWNP9MVfucKvH72eFM1W5v6cEUD46mTJmCs2fPlnjb+fPnMWXKFKeLqgzyCvKQMDcBCXMTkFfg/GJ6YqakDS36ir0qHzOVj5mqp3i1mihjffb169fLXd3mSQJ9A11dgsdxu0yd2YbD+ltYJd0OxFNo0Vdu0atafNO3nqf9/Iv6WKPzj0nNVKtzpFWy+bpFn1ZCDg2ODh8+jEOHDln+3rhxI/744w+bafLy8vD555+jXr16Ugt0V0H6IOSOz3V1GR6FmZIW7PtK1ulD2KtyMVP5mKl6Dg2OVq9ebVldptPpSt2uKCAgAIsWLZJXHZFsjn47U7ttglb1EKlVhc5CD6D4+7C896Xa529/P0/LsYpzaHD05JNPIjk5GUIItG3bFosWLUKzZs1spvHz80O9evUq7QloiYiIiAAHB0dRUVGIiooCAGzfvh2tWrVCSEiIpoW5u3xjPoavGw4AeP+e9+Hn4+fiiio/t820tG+eSr4hKl1ixW+f0uQb8/HsxmcBFPaVFvOU1qsyl2yWtgSlvCVIzj6uSpplqjVHc3bBkqZKm6kbULy3Wrdu3ar8wAgAjGYjFh5ciIUHF8JoNrq6HI/ATEkLWvQVe1U+ZiofM1XPoSVHTzzxBF577TUkJCTgiSeeKHNanU6HTz75REpx7szX2xfTEqdZLpPzNM1U6XYIFU3reirxkZOdZd9XJjh/jqkKe/9r2Rdu9h5wOlM3ez4O07BupzKVtd1lJf3ccWhwtH37dowaNQoA8MMPP0BXRmhl3eZJ9N56TOg6wdVleBRmSlqw7ysDnD87OXtVPmYqHzNVz6HB0alTpyyX09PTtaqFyPNxmyLSmqN7Z5GtisqF+Rcq6zhwjnxOlrYNV3n3c5Dig0BSISEEMnMLT6MSHhheZZaYaYmZkhaEELh84zKAwr7SYp7sVecxU/mYqXqKN8hOTU3Fl19+WeJtX375JX7++Weni6oMbhTcQOQ7kYh8JxI3Cm64uhyPoEmmWp4vyJl58zxGFUaLvtL8/V8F+8PtPlOdfQ2sz1fmotfSrTJ1JIfSprHP0v4o7iVtU+pk9oqXHI0fPx6dOnXCQw89VOy2o0eP4uOPP8bWrVtVFVMZFJ0+JScnB0WbLmRnZ8Okd34jz0orO/t/vwp/l3WKmZIw0xJIyrTo/tbzrGpyb+ba9pWhsK+cydT7pjd7FZDap26faUW9fzwtU/vcSsqxtGxlZa4mU6FQzZo1xYYNG0q8bePGjSIiIkLpLCuVjIwMAYA/ZfxkZGQwU2bq9j/MlJlWhh9m6ppMFS85ys3NhY9PyXfz8vIq/PbvwaKjo5GRkYGQkBCuv7UjhEBOTg6io6MV3Y+Zlo6ZysdM5WOm8jFT+ZRkqhNC2TK7Jk2a4N5778WMGTOK3TZ27FisWbOm2ElpiYiIiCoLxRtkDxgwALNnzy52gtnFixdjzpw5GDhwoLTiiIiIiCqa4iVHN2/exN13340dO3YgICAA0dHROH/+PAwGA7p3745NmzZBr9drVS8RERGRphQPjgDAZDJh+fLl2Lx5MzIzMxEREYGkpCQMHDgQ3t7eWtRJREREVCFUDY6IiIiIPJXibY6IiIiIPJlDu/L36NED8+bNQ6NGjdCjR48yp9XpdPj++++lFEdERERU0RwaHFmveTObzWUeO8HT19KZzWacP3+ex5AogfUxJLy8HF8oyUxLx0zlY6byMVP5mKl8ijJVdOhN4tFHJR19lJkyU1f/MFNmWhl+mKlrMlV8hOxdu3ahVatWCA4OLnZbbm4u9u/fj65duyqdbaUREhICAMjIyEBoaKiLq3Ev2dnZiI2NtWTkKGZaOmYqHzOVj5nKx0zlU5Kp4sFRYmIi9u7di7Zt2xa77Y8//kBiYiJMJjc7YaBERYspQ0ND2XilULool5mWj5nKx0zlY6byMVP5HMlU8eBIlLFNUUFBgaJ1o5WZ0WzEmj/WAACSGyTDx0txlGSHmcrHTEkLRrMRG/7cAKCwr7SYJ3vVeaoyLRo4ePj2w+VxqPuys7ORlZVl+fvixYs4c+aMzTR5eXlYsmQJateuLbVAd+Xj5YOURimuLsOjMFP5mClpQYu+Yq/Kx0zVc2hwNHv2bEydOhVA4eKofv36lTidEALjx4+XVx0RERFRBXNocHTXXXchODgYQgi88soreO655xAXF2czjZ+fH5o3b45u3bppUqi7MZlN2JG+AwDQJa4LvL142hRnMVP5mClpwWQ2YfeZ3QAK+0qLebJXncdM1XNocNShQwd06NABQOEeacOHD0d0dLSmhbk7g9GAxCWJAIDr464jSB/k4ooqP2YqHzMlLdj3lRbzZK86j5mqp3iLt0mTJhW7zmAwID09HfXr168yJ57V6XRoEtHEcpmcx0zlY6akBS36ir0qHzNVT/Hg6L333kNWVhZee+01AMD+/ftx99134+rVq6hbty527NiB2NhY6YW6m0DfQBwZccTVZXgUZiofMyUt2PdVdl629HmS85ipeor3u1+4cCGqVatm+fvVV19FjRo1MHv2bAghMG3aNJn1EREREVUoxUuOzpw5g0aNGgEAcnJysGvXLqxYsQL3338/qlevjtdff116kUREREQVRfGSo/z8fPj6+gIA9u7dC7PZjDvvvBMAULduXVy8eFFuhW4qryAPvT7rhV6f9UJeQZ6ry/EIzFQ+Zkpa0KKv2KvyMVP1FC85iouLw+7du9G9e3esXbsWt912m+UQ5ZmZmVXmcOVmYca2k9ssl8l5zFQ+Zkpa0KKv2KvyMVP1FA+OHnnkEUyZMgVr1qzBr7/+infeecdy2759+9CgQQOpBborPx8/LOu3zHKZnMdM5WOmpAX7vrqBG9LnSc5jpuopHhxNmDABPj4+2LNnD/r164fnn3/ectvvv/+OBx54QGqB7srHyweDWwx2dRkehZnKx0xJC1r0FXtVPs0z9eDzsCkeHOl0OowdO7bE29atW+d0QURERESuxNMeq2Qym5B2Lg0A0CqqFQ/LLgEzlY+ZkhZMZhMOXDgAoLCvtJgne9V5zFQ9VYOjEydO4KOPPsKxY8eQl2e7BbxOp8P3338vpTh3ZjAa0HZhWwA8LLsszFQ+ZkpasO8rLebJXnUeM1VP8eDo999/R/v27VGnTh389ddfaNGiBS5fvoxz584hNjYW9erV06JOt6PT6RAfFm+5TM5jpvIxU9KCFn3FXpWPmaqneHA0fvx49O7dGytXroRer8cnn3yCVq1a4dtvv8UTTzxRZY6QHegbiPTR6a4uw6MwU/mYKWnBvq9knT6EvSoXM1VP8UEgDxw4gCFDhsDLq/CuZnPhsRP69OmDl156CePGjZNbIREREbkPne7/91TzUIoHR//88w9q1KgBLy8v+Pr64p9//rHc1qZNGxw4cEBqgUREREQVSfHgqE6dOrh8+TIA4NZbb8WuXbsstx0+fBjBwcHyqnNjBqMBKStSkLIiBQajwdXleARmKh8zJS1o0VfsVfkqLFMPXJKkeJujzp07Y8+ePUhJScHgwYMxadIkXLhwAXq9HosXL8YjjzyiRZ1ux2Q2Ye3xtZbL5DxmKh8zJS1o0VfsVfmYqXqqjpB9/vx5AMCrr76Kixcv4vPPP4dOp8NDDz1kczoRT6b31mNB8gLLZXIeM5WPmZIW7PsqD86f1JS9Kh8zVU8nhAce91uB+fPnY/78+UhPTwcANG3aFK+//jqSkpJKnD47OxthYWG4du1alTnJrqPUZsNMS8dM5WOm8jFT+VyWqSOnBCltFZqbDyeUZKN4myNPExMTgxkzZmDfvn3Yt28fevTogfvuuw9HjhxxdWlERETkAg6tVlu6dKmimT722GOqinGFvn372vz95ptvYv78+UhNTUXTpk1LvZ9ZmHHk78IBVOOIxvDSVflxptOYqXzMlLRgFmYcyzwGoLCvtJgne9V5zFQ9hwZHQ4cOdXiGOp2uUg2OrJlMJnz11VfIzc1Fhw4dypw2ryAPzeY3A8DDssvCTOVjpqQF+77SYp6a9Wp5q4086EzzfP+r59Dg6NSpU1rX4VK//fYbOnToAIPBgODgYKxevRpNmjQp937hgeEVUF3VwkzlY6akBS36ir0qHzNVx6HBUXx8vNZ1uFTDhg1x6NAhZGVl4ZtvvsGQIUOwc+fOMgdIQfogZL6cWYFVej5mKh8zJS3Y91W2wfnTh7BX5ZOaqYcdx6g8inflL3Lt2jWkpqbi8uXLuOeee1C9enWZdVUovV6PW2+9FUDhUb7T0tIwd+5cfPTRRy6ujIiIiCqaqq2z3njjDURHRyMpKQmPPfaYZbVbz549MWPGDKkFuoIQAvn5+a4ug6jy8cAj5ZKHYq9SGRQPjubNm4cpU6Zg2LBh+Pbbb2F9mKTk5GR8++23UgvU2vjx47F7926kp6fjt99+w4QJE7Bjxw4MHjy4zPsZjAYMXjUYg1cN5qHuJWGm8jFT0oIWfcVelY+Zqqd4tdr777+PMWPGYObMmTCZbA9HXr9+fZw4cUJacRXh0qVLePTRR3HhwgWEhYWhRYsW2Lx5M3r16lXm/UxmE5b/thwALEcgJee4NFP7b5BK9lRx471bXJZpaZm4cVbkOC36ip+p8jFT9RQPjk6ePInevXuXeFtISAiysrKcralCffLJJ6rup/fWY3bv2ZbL5DxmKh8zJS3Y95Ws04ewV+VipuopHhyFhYXh0qVLJd6Wnp6OyMhIp4uqDHy9fTG6/WhXl+FRmKl8Ls+US4o8kn1fyRgcubxXPRAzVU/xNkc9e/bEzJkzkZuba7lOp9PBaDRi/vz5pS5VIiIiIqoMFC85mjp1Ku644w40adIE/fr1g06nw/vvv4+DBw/izJkz+PLLL7Wo0+2YhRnpWekAgLiwuKp7WHadTtpSAamZOrrEQou9VdxoaYnb9qmSk1u6QY5kyyzMOHPtDIDCvtJintJ7Ve17vRL3ocOZcq+9YhR336233oqffvoJjRs3xrx58yCEwNKlSxEeHo7du3cjLk7OG8Xd5RXkIWFuAhLmJiCvwPlFysRMtcBMSQta9BV7VT5mqp6qg0A2adIEmzdvRn5+Pq5cuYLq1asjICBAdm1uL9A30NUleBy3y9SZJWPW38Zc+K3TqUxlfWt25pup2hoq8Tf+ykCL96rUeWr1+ley+brdZ2olofoI2QDg5+cHo9EIX19fWfVUGkH6IOSOzy1/QnIYM5WPmZIW7PtK1ulD2KtyMVP1nFqpazKZkJCQgMOHD8uqh0hbjh4VV83RcyvqPu5Ay7qdnXdlzZTksH/9y+sHtf2i9HGoUnF6izfBRdZERETkQdxk15XKJ9+Yj+HrhmP4uuHIN/I8bDK4baZF3wjtf9TMQ9Z0DnLbTIuoyVVpRvxGL50WfeX2vVoaR5cguWBJU6XN1A1wcKSS0WzEwoMLsfDgQhjNRleX4xGYqXzMlLSgRV+xV+Vjpuo5tUG2t7c3tm/fjoYNG8qqp9Lw9fbFtMRplsvkPE0ztf+G5m5LEjSqR1qm7rznlzvX5qHs+8oEUzn3UD5PxdztPe0oDet2KlO122HZq6TvS6cGRwDQrVs3GXVUOnpvPSZ0neDqMjwKM5WPmZIW7PvKAOfP+M5elY+ZqufQ4GjXrl1o1aoVgoODsWvXrnKn79q1q9OFEXmkyr6Uw5Xfzt196R9VbhXVT+zbQmUdB07NEfQlH1fOocFR9+7dkZqairZt26J79+7QlfLiCiGg0+lgMjm/iNXdCSGQmZsJAAgPDC81E3IcM5WPmZIWhBC4fOMygMK+0mKe7FXnMVP1HBocbd++HU2aNLFcJuBGwQ1Ez4kGAFwfdx1B+iAXV1T5aZKplh8GMo76rDHN+9SdPmzdqRYPd6PgBiLfiQRQ2FdazNPln6nO9pMb9KNbZapkaZCj15d2m/11KpYkOTQ4st6uqKpuY1Sk6LhOOTk5KFrNnp2dDZPe85eWlSo7+3+/Cn8rPfYVMy0BM5VPUqZF96/Kcm/m2vaVobCvnMnU+6a3e/dqRb3uEvvULTK1z62kHEvLVlbmajIVpEhGRoYAwJ8yfjIyMpgpM3X7H2bKTCvDDzN1TaY6IZQvbzp48CCWL1+O06dPw2Cw3UtBp9Nh7dq1SmdZaZjNZpw/fx4hISFcf2tHCIGcnBxER0fDy8vxQ2gx09IxU/mYqXzMVD5mKp+STBUPjpYuXYrHH38cXl5eiIyMhF6vt52hToeTJ08qr5qIiIjIDSgeHDVs2BANGzbEkiVLUL16da3qIiIiInIJxQeBPHfuHD744AMOjIiIiMgjKT632u23345z585pUQsRERGRyykeHM2aNQszZszA4cOHtaiHiIiIyKUUr1Zr37497r//ftx+++2IiopCjRo1bG7X6XT49ddfpRVIREREVJEUD47efvttTJ8+HREREYiPjy+2txoRERFRZaZ4b7Xo6Gjcc889+Oijj+Dt7a1VXUREREQuoXjJUXZ2NgYNGlRlB0Y8wFbpeNAy+ZipfMxUPmYqHzOVT0mmigdHnTt3xtGjR9GjRw/VBVZm58+fR2xsrKvLcGsZGRmIiYlxeHpmWj5mKh8zlY+ZysdM5XMkU8WDo7lz5+KBBx5AbGwskpKSqtw2RyEhIQAKww0NDXVxNe4lOzsbsbGxlowcxUxLx0zlY6byMVP5mKl8SjJVPDhq06YNCgoKcP/990On0yEwMNDmdp1Oh2vXrimdbaVRtJjS298bYbPDAADXx11HkD7IlWW5FaWLcplp+ZipfGozDQ0Nhbe/N4KnBwNgrtacybSq/yPPvZlr01OhKMyjwvu06PGUn3a10nAkU8WDowceeIDrMQH4+fhh9cOrLZfJecxUPmaqDeZKstn31A3ckD5PcpziwdHixYs1KKPy8fHyQUqjFFeX4VGYqXzMVBvMlWTToqfYp+opPkI2ERERkSdTNTj6448/MHDgQERFRUGv1+PAgQMAgClTpmD79u1SC3RXJrMJO9J3YEf6DpjMJleX4xGYqXzMVBvMlWTToqfYp+opXq126NAhdOnSBSEhIejevTu+/PJLy23Xr1/Hhx9+iMTERKlFuiOD0YDEJYXPkxtkysFM5WOm2mCuJJt9T2kxT/ap4xQPjsaOHYsWLVpg69at0Ov1WLlypeW2tm3b4ptvvpFaoLvS6XRoEtHEcpmcx0zlY6baYK4kmxY9xT5VT/Hg6KeffsKyZcsQGBgIk8l2MV2tWrVw8eJFacW5s0DfQBwZccTVZXgUZiofM9UGcyXZ7HsqOy9b+jzJcYq3ORJClHrgx3/++Qd+ftxdkIiIiCovxYOjFi1aYPXq1SXetnnzZrRu3drpooiIiIhcRfFqtVGjRmHQoEEICgrCo48+CgA4c+YMfvjhB3z66af4+uuvpRfpjvIK8vDAZw8AANYNWIcA3wAXV1T5MVP5mKk28grycO+KewEwV5LDvqe0mCf71HGKB0cPP/ww/vvf/2Ly5Mn4z3/+A6DwqNk+Pj6YMmUK+vbtK71Id2QWZmw7uc1ymZzHTOVjptpgriSbFj3FPlVP8eAIAMaPH4/HHnsM3333HS5duoTw8HD07t0b8fHxsutzW34+fljWb5nlMjmPmcrHTLXBXEk2+56SdfoQTfvUg8/DpmpwBAAxMTEYNmyYzFoqFR8vHwxuMdjVZXgUZiofM9UGcyXZtOgp9ql6Tp0+5OrVqxg7diySk5Px1FNP4cgR7jJIRERElZtDS45eeuklfPnllzhz5ozlutzcXLRp0wanT5+G+N8itRUrVuCXX35Bw4YNtanWjZjMJqSdSwMAtIpqBW8vbxdXVPkxU/mYqTZMZhMOXCg8bRJzJRnse0qLebJPHefQkqM9e/ZgwIABNte9//77SE9Px+jRo5GVlYU9e/YgODgYM2bM0KRQd2MwGtB2YVu0XdgWBqPB1eV4BGYqHzPVBnMl2bToKfapeg4Njk6ePIk2bdrYXLd+/XpERERg5syZCA0NRfv27TFmzBjs2LFDizorzPTp06HT6TB69Ogyp9PpdIgPi0d8WDwPyy4JM5WPmWqDuZJsWvQU+1Q9h1arZWVlISoqyvK30WhEWloaUlJS4O39/4vpbr/9dly4cEF+lRUkLS0NCxYsQIsWLcqdNtA3EOmj07UvqgphpvIxU20wV5LNvqdknT5Ekz6tAgMth5Yc1apVy2bQc+DAARQUFBRbmuTl5VVpTx9y/fp1DB48GB9//DGqV6/u6nKIiIjIRRwaHLVu3Roff/yxZcPrzz//HDqdDj179rSZ7o8//rBZwlSZjBw5En369MGdd97p6lKIiIgqD53O45YmObRa7dVXX0WnTp3QsGFDhIeHIzU1FV26dEGrVrZb1K9fvx533HGHJoVqacWKFdi/fz/27dvn8H0MRgMeW/FY4f37r4C/j79W5VUZzFQ+ZqoNg9GAAV8X7qTCXEkG+57SYp7sU8c5NDhq164d1q5di1mzZuHKlSv417/+VWyvtIsXL+Ls2bN4/PHHNSlUKxkZGRg1ahS2bNkCf3/HG8dkNmHt8bWWy+Q8ZiofM9UGcyXZtOgp9ql6Dh8hu0+fPujTp0+pt9euXRu//vqrlKIq0v79+/H333+jdevWlutMJhN27dqF999/H/n5+TYbnRfRe+uxIHmB5TI5j5nKx0y1wVxJNvueykOe9HmS41SfPsRT9OzZE7/99pvNdY8//jgaNWqEV199tcSBEQD4evtieOvhFVFilcFM5WOm2mCuJJt9T8kYHLFP1avyg6OQkBA0a9bM5rqgoCDUrFmz2PVERETk+ar84EgtszDjyN+F55JrHNEYXjqnTlNHYKZaYKbaMAszjmUeA8BcSQ77ntJinuxTx3FwVAJHjvKdV5CHZvMLlyxdH3cdQfogjavyfMxUPmaqDeZKstn3lBbzZJ86joMjJ4QHhru6BI/DTOVjptpgriSbFj3FPlWHgyOVgvRByHw509VleBRmKh8z1QZzJdnseyrb4PzpQ6T2qYcd5LE8XAFJREREZIWDIyKSxwNPI0BEVQ8HRyoZjAYMXjUYg1cNhsFocHU5HoGZysdMtcFcSTYteop9qh4HRyqZzCYs/205lv+2nIdll8SlmRYt8VCz5MONl5a4LNPSMnHjrJTg+59k06Kn2KfqcYNslfTeeszuPdtymZzHTOVjptpgriSbfU/JOn0I+1QdDo5U8vX2xej2o11dhkdhpvK5PNOipURCuK4GDbg819LodB6XdVVh31OyTh/iln1aCXC1GhEREZEVLjlSySzMSM9KBwDEhcVV3cOyS/ymKjVTR5dYaLH9ixstLXHbPnUkIzfK0Z5ZmHHm2hkAEnsVcLxftc7EjbP3VPY9pcU8S+1TD9gOUDYOjlTKK8hDwgcJAHhYdlmYqXzMVBt5BXlImMtcSR77ntJinuxTx3Fw5IRA30BXl+Bx3C5TZ5aMKVkioCGnMpW1BMGZb6Zqa9B46Ydb9KqWSz5Lmn9RllyypAktesot+rQS4uBIpSB9EHLH57q6DI/CTOVjptpgriSbfU/JOn0I+1QdDo6oatFyWyRn7lPZvoFrWbezS0O4/YSt8l6rytqDRBpyk60ziYiIiNwDB0cq5RvzMXzdcAxfNxz5xnxXl+MR3DZT+6Nna3kUbclHkHbbTIuoyVVpRhoclVuzXGVmYD9NefdRM0+SRouecvv3vxvj4Eglo9mIhQcXYuHBhTCaja4uxyMwU/mYqTaYK8mmRU+xT9XjNkcq+Xr7YlriNMtlcp6mmdp/23W3b78a1SMtU3feLsUFtVXY+1/LPnW390AVZ99TJjh/LjSn+lTWdpfu+JnhAA6OVNJ76zGh6wRXl+FRmKl8zFQbzJVks+8pAwzS50mO4+CIqCK58xIYR7hyaYO7L/1zF45sN0TkamUdB07NEfQlH1eOgyOVhBDIzM0EAIQHhkPHDxynMVP5mKk2hBC4fOMyAOZKctj3lBbzZJ86joMjlW4U3ED0nGgAPCy7LJpk6q7bbFTQh5TmfepOH7YVWMuNghuIfCcSQBXIlSqEfU9pMU+X/p9SsjTI0etLu620I7srwMGRQuJ/Iefk5KBolXB2djZMeuc3nqu0srP/96vwt1DYiMy0BMxUPkmZZmdnw/umN3MFpGZa1eXezLXtKUNhT1X6PrV/bUt6rUt7/WX1hZo+FaRIRkaGAMCfMn4yMjKYKTN1+x9mykwrww8zdU2mOiEq65ahrmE2m3H+/HmEhIRw/a0dIQRycnIQHR0NLy/HD6HFTEvHTOVjpvIxU/mYqXxKMuXgiIiIiMgKj5BNREREZIWDIyIiIiIrHBwRERERWeHgiIiIiMgKB0dEREREVjg4IiIiIrLCI2QrxGNIlI7H5ZCPmcrHTOVjpvIxU/mUZMrBkULnz59HbGysq8twaxkZGYiJiXF4emZaPmYqHzOVj5nKx0zlcyRTDo4UCgkJAVAYbmhoqIurcS/Z2dmIjY21ZOQoZlo6ZiofM5WPmcrHTOVTkikHRwoVLaYMDQ1l45VC6aJcZlo+ZiofM5WPmcrHTOVzJFMOjlQymo1Y88caAEByg2T4eDFKZzFT+ZipNoxmIzb8uQEAc5WFmZJs9j2lBLtPJR8vH6Q0SnF1GR6FmcrHTLXBXOVjpiSbMz3FXfmJiIiIrHDJkUomswk70ncAALrEdYG3l7drC/IAzFQ+ZqoNk9mE3Wd2A2CusjBTks2+p5Tg4Eglg9GAxCWJAIDr464jSB/k4ooqP2YqHzPVBnOVj5mSbPY9pQQHRyrpdDo0iWhiuUzOY6byMVNtMFf5mCnJ5kxPcXCkUqBvII6MOOLqMjwKM5WPmWqDucrHTEk2+57Kzst2+L7cIJuIiIjICgdHRERERFY4OFIpryAPvT7rhV6f9UJeQZ6ry/EIzFQ+ZqoN5iofMyXZnOkpbnOkklmYse3kNstlch4zlY+ZaoO5ysdMSTZneoqDI5X8fPywrN8yy2VyHjOVj5lqg7nKx0xJNvueuoEbDt+XgyOVfLx8MLjFYFeX4VGYqXzMVBvMVT5mSrI501Pc5oiIiIjICpccqWQym5B2Lg0A0CqqFQ91LwEzlY+ZasNkNuHAhQMAmKsszJRks+8pJTg4UslgNKDtwrYAeKh7WZipfMxUG8xVPmZKstn3lBIcHKmk0+kQHxZvuUzOY6byMVNtMFf5mCnJ5kxPcXCkUqBvINJHp7u6DI/CTOVjptpgrvIxU5LNvqd4+hAiIiIilTg4IiIiIrLCwZFKBqMBKStSkLIiBQajwdXleARmKh8z1QZzlY+ZkmzO9FSV3+Zo8uTJmDJlis11tWrVwsWLF8u8n8lswtrjay2XyXnMVD5mqg3mKh8zJdmc6akqPzgCgKZNm2Lbtm2Wv729yz++ht5bjwXJCyyXyXnMVD5mqg3mKh8zJdnseyoPjp98loMjAD4+Pqhdu7ai+/h6+2J46+EaVVQ1MVP5mKk2mKt8zJRks+8pJYMjbnME4MSJE4iOjkZCQgIGDBiAkydPurokIiIicpEqPzhq164dli5diu+++w4ff/wxLl68iI4dO+LKlStl3s8szDjy9xEc+fsIzMJcQdV6NmYqHzPVBnOVj5mSbM70VJVfrZaUlGS53Lx5c3To0AH16tXDkiVLMGbMmFLvl1eQh2bzmwHgoe5lYabyMVNtMFf5KizToiMlC6Hudqo07HtKiSo/OLIXFBSE5s2b48SJE+VOGx4YXgEVVS3MVD5mqg3mKh8zJdnU9hQHR3by8/Nx7NgxdOnSpczpgvRByHw5s4KqqhqYqXzMVBvMVT5mSrLZ91S2gacPcdhLL72EnTt34tSpU/j555/Rv39/ZGdnY8iQIa4ujYiIiFygyi85Onv2LAYOHIjLly8jIiIC7du3R2pqKuLj411dGlHlw+01qLJgr1IZqvzgaMWKFaruZzAa8MyqZwAAn9z7Cfx9/GWWVSUxU/mYqTYMRgOGrRsGgLnKwkxJNvueUqLKr1ZTy2Q2Yflvy7H8t+U81L0kLs1Up7P9UXNfN+SyTEvLxI2zUoLvf/mYKcnmTE9V+SVHaum99Zjde7blMjmPmcrHTLXBXOVjpiSbfU/x9CEVwNfbF6Pbj3Z1GR6Fmcrn8kw9dLsOl+daGp2u0mbttplSpWXfUzx9CBEREZFKXHKkklmYkZ6VDgCIC4uDl66KjjMlflOVmqmjSyy02P7FjZaWuG2fOpKRG+VozyzMOHPtDACJvQo43q9aZ+KC7KVmWhK173U37kMqm31PKcHBkUp5BXlI+CABAE8fIAszlY+ZaiOvIA8Jc5mrTMyUZLPvKSU4OHJCoG+gq0vwOG6XqTNLxpQsEdCQU5nK+tbszBI6tTVo/I3fLXpVyyWfJc2/KEuNspWaqVavf2WbbxWntqc4OFIpSB+E3PG5ri7DozBT+ZipNpirfMyUZLPvKSWnD+HgiKoWLbdFcuY+le3bopZ1O7s0xAOOoyRVVTsLvf3rX14/yFoy6Wk5VnFusnUmERERkXvg4EilfGM+hq8bjuHrhiPfmO/qcjyC22Zqf/RsLY+iLfkI0m6baRE1uSrNSIOjcmuWq8wM7Kcp7z5q5imR2/dqaRzNWenrQU5zpqc4OFLJaDZi4cGFWHhwIYxmo6vL8QjMVD5mqg3mKh8zJdmc6Sluc6SSr7cvpiVOs1wm52maqdLtECqaRvVIy9Sdt6dwQW0V9v7Xsk/d7D3gdKZu9nwcVlnrrgTse8oEx8+vxsGRSnpvPSZ0neDqMjwKM5WPmWqDucrHTEk2+54ywODwfTk4IqpI7rwExhGu/Jbr7kv/3IWje2eRrYrKhflXChwcqSSEQGZuJgAgPDAcOja805ipfMxUG0IIXL5xGQBzlYWZkmz2PaUEB0cq3Si4geg50QB4qHtZNMnUXbfZqKAPfs371J3+gVVgLTcKbiDynUgAVSDXCqJ5pkrxeFuVnn1PKcHBkULif6tDcnJyULT6Mjs7Gya94xt6eZzs7P/9KvwtFK4yYqYlYKbySco0Ozsb3je9mStQtTLNdvzoyjIeR0amVV3uzVzbnjIU9pRDmQpSJCMjQwDgTxk/GRkZzJSZuv0PM2WmleGHmbomU50QlXXLUNcwm804f/48QkJCuE7cjhACOTk5iI6OhpeX44fQYqalY6byMVP5mKl8zFQ+JZlycERERERkhUfIJiIiIrLCwRERERGRFQ6OiIiIiKxwcERERERkhYMjIiIiIiscHBERERFZ4eCIiIiIyAoHR0RERERWODgiIiIissLBEREREZEVDo6IiIiIrHBwRERERGSFgyMiIiIiKxwcEREREVnh4IiIiIjICgdHRERERFY4OCIiIiKywsERERERkRUOjoiIiIiscHBEREREZIWDIyIiIiIrHBwRERERWeHgiIiIiMgKB0dEREREVjg4IiIiIrLCwRERERGRFQ6OiIiIiKxwcERERERkhYMjIiIiIiscHBERERFZ4eCIiIiIyAoHR0Qa+fnnn9GvXz/ExcXBz88PtWrVQocOHfDiiy+6pJ4dO3ZAp9Nhx44d0uaZnp4OnU6HxYsXS5untT179mDy5MnIysoqdlv37t3RvXt3xfOsW7cuhg4davn7/PnzmDx5Mg4dOqS6zory1ltvYc2aNZo/ztChQ1G3bl3NH4fIXXFwRKSBb7/9Fh07dkR2djZmzpyJLVu2YO7cuejUqRNWrlzpkppatWqFvXv3olWrVi55fDX27NmDKVOmlDg4mjdvHubNm6d4nqtXr8Zrr71m+fv8+fOYMmUKB0dEZOHj6gKIPNHMmTORkJCA7777Dj4+//82GzBgAGbOnOmSmkJDQ9G+fXuXPLYWmjRpoup+t99+u+RKtJeXl4eAgABXl0FUZXDJEZEGrly5gvDwcJuBUREvr+Jvu5UrV6JDhw4ICgpCcHAwevfujYMHD9pMM3ToUAQHB+OPP/5A7969ERQUhKioKMyYMQMAkJqais6dOyMoKAgNGjTAkiVLbO6vdLXaiRMnMGjQIERGRsLPzw+NGzfGBx98UO79/vrrLzz++OOoX78+AgMDUadOHfTt2xe//fabzXRmsxnTpk1Dw4YNERAQgGrVqqFFixaYO3cuAGDy5Ml4+eWXAQAJCQnQ6XQ29Ze0Wi0/Px9Tp05F48aN4e/vj5o1ayIxMRF79uyxTGO9Wm3Hjh244447AACPP/645TEmT56Mzz77DDqdDnv37i32HKdOnQpfX1+cP3++xAyOHDkCnU6Hr776ynLd/v37odPp0LRpU5tp7733XrRu3dqmvuTkZKxatQq33347/P39MWXKFOh0OuTm5mLJkiWWOtWsVgSA5cuXo0OHDggODkZwcDBuu+02fPLJJ2Xe54MPPkDXrl0RGRmJoKAgNG/eHDNnzkRBQYHNdAcPHkRycrKlb6Kjo9GnTx+cPXvWMs1XX32Fdu3aISwsDIGBgbjlllvwxBNPqHouRFrgkiMiDXTo0AELFy7E888/j8GDB6NVq1bw9fUtcdq33noLEydOxOOPP46JEyfi5s2bmDVrFrp06YJffvnFZglJQUEB7r//fjz99NN4+eWXsXz5cowbNw7Z2dn45ptv8OqrryImJgbvvfcehg4dimbNmtn843XU0aNH0bFjR8TFxeHdd99F7dq18d133+H555/H5cuXMWnSpFLve/78edSsWRMzZsxAREQErl69iiVLlqBdu3Y4ePAgGjZsCKBw6drkyZMxceJEdO3aFQUFBfjjjz8sq9D+9a9/4erVq3jvvfewatUqREVFASh9iZHRaERSUhJ2796N0aNHo0ePHjAajUhNTcWZM2fQsWPHYvdp1aoVFi1aZMm+T58+AICYmBhERkbilVdewQcffIAOHTrYPM5HH32Efv36ITo6usRamjZtiqioKGzbtg0PPvggAGDbtm0ICAjA0aNHcf78eURHR8NoNGLnzp14+umnbe5/4MABHDt2DBMnTkRCQgKCgoKQkpKCHj16IDEx0bJaMDQ0tNTXoTSvv/463njjDdx///148cUXERYWht9//x2nT58u837//e9/MWjQICQkJECv1+PXX3/Fm2++iT/++AOffvopACA3Nxe9evVCQkICPvjgA9SqVQsXL17E9u3bkZOTAwDYu3cvHn74YTz88MOYPHky/P39cfr0afzwww+KnwuRZgQRSXf58mXRuXNnAUAAEL6+vqJjx45i+vTpIicnxzLdmTNnhI+Pj3juueds7p+TkyNq164tHnroIct1Q4YMEQDEN998Y7muoKBARERECADiwIEDluuvXLkivL29xZgxYyzXbd++XQAQ27dvL7f+3r17i5iYGHHt2jWb65999lnh7+8vrl69KoQQ4tSpUwKAWLRoUanzMhqN4ubNm6J+/frihRdesFyfnJwsbrvttjLrmDVrlgAgTp06Vey2bt26iW7duln+Xrp0qQAgPv744zLnGR8fL4YMGWL5Oy0trdTnMGnSJKHX68WlS5cs161cuVIAEDt37izzcR555BFxyy23WP6+8847xfDhw0X16tXFkiVLhBBC/PTTTwKA2LJli0193t7e4vjx48XmGRQUZFO7UidPnhTe3t5i8ODBZU43ZMgQER8fX+rtJpNJFBQUiKVLlwpvb29LP+zbt08AEGvWrCn1vu+8844AILKyslQ9B6KKwNVqRBqoWbMmdu/ejbS0NMyYMQP33Xcf/vzzT4wbNw7NmzfH5cuXAQDfffcdjEYjHnvsMRiNRsuPv78/unXrVmwVmE6nwz333GP528fHB7feeiuioqJstqWpUaMGIiMjy1waIISweUyj0QgAMBgM+P7779GvXz8EBgba3H7PPffAYDAgNTW11PkajUa89dZbaNKkCfR6PXx8fKDX63HixAkcO3bMMl3btm3x66+/YsSIEfjuu++QnZ2tKGN7mzZtgr+/v9TVM8888wwA4OOPP7Zc9/7776N58+bo2rVrmfft2bMnTp48iVOnTsFgMODHH3/E3XffjcTERGzduhVA4dIkPz8/dO7c2ea+LVq0QIMGDaQ9jyJbt26FyWTCyJEjFd/34MGDuPfee1GzZk14e3vD19cXjz32GEwmE/78808AwK233orq1avj1VdfxYcffoijR48Wm0/RasyHHnoIX375Jc6dO+fckyLSAAdHRBpq06YNXn31VXz11Vc4f/48XnjhBaSnp1s2yr506RKAwn8Yvr6+Nj8rV660DKKKBAYGwt/f3+Y6vV6PGjVqFHtsvV4Pg8FQam07d+4s9pjp6em4cuUKjEYj3nvvvWK3Fw3M7OuyNmbMGLz22mtISUnB+vXr8fPPPyMtLQ0tW7ZEXl6eZbpx48bhnXfeQWpqKpKSklCzZk307NkT+/btKyfVkmVmZiI6OrrEbbrUqlWrFh5++GF89NFHMJlMOHz4MHbv3o1nn3223PveeeedAAoHQD/++CMKCgrQo0cP3Hnnnfj+++8tt3Xq1KnYxtZFqxBly8zMBFC42lCJM2fOoEuXLjh37hzmzp1rGfgXbYNW9LqGhYVh586duO222zB+/Hg0bdoU0dHRmDRpkmXbpK5du2LNmjWWLwUxMTFo1qwZvvjiC4nPlMg53OaIqIL4+vpi0qRJmD17Nn7//XcAQHh4OADg66+/Rnx8fIXW07p1a6SlpdlcV7QdjLe3Nx599NFSlzAkJCSUOt9ly5bhsccew1tvvWVz/eXLl1GtWjXL3z4+PhgzZgzGjBmDrKwsbNu2DePHj0fv3r2RkZGBwMBARc8nIiICP/74I8xms9QB0qhRo/DZZ59h7dq12Lx5M6pVq4bBgweXe7+YmBg0aNAA27ZtQ926ddGmTRtUq1YNPXv2xIgRI/Dzzz8jNTUVU6ZMKXZfnU4nrX5rERERAICzZ88iNjbW4futWbMGubm5WLVqlU2flnT4g+bNm2PFihUQQuDw4cNYvHgxpk6dioCAAIwdOxYAcN999+G+++5Dfn4+UlNTMX36dAwaNAh169a12b6LyFU4OCLSwIULF0r89l+0WqloQ97evXvDx8cH//3vf/HAAw9UaI0hISFo06ZNsev1ej0SExNx8OBBtGjRAnq9XtF8dTod/Pz8bK779ttvce7cOdx6660l3qdatWro378/zp07h9GjRyM9PR1NmjSxzMd6iVNpkpKS8MUXX2Dx4sWKVq2V9xitW7dGx44d8fbbb+P333/Hk08+iaCgIIfmfeedd+LLL79EbGysZWPvBg0aIC4uDq+//joKCgosS5gcrdWRLEpz1113wdvbG/Pnz1c0CCkarFm/rkIIm9WNJd2nZcuWmD17NhYvXowDBw4Um8bPzw/dunVDtWrV8N133+HgwYMcHJFb4OCISAO9e/dGTEwM+vbti0aNGsFsNuPQoUN49913ERwcjFGjRgEo3G176tSpmDBhAk6ePIm7774b1atXx6VLl/DLL78gKCioxCULWps7dy46d+6MLl264JlnnkHdunWRk5ODv/76C+vXry9zz6Lk5GQsXrwYjRo1QosWLbB//37MmjWr2Kqcvn37olmzZmjTpg0iIiJw+vRpzJkzB/Hx8ahfvz6AwqUQRfUMGTIEvr6+aNiwIUJCQoo97sCBA7Fo0SI8/fTTOH78OBITE2E2m/Hzzz+jcePGGDBgQIn11qtXDwEBAfj888/RuHFjBAcHIzo62mZPtFGjRuHhhx+GTqfDiBEjHM6xZ8+emDdvHi5fvow5c+bYXL9o0SJUr15d0d6EzZs3x44dO7B+/XpERUUhJCQEDRs2xOnTp1GvXj0MGTKkzF3y69ati/Hjx+ONN95AXl4eBg4ciLCwMBw9ehSXL18utdd69eoFvV6PgQMH4pVXXoHBYMD8+fPxzz//2Ey3YcMGzJs3DykpKbjlllsghMCqVauQlZWFXr16ASjcW+7s2bPo2bMnYmJikJWVhblz58LX1xfdunVzOAsiTbl2e3Aiz7Ry5UoxaNAgUb9+fREcHCx8fX1FXFycePTRR8XRo0eLTb9mzRqRmJgoQkNDhZ+fn4iPjxf9+/cX27Zts0wzZMgQERQUVOy+3bp1E02bNi12fXx8vOjTp4/lbyV7qwlRuCfaE088IerUqSN8fX1FRESE6Nixo5g2bZrNNLDb0+uff/4Rw4YNE5GRkSIwMFB07txZ7N69u9jeZe+++67o2LGjCA8PF3q9XsTFxYlhw4aJ9PR0mzrGjRsnoqOjhZeXl0399vMTQoi8vDzx+uuvi/r16wu9Xi9q1qwpevToIfbs2WOTi/0eX1988YVo1KiR8PX1FQDEpEmTbG7Pz88Xfn5+4u6773YoO+ssvLy8RFBQkLh586bl+s8//1wAEPfff3+x+9i/btYOHTokOnXqJAIDAwUAy/Mveh0c3ZNt6dKl4o477hD+/v4iODhY3H777TavYUl7q61fv160bNlS+Pv7izp16oiXX35ZbNq0yeY1+eOPP8TAgQNFvXr1REBAgAgLCxNt27YVixcvtsxnw4YNIikpSdSpU0fo9XoRGRkp7rnnHrF7926HaieqCDohhHDd0IyIyP2tX78e9957L7799lubvQWJyDNxcEREVIqjR4/i9OnTGDVqFIKCgnDgwAHNNpYmIvfBXfmJiEoxYsQI3HvvvahevTq++OILDoyIqgguOSIiIiKywiVHRERERFY4OCIiIiKywsERERERkRUOjoiIiIiscHBEREREZIWDIyIiIiIrHBwRERERWeHgiIiIiMjK/wGf9hgiH+wQoQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E0p = {j : (E0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E0p[j], num_bins, range = (np.quantile(E0p[j], 0.10), np.quantile(E0p[j], 0.90)), color = 'r', alpha = 1) # Logit is blue\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E1p = {j : (E1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E1p[j], num_bins, range = (np.quantile(E1p[j], 0.10), np.quantile(E1p[j], 0.90)), color = 'b', alpha = 1) # Similarity is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Similarity price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mean elasticities for the Logit Model are given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean elasticity wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean elasticity of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>-0.172311</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>-0.172365</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>-0.172652</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>-0.173053</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>-0.173175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean elasticity wrt. class         0         1         2         3         4  \\\n",
       "Mean elasticity of class                                                       \n",
       "0                          -0.003261  0.001052  0.000998  0.000711  0.000311   \n",
       "1                           0.170102 -0.172311  0.000998  0.000711  0.000311   \n",
       "2                           0.170102  0.001052 -0.172365  0.000711  0.000311   \n",
       "3                           0.170102  0.001052  0.000998 -0.172652  0.000311   \n",
       "4                           0.170102  0.001052  0.000998  0.000711 -0.173053   \n",
       "5                           0.170102  0.001052  0.000998  0.000711  0.000311   \n",
       "\n",
       "Mean elasticity wrt. class         5  \n",
       "Mean elasticity of class              \n",
       "0                           0.000189  \n",
       "1                           0.000189  \n",
       "2                           0.000189  \n",
       "3                           0.000189  \n",
       "4                           0.000189  \n",
       "5                          -0.173175  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(E0.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. class', index = 'Mean elasticity of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Similarity the mean elasticities are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean elasticity wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean elasticity of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003291</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000317</td>\n",
       "      <td>0.000199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.146003</td>\n",
       "      <td>-0.273327</td>\n",
       "      <td>0.058294</td>\n",
       "      <td>0.043596</td>\n",
       "      <td>0.017538</td>\n",
       "      <td>0.007896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.146003</td>\n",
       "      <td>0.063005</td>\n",
       "      <td>-0.255848</td>\n",
       "      <td>0.027030</td>\n",
       "      <td>0.015051</td>\n",
       "      <td>0.004759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.146003</td>\n",
       "      <td>0.067949</td>\n",
       "      <td>0.037333</td>\n",
       "      <td>-0.264566</td>\n",
       "      <td>0.011519</td>\n",
       "      <td>0.001762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.146003</td>\n",
       "      <td>0.063012</td>\n",
       "      <td>0.043911</td>\n",
       "      <td>0.023949</td>\n",
       "      <td>-0.281798</td>\n",
       "      <td>0.004923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.146003</td>\n",
       "      <td>0.047050</td>\n",
       "      <td>0.029886</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.006708</td>\n",
       "      <td>-0.238081</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean elasticity wrt. class         0         1         2         3         4  \\\n",
       "Mean elasticity of class                                                       \n",
       "0                          -0.003291  0.001076  0.000990  0.000710  0.000317   \n",
       "1                           0.146003 -0.273327  0.058294  0.043596  0.017538   \n",
       "2                           0.146003  0.063005 -0.255848  0.027030  0.015051   \n",
       "3                           0.146003  0.067949  0.037333 -0.264566  0.011519   \n",
       "4                           0.146003  0.063012  0.043911  0.023949 -0.281798   \n",
       "5                           0.146003  0.047050  0.029886  0.008434  0.006708   \n",
       "\n",
       "Mean elasticity wrt. class         5  \n",
       "Mean elasticity of class              \n",
       "0                           0.000199  \n",
       "1                           0.007896  \n",
       "2                           0.004759  \n",
       "3                           0.001762  \n",
       "4                           0.004923  \n",
       "5                          -0.238081  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(E1.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. class', index = 'Mean elasticity of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios\n",
    "\n",
    "We now visualize the implied diversion ratios $\\mathcal{D}$. If $\\bar D_{c\\ell}$ denotes the sum of choice probability weigthed diversion ratios, then we have as above that $\\bar D_{c\\ell} = \\sum_{j}\\sum_{k} \\mathrm{1}_{\\{j\\in c\\}} \\mathrm{1}_{\\{k\\in \\ell\\}} q_j q_k \\mathcal{D}_{jk}$ i.e. more generally $\\bar D = (\\psi^{\\text{class}} \\circ q) \\mathcal{D} (\\psi^{\\text{class}} \\circ q).'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiversionRatio_agg(data, Theta, q, x, model, direction_var, market_id = 'market', product_id = 'co', char_number = K-1, mode = 'Similarity', outside_option = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    dq_du_agg = ccp_directionalgrad(data, Theta, q, x, model, direction_var, market_id, product_id, model, outside_option)[1]\n",
    "    D_agg = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        D_agg[t] = -100*np.einsum('cl,c->cl', dq_du_agg[t], 1./np.diag(dq_du_agg[t]))\n",
    "\n",
    "    return D_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_Similarityagg = DiversionRatio_agg(dat, ThetaOptBLP, qOpt, z_logit, Model, 'cla', char_number = pr_index)\n",
    "D_Logitagg = DiversionRatio_agg(dat, LogitBLP_beta, logit_ccp(LogitBLP_beta, z_logit), z_logit, Model, 'cla', char_number = pr_index, mode = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0, D1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    D0[t,:,:] = D_Logitagg[t]\n",
    "    D1[t,:,:] = D_Similarityagg[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkcAAAHlCAYAAAD/f1iPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABuGklEQVR4nO3dd3xT5f4H8E+aNl20BVoKFGjpBRnKEpEpS9ZlCQoquyhDAa8XEZVxlYoICg64yhBREJHhAOSiVmSKUrRMFRHxQm21jFIutJSmNMnz+wObX9KZnDyn5yT9vF+vvEhPTk6++eRJ+OasGIQQAkREREQEAPDTugAiIiIiPWFzREREROSAzRERERGRAzZHRERERA7YHBERERE5YHNERERE5IDNEREREZEDNkdEREREDtgcERERETlgc6Rja9asgcFgwKFDh0q8fcCAAahfv77TtPr162Ps2LFuPc6BAweQmJiIK1euKCuUSvSvf/0LsbGx8Pf3R9WqVbUup0Rjx44tNoY81a1bN3Tr1q3c+erXr48BAwZIfezy7N27FwaDAXv37rVP+/zzz5GYmOjxsos+79TUVBgMBqxZs8bjZatFjde/Is2fPx9bt24tNr2k11ltiYmJMBgMuHTpUoU9Znm1kHJsjnzMli1b8Oyzz7p1nwMHDuD5559ncyTRp59+ihdffBFjxozBvn37sHPnTq1LIgCtW7dGcnIyWrdubZ/2+eef4/nnn5f+WLVr10ZycjL69+8vfdmyPPvss9iyZYvWZShWWnNU0utM5A5/rQsguW6//XatS3BbQUEBDAYD/P19Zzj+9NNPAIDHH38c0dHRGldDhcLDw9G+ffsKeazAwMAKeyxHeXl5CAoKcmnNQYMGDSqgItdYrVZYLBYEBgZ6vKyKfJ3JN3HNkY8pulnNZrNh3rx5aNy4MYKDg1G1alW0aNECS5YsAXBz9etTTz0FAIiPj4fBYHBaHW2z2bBw4UI0adIEgYGBiI6OxpgxY/DHH384Pa4QAvPnz0dcXByCgoLQpk0bfPXVV8U2NRSu7n7//ffx5JNPok6dOggMDMRvv/2GzMxMTJ48GbfeeiuqVKmC6Oho3H333di/f7/TYxVurli0aBFefvll1K9fH8HBwejWrRt+/fVXFBQUYMaMGYiJiUFERATuvfdeXLx40WkZu3fvRrdu3RAZGYng4GDExsZiyJAhuH79epn5upJH/fr18a9//QsAULNmTRgMhjI324wdOxZVqlTBiRMn0KNHD4SGhqJGjRp47LHHitVjNpsxc+ZMxMfHw2QyoU6dOpgyZUqxtX6uvm4lEUJg2bJlaNWqFYKDg1GtWjUMHToUZ86cKTbfwoUL7a9569at8cUXX5S7fHe4+nzz8/Px5JNPolatWggJCUGXLl1w+PDhYu+Hoptbxo4di6VLlwKAfewbDAakpqaWWpOrz7voZrWtW7fCYDBg165dxeZdvnw5DAYDfvjhB/u0Q4cO4Z577kH16tURFBSE22+/HR9++KHT/Qo3ve/YsQMPP/wwatSogZCQEOTn5yMzMxMTJ05EvXr1EBgYiBo1aqBTp05OazFL2qzmauaFm0WTkpLQunVrBAcHo0mTJnj33XdLza5oNgsXLsS8efMQHx+PwMBA7NmzB2azGU8++SRatWqFiIgIVK9eHR06dMCnn37qtAyDwYDc3Fy899579tet8LOmtM1q27ZtQ4cOHRASEoKwsDD06tULycnJTvO4kltZ0tPTcd999yE8PBwREREYNWoUMjMz7bePGzcO1atXL/Gz5u6778Ztt91W7mMkJSWhR48eiIiIQEhICJo2bYoFCxaUeZ9Nmzahd+/eqF27NoKDg9G0aVPMmDEDubm5TvOdOXMGw4YNQ0xMDAIDA1GzZk306NEDx44ds8+j9PPTqwjSrdWrVwsA4uDBg6KgoKDYpV+/fiIuLs7pPnFxcSIhIcH+94IFC4TRaBRz5swRu3btEklJSWLx4sUiMTFRCCFEenq6+Mc//iEAiM2bN4vk5GSRnJwsrl69KoQQYuLEiQKAeOyxx0RSUpJYsWKFqFGjhqhXr57IzMy0P87MmTMFADFx4kSRlJQk3n77bREbGytq164tunbtap9vz549AoCoU6eOGDp0qNi2bZvYvn27yMrKEr/88ouYNGmS2Lhxo9i7d6/Yvn27GDdunPDz8xN79uyxL+Ps2bMCgIiLixMDBw4U27dvF+vWrRM1a9YUjRo1EqNHjxYPP/yw+OKLL8SKFStElSpVxMCBA53uHxQUJHr16iW2bt0q9u7dKz744AMxevRo8b///a/M18SVPI4cOSLGjRsnAIikpCSRnJws0tPTS11mQkKCMJlMIjY2Vrz44otix44dIjExUfj7+4sBAwbY57PZbKJPnz7C399fPPvss2LHjh3ilVdeEaGhoeL2228XZrPZrToLH7voGJowYYIICAgQTz75pEhKShLr168XTZo0ETVr1hTnz5+3zzdnzhwBQIwbN0588cUXYuXKlaJOnTqiVq1aTq95aeLi4kT//v1Lvd2d5zt8+HDh5+cnZsyYIXbs2CEWL14s6tWrJyIiIpzeD4Xjr3A8/fbbb2Lo0KECgH3sJycnOy27KFefd+E4Xb16tRBCiIKCAhEdHS1GjhxZbJlt27YVrVu3tv+9e/duYTKZROfOncWmTZtEUlKSGDt2rNPyhPj/z4g6deqIiRMnii+++EJ8/PHHwmKxiD59+ogaNWqIlStXir1794qtW7eK5557TmzcuNF+/6KvvzuZx8XFibp164pbb71VrF27Vnz55Zfi/vvvFwDEvn37Ss3PMZs6deqI7t27i48//ljs2LFDnD17Vly5ckWMHTtWvP/++2L37t0iKSlJTJ8+Xfj5+Yn33nvPvozk5GQRHBws+vXrZ3/dTpw4UeLrLIQQH3zwgQAgevfuLbZu3So2bdok7rjjDmEymcT+/fvt87mSW0kKx0VcXJx46qmnxJdffilee+01e3Y3btwQQghx/PhxAUC8/fbbTvc/ceKEACCWLl1a5uOsWrVKGAwG0a1bN7F+/Xqxc+dOsWzZMjF58uRitTh64YUXxOuvvy4+++wzsXfvXrFixQoRHx8vunfv7jRf48aNRcOGDcX7778v9u3bJz755BPx5JNP2rP05PPTm7A50rHCD76yLuU1RwMGDBCtWrUq83EWLVokAIizZ886TT958qQA4PSmE0KI7777TgAQs2bNEkIIcfnyZREYGCgefPBBp/mSk5MFgBKboy5dupT7/C0WiygoKBA9evQQ9957r3164Qdry5YthdVqtU9fvHixACDuuecep+VMnTpVALA3fB9//LEAII4dO1ZuDY5czUOI//9wcmxESpOQkCAAiCVLljhNf/HFFwUA8c033wghhEhKShIAxMKFC53m27RpkwAgVq5c6XadRf9zLHzNXn31Vaf7pqeni+DgYPH0008LIYT43//+J4KCgpxeFyGE+Pbbb4u95qUprzly9fkW/qfyzDPPOM23YcMGAaDM5kgIIaZMmVLsP5LSuPO8izZHQggxbdo0ERwcLK5cuWKf9vPPPwsA4o033rBPa9Kkibj99ttFQUGB0+MMGDBA1K5d2z7uCz8jxowZU6zWKlWqiKlTp5b5fIq+/q5mLsTN1y8oKEj8/vvv9ml5eXmievXq4pFHHinzcQuzadCggb1pKE3h58C4cePE7bff7nRbaGio0+tbqOjrbLVaRUxMjGjevLnTZ0ZOTo6Ijo4WHTt2tE9zJbeSFL7nn3jiCafphU3ZunXr7NO6du1a7HN50qRJIjw8XOTk5JT6GDk5OSI8PFzcddddwmazlVtLaWw2mygoKBD79u0TAMTx48eFEEJcunRJABCLFy8u9b5KPz+9DTereYG1a9ciJSWl2OWuu+4q975t27bF8ePHMXnyZHz55ZfIzs52+XH37NkDAMWOfmvbti2aNm1q3zxw8OBB5Ofn44EHHnCar3379qUeCTNkyJASp69YsQKtW7dGUFAQ/P39ERAQgF27duHkyZPF5u3Xrx/8/P5/CDdt2hQAiu0AWzg9LS0NANCqVSuYTCZMnDgR7733XrHNRaVxNQ+lRo4c6fT3iBEjnB539+7dJT7+/fffj9DQUPvje1Ln9u3bYTAYMGrUKFgsFvulVq1aaNmypX0zRXJyMsxmc7GaO3bsiLi4ONefdBlcfb779u0DgGLjb+jQodL3Y/P0eT/88MPIy8vDpk2b7NNWr16NwMBA++v922+/4ZdffrE/huPr0K9fP5w7dw6nTp1yWm5J76e2bdtizZo1mDdvHg4ePIiCgoJy63M180KtWrVCbGys/e+goCA0atQIv//+e7mPBQD33HMPAgICik3/6KOP0KlTJ1SpUsX+OfDOO++U+DngilOnTiEjIwOjR492+syoUqUKhgwZgoMHD9o3CSnJzVHRsfHAAw/A39/f/r4EgH/+8584duwYvv32WwBAdnY23n//fSQkJKBKlSqlLvvAgQPIzs7G5MmT3T4a7cyZMxgxYgRq1aoFo9GIgIAAdO3aFQDsuVavXh0NGjTAokWL8Nprr+Ho0aOw2WxOy1H6+elt2Bx5gaZNm6JNmzbFLhEREeXed+bMmXjllVdw8OBB9O3bF5GRkejRo0eppwdwlJWVBeDmUTdFxcTE2G8v/LdmzZrF5itpWmnLfO211zBp0iS0a9cOn3zyCQ4ePIiUlBT8/e9/R15eXrH5q1ev7vS3yWQqc7rZbAZwcyfUnTt3Ijo6GlOmTEGDBg3QoEED+35YpXE1DyX8/f0RGRnpNK1WrVpOj5uVlQV/f3/UqFHDaT6DwYBatWoVez2U1HnhwgUIIVCzZk0EBAQ4XQ4ePGg/TLlwGYU1llS3p9x9vkXHWkmZyqgJUP68b7vtNtx5551YvXo1gJs7Ia9btw6DBg2yj9sLFy4AAKZPn17sNZg8eTIAFDtcvKTXetOmTUhISMCqVavQoUMHVK9eHWPGjMH58+fLfH6uZF6opHwDAwNLfL+WpKS6N2/ejAceeAB16tTBunXrkJycjJSUFDz88MP297C7yntP2Gw2/O9//wOgLDdHRcdB4Th0zG7QoEGoX7++fX+3NWvWIDc3F1OmTClz2YX7LtWtW9elWgpdu3YNnTt3xnfffYd58+Zh7969SElJwebNmwHA/noV7hPXp08fLFy4EK1bt0aNGjXw+OOPIycnB4Dyz09v4zuHB1GJ/P39MW3aNEybNg1XrlzBzp07MWvWLPTp0wfp6ekICQkp9b6FH3znzp0r9mbMyMhAVFSU03yFH+qOzp8/X+Lao5K+9axbtw7dunXD8uXLnaYXvill6ty5Mzp37gyr1YpDhw7hjTfewNSpU1GzZk0MGzasxPu4mocSFosFWVlZTv/ZFH4YF06LjIyExWJBZmam039eQgicP38ed955p8d1RkVFwWAwYP/+/SUeNVQ4rfAxSvoPo7TX3F3uPt8LFy6gTp069vkKM5VJxvN+6KGHMHnyZJw8eRJnzpzBuXPn8NBDD9lvL3x9Zs6cifvuu6/EZTRu3Njp75LeT1FRUVi8eDEWL16MtLQ0bNu2DTNmzMDFixeRlJRU6vNzJXNZSvsciI+Px6ZNm5xuz8/PV/w4ju+JojIyMuDn54dq1aoBUJabo/Pnz5c4Dh3f235+fpgyZQpmzZqFV199FcuWLUOPHj2Kva5FFb4mrhxY4Wj37t3IyMjA3r177WuLAJR4+pa4uDi88847AIBff/0VH374IRITE3Hjxg2sWLECgLLPT2/DNUeVSNWqVTF06FBMmTIFly9fth+RU/gfXtFve3fffTeAmx9WjlJSUnDy5En06NEDANCuXTsEBgY6bSoAbm5uc3X1OnDzg7Lof8g//PBDsaNJZDIajWjXrp39G9yRI0dKndfVPJT64IMPnP5ev349ANiPwClcftHH/+STT5Cbm2u/3ZM6BwwYACEE/vzzzxLXVjZv3hzAzU2mQUFBxWo+cOCAW695WVx9vl26dAGAYuPv448/hsViKfdxShv/JZHxvIcPH46goCCsWbMGa9asQZ06ddC7d2/77Y0bN8Ytt9yC48ePl/gatGnTBmFhYS49VqHY2Fg89thj6NWrV5lj3NXM1WQwGGAymZwao/Pnzxc7Wg1wfS1V48aNUadOHaxfvx5CCPv03NxcfPLJJ/Yj2IpyNTdHRcfGhx9+CIvFUuzEqOPHj4fJZMLIkSNx6tQpPPbYY+Uuu2PHjoiIiMCKFSucnkd5CrMs+vn61ltvlXm/Ro0a4V//+heaN29e4vN35/PT23DNkY8bOHAgmjVrhjZt2qBGjRr4/fffsXjxYsTFxeGWW24BAPt/eEuWLEFCQgICAgLQuHFjNG7cGBMnTsQbb7wBPz8/9O3bF6mpqXj22WdRr149PPHEEwBubsaaNm0aFixYgGrVquHee+/FH3/8geeffx61a9d22sZflgEDBuCFF17AnDlz0LVrV5w6dQpz585FfHy8S//JuWrFihXYvXs3+vfvj9jYWJjNZvvhxz179iz1fq7moYTJZMKrr76Ka9eu4c4778SBAwcwb9489O3b175vWa9evdCnTx8888wzyM7ORqdOnfDDDz9gzpw5uP322zF69GiP6+zUqRMmTpyIhx56CIcOHUKXLl0QGhqKc+fO4ZtvvkHz5s0xadIkVKtWDdOnT8e8efMwfvx43H///UhPT0diYqJbm9XOnz+Pjz/+uNj0+vXru/x8b7vtNgwfPhyvvvoqjEYj7r77bpw4cQKvvvoqIiIiyh1/heP/5ZdfRt++fWE0GtGiRQv75lhHMp531apVce+992LNmjW4cuUKpk+fXqzGt956C3379kWfPn0wduxY1KlTB5cvX8bJkydx5MgRfPTRR2U+xtWrV9G9e3eMGDECTZo0QVhYGFJSUpCUlFTq2ijA9TGmpgEDBmDz5s2YPHkyhg4divT0dLzwwguoXbs2Tp8+7TRv8+bNsXfvXvznP/9B7dq1ERYWVuLaFz8/PyxcuBAjR47EgAED8MgjjyA/Px+LFi3ClStX8NJLLwFQnpujzZs3w9/fH7169cKJEyfw7LPPomXLlsX2iatatSrGjBmD5cuXIy4uDgMHDix32VWqVMGrr76K8ePHo2fPnpgwYQJq1qyJ3377DcePH8ebb75Z4v06duyIatWq4dFHH8WcOXMQEBCADz74AMePH3ea74cffsBjjz2G+++/H7fccgtMJhN2796NH374ATNmzACg/PPT62i4MziVo/BIlJSUlBJv79+/f7lHq7366quiY8eOIioqyn64+Lhx40RqaqrT/WbOnCliYmKEn59fsaM8Xn75ZdGoUSMREBAgoqKixKhRo4odmm6z2cS8efNE3bp1hclkEi1atBDbt28XLVu2dDqyp/Aoko8++qjY88nPzxfTp08XderUEUFBQaJ169Zi69atxY6oKTzSZdGiRU73L23ZRXNMTk4W9957r4iLixOBgYEiMjJSdO3aVWzbtq3EnB25moe7R6uFhoaKH374QXTr1k0EBweL6tWri0mTJolr1645zZuXlyeeeeYZERcXJwICAkTt2rXFpEmTih1C62qdJR3KL4QQ7777rmjXrp0IDQ0VwcHBokGDBmLMmDHi0KFD9nlsNptYsGCBqFevnv01/89//iO6du3q8tFqKOUozMIx7OrzNZvNYtq0aSI6OloEBQWJ9u3bi+TkZBEREeF09FBJR6vl5+eL8ePHixo1agiDwVDikZuOXH3eJR2tVmjHjh325/rrr7+W+DjHjx8XDzzwgIiOjhYBAQGiVq1a4u677xYrVqywz1PaZ4TZbBaPPvqoaNGihQgPDxfBwcGicePGYs6cOSI3N9c+X0mvv6uZl3a0oSuvf2nv4UIvvfSSqF+/vggMDBRNmzYVb7/9dolHYB07dkx06tRJhISEOB0tWNLrLIQQW7duFe3atRNBQUEiNDRU9OjRQ3z77bdu51aSwvoOHz4sBg4cKKpUqSLCwsLE8OHDxYULF0q8z969ewUA8dJLL5W57KI+//xz0bVrVxEaGipCQkLErbfeKl5++eVitTg6cOCA6NChgwgJCRE1atQQ48ePF0eOHHEaoxcuXBBjx44VTZo0EaGhoaJKlSqiRYsW4vXXXxcWi0UI4dnnpzcxCOHGujkiN5w9exZNmjTBnDlzMGvWLK3L0a2xY8fi448/xrVr17QuxaccOHAAnTp1wgcffGA/EoxIT5588kksX74c6enp0g8eIM9wsxpJcfz4cWzYsAEdO3ZEeHg4Tp06hYULFyI8PBzjxo3TujzycV999RWSk5Nxxx13IDg4GMePH8dLL72EW265xeXNIUQV5eDBg/j111+xbNkyPPLII2yMdIjNEUkRGhqKQ4cO4Z133sGVK1cQERGBbt264cUXXyz1cH4iWcLDw7Fjxw4sXrwYOTk5iIqKQt++fbFgwQIEBQVpXR6Rk8IdwAcMGIB58+ZpXQ6VgJvViIiIiBzwUH4iIiIiB2yOiIiIiBywOSIiIiJywOaIiIiIyAGbIyIiIiIHbI6IiIiIHLA5IiIiInLA5oiIiIjIAZsjIiIiIgdsjoiIiIgcsDkiIiIicsDmiIiIiMgBmyMiIiIiB2yOiIiIiBywOSIiIiJywOaIiIiIyAGbIy+2bNkyxMfHIygoCHfccQf279+vdUk+5euvv8bAgQMRExMDg8GArVu3al2Sz1mwYAHuvPNOhIWFITo6GoMHD8apU6e0LkvXli9fjhYtWiA8PBzh4eHo0KEDvvjiC63L8mkLFiyAwWDA1KlTtS5FtxITE2EwGJwutWrV0rosxdgcealNmzZh6tSpmD17No4ePYrOnTujb9++SEtL07o0n5Gbm4uWLVvizTff1LoUn7Vv3z5MmTIFBw8exFdffQWLxYLevXsjNzdX69J0q27dunjppZdw6NAhHDp0CHfffTcGDRqEEydOaF2aT0pJScHKlSvRokULrUvRvdtuuw3nzp2zX3788UetS1LMIIQQWhdB7mvXrh1at26N5cuX26c1bdoUgwcPxoIFCzSszDcZDAZs2bIFgwcP1roUn5aZmYno6Gjs27cPXbp00bocr1G9enUsWrQI48aN07oUn3Lt2jW0bt0ay5Ytw7x589CqVSssXrxY67J0KTExEVu3bsWxY8e0LkUKrjnyQjdu3MDhw4fRu3dvp+m9e/fGgQMHNKqKyHNXr14FcPM/eyqf1WrFxo0bkZubiw4dOmhdjs+ZMmUK+vfvj549e2pdilc4ffo0YmJiEB8fj2HDhuHMmTNal6SYv9YFkPsuXboEq9WKmjVrOk2vWbMmzp8/r1FVRJ4RQmDatGm466670KxZM63L0bUff/wRHTp0gNlsRpUqVbBlyxbceuutWpflUzZu3IjDhw/j0KFDWpfiFdq1a4e1a9eiUaNGuHDhAubNm4eOHTvixIkTiIyM1Lo8t7E58mIGg8HpbyFEsWlE3uKxxx7DDz/8gG+++UbrUnSvcePGOHbsGK5cuYJPPvkECQkJ2LdvHxskSdLT0/HPf/4TO3bsQFBQkNbleIW+ffvarzdv3hwdOnRAgwYN8N5772HatGkaVqYMmyMvFBUVBaPRWGwt0cWLF4utTSLyBv/4xz+wbds2fP3116hbt67W5eieyWRCw4YNAQBt2rRBSkoKlixZgrfeekvjynzD4cOHcfHiRdxxxx32aVarFV9//TXefPNN5Ofnw2g0alih/oWGhqJ58+Y4ffq01qUown2OvJDJZMIdd9yBr776ymn6V199hY4dO2pUFZH7hBB47LHHsHnzZuzevRvx8fFal+SVhBDIz8/Xugyf0aNHD/z44484duyY/dKmTRuMHDkSx44dY2Pkgvz8fJw8eRK1a9fWuhRFuObIS02bNg2jR49GmzZt0KFDB6xcuRJpaWl49NFHtS7NZ1y7dg2//fab/e+zZ8/i2LFjqF69OmJjYzWszHdMmTIF69evx6effoqwsDD72tCIiAgEBwdrXJ0+zZo1C3379kW9evWQk5ODjRs3Yu/evUhKStK6NJ8RFhZWbL+30NBQREZGcn+4UkyfPh0DBw5EbGwsLl68iHnz5iE7OxsJCQlal6YImyMv9eCDDyIrKwtz587FuXPn0KxZM3z++eeIi4vTujSfcejQIXTv3t3+d+F284SEBKxZs0ajqnxL4akounXr5jR99erVGDt2bMUX5AUuXLiA0aNH49y5c4iIiECLFi2QlJSEXr16aV0aVWJ//PEHhg8fjkuXLqFGjRpo3749Dh486LX/J/E8R0REREQOuM8RERERkQM2R0REREQO2BwREREROeAO2W6y2WzIyMhAWFgYT7hYhBACOTk5iImJgZ+f6303My0dM5WPmcrHTOVjpvK5kymbIzdlZGSgXr16Wpeha+np6W6dyI+Zlo+ZysdM5WOm8jFT+VzJlM2Rm8LCwgDcDDc8PFzjavQlOzsb9erVs2fkKmZaOmYqHzOVj5nKx0zlcydTNkduKlxNGR4ezoFXCndX5TLT8jFT+ZipfMxUPmYqnyuZsjki3bDYLNj6y1YAwIBGA+Dvx+HpKWaqDovNgu2/bgfAXGVhpvIxU+WYFOmGv58/BjcZrHUZPoWZqoO5ysdM5WOmyvFQfiIiIiIHXHNEumG1WbE3dS8AoHNsZxj9+MvXnmKm6rDarNifth8Ac5WFmcrHTJVjc0S6YbaY0f29mz/0em3mNYSaQjWuyPsxU3UwV/mYqXzMVDk2R6QbBoMBt9a41X6dPMdM1cFc5WOm8jFT5dgckW6EBITgxOQTWpfhU5ipOpirfMxUPmaqHHfIJiIiInLA5oiIiIjIAZsj0o28gjz0er8Xer3fC3kFeVqX4xOYqTqYq3zMVD5mqhz3OSLdsAkbdp7Zab9OnmOm6mCu8jFT+ZipcmyOSDcC/QOx7t519uvkOWaqDuYqHzOVj5kqx+aIdMPfzx8jW4zUugyfwkzVwVzlY6byMVPluM8RERERkQOuOSLdsNqsSPkzBQDQunZrnupeAmaqDqvNiiPnjgBgrrIwU/mYqXJsjkg3zBYz2q5qC4CnupeFmaqDucrHTOVjpsqxOSLdMBgMiIuIs18nzzFTdTBX+ZipfMxUOTZHpBshASFInZqqdRk+hZmqg7nKx0zlY6bKcYdsIiIiIgdsjoiIiIgcsDki3TBbzBi8cTAGbxwMs8WsdTk+gZmqg7nKx0zlY6bKcZ+jvyxbtgyLFi3CuXPncNttt2Hx4sXo3Lmz1mVVKlabFZ+e+tR+nTzHTNXBXOVjpvIxU+XYHAHYtGkTpk6dimXLlqFTp05466230LdvX/z888+IjY3VurxKw2Q0YeWAlfbruld49IcQ2tZRBq/L1EswV/mYqXzMVDmDEDr+ZK8g7dq1Q+vWrbF8+XL7tKZNm2Lw4MFYsGCB07zZ2dmIiIjA1atXER4eXtGl6prSbLw20wpojipdphWAmcrHTOVjpvK5k02l3+foxo0bOHz4MHr37u00vXfv3jhw4IBGVREREZFWKv1mtUuXLsFqtaJmzZpO02vWrInz589rVFXlZBM2nLh4AgDQtEZT+Bkqfe/uMWaqDpuw4WTmSQDMVRZmKh8zVa7SN0eFip49VAjBM4pWsLyCPDRb3gwAT3UvS6XK1GCosP2/KlWuFYSZyic1Uy/Yx1KmSt8cRUVFwWg0FltLdPHixWJrk0h9USFRWpfgc5ipOpirfMxUPmaqTKVvjkwmE+644w589dVXuPfee+3Tv/rqKwwaNEjDyiqfUFMoMp/K1LoMn+JTmerom6vP5cpMfZIqmRbdoqKDsaOGSt8cAcC0adMwevRotGnTBh06dMDKlSuRlpaGRx99VOvSiIiIqIKxOQLw4IMPIisrC3PnzsW5c+fQrFkzfP7554iLi9O6NCIidehkjZHX0tGaTN3xgWy46/pfJk+ejNTUVOTn5+Pw4cPo0qWL1iVVOmaLGSM3j8TIzSN5qntJmKk6mKt8zFQ+ZqocmyPSDavNivU/rsf6H9fzVPeSMFN1MFf5vCJTg6H4Pjflza8hr8hUp7hZjXTDZDTh9T6v26+T55ipOpirfMxUPmaqHJsj0o0AYwCmtp+qdRk+xacz1XC/Bp/OVSM+kanO9uOqkEw9ec463jeJm9WIiIiIHHDNEemGTdiQeiUVABAbEauPU927+q2ovPk0+kapy0xLo7Nv3WWxCRvSrqYB8IJcXeG4b4xGr4EuMi06BmXkouHakQrPtOg+Vl58TiQ2R6QbeQV5iF8aD4A/HyALM1VHXkEe4pcwV5mYqXzMVDk2R6QrIQEhWpfgmcJvno7fFjU+YsVnMq3o+5bD63PVIWYqHzNVhs0R6UaoKRS5s3K1LsOnMFN1MFf5mKl8zFQ5L99QTqQRx7VB7p73ROl9K5PCXErKR0lmzLnkc/SUlq/s/H0B37eeK+08UUWz1UG+bI6IiIiIHLA5IsXyLfmYsG0CJmybgHxLvu6WJ13hN5ryvtW4+g2ztG/nEr816T7Tsriagwbf6L0m17LWALl638LrKmerWaaePi9X378lzef4+qiQsW7HqTvPVaM1SWyOSDGLzYJVR1dh1dFVsNgsulseMVO1MFf5mKl8zFQ57pBNigUYAzCv+zz7dSs8++2eosvTDTW+tai4tsiRbjMFSj6SrLSjy8rLq6y1Rypkq+tcXeHp74OpcBSgrjJ1d8woWctZASosUx3sIwRA6jml2ByRYiajCbO7zLb/bYZnv/pcdHnkOWaqDuYqHzOVj5kqx+aIqKLo5duVnvBoqIrnSb6++trI3m/NV3NSk7tr31z95QJX5y2CzREpJoTApeuXAABRIVFSlpeZm2lfnoEfMB5jpuooOvaZq+eYqXzMVDk2R6TY9YLriH4lGsDNU9PLWF7M4hj78rzuVPc6/ODx+kx1qujYr5S5St7viJnK51OZlrYvYUm/QiDhs5jNkZvEXx8G2dnZGleivdwbuSjczSg7OxtW880dsoWbH5iF8+fk5Dgvz+TZDt5e7a/xVTjOmKkEkjLNzs6G8YbRe3JV87OqsmYqU9HXh5ne5Oq4dWU+JZkKckt6eroAwEsZl/T0dGbKTHV/YabM1BsuzFSbTA1CqPSrjD7KZrMhIyMDYWFh3H5bhBACOTk5iImJgZ+f66fQYqalY6byMVP5mKl8zFQ+dzJlc0RERETkgGfIJiIiInLA5oiIiIjIAZsjIiIiIgdsjoiIiIgcsDkiIiIicsDmiIiIiMgBmyMiIiIiB/z5EDfxBFul40nL5GOm8jFT+ZipfMxUPncyZXPkpoyMDNSrV0/rMnQtPT0ddevWdXl+Zlo+ZiofM5WPmcrHTOVzJVM2R24KCwsDcDPc8PBwjavRl+zsbNSrV8+ekauYaemYqXzMVD5mKh8zlc+dTNkcualwNaUxyIiI1yMAANdmXkOoKVTLsnTF3VW5hfOHh4fzzVwKpZlynJbOk3FqDDKiyoIqAJirI2YqX4VnWvh4PvzLYq5kyuZIoUD/QGx5cIv9OpEecZyqg7nKx0zlY6bKsTlSyN/PH4ObDNa6DKIycZyqg7nKx0zlY6bK8VB+IiIiIgdcc6SQ1WbF3tS9AIDOsZ1h9DNqWxBRCThO1WG1WbE/bT8A5ioLM5WPmSrH5kghs8WM7u91B8CdB0m/OE7VwVzlY6byMVPl2BwpZDAYcGuNW+3XifSI41QdzFU+ZiofM1WOzZFCIQEhODH5hNZlEJWJ41QdzFU+ZiofM1WOO2QTEREROWBzREREROSAzZFCeQV56PV+L/R6vxfyCvK0LoeoRByn6mCu8jFT+VTP1GD4/zNq+xjuc6SQTdiw88xO+3UiPeI4VQdzlY+ZysdMlWNzpFCgfyDW3bvOfp1IjzhO1cFc5WOm8jFT5dgcKeTv54+RLUZqXQZRmThO1cFc5WOm8jFT5bjPEREREZEDrjlSyGqzIuXPFABA69qteVp20iWOU3VYbVYcOXcEAHOVhZnKx0yVY3OkkNliRttVbQHwtOykXxyn6mCu8jFT+ZipcmyOFDIYDIiLiLNfJ9IjjlN1MFf5mKl8zFQ5NkcKhQSEIHVqqtZlEJWJ41QdzFU+ZiofM1WOO2QTEREROeCaIyIiInJd0U10hX8LUfG1qIRrjhQyW8wYvHEwBm8cDLPFrHU5RCXiOFUHc5WPmcrHTJXjmiOFrDYrPj31qf06kR5xnKqDucrHTOVjpsq53RxlZGQgJycHjRs3BgBYrVa8+uqrOHLkCHr37o2HH35YepF6ZDKasHLASvt1Ij3iOFUHc5WPmcrHTJVzuzl65JFHEBsbi6VLlwIAXnjhBcydOxdVq1bFRx99BJPJhFGjRkkvVG8CjAGYcMcErcsgKhPHqTqYq3zMVD5mqpzb+xwdOXIE3bt3t//99ttv44knnsDly5cxceJEe9PkTb7++msMHDgQMTExMBgM2Lp1q9YlERERkUbcbo6ysrJQq1YtAMDJkydx7tw5jB07FgAwZMgQnDp1SmqBFSE3NxctW7bEm2++6fJ9bMKGExdP4MTFE7AJm4rVESnHcaoO5ipfhWVqMBQ/2spHcZwq5/ZmtYiICFy8eBHAzTUu1atXR/PmzQHcPAPnjRs35FZYAfr27Yu+ffu6dZ+8gjw0W94MAE/LTvrFcaoO5iofM5WPmSrndnPUtm1bvPzyywgICMCSJUvQu3dv+21nzpxBTEyM1AL1LCokSusSiMrFcaoO5ipfhWZa2rl5fOycPdIyrSRr2wq53Ry98MIL6NWrFwYNGoRq1aph9uzZ9tu2bt2Ktm3bSi1Qr0JNoch8KlPrMojKxHGqDuYqHzOVj5kq53Zz1KpVK/z+++/45Zdf0LBhQ4SHh9tvmzx5Mm655RapBRIRERFVJEUngQwJCUHr1q2LTe/fv7/HBRERERFpye2j1Xbv3o2PPvrI/veFCxfQr18/1KpVC2PGjIHZXDlOUW62mDFy80iM3DySp2Un3arwcVp4JFBpRwT5yJFCfP/Lx0zlY6bKud0cPffcc/j555/tfz/99NPYv38/OnbsiI8//hiLFi2SWmBFuHbtGo4dO4Zjx44BAM6ePYtjx44hLS2t1PtYbVas/3E91v+4nqdlJ93iOFUHc5WPmcrHTJVze7Par7/+imeeeQYAYLFYsGXLFrz88suYPHkyXnnlFbz77rt49tlnpReqpkOHDjmd2HLatGkAgISEBKxZs6bE+5iMJrze53X7dSI90nyc+tiRP4U0z1VtGrxuPp+pBpipcm43R9nZ2ahatSoA4PDhw8jNzcU999wD4OZh/omJiTLrqxDdunWDcPNDIMAYgKntp6pTEJEkHKfqYK7yMVP5mKlybm9Wi46OxunTpwEAO3fuRFxcHOrWrQsAyMnJQUBAgNwKSd98YP8R0oCP7HskRVn7aFXkMhyX5Ys45konc/z4CLfXHP3973/HrFmzcOLECaxZswYJCQn223755RfUr19fZn26ZRM2pF5JBQDERsTCz+B2n0mkOo5TddiEDWlXb+6TyFzlYKbyMVPl3G6O5s+fj7S0NLz99tto27Yt/vWvf9lvW79+PTp27Ci1QL3KK8hD/NJ4ADwtO+mXx+NU1r4n5X0jLfo4Jc3vbg0q7jeTV5CH+CUavP/1sg+XCnVIz1T22C06NrV+DVyg2Tj1AW43R1FRUUhKSirxtj179iAoKMjjorxFSECI1iUQlYvjVB3MVT5mKh8zVUbRSSBL43i2bF8XagpF7qxcrcsgKhPHqTqYq3zMVD5mqpyi5shqteKLL77AyZMnkZeX53SbwWDwukP5ichNet60UBl2KnXnORad153XzJP7aqVozWqNB9mb2/T8nqqE3G6OsrKy0LlzZ/zyyy8wGAz2Q+ANDgOQzRERERF5K7d3XZ89ezaCgoLw+++/QwiB7777DqdPn8a0adPQqFGjMs8q7UvyLfmYsG0CJmybgHxLvtblkI+QPa50P06LHj7syuHE7h5yrMIhyqrlqsXPrpT3ky8VVIdmY1XWaRSULs8XM/UBbjdHu3btwrRp0xATE3NzAX5+aNCgARYtWoSePXti+vTp0ovUI4vNglVHV2HV0VWw2Cxal0M+Qva44jhVB3OVj5nKx0yVc3uz2h9//IH69evDaDTCz88Pubn/v7PXwIEDMWLECKkF6lWAMQDzus+zXyeSoei4ssKz30OSNk71vD+EBrVV2PtfyRoFV++js32zPM7U0+ejdh4a5O1RprLGnh4/M1yg6FD+q1evAgBiYmLw008/oUuXLgCAy5cvw2KpHN2pyWjC7C6ztS6DfEzRcWWGZ7+kzXGqDuYqHzOVj5kq53ZzdMcdd+DEiRPo378/+vXrh7lz5yI8PBwmkwmzZs1C+/bt1aiTiPSgtG+TFfGtuKKOQvJ1zM01snJi3iVzzKXo2iVX1gaXNY+EtcluN0ePPfYY/vvf/wIAXnjhBRw8eBBjxowBADRo0ABLlixRXIw3EUIgMzcTABAVEuV0tB6RUkIIXLp+CcDNcSVjeRyn8hV9nZir55ipfMxUObebo549e6Jnz54AgBo1auDo0aP46aefYDAY0KRJE/j7Sz2vpG5dL7iOmMU3d0rnadlJlusF1xH9SjSAm+NKxvJUHad6+rCtwFqKvk4+nWsFUT3TiqaD11BXmbqzNsjV6a7epmANksedjMFgQPPmzT1djNcoPK9TTk4OCncHyc7OhtXk2Y6zXi07+69/bv4r3ByIhfMX3r8yy72R6zyuzDfHldJMOU4dSBynxhtG5gpUrkwr6vPJ1zItmltJOZaWrazMFWRqEC7M5e65i2JjY92a35v88ccfqFevntZl6Fp6ejrq1q3r8vzMtHzMVD5mKh8zlY+ZyudKpi41R35+fm5tq7RaddbxS2Sz2ZCRkYGwsDBuvy1CCIGcnBzExMTAz8/1U2gx09IxU/mYqXzMVD5mKp87mbrUHK1Zs8atkBMSElyel4iIiEhPXGqOiIiIiCoLt38+hIiIiMiXud0cTZs2DSNHjizxtlGjRuGpp57yuCgiIiIirbjdHG3btg29e/cu8bbevXvj008/9bgoIiIiIq243Rz9+eefqF+/fom3xcXF4Y8//vC0JiIiIiLNuN0chYaGIj09vcTb0tLSEBQU5HFRRERERFpx+2i1gQMH4o8//sD333+PgIAA+/SCggK0a9cOMTEx2L59u/RC9YLnkCgdz8shHzOVj5nKx0zlY6byuZWpcNPBgweFyWQSjRo1Ei+//LJYt26deOmll0SjRo1EYGCg+O6779xdpFdJT08XAHgp45Kens5MmanuL8yUmXrDhZlqk6nbv63Wrl07bNu2DVOmTMGMGTPs0xs0aIBt27ahbdu27i7Sq4SFhQG4efrx8PBwjavRl+zsbNSrV8+ekauYaemYqXzMVD5mKh8zlc+dTBX98GyfPn3w22+/4fTp08jMzESNGjVwyy23KFmU1ylcTRkeHs6BVwp3V+Uy0/IxU/mYqXzMVD5mKp8rmSpqjgrdcsstlaYpKspis2DrL1sBAAMaDYC/n0dREpgpeQ+LzYLtv97ct5JjVQ5mKp+iTAsbh0r+4xkcfQr5+/ljcJPBWpfhU5gpeQuOVfmYqXzMVDn+fAgRERGRA645Ushqs2Jv6l4AQOfYzjD6GbUtyAcwU/IWVpsV+9P2A+BYlYWZysdMlWNzpJDZYkb397oDAK7NvIZQU6jGFXk/ZkregmNVPmYqHzNVjs2RQgaDAbfWuNV+nTzHTMlbcKzKx0zlY6bKKWqOCgoKsHbtWuzatQtZWVmIiopCz549MWrUKKezZvuykIAQnJh8QusyfAozJW/BsSofM5WPmSrndnN09epV9OjRA0eOHEFoaChq1aqFAwcOYMOGDVi2bBl27drFcysQERGR13L7aLXZs2fj1KlT2LRpE3JycnD69Gnk5OTgww8/xKlTpzB79mw16iQiIiKqEG43R1u3bsXcuXNx//33O00fOnQoEhMTsWXLFmnF6VleQR56vd8Lvd7vhbyCPK3L8QnMlLwFx6p8zFQ+Zqqc25vVMjMz0aJFixJva9myJS5duuRxUd7AJmzYeWan/Tp5jpmSt+BYlY+Zyqd6pj58Nm23m6M6dergm2++QY8ePYrd9u233yImJkZKYXoX6B+Idfeus18nzzFT8hYcq/IxU/mYqXJuN0cPPvgg5s+fj7CwMCQkJCAyMhJZWVlYt24d5s+fj2nTpqlRp+74+/ljZIuRWpfhU5gpeQuOVfmYqXzMVDm3m6PExEQcPXoU06dPx1NPPQV/f39YLBYIIdCnTx8kJiaqUCYRERFRxXC7OQoMDERSUhK+/PJL7NmzB1lZWYiMjESPHj3Qq1cvNWrUJavNipQ/UwAArWu35mnZJWCm5C2sNiuOnDsCgGNVFmYqHzNVzu3mKC0tDbVr10afPn3Qp08fp9ssFgsyMjIQGxsrrUC9MlvMaLuqLQCell0WZkregmNVPmYqHzNVzu3mKD4+HsnJyWjbtm2x244fP462bdvCarVKKU7PDAYD4iLi7NfJc8yUvAXHqnzMVD5mqpzbzZEo45A9q9VaaV6AkIAQpE5N1boMn8JMyVtwrMrHTOVjpsq5fRJIoOQOND8/H1988QWioqI8LoqIiIhIKy41R88//zyMRiOMRiMMBgPat29v/7vwEhISgrlz52LQoEFq1yzVggULcOeddyIsLAzR0dEYPHgwTp06pXVZREREpBGXNqu1bdsWkydPhhACy5Ytw9ChQ1GzZk2neQIDA9G8eXOMGDFClULVsm/fPkyZMgV33nknLBYLZs+ejd69e+Pnn39GaGjpO6+ZLWaM2TgGALBx6EYE+QdVVMk+i5mStzBbzBj28TAAHKuyMFP5VMu06NYjHzxTtkvNUd++fdG3b18AQG5uLp577jnEx8erWlhFSUpKcvp79erViI6OxuHDh9GlS5dS72e1WfHpqU/t18lzzJS8BceqfMxUPmaqnNs7ZK9evVqNOnTj6tWrAIDq1auXOZ/JaMLKASvt18lzzJS8BceqfMxUPmaqnNvNkS8TQmDatGm466670KxZszLnDTAGYMIdEyqossqBmZK34FiVj5nKx0yVY3Pk4LHHHsMPP/yAb775RutSiIiISCNsjv7yj3/8A9u2bcPXX3+NunXrlju/Tdhw4uIJAEDTGk3hZ1B0VgRywEzJW9iEDSczTwLgWJWFmcrHTJWr9M2REAL/+Mc/sGXLFuzdu9flHc3zCvLQbPnNTW88LbsczJS8BceqfBWWqQ8eWVUajlPlKn1zNGXKFKxfvx6ffvopwsLCcP78eQBAREQEgoODy7xvVAhPeCkbMyVvwbEqHzOVj5kqo6g5KigowNq1a7Fr1y5kZWUhKioKPXv2xKhRoxAQECC7RlUtX74cANCtWzen6atXr8bYsWNLvV+oKRSZT2WqWFnlw0zJW3CsyqebTH1ozZLUTCvJT4MVcrs5unr1Knr06IEjR44gNDQUtWrVwoEDB7BhwwYsW7YMu3btQnh4uBq1qqKs34ojIiKiysftvbNmz56NU6dOYdOmTcjJycHp06eRk5ODDz/8EKdOncLs2bPVqJNIXQZDpftmRFSp8T1PZXC7Odq6dSvmzp2L+++/32n60KFDkZiYiC1btkgrTs/MFjNGbh6JkZtHwmwxa12OT2Cm5C04VuVjpvIxU+Xcbo4yMzPRokWLEm9r2bIlLl265HFR3sBqs2L9j+ux/sf1PC27JF6bKb+BVjpeO1Z1jJnKx0yVc3ufozp16uCbb75Bjx49it327bffIiYmRkphemcymvB6n9ft18lzzJS8BceqfMxUPmaqnNvN0YMPPoj58+cjLCwMCQkJiIyMRFZWFtatW4f58+dj2rRpatSpOwHGAExtP1XrMnyKLjKVcaSKDx3tQiXTxVgticHgteNOt5l6MWaqnNvNUWJiIo4ePYrp06fjqaeegr+/PywWC4QQ6NOnDxITE1Uok4iIiKhiuN0cBQYGIikpCV9++SX27NmDrKwsREZGokePHujVq5caNeqSTdiQeiUVABAbEcvTsksgNVOuASIV2YQNaVfTAEgcq4A+xprj2qcKfA9IzbQklXC/QJczrYTZlEfxGbL79OmDPn36yKzFq+QV5CF+6c2fGuFp2eVgpuQt8gryEL+EY1UmZiofM1Wu0v98iCdCAkK0LsHnVHiman5jcmWNANdOeS1N3v+ljRfZ47i85ak0bqVmKqvGossp7zVQ+njekGkl4lJz9Le//Q1btmxBy5YtER8fD0MZbxyDwYD//ve/0grUq1BTKHJn5Wpdhk9hpuQtOFblY6byMVPlXGqOunbtav9JkK5du5bZHBHpmqvfzjjGSc/cGZ9F51WyxsOb1nAWfb5qrQVTaw0S6YJLzdHq1avt19esWaNWLURERESac/twgLVr1yIrK6vE2y5fvoy1a9d6XJQ3yLfkY8K2CZiwbQLyLflal+MTKjRTJWe1LrxPafcta5muPh7Pti2dGuNKtbGqZGy5uyxP5ldxfPr8Z2rR7Crgve7zmarI7ebooYceKnWforNnz+Khhx7yuChvYLFZsOroKqw6ugoWm0XrcnwCMyU1qDGuOFblY6byMVPl3D5aTZSxHdVsNsNoNHpUkLcIMAZgXvd59uvkOVUz9eQbGtfkeLWi48oKz39jqsLe/xV1NKUOeJypp89H6f21elwXeJSpkrpKuo+X7nvlUnOUlpaG1NRU+99Hjx6F2ez8C795eXlYuXIlYmNjpRaoVyajCbO7zNa6DJ/CTEkNRceVGZ7/OjnHqnzMVD5mqpzLO2Q///zzMBgMMBgMmDx5crF5CtcoLVmyRG6FpG9e/FtOmqio89RQ5cWxpG98fW4q6zxwSo6glHymeZeaowceeADNmjWDEAIPPPAA5s+fj1tuucVpnsDAQDRr1gz169f3uChvIIRAZm4mACAqJIqnN5CAmZIahBC4dP0SgJvjSo1lcqx6jpnKx0yVc6k5atq0KZo2bQrg5lqkAQMGIDIyUtXC9O56wXXELI4BwNOyy6JKpnrdZ4MfUhXmesF1RL8SDeDmuFJjmdLf/3odtypSPVN3uXu+JFeXU4F0lak7a4NcnV7abeWd28sFbu+QnZCQ4PaD+JLCzYc5OTko3HUhOzsbVpPnO3l6rezsv/65+W9ZO+2XhJmWQFKmhfevzHJv5DqPK/PNceVJpsYbRo5VQOo49blMlb73fC3TojmUlEtpWcn6/FKQqUG4mzxuns9o/fr1OHnyJPLy8pwXaDDgnXfecXeRXuOPP/5AvXr1tC5D19LT01G3bl2X52em5WOm8jFT+ZipfMxUPlcydbs5SktLw5133onr16/j+vXriIqKwuXLl2G1WlGtWjVERETgzJkzHhWuZzabDRkZGQgLC+P22yKEEMjJyUFMTAz8/Fw/hRYzLR0zlY+ZysdM5WOm8rmTqdvN0YgRI3D+/Hls374dVapUwaFDh9CsWTO8/fbbmD9/Pnbu3GnfP4mIiIjI27h9huzk5GRMmjQJQUFBAG52YiaTCVOmTMG4cePw1FNPSS+SiIiIqKK43RxduHABtWvXhp+fH4xGo9MOn127dsU333wjtUAiIiKiiuR2c1SzZk1cvnwZAFC/fn0cOnTIfltqair8/d0+AI6IiIhIN9zuZNq3b4+jR4/innvuwX333Ye5c+ciPz8fJpMJixYtwt13361GnUREREQVwu0dsg8fPozU1FQMGTIEubm5GD58OD777DMIIdClSxds2LABtWvXVqteIiIiIlUpOs9RUdnZ2TAYDAgLC5NRExEREZFm3GqO8vLy0LBhQ6xYsQIDBw5Usy7d4jkkSsfzcsjHTOVjpvIxU/mYqXzuZOrWPkfBwcHIy8tDaGjl/R2xjIwMnn20HO6e0ZWZlo+ZysdM5WOm8jFT+VzJ1O0dsnv06IGdO3dW2h2vCzcdpqenIzw8XONq9CU7Oxv16tVze/MqMy0dM5WPmcrHTOVjpvK5k6nbzdGsWbMwZMgQBAUF4b777kPt2rWLrbqrXr26u4v1GoXPNTw8nAOvFO6uymWm5WOm8jFT+ZipfMxUPlcydbs5uuOOOwAAiYmJeP7550ucx2r18l9TdoHFZsHWX7YCAAY0GgB/P57fyVPMVD5mSt7CYrNg+6/bAXCsyqIo08LGwfNjtbya26Pvueee405eAPz9/DG4yWCty/ApzFQ+ZkregmNVPmaqnNvNUWJiogplEBEREekD11sqZLVZsTd1LwCgc2xnGP2M2hbkA5ipfMyUvIXVZsX+tP0AOFZlYabKsTlSyGwxo/t73QEA12ZeQ6ip8p7eQBZmKh8zJW/BsSofM1WOzZFCBoMBt9a41X6dPMdM5WOm5C04VuVjpsqxOVIoJCAEJyaf0LoMn8JM5WOm5C04VuVjpsq5fk5yIiIiokqAzRERERG5z2D4//Mi+RhFm9WEEEhJScHvv/+OvLy8YrePGTPG48L0Lq8gD0PeHwIA2DZsG4IDgjWuyPsxU/mYKXmLvII83LPxHgAcq7IwU+Xcbo5+/fVX3HPPPTh9+jRECWfQNBgMlaI5sgkbdp7Zab9OnmOm8jFT8hYcq/IxU+Xcbo6mTJkCs9mMTZs2oUWLFggMDFSjLt0L9A/EunvX2a+T55ipfMyUvAXHqnzMVDm3m6Pvv/8eb7/9NoYOHapGPV7D388fI1uM1LoMn8JM5WOm5C04VuVjpsq5vUN2lSpV+Eu/RERE5LPcbo4eeughrF+/Xo1avIrVZkXKnylI+TMFVptV63J8AjOVj5mSt+BYlY+ZKuf2ZrVmzZphw4YNuOeeezBw4EBERkYWm+e+++6TUpyemS1mtF3VFgBPyy4LM5WPmZK34FiVj5kq53ZzNGLECADA2bNnsX379mK3GwwGWK2+36EaDAbERcTZr5PnmKl8zJS8BceqfMxUObeboz179qhRh9cJCQhB6tRUrcvwKcxUPmZK3oJjVT5mqpzbzVHXrl3VqIOIiIhIFxT/8GxOTg6Sk5ORlZWFqKgotG/fHmFhYTJrIyIiIqpwin5b7ZVXXkFMTAz69u2LkSNHok+fPoiJicFrr70muz7dMlvMGLxxMAZvHAyzxax1OT6BmcrHTMlbcKzKp1qmPvybaoXcXnO0du1aPP300+jbty/Gjh2LmJgYZGRk4L333sNTTz2FGjVqYPTo0WrUqitWmxWfnvrUfp08x0zlY6bkLThW5WOmyrndHL3++usYMWIE1q1b5zT9/vvvx6hRo/D6669XiubIZDRh5YCV9uvkOWYqHzMlb8GxKl+FZVq4FqmE31v1Vm43R7/88gsWLFhQ4m2jRo3Cvffe63FRFWn58uVYvnw5UlNTAQC33XYbnnvuOfTt27fM+wUYAzDhjgkVUGHlwUzlY6bkLThW5WOmyrm9z1FwcDAuX75c4m2XL19GcHCwx0VVpLp16+Kll17CoUOHcOjQIdx9990YNGgQTpw4oXVpREREpAG3m6POnTsjMTERGRkZTtPPnz+PuXPnokuXLtKKqwgDBw5Ev3790KhRIzRq1AgvvvgiqlSpgoMHD5Z5P5uw4cTFEzhx8QRswlZB1fo2ZiofMyVvwbEqHzNVzu3NavPnz0fHjh3RsGFD9OjRA7Vr18a5c+ewe/duBAQEYPPmzWrUWSGsVis++ugj5ObmokOHDmXOm1eQh2bLmwHgadllYabyMVPyFhyr8jFT5dxujm677TakpKRgzpw52LNnD7KyshAZGYnBgwdjzpw5aNSokRp1qurHH39Ehw4dYDabUaVKFWzZsgW33nprufeLComqgOoqF2YqHzMlb8GxKh8zVUbRSSAbNWqEDRs2yK5FM40bN8axY8dw5coVfPLJJ0hISMC+ffvKbJBCTaHIfCqzAqv0fcxUPmZK3qLCxmp5R1b50JFXUjP18fMaFaX4DNm+xGQyoWHDhgCANm3aICUlBUuWLMFbb72lcWVERERU0VxqjubOnYvx48cjJiYGc+fOLXNeg8GAZ599VkpxWhFCID8/X+syqCL50LdFInIB3/NUBpeao8TERPz9739HTEwMEhMTy5zX25qjWbNmoW/fvqhXrx5ycnKwceNG7N27F0lJSWXez2wxY9LmSQCAd+55B0H+QRVRrk9jpvIxU/IWZosZ47aNA8CxKgszVc6l5shms5V43RdcuHABo0ePxrlz5xAREYEWLVogKSkJvXr1KvN+VpsV639cDwD2M5CSZ7w2Ux1/A63wTIvul1A0Ex1nRdry2ve/jjFT5Sr9PkfvvPOOovuZjCa83ud1+3XyHDOVj5mSt+BYlY+ZKud2c2Q2m3Hjxg2Eh4fbp3344Yc4cuQIevbsiZ49e0otUK8CjAGY2n6q1mX4FF1kKmPNho7WjmieqY6yIH1TfaxWsqOtAB28/72Y22fIHj16NB5//HH73//+978xbNgwLFy4EH369MHnn38utUAiIiKiiuR2c/T999/j73//u/3vf//73xg1ahSuXLmC++67D6+88orUAvXKJmxIvZKK1CupPC27JFIzNRgq5TfFonQ7Tl15ffgaVipePVZ1yuVMC5+jlz5PNbi9WS0zMxN16tQBAJw9exZnzpzBhg0bEB4ejnHjxmHMmDHSi9SjvII8xC+NB8DTssvCTOVjpuQt8gryEL+EY1UmZqqc281RSEgIrl69CgDYv38/qlSpgjZt2gAAgoKCcO3aNbkV6lhIQIjWJWjPYJC6P0mFZ1rWNyVP95dxXLaGZ+P1KFNZ9XnyjVRpDdzfyetIff+r9fp72XL5/5QybjdHzZs3x9KlSxEXF4dly5ahe/fuMPz1oqalpaFWrVrSi9SjUFMocmflal2GT2Gm8jFT8hYcq/IxU+Xcbo6effZZDBgwAK1atYLJZMLOnTvtt3322Wdo3bq11AKJpHL12xm3vZdN7W/PWi+DvFfR19+VfdsAz9dMck2lT3G7Obr77rtx8uRJHD58GK1atcLf/vY3p9tatWolsz4iIiKiCuXW0Wp5eXkYMWIE0tPTcd999zk1RgDwyCOPoF27dlIL1Kt8Sz4mbJuACdsmIN/C32GToUIzdefIDMcjOco6qqOsZbr6eJKPGNH9OC0v07Lu4+5jkDRqjCvdj9XSKH1vV8C49NpMdcCt5ig4OBiffvqpz/2EiBIWmwWrjq7CqqOrYLFZtC7HJzBT+ZgpqUGNccWxKh8zVc7tzWqtWrXCTz/9hC5duqhRj9cIMAZgXvd59uvkOVUzraRrDqRlquf9KfRcm48qOq6ssEpfpts8fY/L+oxwdzyq+NnkUaZK6irpPl76vnS7OXrppZcwevRo3HbbbejatasaNXkFk9GE2V1ma12GT2Gm8jFTUkPRcWWGWfoyyXPMVDm3m6PJkyfj2rVruPvuu1GtWjXUrl3bfig/ABgMBhw/flxqkUQ+o7Rvld6yZqusfaoq+rG9JTOqnLz9va62ss4D58rat9KOFizvfi5yuzmKjIxEVFSUxw/s7YQQyMzNBABEhUQ5NYikDDOVj5mSGoQQuHT9EoCb40qNZXKseo6ZKud2c7R3714VyvA+1wuuI2ZxDACell0WVTJV88NAxlmfVab6ONXTh62eavFx1wuuI/qVaAA3x5Uay9T8M1X2Pkga0FWm7qwNcnV6abcVnaZgTZLbzVFlJ/4KOScnB4Wb2bOzs2E1eb5DotfKzv7rn5v/CjcHIjMtATOVT1KmhfevzHJv5DqPK/PNceVJpsYbRn2P1Yp63SWOU11kWjS3knIsLVtZmSvJVChw8eJFMWPGDNG+fXvRsGFD8dNPPwkhhFixYoU4cuSIkkV6jfT0dAGAlzIu6enpzJSZ6v7CTJmpN1yYqTaZGoRwry09e/YsOnXqhKtXr6Jly5b47rvvkJKSgtatW2PKlCm4fv06Vq9e7c4ivYrNZkNGRgbCwsK4/bYIIQRycnIQExMDPz/XT6HFTEvHTOVjpvIxU/mYqXzuZOp2c3T//ffjxIkT2LlzJ6Kjo2EymXDo0CG0bt0aGzZswJw5c/Drr7969ASIiIiItOL2Pke7du3C8uXLERMTA6vVeftl7dq1kZGRIa04IiIioorm1s+HAIDZbEb16tVLvC03N9et1X9EREREeuN2J9O4cWPs3LmzxNu+/vprNGvWzOOiiIiIiLTi9ma1CRMmYNq0aYiJicHIkSMBADdu3MDHH3+MZcuW4c0335ReJBEREVFFcXuHbACYOHEiVq1aBT8/P9hsNvj5+UEIgQkTJmDFihVq1ElERERUIRQ1RwBw8OBBfPbZZ7hw4QKioqIwYMAAdOzYUXZ9RERERBVKcXNERERE5Ivc3ueoTZs2ePjhhzF8+HBUq1ZNjZp0jSfYKh1PWiYfM5WPmcrHTOVjpvK5lalb5yUXQrRt21YYDAYRFBQkhg0bJr788kths9ncXYzX4qnZ5ZyanZkyU60vzJSZesOFmWqTqdtrjr777jucOnUK7777LtatW4cPP/wQMTExGDt2LBISEtCwYUN3F+lVwsLCAADp6ekIDw/XuBp9yc7ORr169ewZuYqZlo6ZysdM5WOm8jFT+dzJ1O3mCLh5rqOXX34ZCxYsQFJSElavXo1XXnkF8+fPx1133YV9+/YpWaxXKFxNaQwyIuL1CADAtZnXEGoK1bIsXXF3VS4zLR8zlU9ppuHh4TAGGVFlQRUAzNWRJ5nyP/KSVfg4LXw8H94d2ZVMFTVHhfz8/NCvXz/069cP3377LYYPH45vvvnGk0V6jUD/QGx5cIv9OnmOmcrHTNXBXMkbcJwq51FzlJOTg40bN2L16tX47rvvEBQUhOHDh8uqTdf8/fwxuMlgrcvwKcxUPmaqDuZK3oDjVDlFP4S2e/dujB49GrVq1cIjjzwCm82GZcuW4dy5c1i3bp3sGomIiIgqjNtrjurXr4/09HRER0dj8uTJePjhh9G0aVM1atM1q82Kval7AQCdYzvD6GfUtiAfwEzlY6bqsNqs2J+2HwBzJf3iOFXO7ebo9ttvxxtvvIF+/frBaKy8QZstZnR/rzsA7pApCzOVj5mqg7mSN+A4Vc7t5mjLli1q1OF1DAYDbq1xq/06eY6ZysdM1cFcyRtwnCrn0Q7ZlVlIQAhOTD6hdRk+hZnKx0zVwVzJG3CcKufSDtlGoxHff//9zTv4+cFoNJZ68fdnv0VERETey6VO5rnnnkPdunXt17l6joiIiHyVS83RnDlz7NcTExPVqsWr5BXkYcj7QwAA24ZtQ3BAsMYVeT9mKh8zVUdeQR7u2XgPAOZK+qX6OPXhs2lzG5hCNmHDzjM77dfJc8xUPmaqDuZK3oDjVDm3mqPMzEy89dZb+Prrr5GRkQEAiImJQffu3TFx4kRERkaqUqQeBfoHYt296+zXyXPMVD5mqg7mSt6A41Q5l5ujXbt2YciQIcjOzobRaERUVBSEEDh16hR27tyJV155BVu2bEGXLl3UrFc3/P38MbLFSK3L8CnMVD5mqg7mSt6A41Q5l45Wy8zMxIMPPoiIiAh8+OGHuHr1Ks6dO4fz58/j6tWr2LhxI0JDQzF06FBkZWWpXTMRERGRalxqjt555x1YrVZ8++23GDp0KEJCQuy3hYSE4IEHHsA333yDgoICvPPOO6oVqydWmxUpf6Yg5c8UWG1WrcvxCcxUPmaqDuZK3oDjVDmXmqMdO3bg4Ycfth/OX5LY2Fg89NBDSEpKklacnpktZrRd1RZtV7WF2WLWuhyfwEzlY6bqYK7kDThOlXOpOTp58iTuuuuucufr3LkzTp486XFRWlqwYAEMBgOmTp1a5nwGgwFxEXGIi4jjeZ8kYabyMVN1MFfyBhynyrm0Q/aVK1cQHR1d7nzR0dG4cuWKpzVpJiUlBStXrkSLFi3KnTckIASpU1PVL6oSYabyMVN1MFfyBqqN00rQaLm05ig/Px8BAQHlzufv748bN254XJQWrl27hpEjR+Ltt99GtWrVtC6HiIiINOLyofynTp0q93fTfvnlF48L0sqUKVPQv39/9OzZE/PmzdO6HCIiItKIy83R2LFjy51HCOGV2zU3btyIw4cP49ChQy7fx2wxY8zGMTfvP3QjgvyD1Cqv0mCm8jFTdZgtZgz7eBgA5kr6xXGqnEvN0erVq9WuQzPp6en45z//iR07diAoyPWBY7VZ8empT+3XyXPMVD5mqg7mSt6gwsapD/7GmkvNUUJCgtp1aObw4cO4ePEi7rjjDvs0q9WKr7/+Gm+++Sby8/NhNBqL3c9kNGHlgJX26+Q5ZiofM1UHcyVvwHGqXKX/4dkePXrgxx9/dJr20EMPoUmTJnjmmWdKbIwAIMAYgAl3TKiIEisNZiofM1UHcyVvwHGqXKVvjsLCwtCsWTOnaaGhoYiMjCw2nYiIiHxfpW+OlLIJG05cPAEAaFqjKfwMLp0VgcrATOVjpuqwCRtOZt484S1zJb3iOFWOzVEJ9u7dW+48eQV5aLb85pqlazOvIdQUqnJVvo+ZysdM1cFcyRtwnCrH5sgDUSFRWpfgc5ipfMxUHcyVvAHHqTJsjhQKNYUi86lMrcvwKcxUPmaqDuZK3kDqOPXCcxh6ghsgiYiIiBywOSICbn4rqmTfjIiIqGRsjhQyW8wYuXkkRm4eCbPFrHU5PoGZysdM1cFcyRtwnCrH5kghq82K9T+ux/of1/PnAyTx2kx1vNapwjMtzKK0THSclTu8dqxSpcJxqhx3yFbIZDTh9T6v26+T55ipfMxUHcyVvAHHqXJsjhQKMAZgavupWpfhU3SRqY/9gKLmmfpYnoU0z7U0BoPPZU3K6XacegFuViMiIiJywDVHCtmEDalXUgEAsRGxlfe07BK/qUrN1NU1Fmrs/6KjtSW6HaeuZKSjHIuyCRvSrqYBkDhWAf08Vx1nT65zeZz6wH6AsrE5UiivIA/xS+MB8LTssjBT+ZipOvIK8hC/hLmSvnGcKsfmyAMhASFal+BzdJepJ2vGdLJGwKNMZa1B8OSbqdIaVF77oclYLfqc1FzzWdLyiz4u1yzpnu4+U70EmyOFQk2hyJ2Vq3UZPoWZysdM1cFcyRtwnCrH5ogqFzX3RfLkPt72DVx23TKXx/0nnJWXrbeOQSIV6WTvTCIiIiJ9YHOkUL4lHxO2TcCEbROQb8nXuhyfoNtMi571WclZnl29j+QzSOs200JFn68r+bqbkQpn5VYtV1fPLO7Kc1IjJx85w7keqTGmdP/+1zE2RwpZbBasOroKq46ugsVm0bocn8BM5WOm6mCuJJsaY4rjVDnuc6RQgDEA87rPs18nz6maadFvu3r79qtSPdIy1fN+KRrUVmHvf5njQm9jnpwUHVNWeP5baB6NU1n7XerxM8MFbI4UMhlNmN1lttZl+BRmKh8zVQdzJdmKjikzzNKXSa5jc0RUkfS8BsYVpX2brIi1Enpf+6cXzIW8QVnngVNyBn3J55Vjc6SQEAKZuZkAgKiQKBj4geQxZiofM1WHEAKXrl8CwFxJjqJjSo1lcpy6js2RQtcLriNmcQwAnpZdFlUyVfPDQMZZn1Wm+jjV04dtBdZyveA6ol+JBlAJcqUKUXRMqbFMTf+fcmdtkKvTS7uttDO7u4HNkZvEXyHn5OSgcJNwdnY2rCbPd57zWtnZf/1z81/h5kBkpiVgpvJJyjQ7OxvGG0bmCkjNtLLLvZHrPKbMN8eU14/Toq9tSa91aa+/rHGhZJwKckt6eroAwEsZl/T0dGbKTHV/YabM1BsuzFSbTA1CeOueodqw2WzIyMhAWFgYt98WIYRATk4OYmJi4Ofn+im0mGnpmKl8zFQ+ZiofM5XPnUzZHBERERE54BmyiYiIiBywOSIiIiJywOaIiIiIyAGbIyIiIiIHbI6IiIiIHLA5IiIiInLAM2S7ieeQKB3PyyEfM5WPmcrHTOVjpvK5kymbIzdlZGSgXr16Wpeha+np6ahbt67L8zPT8jFT+ZipfMxUPmYqnyuZsjlyU1hYGICb4YaHh2tcjb5kZ2ejXr169oxcxUxLx0zlY6byMVP5mKl87mTK5shNhaspw8PDOfBK4e6qXGZaPmYqHzOVj5nKx0zlcyVTNkcKWWwWbP1lKwBgQKMB8PdjlJ5ipvIxU3VYbBZs/3U7AOYqCzMlPeHoU8jfzx+DmwzWugyfwkzlY6bqYK7yMVPSEx7KT0REROSAa44Ustqs2Ju6FwDQObYzjH5GbQvyAcxUPmaqDqvNiv1p+wEwV1mYKekJmyOFzBYzur/XHQBwbeY1hJpCNa7I+zFT+ZipOpirfMyU9ITNkUIGgwG31rjVfp08x0zlY6bqYK7yMVPSEzZHCoUEhODE5BNal+FTmKl8zFQdzFU+Zkp6wh2yiYiIiBywOSIiIiJywOZIobyCPPR6vxd6vd8LeQV5WpfjE5ipfMxUHcxVPmZKesJ9jhSyCRt2ntlpv06eY6byMVN1MFf5mCnpCZsjhQL9A7Hu3nX26+Q5ZiofM1UHc5WPmZKesDlSyN/PHyNbjNS6DJ/CTOVjpupgrvIxU9IT7nNERERE5IBrjhSy2qxI+TMFANC6dmue6l4CZiofM1WH1WbFkXNHADBXWZgp6QmbI4XMFjParmoLgKe6l4WZysdM1cFc5WOmpCdsjhQyGAyIi4izXyfPMVP5mKk6mKt8zJT0hM2RQiEBIUidmqp1GT6FmcrHTNXBXOVjpqQn3CGbiIiIyAGbIyIiIiIHbI4UMlvMGLxxMAZvHAyzxax1OT6BmcrHTNXBXOVjpqQnlX6fo8TERDz//PNO02rWrInz58+XeT+rzYpPT31qv06eY6byMVN1MFf5mCnpSaVvjgDgtttuw86dO+1/G43ln1/DZDRh5YCV9uvkOWYqHzNVB3OVj5mSnrA5AuDv749atWq5dZ8AYwAm3DFBpYoqJ2YqHzNVB3OVj5mSnnCfIwCnT59GTEwM4uPjMWzYMJw5c0brkoiIiEgjlb45ateuHdauXYsvv/wSb7/9Ns6fP4+OHTsiKyurzPvZhA0nLp7AiYsnYBO2CqrWtzFT+ZipOpirfMyU9KTSb1br27ev/Xrz5s3RoUMHNGjQAO+99x6mTZtW6v3yCvLQbHkzADzVvSzMVD5mqg7mKh8zJT2p9M1RUaGhoWjevDlOnz5d7rxRIVEVUFHlwkzlY6bqYK7yMVPSCzZHReTn5+PkyZPo3LlzmfOFmkKR+VRmBVVVOTBT+ZipOpirfBWWaeHvtgmh7HaqFCr9PkfTp0/Hvn37cPbsWXz33XcYOnQosrOzkZCQoHVpREREpIFKv+bojz/+wPDhw3Hp0iXUqFED7du3x8GDBxEXF6d1ad7BYPCNb1j8tigHcyRvwbFKZaj0zdHGjRsV3c9sMWPS5kkAgHfueQdB/kEyy6qUmKl8zFQdZosZ47aNA8BcZWGmpCeVfrOaUlabFet/XI/1P67nqe4l8dpMDYb//xaqM5plWlomOs7KHV47VnWMmZKeVPo1R0qZjCa83ud1+3XyHDOVj5mqg7nKx0xJT9gcKRRgDMDU9lO1LsOn6CJTGfsh6GhfBs0z1VEWMmmea2m8eB9A1TP1gTWWVHG4WY2IiIjIAdccKWQTNqReSQUAxEbEws/APtNTUjN1dY2Fj3+b1O04deX10fFaJ5uwIe1qGgCJYxXQz3PVIHupmcpUUVnoeLxXRmyOFMoryEP80ngAPNW9LMxUPmaqjryCPMQvYa4yMVPSEzZHHggJCNG6BJ+ju0w92YdDJ2sEPMpU1rdZT9bQKa1B5W/imozVos9JjTWfjsssuvyijys5W6mZqvX6cw1PpcDmSKFQUyhyZ+VqXYZPYabyMVN1MFf5mCnpCZsjqlz0ti+St34LlV23zOX5+H5kbqtsvyVW9PUvbzzIWjPpazlWcjrZ442IiIhIH9gcKZRvyceEbRMwYdsE5FvytS7HJ+g208KzOhe9lDZfWctw9bEk0W2mhYo+37LyLe0+7j6GBKrl6urYcuU5qZGTimc41/1YLY2rr42S15A84smYYnOkkMVmwaqjq7Dq6CpYbBaty/EJzFQ+ZqoO5iofMyXZPBlT3OdIoQBjAOZ1n2e/Tp5TNVN390OoaCrVIy1TPe9PoUFtFfb+lzku9Dbmi/A4U0+fn87zIfcVHVNWuP6bfWyOFDIZTZjdZbbWZfgUZiofM1UHc5WPmZJsRceUGWaX78vmiKgi6XkNjCu0/Hat97V/esFc9I3j2CuwOVJICIHM3EwAQFRIFAwc4B5jpvIxU3UIIXDp+iUAzFUWZkqyFR1T7mBzpND1guuIWRwDgKe6l0WVTNX8gJVx1meVqT5O9fQfWAXWcr3gOqJfiQZQCXKtIKpn6i7uw+T1io4pd7A5cpP4a3NITk4OCjdfZmdnw2pyfUcvn5Od/dc/N/8Vbm4yYqYlYKbySco0OzsbxhtG5gpUrkz/eo4V9TgyMq3scm/kOo8p880x5VKmgtySnp4uAPBSxiU9PZ2ZMlPdX5gpM/WGCzPVJlODEN66Z6g2bDYbMjIyEBYWxm3iRQghkJOTg5iYGPj5uX4KLWZaOmYqHzOVj5nKx0zlcydTNkdEREREDniGbCIiIiIHbI6IiIiIHLA5IiIiInLA5oiIiIjIAZsjIiIiIgdsjoiIiIgcsDkiIiIicsDmiIiIiMgBmyMiIiIiB2yOiIiIiBywOSIiIiJywOaIiIiIyAGbIyIiIiIHbI6IiIiIHLA5IiIiInLA5oiIiIjIAZsjIiIiIgdsjoiIiIgcsDkiIiIicsDmiIiIiMgBmyMiIiIiB2yOiIiIiBywOSIiIiJywOaIiIiIyAGbIyIiIiIHbI6IiIiIHLA5IiIiInLA5oiIiIjIAZsjIiIiIgdsjoiIiIgcsDkiIiIicsDmiEiSNWvWwGAw2C9BQUGoVasWunfvjgULFuDixYvF7pOYmAiDwaBBta4zGAxITEzUugxFfv75ZyQmJiI1NbXYbWPHjkX9+vWlPt4bb7yBhg0bwmQywWAw4MqVK1KXX9EKx3RJ+RH5MjZHRJKtXr0aycnJ+Oqrr7B06VK0atUKL7/8Mpo2bYqdO3c6zTt+/HgkJydrVKlrkpOTMX78eK3LUOTnn3/G888/X+J/7s8++yy2bNki7bGOHTuGxx9/HN27d8fu3buRnJyMsLAwacsnoorjr3UBRL6mWbNmaNOmjf3vIUOG4IknnsBdd92F++67D6dPn0bNmjUBAHXr1kXdunUrvMbr168jJCTEpXnbt2+vcjWuc6fu8jRo0EDKcgqdOHECADBhwgS0bdu2zHllPg8iko9rjogqQGxsLF599VXk5OTgrbfesk8vullt8ODBiIuLg81mK7aMdu3aoXXr1va/hRBYtmwZWrVqheDgYFSrVg1Dhw7FmTNnnO7XrVs3NGvWDF9//TU6duyIkJAQPPzwwwCA3bt3o1u3boiMjERwcDBiY2MxZMgQXL9+3X7/kjar/fTTTxg0aBCqVauGoKAgtGrVCu+9957TPHv37oXBYMCGDRswe/ZsxMTEIDw8HD179sSpU6fKzawwmyNHjmDo0KGoVq2avaE5dOgQhg0bhvr16yM4OBj169fH8OHD8fvvv9vvv2bNGtx///0AgO7du9s3d65ZswZAyZvVzGYzZs6cifj4eJhMJtSpUwdTpkwpd/NYt27dMGrUKAA3XyeDwYCxY8eWm39aWhpGjRqF6OhoBAYGomnTpnj11VedXv/U1FQYDAYsWrQIL7/8sv05d+vWDb/++isKCgowY8YMxMTEICIiAvfee2+Jm3BL8t1332HgwIGIjIxEUFAQGjRogKlTp5Z5n6+++gqDBg1C3bp1ERQUhIYNG+KRRx7BpUuXnObLzMzExIkTUa9ePQQGBqJGjRro1KmT09rTo0ePYsCAAfbnHxMTg/79++OPP/5wqX4itXDNEVEF6devH4xGI77++utS53n44YcxaNAg7N69Gz179rRP/+WXX/D999/j3//+t33aI488gjVr1uDxxx/Hyy+/jMuXL2Pu3Lno2LEjjh8/bl87BQDnzp3DqFGj8PTTT2P+/Pnw8/NDamoq+vfvj86dO+Pdd99F1apV8eeffyIpKQk3btwodc3GqVOn0LFjR0RHR+Pf//43IiMjsW7dOowdOxYXLlzA008/7TT/rFmz0KlTJ6xatQrZ2dl45plnMHDgQJw8eRJGo7Hc3O677z4MGzYMjz76KHJzcwHcbBgaN26MYcOGoXr16jh37hyWL1+OO++8Ez///DOioqLQv39/zJ8/H7NmzcLSpUvtjWVpa4yEEBg8eDB27dqFmTNnonPnzvjhhx8wZ84cJCcnIzk5GYGBgSXed9myZdiwYQPmzZuH1atXo0mTJqhRo0aZ+WdmZqJjx464ceMGXnjhBdSvXx/bt2/H9OnT8d///hfLli1zeoylS5eiRYsWWLp0Ka5cuYInn3wSAwcORLt27RAQEIB3330Xv//+O6ZPn47x48dj27ZtZeb65ZdfYuDAgWjatClee+01xMbGIjU1FTt27Cjzfv/973/RoUMHjB8/HhEREUhNTcVrr72Gu+66Cz/++CMCAgIAAKNHj8aRI0fw4osvolGjRrhy5QqOHDmCrKwsAEBubi569eqF+Ph4LF26FDVr1sT58+exZ88e5OTklFkDkeoEEUmxevVqAUCkpKSUOk/NmjVF06ZN7X/PmTNHOL4NCwoKRM2aNcWIESOc7vf0008Lk8kkLl26JIQQIjk5WQAQr776qtN86enpIjg4WDz99NP2aV27dhUAxK5du5zm/fjjjwUAcezYsTKfFwAxZ84c+9/Dhg0TgYGBIi0tzWm+vn37ipCQEHHlyhUhhBB79uwRAES/fv2c5vvwww8FAJGcnFzm4xZm89xzz5U5nxBCWCwWce3aNREaGiqWLFlin/7RRx8JAGLPnj3F7pOQkCDi4uLsfyclJQkAYuHChU7zbdq0SQAQK1euLLOG0l7/0vKfMWOGACC+++47p+mTJk0SBoNBnDp1SgghxNmzZwUA0bJlS2G1Wu3zLV68WAAQ99xzj9P9p06dKgCIq1evlllvgwYNRIMGDUReXl65z+ns2bMl3m6z2URBQYH4/fffBQDx6aef2m+rUqWKmDp1aqnLPnTokAAgtm7dWmadRFrgZjWiCiSEKPN2f39/jBo1Cps3b8bVq1cBAFarFe+//z4GDRqEyMhIAMD27dthMBgwatQoWCwW+6VWrVpo2bIl9u7d67TcatWq4e6773aa1qpVK5hMJkycOBHvvfdesc1xpdm9ezd69OiBevXqOU0fO3Ysrl+/XmwH83vuucfp7xYtWgCA0yawsgwZMqTYtGvXruGZZ55Bw4YN4e/vD39/f1SpUgW5ubk4efKkS8stavfu3QBg3xxW6P7770doaCh27dqlaLlAyfnv3r0bt956a7H9k8aOHQshhL2eQv369YOf3/9/ZDdt2hQA0L9/f6f5CqenpaWVWs+vv/6K//73vxg3bhyCgoLcei4XL17Eo48+inr16sHf3x8BAQGIi4sDAKfs27ZtizVr1mDevHk4ePAgCgoKnJbTsGFDVKtWDc888wxWrFiBn3/+2a06iNTE5oioguTm5iIrKwsxMTFlzvfwww/DbDZj48aNAG5u/jh37hweeugh+zwXLlyAEAI1a9ZEQECA0+XgwYPF9v+oXbt2scdp0KABdu7ciejoaEyZMgUNGjRAgwYNsGTJkjLry8rKKnF5hc+rcLNJocKGrlDhpqm8vLwyH6es2keMGIE333wT48ePx5dffonvv/8eKSkpqFGjhsvLLSorKwv+/v5Om8OAm/tc1apVq9jzckdJz8HdHKtXr+70t8lkKnO62WwutZ7MzEwAcPtgAJvNht69e2Pz5s14+umnsWvXLnz//fc4ePAgAOfXdNOmTUhISMCqVavQoUMHVK9eHWPGjMH58+cBABEREdi3bx9atWqFWbNm4bbbbkNMTAzmzJlTrJEiqmjc54iognz22WewWq3o1q1bmfMVrk1YvXo1HnnkEaxevRoxMTHo3bu3fZ6oqCgYDAbs37+/xP1gik4r7VxKnTt3RufOnWG1WnHo0CG88cYbmDp1KmrWrIlhw4aVeJ/IyEicO3eu2PSMjAx7bTIVrf3q1avYvn075syZgxkzZtin5+fn4/Lly4ofJzIyEhaLBZmZmU4NkhAC58+fx5133ql42SXlX9E5Oip8fu7u+PzTTz/h+PHjWLNmDRISEuzTf/vtt2LzRkVFYfHixVi8eDHS0tKwbds2zJgxAxcvXkRSUhIAoHnz5ti4cSOEEPjhhx+wZs0azJ07F8HBwU6vLVFF45ojogqQlpaG6dOnIyIiAo888ki58z/00EP47rvv8M033+A///kPEhISnHZeHjBgAIQQ+PPPP9GmTZtil+bNm7tVn9FoRLt27bB06VIAwJEjR0qdt0ePHti9e7f9P/FCa9euRUhIiOqH/hsMBgghijWAq1atgtVqdZrmzlqqHj16AADWrVvnNP2TTz5Bbm6u/XZZevTogZ9//rlY1mvXroXBYED37t2lPp6jRo0aoUGDBnj33XeRn5/v8v0Km7yi2TsegVmS2NhYPPbYY+jVq1eJY8tgMKBly5Z4/fXXUbVq1TLHH1FF4JojIsl++ukn+z5AFy9exP79+7F69WoYjUZs2bKl2GabkgwfPhzTpk3D8OHDkZ+fX2w/mE6dOmHixIl46KGHcOjQIXTp0gWhoaE4d+4cvvnmGzRv3hyTJk0q8zFWrFiB3bt3o3///oiNjYXZbMa7774LAE5HyhU1Z84cbN++Hd27d8dzzz2H6tWr44MPPsBnn32GhQsXIiIiovyQPBAeHo4uXbpg0aJFiIqKQv369bFv3z688847qFq1qtO8zZo1AwCsXLkSYWFhCAoKQnx8fLFNfQDQq1cv9OnTB8888wyys7PRqVMn+9Fqt99+O0aPHi31eTzxxBNYu3Yt+vfvj7lz5yIuLg6fffYZli1bhkmTJqFRo0ZSH6+opUuXYuDAgWjfvj2eeOIJxMbGIi0tDV9++SU++OCDEu/TpEkTNGjQADNmzIAQAtWrV8d//vMffPXVV07zXb16Fd27d8eIESPQpEkThIWFISUlBUlJSbjvvvsA3NxvbtmyZRg8eDD+9re/QQiBzZs348qVK+jVq5eqz52oPGyOiCQr3DfIZDKhatWqaNq0KZ555hmMHz/epcYIgP18NevXr0enTp1K/I/yrbfeQvv27fHWW29h2bJlsNlsiImJQadOnco9CSFwc4fsHTt2YM6cOTh//jyqVKmCZs2aYdu2bU6b8Ipq3LgxDhw4gFmzZmHKlCnIy8tD06ZNsXr16mJNnFrWr1+Pf/7zn3j66adhsVjQqVMnfPXVV8V2To6Pj8fixYuxZMkSdOvWDVartdQ6DQYDtm7disTERKxevRovvvgioqKiMHr0aMyfP7/Uw/iVqlGjBg4cOICZM2di5syZyM7Oxt/+9jcsXLgQ06ZNk/pYJenTpw++/vprzJ07F48//jjMZjPq1q1bbAd6RwEBAfjPf/6Df/7zn3jkkUfg7++Pnj17YufOnYiNjbXPFxQUhHbt2uH9999HamoqCgoKEBsbi2eeecZ+qodbbrkFVatWxcKFC5GRkQGTyYTGjRsX22RHpAWDKO/wGSIiIqJKhPscERERETlgc0RERETkgM0RERERkQM2R0REREQO2BwREREROWBzREREROSAzRERERGRAzZHRERERA7YHBERERE5YHNERERE5IDNEREREZGD/wNJsk38WBAfOQAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D0p = {j : (D0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D0p[j], num_bins, range = (np.quantile(D0p[j], 0.10), np.quantile(D0p[j], 0.90)), color = 'r', alpha = 1) # Logit is red\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit diversion ratios by class')\n",
    "fig.supxlabel('Diversion ratio from class')\n",
    "fig.supylabel('Diversion ratio to class')\n",
    "\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D1p = {j : (D1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg, sharex=False, sharey=False)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D1p[j], num_bins, range = (np.quantile(D1p[j], 0.10), np.quantile(D1p[j], 0.90)), color = 'b', alpha = 1) # Similarity is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Similarity diversion ratios by class')\n",
    "fig.supxlabel('Diversion ratio from class')\n",
    "fig.supylabel('Diversion ratio to class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also calculate the mean diversion ratios within each class. For the Logit Model these are given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean diversion ratio wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean diversion ratio of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>32.977441</td>\n",
       "      <td>30.037606</td>\n",
       "      <td>21.805955</td>\n",
       "      <td>9.610813</td>\n",
       "      <td>5.568185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.716873</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.579788</td>\n",
       "      <td>0.413296</td>\n",
       "      <td>0.180329</td>\n",
       "      <td>0.109713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.685594</td>\n",
       "      <td>0.611145</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.413182</td>\n",
       "      <td>0.180340</td>\n",
       "      <td>0.109739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.522081</td>\n",
       "      <td>0.609950</td>\n",
       "      <td>0.578479</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.179993</td>\n",
       "      <td>0.109496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.294337</td>\n",
       "      <td>0.608266</td>\n",
       "      <td>0.576919</td>\n",
       "      <td>0.411271</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.109208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98.225371</td>\n",
       "      <td>0.607820</td>\n",
       "      <td>0.576488</td>\n",
       "      <td>0.410944</td>\n",
       "      <td>0.179377</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean diversion ratio wrt. class           0           1           2  \\\n",
       "Mean diversion ratio of class                                         \n",
       "0                               -100.000000   32.977441   30.037606   \n",
       "1                                 98.716873 -100.000000    0.579788   \n",
       "2                                 98.685594    0.611145 -100.000000   \n",
       "3                                 98.522081    0.609950    0.578479   \n",
       "4                                 98.294337    0.608266    0.576919   \n",
       "5                                 98.225371    0.607820    0.576488   \n",
       "\n",
       "Mean diversion ratio wrt. class           3           4           5  \n",
       "Mean diversion ratio of class                                        \n",
       "0                                 21.805955    9.610813    5.568185  \n",
       "1                                  0.413296    0.180329    0.109713  \n",
       "2                                  0.413182    0.180340    0.109739  \n",
       "3                               -100.000000    0.179993    0.109496  \n",
       "4                                  0.411271 -100.000000    0.109208  \n",
       "5                                  0.410944    0.179377 -100.000000  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D0.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. class', index = 'Mean diversion ratio of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Similarity Model the mean diversion ratios are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean diversion ratio wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean diversion ratio of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>33.743095</td>\n",
       "      <td>29.653807</td>\n",
       "      <td>21.433478</td>\n",
       "      <td>9.574822</td>\n",
       "      <td>5.594798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.505070</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.667742</td>\n",
       "      <td>0.479418</td>\n",
       "      <td>0.213535</td>\n",
       "      <td>0.134236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.447235</td>\n",
       "      <td>0.725560</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.479247</td>\n",
       "      <td>0.213606</td>\n",
       "      <td>0.134352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.262319</td>\n",
       "      <td>0.724218</td>\n",
       "      <td>0.666228</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.213190</td>\n",
       "      <td>0.134046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.002884</td>\n",
       "      <td>0.722113</td>\n",
       "      <td>0.664362</td>\n",
       "      <td>0.476963</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.133677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>97.925531</td>\n",
       "      <td>0.721578</td>\n",
       "      <td>0.663872</td>\n",
       "      <td>0.476580</td>\n",
       "      <td>0.212438</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean diversion ratio wrt. class           0           1           2  \\\n",
       "Mean diversion ratio of class                                         \n",
       "0                               -100.000000   33.743095   29.653807   \n",
       "1                                 98.505070 -100.000000    0.667742   \n",
       "2                                 98.447235    0.725560 -100.000000   \n",
       "3                                 98.262319    0.724218    0.666228   \n",
       "4                                 98.002884    0.722113    0.664362   \n",
       "5                                 97.925531    0.721578    0.663872   \n",
       "\n",
       "Mean diversion ratio wrt. class           3           4           5  \n",
       "Mean diversion ratio of class                                        \n",
       "0                                 21.433478    9.574822    5.594798  \n",
       "1                                  0.479418    0.213535    0.134236  \n",
       "2                                  0.479247    0.213606    0.134352  \n",
       "3                               -100.000000    0.213190    0.134046  \n",
       "4                                  0.476963 -100.000000    0.133677  \n",
       "5                                  0.476580    0.212438 -100.000000  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D1.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. class', index = 'Mean diversion ratio of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "To conclude we compare parameter estimates from the above models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit_MLE</th>\n",
       "      <th>Similarity_MLE</th>\n",
       "      <th>FKN</th>\n",
       "      <th>Logit_BLP</th>\n",
       "      <th>Similarity_BLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.468*** (0.2755)</td>\n",
       "      <td>-2.55*** (0.43873)</td>\n",
       "      <td>-10.459*** (0.00342)</td>\n",
       "      <td>-14.929*** (0.05049)</td>\n",
       "      <td>-11.798*** (0.03458)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.318** (0.17503)</td>\n",
       "      <td>-0.318 (0.22618)</td>\n",
       "      <td>-0.501*** (0.0017)</td>\n",
       "      <td>-2.359** (0.02596)</td>\n",
       "      <td>-0.755*** (0.02002)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.457*** (0.18516)</td>\n",
       "      <td>-0.457* (0.27293)</td>\n",
       "      <td>-3.509*** (0.00255)</td>\n",
       "      <td>-6.764*** (0.0296)</td>\n",
       "      <td>-5.587*** (0.02601)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.948*** (0.15371)</td>\n",
       "      <td>-0.946*** (0.22234)</td>\n",
       "      <td>0.069*** (0.00162)</td>\n",
       "      <td>0.03 (0.02894)</td>\n",
       "      <td>0.257*** (0.02089)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.928*** (0.20096)</td>\n",
       "      <td>-1.957*** (0.30143)</td>\n",
       "      <td>-2.216*** (0.00171)</td>\n",
       "      <td>-2.052* (0.03436)</td>\n",
       "      <td>-2.405*** (0.02323)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.056*** (0.24525)</td>\n",
       "      <td>-2.097*** (0.42346)</td>\n",
       "      <td>5.496*** (0.00318)</td>\n",
       "      <td>10.847*** (0.05173)</td>\n",
       "      <td>5.971*** (0.03394)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.021*** (0.25088)</td>\n",
       "      <td>-2.085*** (0.31568)</td>\n",
       "      <td>0.342*** (0.00197)</td>\n",
       "      <td>-1.041 (0.04148)</td>\n",
       "      <td>0.87*** (0.02738)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.715*** (0.08227)</td>\n",
       "      <td>-0.744*** (0.11864)</td>\n",
       "      <td>-1.024*** (0.00111)</td>\n",
       "      <td>-0.583 (0.01771)</td>\n",
       "      <td>-0.77*** (0.01218)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.373*** (0.2058)</td>\n",
       "      <td>-1.392*** (0.27032)</td>\n",
       "      <td>3.283*** (0.00229)</td>\n",
       "      <td>5.153*** (0.03692)</td>\n",
       "      <td>5.02*** (0.02657)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.01 (0.08087)</td>\n",
       "      <td>-0.055 (0.10572)</td>\n",
       "      <td>0.78*** (0.00081)</td>\n",
       "      <td>0.518 (0.01579)</td>\n",
       "      <td>1.138*** (0.01299)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.847*** (0.09149)</td>\n",
       "      <td>0.844*** (0.09099)</td>\n",
       "      <td>-0.212*** (0.00104)</td>\n",
       "      <td>-0.173** (0.00369)</td>\n",
       "      <td>-0.149*** (0.00237)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.107*** (0.03976)</td>\n",
       "      <td>0.717*** (0.00025)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.843*** (0.00209)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.01 (0.0255)</td>\n",
       "      <td>-0.135*** (0.00017)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.093*** (0.00143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.025 (0.03359)</td>\n",
       "      <td>0.123*** (0.00026)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.206*** (0.00219)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.061* (0.03187)</td>\n",
       "      <td>0.006*** (0.0002)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.007*** (0.00189)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.077*** (0.02724)</td>\n",
       "      <td>-0.111*** (0.00016)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.049*** (0.00172)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.129*** (0.01933)</td>\n",
       "      <td>-0.033*** (0.00011)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.067*** (0.00101)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.053*** (0.01413)</td>\n",
       "      <td>-0.044*** (7e-05)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.061*** (0.00068)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.011 (0.01348)</td>\n",
       "      <td>0.056*** (0.00011)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.022*** (0.00092)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.073*** (0.02374)</td>\n",
       "      <td>-0.054*** (0.00017)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.057*** (0.00163)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.181*** (0.01269)</td>\n",
       "      <td>-0.033*** (0.0001)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.04*** (0.00109)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.044** (0.01847)</td>\n",
       "      <td>0.266*** (0.00023)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.188*** (0.00097)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.019 (0.01933)</td>\n",
       "      <td>-0.229*** (0.00023)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.392*** (0.00119)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.153*** (0.03315)</td>\n",
       "      <td>-0.06*** (0.00013)</td>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.097*** (0.00142)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Logit_MLE       Similarity_MLE                   FKN  \\\n",
       "in_out         -2.468*** (0.2755)   -2.55*** (0.43873)  -10.459*** (0.00342)   \n",
       "cy             -0.318** (0.17503)     -0.318 (0.22618)    -0.501*** (0.0017)   \n",
       "hp            -0.457*** (0.18516)    -0.457* (0.27293)   -3.509*** (0.00255)   \n",
       "we            -0.948*** (0.15371)  -0.946*** (0.22234)    0.069*** (0.00162)   \n",
       "le            -1.928*** (0.20096)  -1.957*** (0.30143)   -2.216*** (0.00171)   \n",
       "wi            -2.056*** (0.24525)  -2.097*** (0.42346)    5.496*** (0.00318)   \n",
       "he            -2.021*** (0.25088)  -2.085*** (0.31568)    0.342*** (0.00197)   \n",
       "li            -0.715*** (0.08227)  -0.744*** (0.11864)   -1.024*** (0.00111)   \n",
       "sp             -1.373*** (0.2058)  -1.392*** (0.27032)    3.283*** (0.00229)   \n",
       "ac                 0.01 (0.08087)     -0.055 (0.10572)     0.78*** (0.00081)   \n",
       "pr             0.847*** (0.09149)   0.844*** (0.09099)   -0.212*** (0.00104)   \n",
       "group_in_out                - (-)   0.107*** (0.03976)    0.717*** (0.00025)   \n",
       "group_cy                    - (-)        0.01 (0.0255)   -0.135*** (0.00017)   \n",
       "group_hp                    - (-)      0.025 (0.03359)    0.123*** (0.00026)   \n",
       "group_we                    - (-)     0.061* (0.03187)     0.006*** (0.0002)   \n",
       "group_le                    - (-)  -0.077*** (0.02724)   -0.111*** (0.00016)   \n",
       "group_wi                    - (-)  -0.129*** (0.01933)   -0.033*** (0.00011)   \n",
       "group_he                    - (-)  -0.053*** (0.01413)     -0.044*** (7e-05)   \n",
       "group_li                    - (-)      0.011 (0.01348)    0.056*** (0.00011)   \n",
       "group_sp                    - (-)  -0.073*** (0.02374)   -0.054*** (0.00017)   \n",
       "group_ac                    - (-)  -0.181*** (0.01269)    -0.033*** (0.0001)   \n",
       "group_brand                 - (-)    0.044** (0.01847)    0.266*** (0.00023)   \n",
       "group_home                  - (-)      0.019 (0.01933)   -0.229*** (0.00023)   \n",
       "group_cla                   - (-)   0.153*** (0.03315)    -0.06*** (0.00013)   \n",
       "\n",
       "                         Logit_BLP        Similarity_BLP  \n",
       "in_out        -14.929*** (0.05049)  -11.798*** (0.03458)  \n",
       "cy              -2.359** (0.02596)   -0.755*** (0.02002)  \n",
       "hp              -6.764*** (0.0296)   -5.587*** (0.02601)  \n",
       "we                  0.03 (0.02894)    0.257*** (0.02089)  \n",
       "le               -2.052* (0.03436)   -2.405*** (0.02323)  \n",
       "wi             10.847*** (0.05173)    5.971*** (0.03394)  \n",
       "he                -1.041 (0.04148)     0.87*** (0.02738)  \n",
       "li                -0.583 (0.01771)    -0.77*** (0.01218)  \n",
       "sp              5.153*** (0.03692)     5.02*** (0.02657)  \n",
       "ac                 0.518 (0.01579)    1.138*** (0.01299)  \n",
       "pr              -0.173** (0.00369)   -0.149*** (0.00237)  \n",
       "group_in_out                 - (-)    0.843*** (0.00209)  \n",
       "group_cy                     - (-)   -0.093*** (0.00143)  \n",
       "group_hp                     - (-)    0.206*** (0.00219)  \n",
       "group_we                     - (-)   -0.007*** (0.00189)  \n",
       "group_le                     - (-)   -0.049*** (0.00172)  \n",
       "group_wi                     - (-)   -0.067*** (0.00101)  \n",
       "group_he                     - (-)   -0.061*** (0.00068)  \n",
       "group_li                     - (-)    0.022*** (0.00092)  \n",
       "group_sp                     - (-)   -0.057*** (0.00163)  \n",
       "group_ac                     - (-)    -0.04*** (0.00109)  \n",
       "group_brand                  - (-)    0.188*** (0.00097)  \n",
       "group_home                   - (-)   -0.392*** (0.00119)  \n",
       "group_cla                    - (-)   -0.097*** (0.00142)  "
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_index = pr_index + 1\n",
    "\n",
    "# arrange lgoit columns\n",
    "Logit_nest = ['-' for i in np.arange(G)]\n",
    "\n",
    "Logitbeta_show = [*(np.round(Logit_beta[:beta_index], decimals = 3).astype('str')), *Logit_nest]\n",
    "Logitse_show = [*(np.round(Logit_SE[:beta_index], decimals=5).astype('str')), *Logit_nest]\n",
    "Logitp_show = [*Logit_p[:beta_index], *[1 for i in np.arange(G)]]\n",
    "\n",
    "LogitBLPbeta_show =[*(np.round(LogitBLP_beta[:beta_index], decimals = 3).astype('str')), *Logit_nest]\n",
    "LogitBLPse_show = [*(np.round(LogitBLP_SE[:beta_index], decimals=5).astype('str')), *Logit_nest]\n",
    "LogitBLPp_show = [*LogitBLP_p[:beta_index], *[1 for i in np.arange(G)]]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Logit_MLE' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip(Logitp_show, Logitbeta_show, Logitse_show)],\n",
    "    'Similarity_MLE': [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*Similarity_p[:beta_index], *Similarity_p[K:]], [*(np.round(Similarity_theta[:beta_index], decimals = 3).astype('str')), *(np.round(Similarity_theta[K:], decimals = 3).astype('str'))], [*(np.round(Similarity_SE[:beta_index], decimals=5).astype('str')), *(np.round(Similarity_SE[K:], decimals=5).astype('str'))])],\n",
    "    'FKN' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*FKN_p[:beta_index], *FKN_p[K:]], [*(np.round(FKN_theta[:beta_index], decimals = 3).astype('str')), *(np.round(FKN_theta[K:], decimals = 3).astype('str'))], [*(np.round(FKN_SE[:beta_index], decimals=5).astype('str')), *(np.round(FKN_SE[K:], decimals=5).astype('str'))])],\n",
    "    'Logit_BLP': [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip(LogitBLPp_show, LogitBLPbeta_show, LogitBLPse_show)],\n",
    "    'Similarity_BLP' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*OptBLP_p[:beta_index], *OptBLP_p[K:]], [*(np.round(ThetaOptBLP[:beta_index], decimals = 3).astype('str')), *(np.round(ThetaOptBLP[K:], decimals = 3).astype('str'))], [*(np.round(SEOptBLP[:beta_index], decimals=5).astype('str')), *(np.round(SEOptBLP[K:], decimals=5).astype('str'))])]\n",
    "}, \n",
    "index = [*x_vars[:beta_index], *['group_' + par for par in nest_vars]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
