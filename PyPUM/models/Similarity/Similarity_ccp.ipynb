{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Model choice probabilities\n",
    "\n",
    "This notebook introduces computational methods for calculating choice probabilities in the Similarity Model of Fosgerau & Nielsen (2023). the methods will be illustrated using publically available data on the European car market from Frank Verboven's website at https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/car_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\models\\Similarity\\Similarity_book.ipynb Cell 4\u001b[0m line \u001b[0;36m<cell line: 19>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m module_path \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m sys\u001b[39m.\u001b[39mpath:\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(module_path)\n",
      "\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutilities\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mLogit_file\u001b[39;00m \u001b[39mimport\u001b[39;00m estimate_logit, logit_se, logit_t_p, q_logit, logit_score, logit_score_unweighted, logit_ccp, LogitBLP_estimator, LogitBLP_se\n",
      "\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mEurocarsdata_file\u001b[39;00m \u001b[39mimport\u001b[39;00m Eurocars_cleandata\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\utilities\\Logit_file.py:67\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[0;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mitertools\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39miter\u001b[39;00m\n",
      "\u001b[0;32m     21\u001b[0m \u001b[39m# %% [markdown]\u001b[39;00m\n",
      "\u001b[0;32m     22\u001b[0m \u001b[39m# Data\u001b[39;00m\n",
      "\u001b[0;32m     23\u001b[0m \u001b[39m# ====\u001b[39;00m\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m     65\u001b[0m \u001b[39m# %%\u001b[39;00m\n",
      "\u001b[0;32m     66\u001b[0m \u001b[39m# Load dataset and variable names\u001b[39;00m\n",
      "\u001b[1;32m---> 67\u001b[0m dat \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m../data/car_data.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[0;32m     68\u001b[0m lab \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/car_labels.csv\u001b[39m\u001b[39m'\u001b[39m, index_col \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mvariable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;32m     70\u001b[0m \u001b[39m# %%\u001b[39;00m\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n",
      "\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n",
      "\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n",
      "\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n",
      "\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n",
      "\u001b[0;32m    310\u001b[0m     )\n",
      "\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n",
      "\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n",
      "\u001b[0;32m    666\u001b[0m     dialect,\n",
      "\u001b[0;32m    667\u001b[0m     delimiter,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n",
      "\u001b[0;32m    677\u001b[0m )\n",
      "\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n",
      "\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n",
      "\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n",
      "\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n",
      "\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n",
      "\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n",
      "\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n",
      "\u001b[0;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n",
      "\u001b[0;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[0;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n",
      "\u001b[0;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n",
      "\u001b[0;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n",
      "\u001b[1;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n",
      "\u001b[0;32m   1219\u001b[0m     f,\n",
      "\u001b[0;32m   1220\u001b[0m     mode,\n",
      "\u001b[0;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n",
      "\u001b[0;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n",
      "\u001b[0;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n",
      "\u001b[0;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n",
      "\u001b[0;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n",
      "\u001b[0;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n",
      "\u001b[0;32m   1227\u001b[0m )\n",
      "\u001b[0;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n",
      "\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n",
      "\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n",
      "\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n",
      "\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n",
      "\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n",
      "\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n",
      "\u001b[0;32m    787\u001b[0m             handle,\n",
      "\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n",
      "\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n",
      "\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n",
      "\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m    792\u001b[0m         )\n",
      "\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n",
      "\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/car_data.csv'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "import sys\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Files\n",
    "module_path = os.path.abspath(os.path.join('....'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utilities.Logit_file import estimate_logit, logit_se, logit_t_p, q_logit, logit_score, logit_score_unweighted, logit_ccp, LogitBLP_estimator, LogitBLP_se\n",
    "from data.Eurocarsdata_file import Eurocars_cleandata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/eurocars.dta'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)\n",
      "\u001b[1;32mc:\\Users\\mgq977\\OneDrive - University of Copenhagen\\Desktop\\Practice\\GREENCAR_notebooks\\PyPUM\\models\\Similarity\\Similarity_book.ipynb Cell 5\u001b[0m line \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load dataset and variable names\u001b[39;00m\n",
      "\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m descr \u001b[39m=\u001b[39m (pd\u001b[39m.\u001b[39;49mread_stata(\u001b[39m'\u001b[39;49m\u001b[39m../data/eurocars.dta\u001b[39;49m\u001b[39m'\u001b[39;49m, iterator \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m))\u001b[39m.\u001b[39mvariable_labels() \u001b[39m# Obtain variable descriptions\u001b[39;00m\n",
      "\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mgq977/OneDrive%20-%20University%20of%20Copenhagen/Desktop/Practice/GREENCAR_notebooks/PyPUM/models/Similarity/Similarity_book.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dat_file \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39m../data/eurocars.csv\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\stata.py:2004\u001b[0m, in \u001b[0;36mread_stata\u001b[1;34m(filepath_or_buffer, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals, chunksize, iterator, compression, storage_options)\u001b[0m\n",
      "\u001b[0;32m   1988\u001b[0m \u001b[39m@Appender\u001b[39m(_read_stata_doc)\n",
      "\u001b[0;32m   1989\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_stata\u001b[39m(\n",
      "\u001b[0;32m   1990\u001b[0m     filepath_or_buffer: FilePath \u001b[39m|\u001b[39m ReadBuffer[\u001b[39mbytes\u001b[39m],\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m   2001\u001b[0m     storage_options: StorageOptions \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n",
      "\u001b[0;32m   2002\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m StataReader:\n",
      "\u001b[1;32m-> 2004\u001b[0m     reader \u001b[39m=\u001b[39m StataReader(\n",
      "\u001b[0;32m   2005\u001b[0m         filepath_or_buffer,\n",
      "\u001b[0;32m   2006\u001b[0m         convert_dates\u001b[39m=\u001b[39;49mconvert_dates,\n",
      "\u001b[0;32m   2007\u001b[0m         convert_categoricals\u001b[39m=\u001b[39;49mconvert_categoricals,\n",
      "\u001b[0;32m   2008\u001b[0m         index_col\u001b[39m=\u001b[39;49mindex_col,\n",
      "\u001b[0;32m   2009\u001b[0m         convert_missing\u001b[39m=\u001b[39;49mconvert_missing,\n",
      "\u001b[0;32m   2010\u001b[0m         preserve_dtypes\u001b[39m=\u001b[39;49mpreserve_dtypes,\n",
      "\u001b[0;32m   2011\u001b[0m         columns\u001b[39m=\u001b[39;49mcolumns,\n",
      "\u001b[0;32m   2012\u001b[0m         order_categoricals\u001b[39m=\u001b[39;49morder_categoricals,\n",
      "\u001b[0;32m   2013\u001b[0m         chunksize\u001b[39m=\u001b[39;49mchunksize,\n",
      "\u001b[0;32m   2014\u001b[0m         storage_options\u001b[39m=\u001b[39;49mstorage_options,\n",
      "\u001b[0;32m   2015\u001b[0m         compression\u001b[39m=\u001b[39;49mcompression,\n",
      "\u001b[0;32m   2016\u001b[0m     )\n",
      "\u001b[0;32m   2018\u001b[0m     \u001b[39mif\u001b[39;00m iterator \u001b[39mor\u001b[39;00m chunksize:\n",
      "\u001b[0;32m   2019\u001b[0m         \u001b[39mreturn\u001b[39;00m reader\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\stata.py:1151\u001b[0m, in \u001b[0;36mStataReader.__init__\u001b[1;34m(self, path_or_buf, convert_dates, convert_categoricals, index_col, convert_missing, preserve_dtypes, columns, order_categoricals, chunksize, compression, storage_options)\u001b[0m\n",
      "\u001b[0;32m   1148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lines_read \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "\u001b[0;32m   1150\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_native_byteorder \u001b[39m=\u001b[39m _set_endianness(sys\u001b[39m.\u001b[39mbyteorder)\n",
      "\u001b[1;32m-> 1151\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n",
      "\u001b[0;32m   1152\u001b[0m     path_or_buf,\n",
      "\u001b[0;32m   1153\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n",
      "\u001b[0;32m   1154\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n",
      "\u001b[0;32m   1155\u001b[0m     is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n",
      "\u001b[0;32m   1156\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n",
      "\u001b[0;32m   1157\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n",
      "\u001b[0;32m   1158\u001b[0m     \u001b[39m# Copy to BytesIO, and ensure no encoding\u001b[39;00m\n",
      "\u001b[0;32m   1159\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpath_or_buf \u001b[39m=\u001b[39m BytesIO(handles\u001b[39m.\u001b[39mhandle\u001b[39m.\u001b[39mread())\n",
      "\u001b[0;32m   1161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_header()\n",
      "\n",
      "File \u001b[1;32mc:\\Users\\mgq977\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py:795\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n",
      "\u001b[0;32m    786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n",
      "\u001b[0;32m    787\u001b[0m             handle,\n",
      "\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n",
      "\u001b[1;32m   (...)\u001b[0m\n",
      "\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n",
      "\u001b[0;32m    792\u001b[0m         )\n",
      "\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n",
      "\u001b[1;32m--> 795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n",
      "\u001b[0;32m    796\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n",
      "\u001b[0;32m    798\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/eurocars.dta'"
     ]
    }
   ],
   "source": [
    "# Load dataset and variable names\n",
    "descr = (pd.read_stata('../data/eurocars.dta', iterator = True)).variable_labels() # Obtain variable descriptions\n",
    "dat_file = pd.read_csv('../data/eurocars.csv') # reads in the data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              cy            cylinder volume or displacement (in cc)\n",
       "1              hp                                 horsepower (in kW)\n",
       "2              we                                     weight (in kg)\n",
       "3              le                                     length (in cm)\n",
       "4              wi                                      width (in cm)\n",
       "5              he                                     height (in cm)\n",
       "6              li          average of li1, li2, li3 (used in papers)\n",
       "7              sp                            maximum speed (km/hour)\n",
       "8              ac  time to acceleration (in seconds from 0 to 100...\n",
       "9              pr   price (in destination currency including V.A.T.)\n",
       "10          brand                                      name of brand\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            cla                              class or segment code"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Outside option is included if OO == True, otherwise analysis is done on the inside options only.\n",
    "OO = True\n",
    "\n",
    "# Choose which variables to include in the analysis, and assign them either as discrete variables or continuous.\n",
    "\n",
    "x_discretevars = [ 'brand', 'home', 'cla']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "z_IV_contvars = ['xexr']\n",
    "z_IV_discretevars = []\n",
    "x_allvars =  [*x_contvars, *x_discretevars]\n",
    "z_allvars = [*z_IV_contvars, *z_IV_discretevars]\n",
    "\n",
    "if OO:\n",
    "    nest_contvars = [var for var in x_contvars if var != 'pr'] # We nest over all variables other than price, but an alternative list can be specified here if desired.\n",
    "    nest_discvars = ['in_out', *x_discretevars]\n",
    "    nest_vars = ['in_out', *nest_contvars, *x_discretevars]\n",
    "else:\n",
    "    nest_contvars = [var for var in x_contvars if (var != 'pr')]\n",
    "    nest_discvars = x_discretevars # See above\n",
    "    nest_vars = [*nest_contvars, *nest_discvars]\n",
    "\n",
    "G = len(nest_vars)\n",
    "\n",
    "# Print list of chosen variables as a dataframe\n",
    "pd.DataFrame(descr, index=['description'])[x_allvars].transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, dat_org, x_vars, z_vars, N, pop_share, T, J, K = Eurocars_cleandata(dat_file, x_contvars, x_discretevars, z_IV_contvars, z_IV_discretevars, outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of numpy arrays for each market. This allows the size of the data set to vary over markets.\n",
    "\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)} # Dict of explanatory variables\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)} # Dict of market shares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "Suppose we are evaluating the choice probability function $P_t(u|\\theta) = \\arg\\max_{q\\in \\Delta} q'u(\\theta)-\\Omega(q|\\theta)$, where $\\Omega$ is a Similarity Perturbation Function as in ..., at some parameter vector $\\theta$. While it is possible to solve for the choice probabilities explicitly by numerical maximization, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Let $u_t = X_t\\beta$ for some parameter vector $\\beta \\in \\mathbb{R}^{K}$, such that $\\theta = (\\beta', \\lambda')'$, and let $q_t^0$ be an initial guess of the choice probabilities, e.g. $q_t^0\\propto \\exp(X_t\\beta)$. Define further\n",
    "\n",
    "$$\n",
    "a=\\sum_{g:\\lambda_g\\geq 0} \\lambda_g   \\qquad b=\\sum_{g:\\lambda_g<0} |\\lambda_g|.\n",
    "$$\n",
    "\n",
    "The choice probabilities are then updated iteratively as\n",
    "$$\n",
    "q_t^{r} = \\frac{e^{v_t^{r}}}{\\sum_{j\\in \\mathcal J_t} e^{v_{tj}^{r}}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "v_t^{r} =\\ln q_t^{r-1}+\\left(u_t-\\nabla_q \\Omega_t(q^{r-1}_t|\\lambda)\\right)/(1+b).\n",
    "$$\n",
    "The gradient $\\nabla_q \\Omega_t(q_t|\\lambda)$ is easily computed using the formula\n",
    "$$\n",
    "\\nabla_q \\Omega_t(q_t|\\lambda) = \\Gamma' \\ln (\\Psi q_t) - \\delta + \\iota_{J_t}\n",
    "$$\n",
    "For numerical stability, it can be a good idea to also do max-rescaling of $v^r_t$ at every iteration. The Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p_t(\\theta)||q_t^{r})\\leq \\frac{a+b}{1+b}D_{KL}(p_t(\\theta)||q^{r-1}_t).\n",
    "$$\n",
    "This is implemented in the function \"Similarity_ccp\" below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_ccp(Theta, x, model, tol = 1.0e-15, maximum_iterations = 1000):\n",
    "    '''\n",
    "    This function finds approximations to the true conditional choice probabilities given parameters.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        tol: tolerated approximation error\n",
    "        maximum_iterations: a no. of maximum iterations which if reached will stop the algorithm\n",
    "\n",
    "    Output\n",
    "        q_1: a dictionary of T numpy arrays (J[t],) of Similarity choice probabilities for each market t\n",
    "    '''\n",
    "\n",
    "    # Objects in model specification\n",
    "    psi = model['psi']\n",
    "    phi = model['phi']\n",
    "\n",
    "    T = len(x) # Number of markets\n",
    "    K = x[0].shape[1] # Number of car characteristics\n",
    "\n",
    "    # Parameters\n",
    "    Beta = Theta[:K]\n",
    "    Lambda = Theta[K:]\n",
    "    G = len(Lambda)  # Number of groups\n",
    "\n",
    "    # Calculate small b\n",
    "    C_minus = np.array([True if Lambda[g] < 0 else False for g in np.arange(G)])\n",
    "    if C_minus.all() == False:\n",
    "        b = 0\n",
    "    else:    \n",
    "        b = np.abs(Lambda[C_minus]).sum() # sum of absolute value of negative lambda parameters.\n",
    "\n",
    "    # Find the Gamma matrix\n",
    "    Gamma = Create_Gamma(Lambda, model)\n",
    "\n",
    "    u = {t: np.einsum('jk,k->j', x[t], Beta) for t in np.arange(T)} # Calculate linear utilities\n",
    "    q = {t: np.exp(u[t] - u[t].max()) / np.exp(u[t] - u[t].max()).sum() for t in np.arange(T)}\n",
    "    q0 = q\n",
    "    Epsilon = 1.0e-10\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        q1 = {}\n",
    "        for t in np.arange(T):\n",
    "            # Calculate v\n",
    "            psi_q = psi[t] @ q0[t] # Compute matrix product\n",
    "            log_psiq =  np.log(np.abs(psi_q) + Epsilon) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "            delta = phi[t]@Lambda\n",
    "            Grad = (Gamma[t].T @ log_psiq) - delta # Compute matrix product\n",
    "            v = np.log(q0[t] + Epsilon) + (u[t] - Grad)/(1 + b) # Calculate v = log(q) + (u - Gamma^T %o% log(Gamma %o% q) - delta)/(1 + b)\n",
    "            v -= v.max(keepdims = True) # Do max rescaling wrt. alternatives\n",
    "\n",
    "            # Calculate iterated ccp q^k\n",
    "            numerator = np.exp(v)\n",
    "            denom = numerator.sum()\n",
    "            q1[t] = numerator/denom\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.array([np.sum((q1[t]-q0[t])**2/q[t].std()) for t in np.arange(T)])) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "        \n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0 = estimate_logit(q_logit, np.zeros((K,)), y, x, pop_share)['beta']\n",
    "lambda0 = np.zeros((G,))\n",
    "theta0 = np.append(beta0, lambda0)\n",
    "q0 = Similarity_ccp(theta0, x, Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice probability distribution \n",
    "\n",
    "We may plot the distribution of choice probabilities ordered according to price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
