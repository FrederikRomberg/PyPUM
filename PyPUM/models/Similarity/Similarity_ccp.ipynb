{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity Model choice probabilities\n",
    "\n",
    "This notebook introduces computational methods for calculating choice probabilities in the Similarity Model of Fosgerau & Nielsen (2023). the methods will be illustrated using publically available data on the European car market from Frank Verboven's website at https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "Suppose we are evaluating the choice probability function $P_t(u|\\theta) = \\arg\\max_{q\\in \\Delta} q'u(\\theta)-\\Omega(q|\\theta)$, where $\\Omega$ is a Similarity Perturbation Function as in ..., at some parameter vector $\\theta$. While it is possible to solve for the choice probabilities explicitly by numerical maximization, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Let $u_t = X_t\\beta$ for some parameter vector $\\beta \\in \\mathbb{R}^{K}$, such that $\\theta = (\\beta', \\lambda')'$, and let $q_t^0$ be an initial guess of the choice probabilities, e.g. $q_t^0\\propto \\exp(X_t\\beta)$. Define further\n",
    "\n",
    "$$\n",
    "a=\\sum_{g:\\lambda_g\\geq 0} \\lambda_g   \\qquad b=\\sum_{g:\\lambda_g<0} |\\lambda_g|.\n",
    "$$\n",
    "\n",
    "The choice probabilities are then updated iteratively as\n",
    "$$\n",
    "q_t^{r} = \\frac{e^{v_t^{r}}}{\\sum_{j\\in \\mathcal J_t} e^{v_{tj}^{r}}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "v_t^{r} =\\ln q_t^{r-1}+\\left(u_t-\\nabla_q \\Omega_t(q^{r-1}_t|\\lambda)\\right)/(1+b).\n",
    "$$\n",
    "The gradient $\\nabla_q \\Omega_t(q_t|\\lambda)$ is easily computed using the formula\n",
    "$$\n",
    "\\nabla_q \\Omega_t(q_t|\\lambda) = \\Gamma' \\ln (\\Psi q_t) - \\delta + \\iota_{J_t}\n",
    "$$\n",
    "For numerical stability, it can be a good idea to also do max-rescaling of $v^r_t$ at every iteration. The Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p_t(\\theta)||q_t^{r})\\leq \\frac{a+b}{1+b}D_{KL}(p_t(\\theta)||q^{r-1}_t).\n",
    "$$\n",
    "This is implemented in the function \"Similarity_ccp\" below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_ccp(Theta, x, model, tol = 1.0e-15, maximum_iterations = 1000):\n",
    "    '''\n",
    "    This function finds approximations to the true conditional choice probabilities given parameters.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        tol: tolerated approximation error\n",
    "        maximum_iterations: a no. of maximum iterations which if reached will stop the algorithm\n",
    "\n",
    "    Output\n",
    "        q_1: a dictionary of T numpy arrays (J[t],) of Similarity choice probabilities for each market t\n",
    "    '''\n",
    "\n",
    "    # Objects in model specification\n",
    "    psi = model['psi']\n",
    "    phi = model['phi']\n",
    "\n",
    "    T = len(x) # Number of markets\n",
    "    K = x[0].shape[1] # Number of car characteristics\n",
    "\n",
    "    # Parameters\n",
    "    Beta = Theta[:K]\n",
    "    Lambda = Theta[K:]\n",
    "    G = len(Lambda)  # Number of groups\n",
    "\n",
    "    # Calculate small b\n",
    "    C_minus = np.array([True if Lambda[g] < 0 else False for g in np.arange(G)])\n",
    "    if C_minus.all() == False:\n",
    "        b = 0\n",
    "    else:    \n",
    "        b = np.abs(Lambda[C_minus]).sum() # sum of absolute value of negative lambda parameters.\n",
    "\n",
    "    # Find the Gamma matrix\n",
    "    Gamma = Create_Gamma(Lambda, model)\n",
    "\n",
    "    u = {t: np.einsum('jk,k->j', x[t], Beta) for t in np.arange(T)} # Calculate linear utilities\n",
    "    q = {t: np.exp(u[t] - u[t].max()) / np.exp(u[t] - u[t].max()).sum() for t in np.arange(T)}\n",
    "    q0 = q\n",
    "    Epsilon = 1.0e-10\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        q1 = {}\n",
    "        for t in np.arange(T):\n",
    "            # Calculate v\n",
    "            psi_q = psi[t] @ q0[t] # Compute matrix product\n",
    "            log_psiq =  np.log(np.abs(psi_q) + Epsilon) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "            delta = phi[t]@Lambda\n",
    "            Grad = (Gamma[t].T @ log_psiq) - delta # Compute matrix product\n",
    "            v = np.log(q0[t] + Epsilon) + (u[t] - Grad)/(1 + b) # Calculate v = log(q) + (u - Gamma^T %o% log(Gamma %o% q) - delta)/(1 + b)\n",
    "            v -= v.max(keepdims = True) # Do max rescaling wrt. alternatives\n",
    "\n",
    "            # Calculate iterated ccp q^k\n",
    "            numerator = np.exp(v)\n",
    "            denom = numerator.sum()\n",
    "            q1[t] = numerator/denom\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.array([np.sum((q1[t]-q0[t])**2/q[t].std()) for t in np.arange(T)])) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "        \n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta0 = estimate_logit(q_logit, np.zeros((K,)), y, x, pop_share)['beta']\n",
    "lambda0 = np.zeros((G,))\n",
    "theta0 = np.append(beta0, lambda0)\n",
    "q0 = Similarity_ccp(theta0, x, Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice probability distribution \n",
    "\n",
    "We may plot the distribution of choice probabilities ordered according to price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
