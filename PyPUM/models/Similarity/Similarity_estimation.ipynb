{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation of the Similarity Model\n",
    "\n",
    "This notebook discusses two estimation methods of parameters for the Similarity model: the Maximimum Likelihood Estimator and the 'FKN' Estimator of Fosgerau et al. (2023). We implement both of these on the publically available data on the European car market from Frank Verboven's website at https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Maximum Likelihood Estimation of Similarity\n",
    "\n",
    "Suppose we want to estimate the parameters $\\theta = (\\beta', \\lambda')'$, where $\\beta$ is a vector of characteristics parameters and $\\lambda$ is a vector of nesting parameters. The log-likelihood contribution of market $t$ in the Similarity Model is then:\n",
    "$$\n",
    "\\ell_t(\\theta)=y_t'\\ln P_t(u_t|\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_t$ and $\\theta$ - calculates $u_t=\\mathbf{X}_t\\beta$ and then calls the fixed point routine described in .... That routine will return the choice probabilities $P_t(u_t|\\theta)$, and we can then evaluate $\\ell_t(\\theta)$.\n",
    "\n",
    "In addition, when maximizing the likelihood we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_t= P_t(u_t|\\theta)$; then the dervative of choice probabilities wrt. parameters is given by,\n",
    "$$\n",
    "\\nabla_\\theta \\ln P_t(u_t|\\theta)=\\mathrm{diag}(q_t)^{-1}\\left(\\nabla_{qq}^2\\Omega_t(q_t|\\lambda)^{-1}-q_tq_t' \\right)\\left[\\mathbf{X}_t,-\\nabla_{q,\\lambda}^2 \\Omega_t(q_t|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P_t(u_t|\\theta)$ and the last term is a block matrix of size $J_t\\times (K+G)$. The latter cross derivative of the Similarity Pertubation Function (see e.g. ...) $\\nabla_{q,\\lambda}^2 \\Omega_t(q_t|\\lambda)$ may be computed using the identity $\\nabla_{q,\\lambda}^2 \\Omega_t(q_t|\\lambda)_g = -\\ln(q) + (\\Psi^g)' \\ln(\\Psi^g q) - \\varphi^g$ for each row $g=1,\\ldots,G$. The derivative of the log-likelihood function can then be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_t(\\theta)= y_t' \\left(\\nabla_\\theta \\ln P_t(u_t|\\theta)\\right) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pertubation_hessian(q, x, Theta, model):\n",
    "    '''\n",
    "    This function calucates the hessian of the pertubation function \\Omega\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    \n",
    "    Returns\n",
    "        Hess: a dictionary of T numpy arrays (J[t],J[t]) of second partial derivatives of the pertubation function \\Omega for each market t\n",
    "    '''\n",
    "    psi = model['psi']\n",
    "    T = len(q.keys())\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    Gamma = Create_Gamma(Theta[K:], model) # Find the \\Gamma matrices \n",
    "    \n",
    "    Hess={}\n",
    "    for t in np.arange(T):\n",
    "        psi_q = np.einsum('cj,j->c', psi[t], q[t]) # Compute a matrix product\n",
    "        Hess[t] = np.einsum('cj,c,cl->jl', Gamma[t], 1/psi_q, psi[t], optimize=True) # Computes the product \\Gamma' diag(\\psi q)^{-1} \\psi (but faster)\n",
    "        \n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_gradient(q, x, Theta, model):\n",
    "    \n",
    "    '''\n",
    "    This function calucates the gradient of the choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    \n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K) of partial derivatives of the choice proabilities wrt. utilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Grad = {}\n",
    "    Hess = compute_pertubation_hessian(q, x, Theta, model) # Compute the hessian of the pertubation function\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        inv_omega_hess = la.inv(Hess[t]) # (J,J) for each t=1,...,T , computes the inverse of the Hessian\n",
    "        qqT = q[t][:,None]*q[t][None,:] # (J,J) outerproduct of ccp's for each market t\n",
    "        Grad[t] = inv_omega_hess - qqT  # Compute Similarity gradient of ccp's wrt. utilities\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_u_grad_Log_ccp(q, x, Theta, model):\n",
    "    '''\n",
    "    This function calucates the gradient of the log choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    \n",
    "    Returns\n",
    "        Epsilon: a dictionary of T numpy arrays (J[t],J[t]) of partial derivatives of the log choice proabilities of products j wrt. utilites of products k for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = ccp_gradient(q, x, Theta, model) # Find the gradient of ccp's wrt. utilities\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = Grad[t]/q[t][:,None] # Computes diag(q)^{-1}Grad[t]\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_elasticity(q, x, Theta, model, char_number = K-1):\n",
    "    ''' \n",
    "    This function calculates the elasticity of choice probabilities wrt. any characteristic or nest grouping of products\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        char_number: an integer which is an index of the parameter in theta wrt. which we wish calculate the elasticity. Default is the index for the parameter of 'pr'.\n",
    "\n",
    "    Returns\n",
    "        a dictionary of T numpy arrays (J[t],J[t]) of choice probability semi-elasticities for each market t\n",
    "    '''\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = Similarity_u_grad_Log_ccp(q, x, Theta, model) # Find the gradient of log ccp's wrt. utilities\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = Grad[t]*Theta[char_number] # Calculate semi-elasticities\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_loglikelihood(Theta, y, x, sample_share, model):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (T,) of Similarity loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "    ccp_hat = Similarity_ccp(Theta, x, model) # Find Similarity choice probabilities\n",
    "    sum_lambdaplus = np.array([theta for theta in Theta[K:] if theta >0]).sum() # Get sum of positive Lambda's\n",
    "    \n",
    "    ll=np.empty((T,))\n",
    "    for t in np.arange(T):\n",
    "        ll[t] = sample_share[t]*(y[t].T@np.log(ccp_hat[t])) # np.einsum('j,j', y[t], np.log(ccp_hat[t], out = -np.inf*np.ones_like(ccp_hat[t]), where = (ccp_hat[t] > 0)))\n",
    "\n",
    "    print([sum_lambdaplus, -ll.mean()])\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_Similarity(Theta, y, x, sample_share, model):\n",
    "    ''' The negative loglikelihood criterion to minimize\n",
    "    '''\n",
    "    Q = -Similarity_loglikelihood(Theta, y, x, sample_share, model)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the derivative of the loglikehood wrt. parameters $\\nabla_\\theta \\ell_t(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation(q, model):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    # Get psidim object from the model specification\n",
    "    psidim = model['psi_3d']\n",
    "    phi = model['phi']\n",
    "\n",
    "    # Get the amount of markets\n",
    "    T = len(q.keys())\n",
    "\n",
    "    # Initialize Z; the cross differential of the pertubation function\n",
    "    Z = {}\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        log_q = log_q = np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0))\n",
    "        psidim_t = psidim[t][1:,:,:] # Get each of the Psi^g nesting matrices\n",
    "        psiq = psidim_t @ q[t] # Computes a matrix product\n",
    "        log_psiq = np.log(psiq, out = np.NINF*np.ones_like(psiq), where = (psiq > 0))\n",
    "        Z[t] = - log_q[:,None] + np.einsum('dkj,dk->jd', psidim_t, log_psiq) - phi[t] # Compute cross differential\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation_old(q, psi, phi):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        psi: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    J = np.array([q[t].shape[0] for t in np.arange(T)])\n",
    "    G = np.int32((psi[0].shape[0] / J[0]) - 1)\n",
    "    \n",
    "    Z = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        log_q = np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0))\n",
    "        Psi_t = psi[t]\n",
    "        Z_t = np.empty((J[t], G))\n",
    "\n",
    "        for g in np.arange(1,G+1):\n",
    "            Psi_d = Psi_t[g*J[t]:(g+1)*J[t],:]\n",
    "            Psiq = np.einsum('cj,j->c', Psi_d, q[t])\n",
    "            log_psiq = np.log(Psiq, out = -np.inf*np.ones_like(Psiq), where = (Psiq > 0))\n",
    "            Z_t[:,g-1] = -log_q + np.einsum('cj,c->j', Psi_d, log_psiq) - phi[t][:,g-1]\n",
    "\n",
    "        Z[t] = Z_t\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_theta_grad_log_ccp(Theta, x, model):\n",
    "    '''\n",
    "    This function calculates the derivative of the Similarity log ccp's wrt. parameters theta\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K+G) of derivatives of the Similarity log ccp's wrt. parameters theta for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "\n",
    "    q = Similarity_ccp(Theta, x, model) # Find choice probabilities\n",
    "\n",
    "    Z = cross_grad_pertubation(q, model) # Find cross differentials of the pertubation function\n",
    "    u_grad = Similarity_u_grad_Log_ccp(q, x, Theta, model)  # Find the gradient of log ccp's wrt. utilities\n",
    "    Grad={}\n",
    "\n",
    "    for t in range(T):\n",
    "        G = np.concatenate((x[t], -Z[t]), axis = 1) # Compute the block matrix\n",
    "        Grad[t] = u_grad[t] @ G # Compute the derivative wrt. parameters\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_score(Theta, y, x, sample_share, model):\n",
    "    '''\n",
    "    This function calculates the score of the Similarity loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of Similarity scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = Similarity_theta_grad_log_ccp(Theta, x, model) # Find derivatives of Similarity log ccp's wrt. parameters theta\n",
    "    D = len(Theta) # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] =sample_share[t]*(log_ccp_grad[t].T@y[t]) #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_score_unweighted(Theta, y, x, model):\n",
    "    '''\n",
    "    This function calculates the score of the Similarity loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of Similarity scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = Similarity_theta_grad_log_ccp(Theta, x, model) # Find derivatives of Similarity log ccp's wrt. parameters theta\n",
    "    D = log_ccp_grad[0].shape[1] # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] = log_ccp_grad[t].T@y[t] #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_Similarity_score(Theta, y, x, sample_share, model):\n",
    "    ''' The derivative of the negative loglikelihood criterion\n",
    "    '''\n",
    "    return -Similarity_score(Theta, y, x, sample_share, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the score function\n",
    "\n",
    "To make sure that our optimization procedure works as intended and is precise, we may calculate the normed difference of the numerical and our analytical gradients of the likelihood function $\\ell_t(\\theta)$ at some parameter $\\hat \\theta^0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analyticgrad(y, x, theta, sample_share, model, delta = 1.0e-8):\n",
    "    ''' \n",
    "    This function calculates the numerical and the analytical score functions at a given parameter \\theta aswell the norm of their difference\n",
    "\n",
    "    Args:\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        delta: the incremental change in the argument, a float, used in calculating numerical gradients\n",
    "\n",
    "    Returns.\n",
    "        normdiff: a float of the euclidean norm of the difference between the numerical and analytical score functions at \\theta\n",
    "        angrad: a numpy array (T,K+G) of analytical Similarity scores\n",
    "        numgrad: a numpy array (T,K+G) of numerical Similarity scores\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "    G = len(theta[K:])\n",
    "\n",
    "    numgrad = np.empty((T, K+G))\n",
    "\n",
    "    for i in np.arange(K+G):\n",
    "        vec = np.zeros((K+G,))\n",
    "        vec[i] = 1\n",
    "        numgrad[:,i] = (Similarity_loglikelihood(theta + delta*vec, y, x, sample_share, model) - Similarity_loglikelihood(theta, y, x, sample_share, model)) / delta\n",
    "\n",
    "    angrad = Similarity_score(theta, y, x, sample_share, model)\n",
    "\n",
    "    normdiff = la.norm(angrad - numgrad)\n",
    "    \n",
    "    return normdiff, angrad, normdiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "diff, an, num = test_analyticgrad(y, x, theta0, pop_share, Model)\n",
    "diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors in Maximum Likelihood estimation\n",
    "\n",
    "As usual we may consistently estimate the covariance matrix of the Similarity maximum likelihood estimator $\\hat \\theta^{\\text{MLE}}$ by the inverse information matrix:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\left( \\sum_{t=1}^T s_t\\nabla_\\theta \\ell_t \\left(\\hat \\theta^{\\text{MLE}}\\right) \\nabla_\\theta \\ell_t \\left(\\hat \\theta^{\\text{MLE}}\\right)' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Where $s = (s_1, \\ldots, s_T)'$ is a vector of the fractions of total observations observed in each market $t$. Accordingly we may find the estimated standard error of parameter $d = 1,\\ldots,K+G$ as the squareroot of the $d$'th diagonal entry of $\\hat \\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_d = \\sqrt{\\hat \\Sigma_{dd}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_se(score, sample_share, N):\n",
    "    '''\n",
    "    This function computes the asymptotic standard errors of the MLE.\n",
    "\n",
    "    Args.\n",
    "        score: a numpy array (T,K+G) of Similarity scores as outputted by the function 'Similarity_score_unweighted'.\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns\n",
    "        SE: a numpy array (K+G,) of asymptotic Similarity MLE standard errors\n",
    "    '''\n",
    "\n",
    "    SE = np.sqrt(np.diag(la.inv(np.einsum('td,tm->dm', sample_share[:,None]*score, score))) / N) # Compute standard errors by taking the squareroot of the diagonal elements of the variance estimate\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        SE: a numpy array (K+G,) of asymptotic Similarity MLE standard errors\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        N: an integer giving the number of observations\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated asymptotic p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE # Compute two-sided t-tests\n",
    "    p = 2*scstat.t.sf(T, df = N-1) # Compute p-values\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_Similarity(f, Theta0, y, x, sample_share, model, N, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given starting values and variables necessary in the Similarity Model specification.\n",
    "\n",
    "    Args:\n",
    "        f: a function to minimize,\n",
    "        Theta0 : a numpy array (K+G,) of initial guess parameters (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations,\n",
    "        Analytic_jac: a boolean. Default value is 'True'. If 'True' the analytic jacobian of the Similarity loglikelihood function is used in estimation. Else the numerical jacobian is used.\n",
    "        options: dictionary with options for the optimizer (e.g. disp=True which tells it to display information at termination.)\n",
    "    \n",
    "    Returns:\n",
    "        res: a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(f(Theta, y, x, sample_share, model))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_Similarity_score(Theta, y, x, sample_share, model), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Theta0.tolist(), options=options, jac=Grad, **kwargs) # optimize.minimize takes a list of parameters Theta0 (not a numpy array) as initial guess.\n",
    "    \n",
    "    # Find estimated standard errors, t-tests, and p-values\n",
    "    se = Similarity_se(Similarity_score_unweighted(result.x, y, x, model), sample_share, N) # Calculate standard errors using the unweighted score contributions from each market\n",
    "    T,p = Similarity_t_p(se, result.x, N)\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'theta': result.x,\n",
    "        'se': se,\n",
    "        't': T,\n",
    "        'p': p,\n",
    "        'success':  result.success, # bool, whether convergence was succesful 1\n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then estimate the model using the corresponding MLE Logit parameters $\\hat \\beta^{\\text{Logit}}$ and nesting parameters $\\hat \\lambda^0 = (0,\\ldots,0)'$ as initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001529\n",
      "         Iterations: 26\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.ones((K,))\n",
    "\n",
    "# Estimate the model\n",
    "Logit_beta = estimate_logit(q_logit, beta_0, y, x, sample_share=pop_share, Analytic_jac=True)['beta']\n",
    "Logit_SE = logit_se(logit_score_unweighted(Logit_beta, y, x), pop_share, N)\n",
    "Logit_t, Logit_p = logit_t_p(Logit_beta, logit_score_unweighted(Logit_beta, y, x), pop_share, N)\n",
    "\n",
    "# Initialize \\theta^0\n",
    "theta0 = np.append(Logit_beta,lambda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.001528535134161618]\n",
      "[3.7176505867959405e-06, 0.001528530844629761]\n",
      "[1.8588252933979702e-05, 0.001528514281678381]\n",
      "[7.807066232271476e-05, 0.0015284575876433064]\n",
      "[0.0002888968393993265, 0.0015283779712649036]\n",
      "[0.0003156820978933802, 0.0015283756589398688]\n",
      "[0.0004228231318695951, 0.001528366650756558]\n",
      "[0.0008514977339305401, 0.0015283344814752554]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.002552430433766674, 0.0015282508819820435]\n",
      "[0.011407411931886541, 0.001528002344870776]\n",
      "[0.0336266372510129, 0.001527432169794866]\n",
      "[0.07652390229927372, 0.0015264248802469536]\n",
      "[0.1598061804339339, 0.0015247252520192492]\n",
      "[0.3261793453534204, 0.0015224054228309036]\n",
      "[0.41714512626147443, 0.0015215190856618535]\n",
      "[0.43667551144630096, 0.0015212756051922887]\n",
      "[0.4294172691651903, 0.0015212519744114967]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001521\n",
      "         Iterations: 11\n",
      "         Function evaluations: 17\n",
      "         Gradient evaluations: 17\n"
     ]
    }
   ],
   "source": [
    "resMLE = estimate_Similarity(q_Similarity, theta0, y, x, pop_share, Model, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_table(theta, se, N, x_vars, nest_vars):\n",
    "    '''\n",
    "    This function constructs a regression table based on Similarity parameter standard error estimates\n",
    "\n",
    "    Args:\n",
    "        theta: a (K+G,) numpy array of estimated parameters\n",
    "        se: a (K+G,) numpy array of estimated standard errors\n",
    "        N: an integer; the number of observations\n",
    "        x_vars: a list containing the names of the covariates\n",
    "        nest_vars: a list containing the names of the nesting groups\n",
    "\n",
    "    Returns.\n",
    "        table: a pandas dataframe structured as a regression table w. parameter estiamtes, standard errors, t-tests, and p-values  \n",
    "    '''\n",
    "    Similarity_t, Similarity_p = Similarity_t_p(se, theta, N) # Get t-test values and p values\n",
    "\n",
    "    if OO:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]] # Set the names of the covariates and the nesting groups as the index\n",
    "    else:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]] # -=-\n",
    "\n",
    "    table  = pd.DataFrame({'theta': [ str(np.round(theta[i], decimals = 4)) + '***' if Similarity_p[i] <0.01 else str(np.round(theta[i], decimals = 3)) + '**' if Similarity_p[i] <0.05 else str(np.round(theta[i], decimals = 3)) + '*' if Similarity_p[i] <0.1 else np.round(theta[i], decimals = 3) for i in range(len(theta))], # Give stars to parameter estimates according to t-tests at levels of significance 0.1, 0.05, and 0.01\n",
    "                'se' : np.round(se, decimals = 5),\n",
    "                't (theta == 0)': np.round(Similarity_t, decimals = 3),\n",
    "                'p': np.round(Similarity_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.5498***</td>\n",
       "      <td>0.43873</td>\n",
       "      <td>5.812</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.318</td>\n",
       "      <td>0.22618</td>\n",
       "      <td>1.408</td>\n",
       "      <td>0.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.457*</td>\n",
       "      <td>0.27293</td>\n",
       "      <td>1.676</td>\n",
       "      <td>0.094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.9464***</td>\n",
       "      <td>0.22234</td>\n",
       "      <td>4.256</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.9569***</td>\n",
       "      <td>0.30143</td>\n",
       "      <td>6.492</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.0975***</td>\n",
       "      <td>0.42346</td>\n",
       "      <td>4.953</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.0849***</td>\n",
       "      <td>0.31568</td>\n",
       "      <td>6.604</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.7439***</td>\n",
       "      <td>0.11864</td>\n",
       "      <td>6.270</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.3924***</td>\n",
       "      <td>0.27032</td>\n",
       "      <td>5.151</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>-0.055</td>\n",
       "      <td>0.10572</td>\n",
       "      <td>0.522</td>\n",
       "      <td>0.601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.8436***</td>\n",
       "      <td>0.09099</td>\n",
       "      <td>9.272</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>0.9939***</td>\n",
       "      <td>0.23615</td>\n",
       "      <td>4.209</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>1.0494***</td>\n",
       "      <td>0.12909</td>\n",
       "      <td>8.130</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>0.7984***</td>\n",
       "      <td>0.11243</td>\n",
       "      <td>7.101</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>0.8222***</td>\n",
       "      <td>0.13417</td>\n",
       "      <td>6.128</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>0.7241***</td>\n",
       "      <td>0.12674</td>\n",
       "      <td>5.713</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>0.9519***</td>\n",
       "      <td>0.18761</td>\n",
       "      <td>5.074</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>0.971***</td>\n",
       "      <td>0.24506</td>\n",
       "      <td>3.962</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>0.909***</td>\n",
       "      <td>0.15489</td>\n",
       "      <td>5.868</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.8222***</td>\n",
       "      <td>0.12488</td>\n",
       "      <td>6.584</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>1.0521***</td>\n",
       "      <td>0.11617</td>\n",
       "      <td>9.056</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>0.8661***</td>\n",
       "      <td>0.16136</td>\n",
       "      <td>5.368</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>0.9185***</td>\n",
       "      <td>0.25339</td>\n",
       "      <td>3.625</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>0.9261***</td>\n",
       "      <td>0.12006</td>\n",
       "      <td>7.714</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>0.9596***</td>\n",
       "      <td>0.23619</td>\n",
       "      <td>4.063</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>0.6353***</td>\n",
       "      <td>0.12252</td>\n",
       "      <td>5.185</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>0.8342***</td>\n",
       "      <td>0.19716</td>\n",
       "      <td>4.231</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.9485***</td>\n",
       "      <td>0.15793</td>\n",
       "      <td>6.006</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>0.8852***</td>\n",
       "      <td>0.14316</td>\n",
       "      <td>6.183</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>0.906***</td>\n",
       "      <td>0.19149</td>\n",
       "      <td>4.731</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>0.9336***</td>\n",
       "      <td>0.12241</td>\n",
       "      <td>7.627</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.8577***</td>\n",
       "      <td>0.11582</td>\n",
       "      <td>7.406</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>1.0311***</td>\n",
       "      <td>0.11766</td>\n",
       "      <td>8.763</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>0.5224***</td>\n",
       "      <td>0.12264</td>\n",
       "      <td>4.260</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>0.8725***</td>\n",
       "      <td>0.23902</td>\n",
       "      <td>3.650</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>0.8056***</td>\n",
       "      <td>0.15549</td>\n",
       "      <td>5.181</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>0.999</td>\n",
       "      <td>0.98588</td>\n",
       "      <td>1.013</td>\n",
       "      <td>0.311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>0.972**</td>\n",
       "      <td>0.41495</td>\n",
       "      <td>2.342</td>\n",
       "      <td>0.019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>0.9887***</td>\n",
       "      <td>0.25101</td>\n",
       "      <td>3.939</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>0.9473***</td>\n",
       "      <td>0.15076</td>\n",
       "      <td>6.283</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>0.9933***</td>\n",
       "      <td>0.18372</td>\n",
       "      <td>5.406</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>0.9673***</td>\n",
       "      <td>0.13301</td>\n",
       "      <td>7.272</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>0.9978***</td>\n",
       "      <td>0.29060</td>\n",
       "      <td>3.433</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>0.8721***</td>\n",
       "      <td>0.10862</td>\n",
       "      <td>8.029</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>0.8744***</td>\n",
       "      <td>0.15332</td>\n",
       "      <td>5.703</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>0.8032***</td>\n",
       "      <td>0.14780</td>\n",
       "      <td>5.435</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>0.971***</td>\n",
       "      <td>0.25528</td>\n",
       "      <td>3.804</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.6015***</td>\n",
       "      <td>0.03366</td>\n",
       "      <td>47.574</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.3074***</td>\n",
       "      <td>0.02881</td>\n",
       "      <td>10.670</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.3473***</td>\n",
       "      <td>0.05590</td>\n",
       "      <td>6.213</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.07304</td>\n",
       "      <td>1.284</td>\n",
       "      <td>0.199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.4534***</td>\n",
       "      <td>0.13370</td>\n",
       "      <td>3.391</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.1073***</td>\n",
       "      <td>0.03976</td>\n",
       "      <td>2.698</td>\n",
       "      <td>0.007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.02550</td>\n",
       "      <td>0.396</td>\n",
       "      <td>0.692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.025</td>\n",
       "      <td>0.03359</td>\n",
       "      <td>0.741</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.061*</td>\n",
       "      <td>0.03187</td>\n",
       "      <td>1.902</td>\n",
       "      <td>0.057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0774***</td>\n",
       "      <td>0.02724</td>\n",
       "      <td>2.842</td>\n",
       "      <td>0.004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.1292***</td>\n",
       "      <td>0.01933</td>\n",
       "      <td>6.685</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0528***</td>\n",
       "      <td>0.01413</td>\n",
       "      <td>3.734</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.01348</td>\n",
       "      <td>0.794</td>\n",
       "      <td>0.427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0728***</td>\n",
       "      <td>0.02374</td>\n",
       "      <td>3.065</td>\n",
       "      <td>0.002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.1813***</td>\n",
       "      <td>0.01269</td>\n",
       "      <td>14.288</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.044**</td>\n",
       "      <td>0.01847</td>\n",
       "      <td>2.361</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>0.019</td>\n",
       "      <td>0.01933</td>\n",
       "      <td>0.972</td>\n",
       "      <td>0.331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>0.1534***</td>\n",
       "      <td>0.03315</td>\n",
       "      <td>4.628</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables          theta       se  t (theta == 0)      p\n",
       "in_out        -2.5498***  0.43873           5.812  0.000\n",
       "cy                -0.318  0.22618           1.408  0.159\n",
       "hp               -0.457*  0.27293           1.676  0.094\n",
       "we            -0.9464***  0.22234           4.256  0.000\n",
       "le            -1.9569***  0.30143           6.492  0.000\n",
       "wi            -2.0975***  0.42346           4.953  0.000\n",
       "he            -2.0849***  0.31568           6.604  0.000\n",
       "li            -0.7439***  0.11864           6.270  0.000\n",
       "sp            -1.3924***  0.27032           5.151  0.000\n",
       "ac                -0.055  0.10572           0.522  0.601\n",
       "pr             0.8436***  0.09099           9.272  0.000\n",
       "brand_2        0.9939***  0.23615           4.209  0.000\n",
       "brand_3        1.0494***  0.12909           8.130  0.000\n",
       "brand_4        0.7984***  0.11243           7.101  0.000\n",
       "brand_5        0.8222***  0.13417           6.128  0.000\n",
       "brand_6        0.7241***  0.12674           5.713  0.000\n",
       "brand_7        0.9519***  0.18761           5.074  0.000\n",
       "brand_8         0.971***  0.24506           3.962  0.000\n",
       "brand_9         0.909***  0.15489           5.868  0.000\n",
       "brand_10       0.8222***  0.12488           6.584  0.000\n",
       "brand_11       1.0521***  0.11617           9.056  0.000\n",
       "brand_12       0.8661***  0.16136           5.368  0.000\n",
       "brand_13       0.9185***  0.25339           3.625  0.000\n",
       "brand_14       0.9261***  0.12006           7.714  0.000\n",
       "brand_15       0.9596***  0.23619           4.063  0.000\n",
       "brand_16       0.6353***  0.12252           5.185  0.000\n",
       "brand_17       0.8342***  0.19716           4.231  0.000\n",
       "brand_18       0.9485***  0.15793           6.006  0.000\n",
       "brand_19       0.8852***  0.14316           6.183  0.000\n",
       "brand_20        0.906***  0.19149           4.731  0.000\n",
       "brand_21       0.9336***  0.12241           7.627  0.000\n",
       "brand_22       0.8577***  0.11582           7.406  0.000\n",
       "brand_23       1.0311***  0.11766           8.763  0.000\n",
       "brand_24       0.5224***  0.12264           4.260  0.000\n",
       "brand_25       0.8725***  0.23902           3.650  0.000\n",
       "brand_26       0.8056***  0.15549           5.181  0.000\n",
       "brand_27           0.999  0.98588           1.013  0.311\n",
       "brand_28         0.972**  0.41495           2.342  0.019\n",
       "brand_29       0.9887***  0.25101           3.939  0.000\n",
       "brand_30       0.9473***  0.15076           6.283  0.000\n",
       "brand_31       0.9933***  0.18372           5.406  0.000\n",
       "brand_32       0.9673***  0.13301           7.272  0.000\n",
       "brand_33       0.9978***  0.29060           3.433  0.001\n",
       "brand_34       0.8721***  0.10862           8.029  0.000\n",
       "brand_35       0.8744***  0.15332           5.703  0.000\n",
       "brand_36       0.8032***  0.14780           5.435  0.000\n",
       "brand_37        0.971***  0.25528           3.804  0.000\n",
       "home_2         1.6015***  0.03366          47.574  0.000\n",
       "cla_2          0.3074***  0.02881          10.670  0.000\n",
       "cla_3          0.3473***  0.05590           6.213  0.000\n",
       "cla_4              0.094  0.07304           1.284  0.199\n",
       "cla_5          0.4534***  0.13370           3.391  0.001\n",
       "group_in_out   0.1073***  0.03976           2.698  0.007\n",
       "group_cy            0.01  0.02550           0.396  0.692\n",
       "group_hp           0.025  0.03359           0.741  0.458\n",
       "group_we          0.061*  0.03187           1.902  0.057\n",
       "group_le      -0.0774***  0.02724           2.842  0.004\n",
       "group_wi      -0.1292***  0.01933           6.685  0.000\n",
       "group_he      -0.0528***  0.01413           3.734  0.000\n",
       "group_li           0.011  0.01348           0.794  0.427\n",
       "group_sp      -0.0728***  0.02374           3.065  0.002\n",
       "group_ac      -0.1813***  0.01269          14.288  0.000\n",
       "group_brand      0.044**  0.01847           2.361  0.018\n",
       "group_home         0.019  0.01933           0.972  0.331\n",
       "group_cla      0.1534***  0.03315           4.628  0.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Similarity_theta = resMLE['theta']\n",
    "Similarity_SE = resMLE['se']\n",
    "Similarity_t, Similarity_p = Similarity_t_p(Similarity_SE, Similarity_theta, N)\n",
    "reg_table(Similarity_theta, Similarity_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4294172691651903"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array([p for p in Similarity_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The FKN Estimator: An alternative approach\n",
    "\n",
    "The log-likelihood function is not globally concave, and finding the global optimum can be difficult. Using the estimation procedure of Fosgerau et. al. (2023 working paper), we can instead fit the parameters using the first-order conditions  for optimality $0=u(X_t,\\beta) - \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)$ and the observed market shares $\\hat q^0_t$ or some non-parametric estimate of the CCP's. The estimator takes the form\n",
    "\n",
    "$$\n",
    "\\hat \\theta^0=\\arg \\min_{\\theta} \\sum_t s_t \\hat \\varepsilon^0_t(\\theta)'\\hat W^0_t\\hat \\varepsilon^0 _t(\\theta),\n",
    "$$\n",
    "where $\\hat W^0_t$ is a positive semidefinite weight matrix, $s_t$ is market $t$'s share of the total population, and\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta) = \\hat D^0_t(u(X_t,\\beta)- \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)) ,\n",
    "$$\n",
    "is the residual of the first-order conditions utilizing the Logit derivatives of CCP's wrt. utilities\n",
    "$$\n",
    "\\hat D^0_t=\\textrm{diag}(\\hat q^0_t)-\\hat q^0_t (\\hat q^0_t)'.\n",
    "$$\n",
    "Using equation (...) above, we have that $\\hat \\epsilon_t$ is a linear function of $\\theta$,\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t \\left(\\hat G^0_t\\theta- \\ln \\hat q^0_t\\right)\\equiv \\hat A^0_t\\theta-\\hat r^0_t.\n",
    "$$\n",
    "With $\\hat G^0_t = [X_t, -\\nabla^2_{q,\\lambda} \\Omega (q^0_t|\\lambda)]$, $\\hat A^0_t = \\hat D^0_t \\hat G^0_t$ and $\\hat r^0_t = \\hat D^0_t \\ln \\hat q^0_t $. Using linearity, the weighted least squares criterion has a unique closed form solution,\n",
    "$$\n",
    "\\hat \\theta^0 =\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat A^0_t \\right)^{-1}\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat r_t^0 \\right)\n",
    "$$\n",
    "\n",
    "In our estimation procedure we will use the inverse of the matrix with the observed market shares CCP's $\\hat q^0_t$ along its main diagonal as our weight matrix $\\hat W^0_t = \\mathrm{diag}(\\hat q^0_t)^{-1}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_array(q, x, model):\n",
    "    ''' \n",
    "    This function calculates the G block matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        G: a dictionary  of T numpy arrays (J[t],K+G): a G matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    Z = cross_grad_pertubation(q, model) # Find the cross derivative of the pertubation function \\Omega wrt. lambda and ccp's q\n",
    "    G = {t: np.concatenate((x[t],-Z[t]), axis=1) for t in np.arange(T)} # Join block matrices along 2nd dimensions  s.t. last dimension is K+G (same dimension as theta)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_array(q):\n",
    "    '''\n",
    "    This function calculates the D matrix - the logit derivative of ccp's wrt. utilities\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "\n",
    "    Returns\n",
    "        D: a dictionary of T numpy arrays (J[t],J[t]) of logit derivatives of ccp's wrt. utilities for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = {t: np.diag(q[t]) - np.einsum('j,k->jk', q[t], q[t]) for t in np.arange(T)} # Compute logit derivatives of ccp's wrt. utilities\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_array(q, x, model):\n",
    "    '''\n",
    "    This function calculates the A matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "\n",
    "    Returns\n",
    "        A: a dictionary  of T numpy arrays (J[t],K+G): an A matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    D = D_array(q) # Compute the derivatives of logit ccp's\n",
    "    G = G_array(q, x, model) # Get the G block matrix\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)} # Compute a matrix product for each market t\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_array(q):\n",
    "    '''\n",
    "    This function calculates 'r'; the logarithm of observed or nonparametrically estimated market shares\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "    \n",
    "    Returns\n",
    "        r: a dictionary of T numpy arrays (J[t],) of the log of ccp's for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = D_array(q) \n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)} # Take logs of ccp-s\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) for t in np.arange(T)} # Compute matrix product\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS_init(q, x, sample_share, model, N):\n",
    "    ''' \n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the initial FKN parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "\n",
    "    A = A_array(q, x, model)\n",
    "    r = r_array(q)\n",
    "\n",
    "    d = A[0].shape[1] # Get the total number of parameters; this is equal to K+G\n",
    "    \n",
    "    # Initialize AWA and AWr matrices\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1/q[t], A[t], optimize = True) # Fast product using that the weights 'W' are diagonal.\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1/q[t], r[t], optimize = True)\n",
    "    \n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0)) # Solve system of equations AWA.sum()*theta = AWr.sum() for parameter estimates theta\n",
    "    #se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "    \n",
    "    return theta_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the observed market shares we may thus find initial parameter estimates $\\hat \\theta^0$ as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFKN0 = WLS_init(y, x, pop_share, Model, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.218988344960131"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.array([p for p in thetaFKN0[K:] if p > 0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for parameter bounds\n",
    "\n",
    "As we see above, the least squares estimator is not guaranteed to respect the parameter bounds $\\sum_g \\hat \\lambda_g<1$. We can use that if we replace $\\hat q^0_t$ with the choice probabilities from the maximum likelihood estimator of the Logit Model, $\\hat q^{logit}_t\\propto \\exp\\{X_t\\hat \\beta^{logit}\\}$, and plug these choice probabilities into the WLS estimator described above, it will return $\\hat \\theta=(\\hat \\beta^{logit},0,\\ldots,0)$ as the parameter estimate. Let $\\hat q_t(\\alpha)$ denote the weighted average of the logit probabilites and the market shares,\n",
    "$$\n",
    "\\hat q_t(\\alpha) =(1-\\alpha) \\hat q^{logit}_t+\\alpha \\hat q^0_t.\n",
    "$$\n",
    " Let $\\hat \\theta^0(\\alpha)$ denote the resulting parameter vector. We perform a line search for values of $\\alpha$, $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\ldots)$ until $\\hat \\theta^0(\\alpha)$ yields a feasible parameter vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogL(Theta, y, x, sample_share, model):\n",
    "    ''' \n",
    "    A function giving the mean Similarity loglikehood evaluated at data and an array of parameters 'Theta'\n",
    "    '''\n",
    "    return np.mean(Similarity_loglikelihood(Theta, y, x, sample_share, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineSearch(Logit_Beta, q_obs, x, sample_share, model, N):\n",
    "    '''\n",
    "    This function performs a line search to find feasible lambda parameters\n",
    "\n",
    "    Args:\n",
    "        Logit_beta: a (K,) numpy array of estimated beta parameters from a corresponding Logit Model\n",
    "        q_obs: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns.\n",
    "        theta_alpha: a (K+G,) numpy array of feasible parameters found by line search\n",
    "    '''\n",
    "\n",
    "    # Get dimensions of data\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit_ccp(Logit_Beta, x)\n",
    "\n",
    "    # Search over alphas s.t. alpha = (1/2)^{k} for some positive integer k\n",
    "    alpha0 = 0.5\n",
    "\n",
    "    for k in np.arange(1,100):\n",
    "\n",
    "        # Set alpha\n",
    "        alpha = alpha0**k \n",
    "        \n",
    "        # Compute convex combination of ccp's\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha = WLS_init(q_alpha, x, sample_share, model, N) # Compute initial FKN parameters but using q_alpha ccp's \n",
    "\n",
    "        lambda_alpha = theta_alpha[K:] # Find lambda parameters\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0]) # Find positive lambda parameters\n",
    "\n",
    "        if pos_pars.sum() <1:\n",
    "            break # Break if positive parameters sum to less than 1\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(Logit_Beta, y, x, sample_share, model, N, num_alpha = 5):\n",
    "    '''\n",
    "    This function performs a grid search on the unit interval to find feasible parameters \\theta\n",
    "\n",
    "    Args:\n",
    "        Logit_beta: a (K,) numpy array of estimated beta parameters from a corresponding Logit Model\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations,\n",
    "        num_alpha: an integer of the number of alphas for which the search is to be performed\n",
    "\n",
    "    Returns.\n",
    "        theta_star: a (K+G,) numpy array of feasible parameters found by grid search\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "    J0 = x[0].shape[0]\n",
    "    psi_3d0 = model['psi_3d'][0]\n",
    "    G = np.int64(psi_3d0.shape[0] - 1)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    LogL_alpha = np.empty((num_alpha,))\n",
    "    theta_alpha = np.empty((num_alpha,K+G))\n",
    "\n",
    "    for k in np.arange(len(alpha_line)):\n",
    "\n",
    "        alpha = alpha_line[k]\n",
    "\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha[k,:] = WLS_init(q_alpha, x, sample_share, model, N)\n",
    "\n",
    "        #lambda_inout = theta_alpha[k,K]\n",
    "        lambda_alpha = theta_alpha[k,K:] # theta_alpha[k,K+1:]\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if (pos_pars.sum() >= 1): #|(lambda_inout >= 1)\n",
    "            LogL_alpha[k] = np.NINF\n",
    "        else:\n",
    "            LogL_alpha[k] = LogL(theta_alpha[k,:], y, x, sample_share, model)\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "    alpha_star = np.argmax(LogL_alpha)\n",
    "    theta_star = theta_alpha[alpha_star,:]\n",
    "\n",
    "    return theta_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the line search method we find corressponding parameters $\\hat \\theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_alpha = LineSearch(Logit_beta, y, x, pop_share, Model, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7549534173977517, 0.0015132588931061905]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0015132588931061905"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_Similarity(theta_alpha, y, x, pop_share, Model).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated FKN estimator\n",
    "\n",
    "The initial estimator $\\hat \\theta^0$ is usually biased. However this can be accommodated for via iterating a similar estimator on the estimator $\\hat \\theta^0$. The iterated estimator is as the initial one, except there is an additional term on $\\hat \\varepsilon$. First, we update the choice probabilities,\n",
    "$$\n",
    "\\hat q^k_t=p(\\mathbf X_t,\\hat \\theta^{k-1})\\\\\n",
    "$$\n",
    "Then we assign\n",
    "$$\n",
    "\\hat D^k_t=\\nabla^2_{qq}\\Omega(\\hat q_t^k|\\hat \\lambda^{k-1})^{-1}-(\\hat q^k_t \\hat q^k_t)'\n",
    "$$\n",
    "and then construct the residual\n",
    "$$\n",
    "\\hat \\varepsilon^k_t(\\theta)=\\hat D^k_t\\left( u(x_t,\\beta)-\\nabla_q \\Omega(\\hat q_t^k|\\lambda)\\right) -y_t+\\hat q_t^k,\n",
    "$$\n",
    "Which can once again be simplified as\n",
    "$$\n",
    "\\hat \\varepsilon^k_t(\\theta)= \\hat A_t^k \\theta-\\hat r^k_t,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat A^k_t=\\hat D_t^k\\hat G^k_t, \\hat r_t^k =\\hat D^k_t\\ln \\hat q_t^k-y_t\n",
    "$$\n",
    "and where $\\hat G^k_t$ is constructed as in the initial estimator. Using the weighted least squares estimator with weights $\\hat W_t^k=\\textrm{diag}(\\hat q^k_t)^{-1}$, we get the estimator\n",
    "$$\n",
    "\\hat \\theta^k = \\arg \\min_{\\theta}\\frac{1}{T}\\sum_t \\hat \\varepsilon^k_t(\\theta)'\\hat W_t^k \\hat \\varepsilon^k_t(\\theta).\n",
    "$$\n",
    "We can once again solve it in closed form as\n",
    "$$\n",
    "\\hat \\theta^k =\\left( \\frac{1}{T}\\sum_t  ( \\hat A^k_t)'\\hat W_t^k \\hat A^k_t\\right)^{-1}\\left( \\frac{1}{T}\\sum_t (\\hat A_t^k)'\\hat W_t^k \\hat r_t^k\\right)\n",
    "$$\n",
    "\n",
    "FKN (2021) show that the estimator $\\hat \\theta^k$ converges to the (true) MLE $\\hat \\theta^{\\text{MLE}}$ as the number of iterations approaches infinity, avoiding thereby \n",
    "\n",
    "Now we implement this procedure and iterate starting from our initial guess $\\hat \\theta^{*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS(Theta, y, x, sample_share, model, N):\n",
    "    '''\n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the iterated parameter estimates.\n",
    "\n",
    "    Args:\n",
    "        Theta: a (K+G,) numpy array of previously estimated \\theta parameters\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations,\n",
    "\n",
    "    Returns.\n",
    "        theta_hat: a (K+G,) numpy array of iterated FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for iterated FKN parameter estimates\n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta.shape[0]\n",
    "    \n",
    "    # Get ccp's\n",
    "    q = Similarity_ccp(Theta, x, model)\n",
    "\n",
    "    # Construct A\n",
    "    D = ccp_gradient(q, x, Theta, model) # A is here constructed using the Similarity derivative of ccp's wrt. utilities instead of the Logit derivative\n",
    "    G = G_array(q, x, model)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "\n",
    "    # Construct r\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where=(q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) + y[t] for t in np.arange(T)} # r = D %o% log(q) + y\n",
    "\n",
    "    # Estimate parameters\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1./q[t], A[t], optimize = True)\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1./q[t], r[t], optimize = True)\n",
    "\n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "\n",
    "    return theta_hat,se_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKN_estimator(logit_beta, q_obs, x, sample_share, model, N, tol = 1.0e-15, max_iters = 1000):\n",
    "    '''\n",
    "    This function estimates the Similarity Model via the FKN estimator\n",
    "\n",
    "    Args:\n",
    "        logit_beta: a (K,) numpy array of estimated beta parameters from a corresponding Logit Model\n",
    "        q_obs: a dictionary of T numpy arrays (J[t],) of observed choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        model: a dictionary of the Similarity Model specification as outputted by 'Similarity_specification'\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns.\n",
    "        res: a dictionary containing FKN parameter estimates, standard errors, iterations to convergence, etc.\n",
    "    '''\n",
    "\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Get initial FKN parameters using observed market shares\n",
    "    theta_init = WLS_init(q_obs, x, sample_share, model, N) \n",
    "    \n",
    "    # If the positive nesting paramters sum to more than 1, then perform a linesearch.\n",
    "    if (np.array([p for p in theta_init[K:] if p>0]).sum() >= 1):\n",
    "        theta_star = LineSearch(logit_beta, q_obs, x, sample_share, model, N)\n",
    "        theta0 = theta_star\n",
    "    else:\n",
    "        theta0 = theta_init\n",
    "\n",
    "    '''logl0 = LogL(theta0, q_obs, x, sample_share, psi, nest_count)'''\n",
    "    \n",
    "    # Debiasing iterations\n",
    "    for k in np.arange(max_iters):\n",
    "        # Compute iterated FKN parameters and standard errors\n",
    "        theta1, se1 = WLS(theta0, q_obs, x, sample_share, model, N)\n",
    "\n",
    "        '''logl1=LogL(theta1, q_obs, x, sample_share, psi, nest_count)\n",
    "        \n",
    "        for m in range(10):\n",
    "            if logl1<logl0:\n",
    "                theta1=0.5*theta0+0.5*theta1\n",
    "                logl1=LogL(theta1, q_obs, x, sample_share, psi, nest_count)\n",
    "            else:\n",
    "                break'''\n",
    "\n",
    "        # Check convergence in an appropriate norm\n",
    "        dist = la.norm(theta1 - theta0)\n",
    "\n",
    "        if dist<tol:\n",
    "            succes = True\n",
    "            iter = k\n",
    "            break\n",
    "        elif k==max_iters:\n",
    "            succes = False\n",
    "            iter = max_iters\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        theta0 = theta1\n",
    "\n",
    "    res = {'theta': theta1,\n",
    "           'se': se1,\n",
    "           'fun': -LogL(theta1, y, x, sample_share, model),\n",
    "           'iter': iter,\n",
    "           'succes': succes}\n",
    "    \n",
    "    return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1675046701290483, 0.0014929418072264493]\n"
     ]
    }
   ],
   "source": [
    "res = FKN_estimator(Logit_beta, y, x, pop_share, Model, N, tol=1.0e-8, max_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-10.4593***</td>\n",
       "      <td>0.00342</td>\n",
       "      <td>3057.771</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.5007***</td>\n",
       "      <td>0.00170</td>\n",
       "      <td>294.137</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-3.5089***</td>\n",
       "      <td>0.00255</td>\n",
       "      <td>1378.193</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.0686***</td>\n",
       "      <td>0.00162</td>\n",
       "      <td>42.379</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.2163***</td>\n",
       "      <td>0.00171</td>\n",
       "      <td>1299.239</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.4956***</td>\n",
       "      <td>0.00318</td>\n",
       "      <td>1727.421</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.342***</td>\n",
       "      <td>0.00197</td>\n",
       "      <td>173.217</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-1.0238***</td>\n",
       "      <td>0.00111</td>\n",
       "      <td>918.509</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>3.2825***</td>\n",
       "      <td>0.00229</td>\n",
       "      <td>1433.185</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.7804***</td>\n",
       "      <td>0.00081</td>\n",
       "      <td>958.274</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.2116***</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>203.542</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.6157***</td>\n",
       "      <td>0.00256</td>\n",
       "      <td>240.561</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>0.1901***</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>379.236</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.6004***</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>911.291</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.2588***</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>541.947</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.1982***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>408.478</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.6187***</td>\n",
       "      <td>0.00141</td>\n",
       "      <td>439.537</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.5005***</td>\n",
       "      <td>0.00167</td>\n",
       "      <td>299.205</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.6751***</td>\n",
       "      <td>0.00224</td>\n",
       "      <td>749.046</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.2893***</td>\n",
       "      <td>0.00057</td>\n",
       "      <td>506.358</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>0.0527***</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>109.716</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.3133***</td>\n",
       "      <td>0.00078</td>\n",
       "      <td>401.886</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-0.8757***</td>\n",
       "      <td>0.00134</td>\n",
       "      <td>652.582</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-1.5846***</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>1160.229</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.6567***</td>\n",
       "      <td>0.00262</td>\n",
       "      <td>631.724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-0.6894***</td>\n",
       "      <td>0.00056</td>\n",
       "      <td>1231.625</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.5225***</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>710.004</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.3154***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>673.229</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-0.9024***</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>806.989</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.0478***</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>74.398</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>0.0098***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>21.159</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.1067***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>217.936</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>0.1704***</td>\n",
       "      <td>0.00052</td>\n",
       "      <td>329.639</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.2503***</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>505.247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.8084***</td>\n",
       "      <td>0.00125</td>\n",
       "      <td>648.105</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.5776***</td>\n",
       "      <td>0.00073</td>\n",
       "      <td>794.027</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.706***</td>\n",
       "      <td>0.03484</td>\n",
       "      <td>77.674</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.6662***</td>\n",
       "      <td>0.00124</td>\n",
       "      <td>536.215</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.3179***</td>\n",
       "      <td>0.01393</td>\n",
       "      <td>166.350</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.3017***</td>\n",
       "      <td>0.00249</td>\n",
       "      <td>522.873</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-0.9614***</td>\n",
       "      <td>0.00236</td>\n",
       "      <td>407.022</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.3562***</td>\n",
       "      <td>0.00083</td>\n",
       "      <td>429.612</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-2.5192***</td>\n",
       "      <td>0.01106</td>\n",
       "      <td>227.763</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.667***</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>1010.355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.2463***</td>\n",
       "      <td>0.00069</td>\n",
       "      <td>355.353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1235***</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>188.905</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.5635***</td>\n",
       "      <td>0.00293</td>\n",
       "      <td>534.335</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.115***</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>2454.987</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>-0.0496***</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>326.455</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.045***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>195.954</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.0481***</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>134.471</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.1448***</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>284.256</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.7171***</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>2871.216</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.1345***</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>801.206</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.1228***</td>\n",
       "      <td>0.00026</td>\n",
       "      <td>466.355</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.0057***</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>29.340</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.1108***</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>674.315</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0325***</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>292.058</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0439***</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>621.558</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.0556***</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>516.183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0543***</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>311.878</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0329***</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>315.408</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.2663***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>1139.753</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.2293***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>1010.956</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.06***</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>467.573</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)    p\n",
       "in_out        -10.4593***  0.00342        3057.771  0.0\n",
       "cy             -0.5007***  0.00170         294.137  0.0\n",
       "hp             -3.5089***  0.00255        1378.193  0.0\n",
       "we              0.0686***  0.00162          42.379  0.0\n",
       "le             -2.2163***  0.00171        1299.239  0.0\n",
       "wi              5.4956***  0.00318        1727.421  0.0\n",
       "he               0.342***  0.00197         173.217  0.0\n",
       "li             -1.0238***  0.00111         918.509  0.0\n",
       "sp              3.2825***  0.00229        1433.185  0.0\n",
       "ac              0.7804***  0.00081         958.274  0.0\n",
       "pr             -0.2116***  0.00104         203.542  0.0\n",
       "brand_2        -0.6157***  0.00256         240.561  0.0\n",
       "brand_3         0.1901***  0.00050         379.236  0.0\n",
       "brand_4        -0.6004***  0.00066         911.291  0.0\n",
       "brand_5        -0.2588***  0.00048         541.947  0.0\n",
       "brand_6        -0.1982***  0.00049         408.478  0.0\n",
       "brand_7        -0.6187***  0.00141         439.537  0.0\n",
       "brand_8        -0.5005***  0.00167         299.205  0.0\n",
       "brand_9        -1.6751***  0.00224         749.046  0.0\n",
       "brand_10        0.2893***  0.00057         506.358  0.0\n",
       "brand_11        0.0527***  0.00048         109.716  0.0\n",
       "brand_12       -0.3133***  0.00078         401.886  0.0\n",
       "brand_13       -0.8757***  0.00134         652.582  0.0\n",
       "brand_14       -1.5846***  0.00137        1160.229  0.0\n",
       "brand_15       -1.6567***  0.00262         631.724  0.0\n",
       "brand_16       -0.6894***  0.00056        1231.625  0.0\n",
       "brand_17       -0.5225***  0.00074         710.004  0.0\n",
       "brand_18        0.3154***  0.00047         673.229  0.0\n",
       "brand_19       -0.9024***  0.00112         806.989  0.0\n",
       "brand_20       -0.0478***  0.00064          74.398  0.0\n",
       "brand_21        0.0098***  0.00047          21.159  0.0\n",
       "brand_22        0.1067***  0.00049         217.936  0.0\n",
       "brand_23        0.1704***  0.00052         329.639  0.0\n",
       "brand_24       -0.2503***  0.00050         505.247  0.0\n",
       "brand_25       -0.8084***  0.00125         648.105  0.0\n",
       "brand_26       -0.5776***  0.00073         794.027  0.0\n",
       "brand_27        -2.706***  0.03484          77.674  0.0\n",
       "brand_28       -0.6662***  0.00124         536.215  0.0\n",
       "brand_29       -2.3179***  0.01393         166.350  0.0\n",
       "brand_30       -1.3017***  0.00249         522.873  0.0\n",
       "brand_31       -0.9614***  0.00236         407.022  0.0\n",
       "brand_32       -0.3562***  0.00083         429.612  0.0\n",
       "brand_33       -2.5192***  0.01106         227.763  0.0\n",
       "brand_34        -0.667***  0.00066        1010.355  0.0\n",
       "brand_35       -0.2463***  0.00069         355.353  0.0\n",
       "brand_36       -0.1235***  0.00065         188.905  0.0\n",
       "brand_37       -1.5635***  0.00293         534.335  0.0\n",
       "home_2           1.115***  0.00045        2454.987  0.0\n",
       "cla_2          -0.0496***  0.00015         326.455  0.0\n",
       "cla_3            0.045***  0.00023         195.954  0.0\n",
       "cla_4           0.0481***  0.00036         134.471  0.0\n",
       "cla_5           0.1448***  0.00051         284.256  0.0\n",
       "group_in_out    0.7171***  0.00025        2871.216  0.0\n",
       "group_cy       -0.1345***  0.00017         801.206  0.0\n",
       "group_hp        0.1228***  0.00026         466.355  0.0\n",
       "group_we        0.0057***  0.00020          29.340  0.0\n",
       "group_le       -0.1108***  0.00016         674.315  0.0\n",
       "group_wi       -0.0325***  0.00011         292.058  0.0\n",
       "group_he       -0.0439***  0.00007         621.558  0.0\n",
       "group_li        0.0556***  0.00011         516.183  0.0\n",
       "group_sp       -0.0543***  0.00017         311.878  0.0\n",
       "group_ac       -0.0329***  0.00010         315.408  0.0\n",
       "group_brand     0.2663***  0.00023        1139.753  0.0\n",
       "group_home     -0.2293***  0.00023        1010.956  0.0\n",
       "group_cla        -0.06***  0.00013         467.573  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "FKN_theta = res['theta']\n",
    "FKN_SE = res['se']\n",
    "FKN_t, FKN_p = Similarity_t_p(FKN_SE, FKN_theta, N)\n",
    "reg_table(FKN_theta, FKN_SE, N, x_vars, nest_vars)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
