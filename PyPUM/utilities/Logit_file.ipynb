{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Demand for Cars with the Multinomial Logit Model\n",
    "\n",
    "In this notebook, we will explore the dataset used in\n",
    "Brownstone and Train (1999). We will estimate the Multinomial Logit Model\n",
    "model given the available data using the functions defined below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import os\n",
    "import sys\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "from scipy.stats import t\n",
    "from IPython import display\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "\n",
    "# Files\n",
    "module_path = os.path.abspath(os.path.join('../'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "data_path = os.path.join(module_path, 'data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimating the logit model\n",
    "\n",
    "We estimate a logit model on the data using maximum likelihood. In doing this we will see our first use of the numpy function 'einsum()' which quickly and easily computes matrix products, outer products, transposes, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def util(Beta, x):\n",
    "    '''\n",
    "    This function finds the deterministic utilities u = X*Beta.\n",
    "    \n",
    "    Args.\n",
    "        Beta: (K,) numpy array of parameters\n",
    "        x: (N,J,K) matrix of covariates\n",
    "\n",
    "    Output\n",
    "        u: (N,J) matrix of deterministic utilities\n",
    "    '''\n",
    "\n",
    "    if isinstance(x, (np.ndarray)):\n",
    "        u = np.einsum('njk,k->nj', x, Beta) # is the same as x @ Beta\n",
    "    else:\n",
    "        T = len(x.keys())\n",
    "        u = {}\n",
    "        for t in np.arange(T):\n",
    "            u[t] = np.dot(x[t], Beta)\n",
    "\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_loglikehood(Beta, y, x, sample_share, MAXRESCALE: bool = True):\n",
    "    '''\n",
    "    This function calculates the likelihood contributions of a Logit model\n",
    "\n",
    "    Args. \n",
    "        Beta: (K,) vector of parameters \n",
    "        x: (N,J,K) matrix of covariates \n",
    "        y: (N,J) matrix of outcomes \n",
    "\n",
    "    Returns\n",
    "        ll_i: (N,) vector of loglikelihood contributions for a Logit\n",
    "    '''\n",
    "\n",
    "    # deterministic utility \n",
    "    v = util(Beta, x)\n",
    "\n",
    "    if isinstance(x, (np.ndarray)):\n",
    "        if MAXRESCALE: \n",
    "            # subtract the row-max from each observation\n",
    "            v -= v.max(axis=1, keepdims=True)  # keepdims maintains the second dimension, (N,1), so broadcasting is successful\n",
    "\n",
    "        # denominator \n",
    "        denom = np.exp(v).sum(axis=1) # NOT keepdims! becomes (N,)\n",
    "\n",
    "        # utility at chosen alternative for each individual i\n",
    "        v_i = np.einsum('nj,nj->n', y, v) # Becomes (N,)\n",
    "\n",
    "        # likelihood \n",
    "        LL = np.einsum('n,n->n', sample_share, v_i - np.log(denom)) # difference between two 1-dimensional arrays\n",
    "        \n",
    "    else:\n",
    "        T = len(x.keys())\n",
    "        ll_i = np.empty((T,))\n",
    "\n",
    "        if MAXRESCALE:\n",
    "            v = {t: v[t] - v[t].max(keepdims=True) for t in np.arange(T)}\n",
    "        \n",
    "        for t in np.arange(T):\n",
    "            denom = np.exp(v[t]).sum()\n",
    "            v_i = np.dot(y[t], v[t])\n",
    "            ll_i[t] = v_i - np.log(denom)\n",
    "            LL = np.einsum('n,n->n', sample_share, ll_i)\n",
    "\n",
    "    return LL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of the likelihood function $\\ell_i (\\theta)$ wrt. parameters in the logit model if individual $i$ chose product $j$ is given by:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\ell_i(\\theta) =y_i'\\left(X - \\left(\\iota \\circ \\frac{\\sum_\\ell e^{ X_\\ell \\theta }X_\\ell}{\\sum_\\ell e^{X_\\ell \\theta }}\\right)\\right)  = X_j - \\frac{\\sum_\\ell e^{ X_\\ell \\theta }X_\\ell}{\\sum_\\ell e^{X_\\ell \\theta }}\n",
    "$$\n",
    "\n",
    "We may then consistently estimate the covariance matrix in the logit model by plugging the MLE $\\hat \\theta$ into the formula:\n",
    "\n",
    "$$\n",
    "\\Sigma(\\theta) = \\left(\\sum_i \\nabla_\\theta \\ell_i(\\theta)\\nabla_\\theta \\ell_i(\\theta)'\\right)^{-1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_score(theta, y, x, sample_share):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    if isinstance(x, (np.ndarray)):\n",
    "        N,J,K = x.shape\n",
    "\n",
    "        numer_term = np.einsum('nj,njk->njk', np.exp(np.einsum('k,njk->nj', theta, x)), x)\n",
    "        numer = np.einsum('j,njk->nk', np.ones((J,)), numer_term)\n",
    "        denom = np.einsum('j,nj->n', np.ones((J,)), np.exp(np.einsum('k,njk->nj', theta, x)))\n",
    "        yLog_grad = np.einsum('nj,njk->nk', y, x - (numer / denom[:,None])[:,None,:])\n",
    "        score = np.einsum('n,nk->nk', sample_share, yLog_grad)\n",
    "        \n",
    "    else:\n",
    "        T = len(x.keys())\n",
    "        yLog_grad = np.empty((T, len(theta)))\n",
    "\n",
    "        for t in np.arange(T):\n",
    "            numer = np.dot(np.exp(np.dot(x[t], theta)), x[t])\n",
    "            denom = np.exp(np.dot(x[t], theta)).sum()\n",
    "            yLog_grad[t,:] = np.dot(y[t], x[t] - np.divide(numer, denom))\n",
    "        \n",
    "        score = np.einsum('n,nd->nd', sample_share, yLog_grad)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_score_unweighted(theta, y, x):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    if isinstance(x, (np.ndarray)):\n",
    "        N,J,K = x.shape\n",
    "\n",
    "        numer_term = np.einsum('nj,njk->njk', np.exp(np.einsum('k,njk->nj', theta, x)), x)\n",
    "        numer = np.einsum('j,njk->nk', np.ones((J,)), numer_term)\n",
    "        denom = np.einsum('j,nj->n', np.ones((J,)), np.exp(np.einsum('k,njk->nj', theta, x)))\n",
    "        yLog_grad = np.einsum('nj,njk->nk', y, x - (numer / denom[:,None])[:,None,:])\n",
    "        score = yLog_grad\n",
    "        \n",
    "    else:\n",
    "        T = len(x.keys())\n",
    "        yLog_grad = np.empty((T, len(theta)))\n",
    "\n",
    "        for t in np.arange(T):\n",
    "            numer = np.dot(np.exp(np.dot(x[t], theta)), x[t])\n",
    "            denom = np.exp(np.dot(x[t], theta)).sum()\n",
    "            yLog_grad[t,:] = np.dot(y[t], x[t] - np.divide(numer, denom))\n",
    "        \n",
    "        score = yLog_grad\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_se(score, sample_share, N):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    Sigma = np.einsum('nk,nm->km', sample_share[:,None]*score, score)\n",
    "    SE = np.sqrt(np.diag(la.inv(Sigma)) / N)\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_t_p(theta, score, sample_share, N, theta_hypothesis = 0):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    if isinstance(x, (np.ndarray)):\n",
    "        D,J,K = x.shape\n",
    "    else:\n",
    "        D = len(x.keys())\n",
    "\n",
    "    SE = logit_se(score, sample_share, N)\n",
    "    T = np.abs(theta - theta_hypothesis) / SE\n",
    "    p = t.sf(T, df = D-1)\n",
    "\n",
    "    return T,p\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_logit(Beta, y, x, sample_share):\n",
    "    \n",
    "    '''\n",
    "    q: Criterion function, passed to estimate_logit().\n",
    "    '''\n",
    "    return -logit_loglikehood(Beta, y, x, sample_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_logit_score(Beta, y, x, sample_share):\n",
    "    ''' \n",
    "    '''\n",
    "    return -logit_score(Beta, y, x, sample_share)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_logit(q, Beta0, y, x, sample_share, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given start values and \n",
    "    variables to calculate the residuals.\n",
    "\n",
    "    Args.\n",
    "        q: a function to minimize,\n",
    "        Beta0 : (K+G,) array of initial guess parameters,\n",
    "        y: array of observed response variables (N,),\n",
    "        x: array of observed explanatory variables (N,J,K),\n",
    "        options: dictionary with options for the optimizer (e.g. disp=True,\n",
    "            which tells it to display information at termination.)\n",
    "    \n",
    "    Returns\n",
    "        res: Returns a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(q(Theta, y, x, sample_share))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_logit_score(Theta, y, x, sample_share), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Beta0.tolist(), options=options, jac = Grad, **kwargs)\n",
    "    pars = result.x\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'beta': pars, # vector of estimated parameters\n",
    "        'success':  result.success, # bool, whether convergence was succesful \n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating a Logit model via maximum likelihood with an initial guess of parameters $\\hat \\beta^0 = 0$ yields estimated parameters $\\hat \\beta^{\\text{logit}}$ given as..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here '$***$', '$**$', and '$*$' indicates that we can reject the hypothesis $\\beta=0$ at levels of significance $\\alpha = 0.01, 0.05, 0.1$, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We then compute the corresponding Logit choice probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_ccp(Beta, x, MAXRESCALE:bool=True):\n",
    "    '''logit_ccp(): Computes the (N,J) matrix of choice probabilities from a logit model\n",
    "    Args. \n",
    "        u: (N,J) matrix of  \n",
    "    \n",
    "    Returns\n",
    "        ccp: (N,J) matrix of probabilities \n",
    "    '''\n",
    "    # deterministic utility \n",
    "    v = util(Beta, x) # (N,J)\n",
    "\n",
    "    if isinstance(x, (np.ndarray)): \n",
    "        if MAXRESCALE: \n",
    "            # subtract the row-max from each observation\n",
    "            v -= v.max(axis=1, keepdims=True)  # keepdims maintains the second dimension, (N,1), so broadcasting is successful\n",
    "        \n",
    "        # denominator \n",
    "        denom = np.exp(v).sum(axis=1, keepdims=True) # (N,1)\n",
    "        \n",
    "        # Conditional choice probabilites\n",
    "        ccp = np.exp(v) / denom\n",
    "    else:\n",
    "        T = len(x.keys())\n",
    "        \n",
    "        if MAXRESCALE:\n",
    "            v = {t: v[t] - v[t].max(keepdims=True) for t in np.arange(T)}\n",
    "        \n",
    "        # denominator\n",
    "        denom = {t: np.exp(v[t]).sum() for t in np.arange(T)}\n",
    "\n",
    "        # Conditional choice probabilites\n",
    "        ccp = {t: np.divide(np.exp(v[t]), denom[t]) for t in np.arange(T)}\n",
    "\n",
    "    \n",
    "    return ccp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our estimates $\\hat \\beta^{\\text{logit}}$, the choice probabilities $\\hat q_i^{logit}$ of products $\\{0,1, \\ldots , 5\\}$ for individuals $i=0,1,\\ldots , 4653$ thus becomes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "logit_q = logit_ccp(logit_beta, x)\n",
    "pd.DataFrame(logit_q).rename_axis(index='individuals', columns='products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logit elasticities\n",
    "\n",
    "The logit (semi-)elasticities of the choice probabilities $q_i = P(u| \\beta)$ for individual i wrt. to the $\\ell$'th characteristic are given by the formula:\n",
    "$ \\mathcal{E}_i= \\nabla_x \\ln P(u| \\beta)= \\left( I_J - \\iota q_i'\\right) \\beta_\\ell$\n",
    "where $()'$ denotes the transpose of a matrix, $\\iota=(1, \\ldots, 1)'$ is the all ones vector in $\\mathbb{R}^{J}$,and $I_J$ is the identity matrix in $\\mathbb{R}^{J\\times J}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we compute the implied price-to-log-income elasticities for our logit model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_elasticity(q, Beta, char_number):\n",
    "    ''' \n",
    "    This function calculates the logit elasticities of choice probabilities wrt. a given charateristic k\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        Beta: a (K,) numpy array of parameters\n",
    "        car_number: an integer k for the index of the characteristic\n",
    "\n",
    "    Output:\n",
    "        Epsilon: a (N,J,J) matrix of logit elasticities of choice probabilities wrt. the charateristic k\n",
    "    '''\n",
    "    if isinstance(q, (np.ndarray)):\n",
    "        assert q.ndim == 2\n",
    "        assert Beta.ndim == 1\n",
    "\n",
    "        N,J = q.shape\n",
    "\n",
    "        iota_q = np.einsum('j,ni->nji', np.ones((J,)), q)\n",
    "        Epsilon = (np.eye(J) - iota_q)*Beta[char_number]\n",
    "    else:\n",
    "        T = len(q.keys())\n",
    "        J = {t: q[t].shape[0] for t in np.arange(T)}\n",
    "        \n",
    "        iota_q = {t: np.outer(np.ones(J[t]), q[t]) for t in np.arange(T)}\n",
    "        Epsilon = {t: np.multiply(np.eye(J[t]) - iota_q[t], Beta[char_number]) for t in np.arange(T)}\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented on our datset, we thus find the elasticities as follows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epsilon_logit = logit_elasticity(logit_q, logit_beta, 0)\n",
    "pd.DataFrame(epsilon_logit[0,:,:]).rename_axis(index = 'Elasticity of products', columns='Elasticity wrt. products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example for individual $i=0$, the $j\\ell$'th entry corresponds to the elasticity of the choice probability of product $j$ with respect to the price-to-log-income (i.e. the $0$'th characteristic) of product $\\ell$ for $j, \\ell \\in \\{0,1, \\ldots ,  5\\}$. Note that the diagonal entries are negative, indicating that all products are normal, and that the cross-elasticities (i.e. $j \\neq \\ell$) with respect to any product $\\ell$ are equal for all $j \\neq \\ell$. Our example thus validates the IIA property of the logit model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Diversion Ratios for Logit\n",
    "\n",
    "The diversion ratio to product j from product k wrt. to the $\\ell$'th characteristic $-100 \\cdot \\frac{\\partial P_j / \\partial x_{k\\ell}}{\\partial P_k / \\partial x_{k\\ell}}$ for the standard logit model is given by equation:\n",
    "\n",
    "$$\n",
    "D^i = -100 \\cdot \\nabla_{x_\\ell} P(u|\\beta)(\\nabla_{x_\\ell} P(u|\\beta) \\circ I_J)^{-1} = -100 \\cdot \\nabla_u P(u|\\beta)(\\nabla_u P(u|\\beta) \\circ I_J)^{-1}\n",
    "$$\n",
    "\n",
    "Where '$\\circ$' is the elementwise product of matrices and $\\nabla_{x_\\ell} P(u|\\beta) = \\beta_\\ell(\\mathrm{diag}(q) - qq')$ is the usual derivative of Logit choice probailities wrt. the $\\ell$'th characteristic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_diversion_ratio(q, Beta):\n",
    "    '''\n",
    "    This function calculates the logit diversion ratios of choice probabilities wrt. a given charateristic k\n",
    "\n",
    "    Args.\n",
    "        q: a (N,J) numpy array of choice probabilities\n",
    "        Beta: a (K,) numpy array of parameters\n",
    "        car_number: an integer k for the index of the characteristic\n",
    "\n",
    "    Output:\n",
    "        DR: a (N,J,J) matrix of logit diversion ratios of choice probabilities wrt. the charateristic k\n",
    "    '''\n",
    "\n",
    "    if isinstance(q, (np.ndarray)):\n",
    "        assert q.ndim == 2\n",
    "        assert Beta.ndim == 1\n",
    "\n",
    "        N,J = q.shape\n",
    "        diag_q = q[:,:,None] * np.eye(J,J)[None, :, :]\n",
    "        qqT = np.einsum('nj,nk->njk', q, q)\n",
    "        Grad = diag_q - qqT\n",
    "        diag_Grad_mat = Grad * np.eye(J,J)[None, :, :]\n",
    "        diag_Grad_vec = np.einsum('njk,k->nj', diag_Grad_mat, np.ones((J,)))\n",
    "        DR = -100 * np.einsum('njk,nj->njk', Grad, 1./diag_Grad_vec)\n",
    "    else:\n",
    "        T = len(q.keys())\n",
    "        J = {t: q[t].shape[0] for t in np.arange(T)}\n",
    "\n",
    "        diag_q = {t: np.multiply(np.eye(J[t]), q[t]) for t in np.arange(T)}\n",
    "        qqT = {t: np.outer(q[t], q[t]) for t in np.arange(T)}\n",
    "        Grad = {t: diag_q[t] - qqT[t] for t in np.arange(T)}\n",
    "        diag_Grad = {t: np.multiply(Grad[t], np.eye(J[t])) for t in np.arange(T)}\n",
    "        DR = {t: np.multiply(-100, np.dot(Grad[t], la.inv(diag_Grad[t]))) for t in np.arange(T)}\n",
    "\n",
    "    return DR\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DR_logit_hat = logit_diversion_ratio(logit_q, logit_beta)\n",
    "pd.DataFrame(DR_logit_hat[0,:,:]).rename_axis(index = 'Div. ratio of products', columns='Div. ratio wrt. products')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "own_DR_logit = {j : (DR_logit_hat.reshape((N, J**2))[:,j]).flatten() for j in np.arange(J**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.  \n",
    "\n",
    "j_pairs = iter.product(np.arange(J), np.arange(J))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(J, J)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(J**2)):\n",
    "    axes[p].hist(own_DR_logit[j], num_bins)\n",
    "    axes[p].vlines(0, 0, 1500, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Logit price-to-log-income diversion ratios')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP Estimation and instruments\n",
    "\n",
    "The principles are pretty similar to what we have been doing already. Define the residual,\n",
    "\n",
    "$$\\xi_m(\\theta) = u(X_m, \\beta)$$\n",
    "\n",
    "In the Logit model, this residual is a linear function of $\\theta$ which has the form\n",
    "\n",
    "$$\\xi_m(\\theta) =  X_m \\beta âˆ’ r_m^0$$\n",
    "\n",
    "where $r^0_m = \\ln q^0_m$  with $q^0_m$ being the observed market shares in market $m$. For the BLP estimator, we set this residual orthogonal to a matrix of instruments $\\hat Z_m$ of size $J_m \\times K$, and find the estimator $ \\hat \\beta^{IV}$ which solves the moment conditions\n",
    "\n",
    "$$\\sum_m  s_m \\hat Z_m' \\xi(\\hat \\beta^{IV}) = 0$$\n",
    "\n",
    "Where $s_m$ denotes the share of observations in our sample which belong to market $m$. Since $\\hat \\xi$ is linear, the moment equations have a unique solution,\n",
    "\n",
    "$$\\hat \\beta^{IV} = \\left(\\sum_m s_m \\hat Z_m' X_m \\right)^{-1}\\left(\\sum_m s_m \\hat Z_m' r^0_m \\right)$$\n",
    "\n",
    "An estimator for the constant $\\sigma^2$ is given by:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{S} \\sum_t\\sum_{j = 1}^{J_t} \\xi_{tj}(\\hat \\beta^{IV})^2\n",
    "$$\n",
    "\n",
    "Where $S = T * \\sum_t J_t$ is the number of observations in each market. An estimate of the variance of the parameter estimates $\\hat \\beta^{IV}$ is then given by:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\hat \\sigma^2 \\left(\\sum_t Z_t'Z_t\\right)^{-1}\n",
    "$$\n",
    "\n",
    "We require an instrument for the price of the goods. This is something which is correlated with the price, but uncorrelated with the error term $\\xi_m$ (in the\n",
    "BLP model, $\\xi_{mj}$ represents unobserved components of car quality). A standard instrument in this case would be a measure of marginal cost (or something which is correlated with marginal cost, like a production price index). For everything other than price, we can simply use the regressor itself as the instrument i.e. $ \\hat Z^{mjk} = X^0_{mjk}$, for all other dimensions than price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogitBLP_estimator(q_obs, z, x, sample_share):\n",
    "    ''' \n",
    "    '''\n",
    "    N = len(z)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    r = {t: np.log(q_obs[t], out = np.NINF*np.ones_like((q_obs[t])), where = (q_obs[t] > 0)) for t in np.arange(N)}\n",
    "    \n",
    "    sZG = np.empty((N,K,K))\n",
    "    sZr = np.empty((N,K))\n",
    "\n",
    "    for t in np.arange(N):\n",
    "        sZG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', z[t], x[t])\n",
    "        sZr[t,:] = sample_share[t]*np.einsum('jd,j->d', z[t], r[t])\n",
    "\n",
    "    theta_hat = la.solve(sZG.sum(axis=0), sZr.sum(axis=0))\n",
    "    \n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogitBLP_se(Beta, q_obs, z, x):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    # Find sample sizes\n",
    "    T = len(x)\n",
    "    S = T*np.array([x[t].shape[0] for t in np.arange(T)]).sum()\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Estimate constant\n",
    "    sum_Jt_xi = np.empty((T))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sum_Jt_xi[t] = ((x[t]@Beta - np.log(q_obs[t]))**2).sum()\n",
    "\n",
    "    sigma2_hat = sum_Jt_xi.sum() / S\n",
    "\n",
    "    # Find Standard errors\n",
    "\n",
    "    ZZ = np.empty((T,K,K))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        ZZ[t,:,:] = (z[t].T)@z[t]\n",
    "\n",
    "    Sigma = sigma2_hat*la.inv(ZZ.sum(axis = 0))\n",
    "    SE = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "    return SE"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
