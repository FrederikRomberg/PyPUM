{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "This file collects a few useful functions for plotting, regression tables, computation, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "import sys\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "\n",
    "# Files\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from utilities.Logit_file import estimate_logit, logit_se, logit_t_p, q_logit, logit_score, logit_score_unweighted, logit_ccp, LogitBLP_estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function tests whether the utility parameters are identified, by looking at the rank of the stacked matrix of explanatory variables.\n",
    "\n",
    "def rank_test(x):\n",
    "    '''\n",
    "    This function tests whether the utility parameters are identified, by looking at the rank of the stacked matrix of explanatory variables.\n",
    "\n",
    "    Args:\n",
    "        x: a dictionary (length T) of (J[t],K) numpy arrays or itself a numpy array (T,J,K) of the Covariates\n",
    "    \n",
    "    Returns.\n",
    "        a print statement concerning the rank of x\n",
    "    '''\n",
    "\n",
    "    if (isinstance(x, (np.ndarray))):\n",
    "        if (x.ndim == 3):\n",
    "            T,J,K = x.shape\n",
    "            xpsied = x.reshape((T*J,K))\n",
    "        else:\n",
    "            print('x is array of dim != 3')\n",
    "    else:\n",
    "        T = len(x)\n",
    "        xpsied = np.concatenate([x[t] for t in np.arange(T)], axis = 0)\n",
    "    \n",
    "    eigs = la.eigvals(xpsied.T@xpsied)\n",
    "\n",
    "    if np.min(eigs)<1.0e-8:\n",
    "        print('x does not have full rank')\n",
    "    else:\n",
    "        print('x has full rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kernel_estimate(x_cont, cont_vars, x_min, x_max, n_points, J, outside_option: bool = True):\n",
    "    '''\n",
    "    This function calculates kernel density estimates of each of the densities governing the distribution of the continuous covariates using a Gaussian kernel function.\n",
    "\n",
    "    Args:\n",
    "        x_cont: a numpy array (J[t],D_cont) of continuous covariates for a given market t\n",
    "        cont_vars: a list of the labels of the continuous covariates\n",
    "        x_min: a numpy array (D_cont,) of the minimum value within each characteristic of x_cont. Should be compatible with the value of outside_option (i.e. if outside_option = True, x_min should be the lowest value in x_cont when disregarding the outside_option)\n",
    "        x_max: a numpy array (D_cont,) of the maximum value within each characteristic of x_cont. Should be compatible with the value of outside_option\n",
    "        n_points_ an integer of the amount of points to evaluate the kernel estimator in.\n",
    "        J: an integer describing the amount of alternatives in the given market t (i.e. J[t]). Should be compatible with the value of outside_option.\n",
    "        outside_option: a boolean of whether outside is included in x_cont or not. If 'True' outside option is included in x_cont.\n",
    "    \n",
    "    Returns.\n",
    "        f_hat: a (G,n_points) array of the kernel estimated density for each continuous variable in cont_vars \n",
    "    '''\n",
    "\n",
    "    D_cont = len(cont_vars)\n",
    "    \n",
    "    if outside_option:\n",
    "        z_cont = np.linspace(x_min, x_max, n_points).transpose()\n",
    "        diff = z_cont[:,:,None]*np.ones((D_cont, n_points, J - 1)) - x_cont.transpose()[:,None,1:]\n",
    "        IQR = scstat.iqr(x_cont[1:,:], axis = 0) # Compute interquartile range of each continuous variable\n",
    "        sd = np.std(x_cont[1:,:], axis = 0) # Compute empirical standard deviation of each continuous variable\n",
    "        h = 0.9*np.fmin(sd, IQR/1.34)/((J-1)**(1/5)) # Silverman's rule of thumb\n",
    "    else:\n",
    "        z_cont = np.linspace(x_min, x_max, n_points).transpose()\n",
    "        diff = z_cont[:,:,None]*np.ones((D_cont, n_points, J)) - x_cont.transpose()[:,None,:]\n",
    "        IQR = scstat.iqr(x_cont, axis = 0) # Compute interquartile range of each continuous variable\n",
    "        sd = np.std(x_cont, axis = 0) # Compute empirical standard deviation of each continuous variable\n",
    "        h = 0.9*np.fmin(sd, IQR/1.34)/(J**(1/5)) # Silverman's rule of thumb\n",
    "\n",
    "    K = np.exp(-(diff**2)/(2*(h[:,None,None]**2))) / (np.sqrt(2*np.pi)*h[:,None,None]) # Use a gaussian kernel function\n",
    "\n",
    "    f_hat = K.mean(axis=2)\n",
    "\n",
    "    return f_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_grad(y, x, theta, sample_share, loglikelihood, specification, model == 'IPDL', delta = 1.0e-8):\n",
    "    ''' \n",
    "    This function calculates the numerical and the analytical score functions at a given parameter \\theta aswell the norm of their difference\n",
    "\n",
    "    Args:\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T\n",
    "        model: a dictionary of the Similarity model specification as outputted by 'Similarity_specification'\n",
    "        delta: the incremental change in the argument, a float, used in calculating numerical gradients\n",
    "\n",
    "    Returns.\n",
    "        normdiff: a float of the euclidean norm of the difference between the numerical and analytical score functions at \\theta\n",
    "        angrad: a numpy array (T,K+G) of analytical Similarity scores\n",
    "        numgrad: a numpy array (T,K+G) of numerical Similarity scores\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "    G = len(theta[K:])\n",
    "\n",
    "    numgrad = np.empty((T, K+G))\n",
    "\n",
    "    if model == 'IPDL':\n",
    "        for i in np.arange(K+G):\n",
    "            vec = np.zeros((K+G,))\n",
    "            vec[i] = 1\n",
    "            numgrad[:,i] = (loglikelihood(theta + delta*vec, y, x, sample_share, specification[0], specification[1]) - loglikelihood(theta, y, x, sample_share, specification[0], specification[1])) / delta\n",
    "\n",
    "    else:\n",
    "        for i in np.arange(K+G):\n",
    "            vec = np.zeros((K+G,))\n",
    "            vec[i] = 1\n",
    "            numgrad[:,i] = (loglikelihood(theta + delta*vec, y, x, sample_share, specification) - loglikelihood(theta, y, x, sample_share, specification)) / delta\n",
    "    \n",
    "    return numgrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Reg_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        SE: a numpy array (K+G,) of asymptotic Similarity MLE standard errors\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        N: an integer giving the number of observations\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated asymptotic p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE # Compute two-sided t-tests\n",
    "    p = 2*scstat.t.sf(T, df = N-1) # Compute p-values\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_table(theta, se, N, x_vars, nest_vars):\n",
    "    '''\n",
    "    This function constructs a regression table based on Similarity parameter standard error estimates\n",
    "\n",
    "    Args:\n",
    "        theta: a (K+G,) numpy array of estimated parameters\n",
    "        se: a (K+G,) numpy array of estimated standard errors\n",
    "        N: an integer; the number of observations\n",
    "        x_vars: a list containing the names of the covariates\n",
    "        nest_vars: a list containing the names of the nesting groups\n",
    "\n",
    "    Returns.\n",
    "        table: a pandas dataframe structured as a regression table w. parameter estiamtes, standard errors, t-tests, and p-values  \n",
    "    '''\n",
    "    Similarity_t, Similarity_p = Reg_t_p(se, theta, N) # Get t-test values and p values\n",
    "\n",
    "    regdex = [*x_vars, *['group_' + var for var in nest_vars]] # Set the names of the covariates and the nesting groups as the index\n",
    "\n",
    "    table  = pd.DataFrame({'theta': [ str(np.round(theta[i], decimals = 4)) + '***' if Similarity_p[i] <0.01 else str(np.round(theta[i], decimals = 3)) + '**' if Similarity_p[i] <0.05 else str(np.round(theta[i], decimals = 3)) + '*' if Similarity_p[i] <0.1 else np.round(theta[i], decimals = 3) for i in range(len(theta))], # Give stars to parameter estimates according to t-tests at levels of significance 0.1, 0.05, and 0.01\n",
    "                'se' : np.round(se, decimals = 5),\n",
    "                't (theta == 0)': np.round(Similarity_t, decimals = 3),\n",
    "                'p': np.round(Similarity_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for parameter bounds\n",
    "\n",
    "As we see above, the least squares estimator is not guaranteed to respect the parameter bounds $\\sum_g \\hat \\lambda_g<1$. We can use that if we replace $\\hat q^0_t$ with the choice probabilities from the maximum likelihood estimator of the logit model, $\\hat q^{logit}_t\\propto \\exp\\{X_t\\hat \\beta^{logit}\\}$, and plug these choice probabilities into the WLS estimator described above, it will return $\\hat \\theta=(\\hat \\beta^{logit},0,\\ldots,0)$ as the parameter estimate. Let $\\hat q_t(\\alpha)$ denote the weighted average of the logit probabilites and the market shares,\n",
    "$$\n",
    "\\hat q_t(\\alpha) =(1-\\alpha) \\hat q^{logit}_t+\\alpha \\hat q^0_t.\n",
    "$$\n",
    " Let $\\hat \\theta^0(\\alpha)$ denote the resulting parameter vector. We perform a line search for values of $\\alpha$, $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\ldots)$ until $\\hat \\theta^0(\\alpha)$ yields a feasible parameter vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineSearch(Logit_Beta, q_obs, x, sample_share, estimator, model, N):\n",
    "    '''\n",
    "    This function performs a line search to find feasible lambda parameters\n",
    "\n",
    "    Args:\n",
    "        Logit_beta: a (K,) numpy array of estimated beta parameters from a corresponding Logit model\n",
    "        q_obs: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns.\n",
    "        theta_alpha: a (K+G,) numpy array of feasible parameters found by line search\n",
    "    '''\n",
    "\n",
    "    # Get dimensions of data\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit_ccp(Logit_Beta, x)\n",
    "\n",
    "    # Search over alphas s.t. alpha = (1/2)^{k} for some positive integer k\n",
    "    alpha0 = 0.5\n",
    "\n",
    "    for k in np.arange(1,100):\n",
    "\n",
    "        # Set alpha\n",
    "        alpha = alpha0**k \n",
    "        \n",
    "        # Compute convex combination of ccp's\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha = estimator(q_alpha, x, sample_share, model, N) # Compute initial FKN parameters but using q_alpha ccp's \n",
    "\n",
    "        lambda_alpha = theta_alpha[K:] # Find lambda parameters\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0]) # Find positive lambda parameters\n",
    "\n",
    "        if pos_pars.sum() <1:\n",
    "            break # Break if positive parameters sum to less than 1\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(Logit_Beta, y, x, sample_share, mean_loglikelihood, estimator, model, N, num_alpha = 5):\n",
    "    '''\n",
    "    This function performs a grid search on the unit interval to find feasible parameters \\theta\n",
    "\n",
    "    Args:\n",
    "        Logit_beta: a (K,) numpy array of estimated beta parameters from a corresponding Logit model\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        sample_share: a numpy array (T,) of the share of observations of each market t = 1,...,T,\n",
    "        model: a dictionary of the Similarity model specification as outputted by 'Similarity_specification',\n",
    "        N: an integer giving the number of observations,\n",
    "        num_alpha: an integer of the number of alphas for which the search is to be performed\n",
    "\n",
    "    Returns.\n",
    "        theta_star: a (K+G,) numpy array of feasible parameters found by grid search\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "    J0 = x[0].shape[0]\n",
    "    psi_3d0 = model['psi_3d'][0]\n",
    "    G = np.int64(psi_3d0.shape[0] - 1)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    LogL_alpha = np.empty((num_alpha,))\n",
    "    theta_alpha = np.empty((num_alpha,K+G))\n",
    "\n",
    "    for k in np.arange(len(alpha_line)):\n",
    "\n",
    "        alpha = alpha_line[k]\n",
    "\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha[k,:] = estimator(q_alpha, x, sample_share, model, N)\n",
    "\n",
    "        #lambda_inout = theta_alpha[k,K]\n",
    "        lambda_alpha = theta_alpha[k,K:] # theta_alpha[k,K+1:]\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if (pos_pars.sum() >= 1): #|(lambda_inout >= 1)\n",
    "            LogL_alpha[k] = np.NINF\n",
    "        else:\n",
    "            LogL_alpha[k] = mean_loglikelihood(theta_alpha[k,:], y, x, sample_share, model)\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "    alpha_star = np.argmax(LogL_alpha)\n",
    "    theta_star = theta_alpha[alpha_star,:]\n",
    "\n",
    "    return theta_star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_comparison(estimator_names, variable_names, theta, se, p):\n",
    "    ''' \n",
    "    This function construct a table comparing estimates, standard errors, and significances.\n",
    "\n",
    "    Args:\n",
    "        estimator_names: a list of names of varies models and/or estimators\n",
    "        variable_names: a list of variables names for parameters\n",
    "        theta: a dictionary of numpy arrays of parameters (ordered according to and congruently with variable names)\n",
    "        se: a dictionary of numpy arrays of parameter standard errors (ordered according to and congruently with variable names)\n",
    "        p: a dictionary of numpy arrays of parameter p-values (ordered according to and congruently with variable names)\n",
    "    \n",
    "    Returns.\n",
    "        reg_comp:\n",
    "    '''\n",
    "    \n",
    "    T = len(estimator_names)\n",
    "    \n",
    "    reg_comp_dict = {}\n",
    "    \n",
    "    # Find the dimension of the full parameter space\n",
    "    d = np.max(np.array([theta[i].shape[0] for i in len(theta)])) \n",
    "    \n",
    "    # Construct columns to show filling in empty nest variables where relevant with a dash '-'\n",
    "    for t in np.arange(T):\n",
    "        if theta[t].shape[0] == d:\n",
    "            theta_show = [*theta[t]]\n",
    "            se_show = [*se[t]]\n",
    "            p_show = [*p[t]]\n",
    "        else:\n",
    "            theta_show = [*theta[t],*['-' for i in np.arange(d - theta[t].shape[0])]]\n",
    "            se_show = [*se[t], *['-' for i in np.arange(d - theta[t].shape[0])]]\n",
    "            p_show = [*p[t], *[ 1 for i in np.arange(d - theta[t].shape[0])]]\n",
    "\n",
    "        reg_comp_dict[estimator_names[t]] = [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip(p_show, theta_show, se_show)]\n",
    "\n",
    "    reg_comp = pd.Dataframe(reg_comp_dict, index = variable_names)\n",
    "\n",
    "    return reg_comp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
