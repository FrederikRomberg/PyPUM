{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Demand for Cars with the Similarity model\n",
    "\n",
    "In this notebook, we will introduce and estimate the Inverse Product Differentiation Logit (Similarity) model of Fosgerau et al. (2023) using publically available data on the European car market from Frank Verboven's website at https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market. We begin by introducing the data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "The dataset consists of approximately 110 vehicle makes per year in the period 1970-1999 in five European markets (Belgium, France, Germany, Italy, and the United Kingdom). The data set includes 47 variables in total. The first four columns are market and product codes for the year, country, and make as well as quantity sold (No. of new registrations) which will be used in computing observed market shares. The remaining variables consist of car characteristics such as prices, horse power, weight and other physical car characteristics as well as macroeconomic variables such as GDP per capita which have been used to construct estimates of the average wage income and purchasing power.\n",
    "\n",
    "We have in total 30 years and 5 countries, totalling $T=150$ year-country combinations, indexed by $t$, and we refer to each simply as market $t$. In market $t$, the choice set is $\\mathcal{J}_t$ which includes the set of available makes as well as an outside option. Let $\\mathcal{J} := \\bigcup_{t=1}^T \\mathcal{J}_t$ be the full choice set and \n",
    " $J:=\\#\\mathcal{J}$ the number of choices which were available in at least one market, for this data set there are $J=357$ choices.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the dataset `eurocars.csv` we thus have a dataframe of $\\sum_{t=1}^T \\#\\mathcal{J}_t = 11459$ rows and $47$ columns. The `ye` column runs through $y=70,\\ldots,99$, the `ma` column runs through $m=1,\\ldots,M$, and the ``co`` column takes values $j\\in \\mathcal{J}$. \n",
    "\n",
    "Because we consider a country-year pair as the level of observation, we construct a `market` column taking values $t=1,\\ldots,T$. In Python, this variable will take values $t=0,\\ldots,T-1$. We construct an outside option $j=0$ in each market $t$ by letting the 'sales' of $j=0$ be determined as \n",
    "\n",
    "$$\\mathrm{sales}_{0t} = \\mathrm{pop}_t - \\sum_{j=1}^J \\mathrm{sales}_{jt}$$\n",
    "\n",
    "where $\\mathrm{pop}_t$ is the total population in market $t$, and the car characteristics of the outside option is set to zero. The market shares of each product in market $t$ can then be found as\n",
    "$$\n",
    "\\textrm{market share}_{jt}=\\frac{\\mathrm{sales_{jt}}}{\\mathrm{pop}_t}.\n",
    "$$\n",
    "We also read in the variable description of the dataset contained in `eurocars.dta`. We will use the list `x_vars` throughout to work with our explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "\n",
    "# Files\n",
    "import Logit_file as logit\n",
    "import Eurocarsdata_file as eurocarsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and variable names\n",
    "# os.chdir('../GREENCAR_notebooks/') # Assigns work directory\n",
    "\n",
    "input_path = os.getcwd() # Assigns input path as current working directory (cwd)\n",
    "descr = (pd.read_stata('eurocars.dta', iterator = True)).variable_labels() # Obtain variable descriptions\n",
    "dat_file = pd.read_csv(os.path.join(input_path, 'eurocars.csv')) # reads in the data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ye</td>\n",
       "      <td>year (=first dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ma</td>\n",
       "      <td>market (=second dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>co</td>\n",
       "      <td>model code (=third dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zcode</td>\n",
       "      <td>alternative model code (predecessors and succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brd</td>\n",
       "      <td>brand code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type</td>\n",
       "      <td>name of brand and model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>name of model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>org</td>\n",
       "      <td>origin code (demand side, country with which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>loc</td>\n",
       "      <td>location code (production side, country where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frm</td>\n",
       "      <td>firm code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qu</td>\n",
       "      <td>sales (number of new car registrations)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pl</td>\n",
       "      <td>places (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>doors (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>li1</td>\n",
       "      <td>measure 1 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>li2</td>\n",
       "      <td>measure 2 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>li3</td>\n",
       "      <td>measure 3 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>princ</td>\n",
       "      <td>=pr/(ngdp/pop): price relative to per capita i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eurpr</td>\n",
       "      <td>=pr/avdexr: price in common currency (in SDR t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>exppr</td>\n",
       "      <td>=pr/avexr: price in exporter currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>avexr</td>\n",
       "      <td>av. exchange rate of exporter country (exporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>avdexr</td>\n",
       "      <td>av. exchange rate of destination country (dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>avcpr</td>\n",
       "      <td>av. consumer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>avppr</td>\n",
       "      <td>av. producer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>avdcpr</td>\n",
       "      <td>av. consumer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>avdppr</td>\n",
       "      <td>av. producer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xexr</td>\n",
       "      <td>avdexr/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tax</td>\n",
       "      <td>percentage VAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pop</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ngdp</td>\n",
       "      <td>nominal gross domestic product of destination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rgdp</td>\n",
       "      <td>real gross domestic product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>engdp</td>\n",
       "      <td>=ngdp/avdexr: nominal gdp in common currency (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ergdp</td>\n",
       "      <td>=rgdp/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>engdpc</td>\n",
       "      <td>=engdp/pop: nominal gdp per capita in common c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ergdpc</td>\n",
       "      <td>=ergdp/pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              ye                   year (=first dimension of panel)\n",
       "1              ma                market (=second dimension of panel)\n",
       "2              co             model code (=third dimension of panel)\n",
       "3           zcode  alternative model code (predecessors and succe...\n",
       "4             brd                                         brand code\n",
       "5            type                            name of brand and model\n",
       "6           brand                                      name of brand\n",
       "7           model                                      name of model\n",
       "8             org  origin code (demand side, country with which c...\n",
       "9             loc  location code (production side, country where ...\n",
       "10            cla                              class or segment code\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            frm                                          firm code\n",
       "13             qu            sales (number of new car registrations)\n",
       "14             cy            cylinder volume or displacement (in cc)\n",
       "15             hp                                 horsepower (in kW)\n",
       "16             we                                     weight (in kg)\n",
       "17             pl             places (number, not reliable variable)\n",
       "18             do              doors (number, not reliable variable)\n",
       "19             le                                     length (in cm)\n",
       "20             wi                                      width (in cm)\n",
       "21             he                                     height (in cm)\n",
       "22            li1  measure 1 for fuel efficiency (liter per km, a...\n",
       "23            li2  measure 2 for fuel efficiency (liter per km, a...\n",
       "24            li3  measure 3 for fuel efficiency (liter per km, a...\n",
       "25             li          average of li1, li2, li3 (used in papers)\n",
       "26             sp                            maximum speed (km/hour)\n",
       "27             ac  time to acceleration (in seconds from 0 to 100...\n",
       "28             pr   price (in destination currency including V.A.T.)\n",
       "29          princ  =pr/(ngdp/pop): price relative to per capita i...\n",
       "30          eurpr  =pr/avdexr: price in common currency (in SDR t...\n",
       "31          exppr              =pr/avexr: price in exporter currency\n",
       "32          avexr  av. exchange rate of exporter country (exporte...\n",
       "33         avdexr  av. exchange rate of destination country (dest...\n",
       "34          avcpr       av. consumer price index of exporter country\n",
       "35          avppr       av. producer price index of exporter country\n",
       "36         avdcpr    av. consumer price index of destination country\n",
       "37         avdppr    av. producer price index of destination country\n",
       "38           xexr                                       avdexr/avexr\n",
       "39            tax                                     percentage VAT\n",
       "40            pop                                         population\n",
       "41           ngdp  nominal gross domestic product of destination ...\n",
       "42           rgdp                        real gross domestic product\n",
       "43          engdp  =ngdp/avdexr: nominal gdp in common currency (...\n",
       "44          ergdp                                        =rgdp/avexr\n",
       "45         engdpc  =engdp/pop: nominal gdp per capita in common c...\n",
       "46         ergdpc                                         =ergdp/pop"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(descr, index=['description']).transpose().reset_index().rename(columns={'index' : 'variable names'}) # Prints data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              cy            cylinder volume or displacement (in cc)\n",
       "1              hp                                 horsepower (in kW)\n",
       "2              we                                     weight (in kg)\n",
       "3              le                                     length (in cm)\n",
       "4              wi                                      width (in cm)\n",
       "5              he                                     height (in cm)\n",
       "6              li          average of li1, li2, li3 (used in papers)\n",
       "7              sp                            maximum speed (km/hour)\n",
       "8              ac  time to acceleration (in seconds from 0 to 100...\n",
       "9              pr   price (in destination currency including V.A.T.)\n",
       "10          brand                                      name of brand\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            cla                              class or segment code"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside option is included if OO == True, otherwise analysis is done on the inside options only.\n",
    "OO = True\n",
    "\n",
    "# Choose which variables to include in the analysis, and assign them either as discrete variables or continuous.\n",
    "\n",
    "x_discretevars = [ 'brand', 'home', 'cla']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "z_IV_contvars = ['xexr']\n",
    "z_IV_discretevars = []\n",
    "x_allvars =  [*x_contvars, *x_discretevars]\n",
    "z_allvars = [*z_IV_contvars, *z_IV_discretevars]\n",
    "\n",
    "if OO:\n",
    "    nest_contvars = [var for var in x_contvars if var != 'pr'] # We nest over all variables other than price, but an alternative list can be specified here if desired.\n",
    "    nest_discvars = ['in_out', *x_discretevars]\n",
    "    nest_vars = ['in_out', *nest_contvars, *x_discretevars]\n",
    "else:\n",
    "    nest_contvars = [var for var in x_contvars if (var != 'pr')]\n",
    "    nest_discvars = x_discretevars # See above\n",
    "    nest_vars = [*nest_contvars, *nest_discvars]\n",
    "\n",
    "G = len(nest_vars)\n",
    "\n",
    "# Print list of chosen variables as a dataframe\n",
    "pd.DataFrame(descr, index=['description'])[x_allvars].transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the data to fit our setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, dat_org, x_vars, z_vars, N, pop_share, T, J, K = eurocarsdata.Eurocars_cleandata(dat_file, x_contvars, x_discretevars, z_IV_contvars, z_IV_discretevars, outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of numpy arrays for each market. This allows the size of the data set to vary over markets.\n",
    "\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)} # Dict of explanatory variables\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)} # Dict of market shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function tests whether the utility parameters are identified, by looking at the rank of the stacked matrix of explanatory variables.\n",
    "\n",
    "def rank_test(x):\n",
    "    x_stacked = np.concatenate([x[t] for t in np.arange(T)], axis = 0)\n",
    "    eigs=la.eig(x_stacked.T@x_stacked)[0]\n",
    "\n",
    "    if np.min(eigs)<1.0e-8:\n",
    "        print('x does not have full rank')\n",
    "    else:\n",
    "        print('x has full rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has full rank\n"
     ]
    }
   ],
   "source": [
    "rank_test(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed utility, logit and nested logit\n",
    "\n",
    "In the following, a vector $z\\in \\mathbb R^d$ is always a column vector. The Similarity model is a discrete choice model, where the probability vector over the alternatives is given by the solution to a utility maximization problem of the form\n",
    "$$\n",
    "p=\\arg\\max_{q\\in \\Delta} q'u-\\Omega(q)\n",
    "$$\n",
    "where $\\Delta$ is the probability simplex over the set of discrete choices, $u$ is a vector of payoffs for each option, $\\Omega$ is a convex function and $q'$ denotes the transpose of $q$. All additive random utility models can be represented in this way (Fosgerau and Sørensen (2021)). For example, the logit choice probabilities result from the perturbation function $\\Omega(q)=q'\\ln q$ where $\\ln q$ is the elementwise logarithm.\n",
    "\n",
    "In the nested logit model, the choice set is divided into a partition $\\mathcal C=\\left\\{C_1,\\ldots,C_L\\right\\}$, and the perturbation function is given by\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\sum_{\\ell =1}^L \\left( \\sum_{j\\in C_\\ell}q_j\\right)\\ln \\left( \\sum_{j\\in C}q_j\\right),\n",
    "$$\n",
    "where $\\lambda\\in [0,1)$ is a parameter. This function can be written equivalently as\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\left(\\psi q\\right)'\\ln \\left( \\psi q\\right),\n",
    "$$\n",
    "where $\\psi$ is a $J \\times L$ matrix, where $\\psi_{j\\ell}=1$ if option $j$ belongs to nest $C_\\ell$ and zero otherwise.\n",
    " This specification generates nested logit choice probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Similarity Model\n",
    "\n",
    "Kernel denisity + Silverman's rule of thumb bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_nests(data, markets_id, products_id, in_out_id, cont_var, disc_var, N, outside_option = True):\n",
    "    '''\n",
    "    This function creates the nest matrices \\Psi^{gt}, and stack them over g for each t.\n",
    "\n",
    "    Args.\n",
    "        data: a pandas DataFrame\n",
    "        markets_id: a string denoting the column of 'data' containing an enumeration t=0,1,...,T-1 of markets\n",
    "        products_id: a string denoting the column of 'data' containing product codes which uniquely identifies products\n",
    "        columns: a list containing the column names of columns in 'data' from which nest groupings g=0,1,...,G-1 for each market t are to be generated\n",
    "        cont_var: a list of the continuous variables in 'columns'\n",
    "        cont_var_bins: a list containing the number of bins to make for each continuous variable in 'columns'\n",
    "        outside_option: a boolean indicating whether the model is estimated with or without an outside option. Default is set to 'True' i.e. with an outside option.\n",
    "\n",
    "    Returns\n",
    "        Psi: a dictionary of length T of the J[t] by J[t] identity stacked on top of the Psi_g matrices for each market t and each gropuing g\n",
    "        nest_dict: a dictionary of length T of pandas series describing the structure of each nest for each market t and each grouping g\n",
    "        nest_count: a dictionary of length T of (G,) numpy arrays containing the amount of nests in each category g\n",
    "    '''\n",
    "\n",
    "    T = data[markets_id].nunique()\n",
    "    J = np.array([data[data[markets_id] == t][products_id].nunique() for t in np.arange(T)])\n",
    "    \n",
    "    # We include nest on outside vs. inside options. The amount of categories varies if the outside option is included in the analysis.\n",
    "    dat = data.sort_values(by = [markets_id, products_id]) # We sort the data in ascending, first according to market and then according to the product id\n",
    "    \n",
    "    Psi = {}\n",
    "    if OO:\n",
    "        in_out_index = [n for n in np.arange(len(disc_var)) if disc_var[n] == in_out_id][0]\n",
    "        non_in_out_indices = np.array([n for n in np.arange(len(disc_var)) if disc_var[n] != in_out_id])\n",
    "\n",
    "    # Assign nests for products in each market t\n",
    "    for t in np.arange(T):\n",
    "        data_t = dat[dat[markets_id] == t] # Subset data on market t\n",
    "\n",
    "        # Estimate discrete kernels\n",
    "        D_disc = len(disc_var)\n",
    "        K_disc = np.empty((D_disc, J[t], J[t]))\n",
    "        C = np.array(data_t[disc_var].nunique())\n",
    "\n",
    "        for d in np.arange(D_disc):\n",
    "            Indicator = pd.get_dummies(data_t[disc_var[d]]).values.reshape((J[t], C[d]))\n",
    "            K_disc[d,:,:] = Indicator@(Indicator.T)\n",
    "\n",
    "        Psidisc_t = np.einsum('djk,dk->djk', K_disc, 1./(K_disc.sum(axis=1)))\n",
    "            \n",
    "        # Estimate continuous kernels\n",
    "        D_cont = len(cont_var)\n",
    "        IQR = scstat.iqr(data_t[cont_var].values, axis = 0) # Compute interquartile range of each continuous variable\n",
    "        sd = np.std(data_t[cont_var].values, axis = 0) # Compute empirical standard deviation of each continuous variable\n",
    "        h = 0.9*np.fmin(sd, IQR/1.34)/(N**(1/5)) # Use Silverman's rule of thumb for bandwidth estimation for each continuous variable\n",
    "        z = data_t[cont_var].values.transpose()\n",
    "        diff = z[:,:,None]*np.ones((D_cont, J[t], J[t])) - z[:,None,:]\n",
    "        K_cont = np.exp(-(diff**2)/(2*h[:,None,None]))\n",
    "\n",
    "        Psicont_t = np.einsum('djk,dk->djk', K_cont, 1./K_cont.sum(axis=1))\n",
    "\n",
    "        # Stack Psi\n",
    "        D = len([*cont_var, *disc_var]) + 1\n",
    "\n",
    "        if outside_option:\n",
    "            Psi[t] = np.concatenate((np.eye(J[t]).reshape((1,J[t],J[t])), Psidisc_t[in_out_index,:,:].reshape((1,J[t],J[t])), Psicont_t, Psidisc_t[non_in_out_indices,:,:]), axis = 0).reshape((D*J[t], J[t]))\n",
    "        else:\n",
    "            Psi[t] = np.concatenate((np.eye(J[t]).reshape((1,J[t],J[t])), Psicont_t, Psidisc_t), axis = 0).reshape((D*J[t], J[t]))\n",
    "\n",
    "    return Psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi = Create_nests(dat, 'market', 'co', 'in_out', nest_contvars, nest_discvars, N, outside_option = OO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributions of the continuous explanatory variables\n",
    "\n",
    "We plot the empirical distributions of the continuous variables against the kernel approximation. Ask Nikolaj for his exact thoughts on these..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the observed x's for each explanatory variable\n",
    "\n",
    "x_cont = {t: dat[dat['market'] == t][nest_contvars].values.reshape((J[t],len(nest_contvars))) for t in np.arange(T)}\n",
    "x_cont0 = np.sort(x_cont[0]) # Get variables for the first market\n",
    "#q_cont0 = np.exp(-(x_cont0**2)/2) / np.exp(-(x_cont0**2)/2).sum(axis=0)[None,:]\n",
    "\n",
    "x_pairs = iter.product(np.arange(3), np.arange(3))\n",
    "num_bins = 20\n",
    "\n",
    "fig, axes = plt.subplots(3, 3)\n",
    "\n",
    "\n",
    "for p,d in zip(x_pairs, np.arange(len(nest_contvars))):\n",
    "    x_cont0d = np.sort(np.unique(x_cont0[:,d]))\n",
    "    IQR = scstat.iqr(x_cont0d)\n",
    "    sd = np.std(x_cont0d)\n",
    "    h = 0.9*np.fmin(sd, IQR/1.34)/(N**(1/5))\n",
    "    q_cont0d = np.exp(-(x_cont0d**2)/(2*h)) / np.exp(-(x_cont0d**2)/(2*h)).sum()\n",
    "    X_Y_spline = make_interp_spline(x_cont0d, q_cont0d)\n",
    "    X_ = np.linspace(x_cont0d.min(), x_cont0d.max(), 500)\n",
    "    Y_ = X_Y_spline(X_)\n",
    "\n",
    "    axes[p].hist(x_cont0d, num_bins, color = 'r', alpha = 1)\n",
    "    axes[p].plot(X_, Y_)\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E0p = {j : (E0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E0p[j], num_bins, range = (np.quantile(E0p[j], 0.10), np.quantile(E0p[j], 0.90)), color = 'r', alpha = 1) # Logit is blue\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Gamma(Lambda, Psi):\n",
    "    '''\n",
    "    This function \n",
    "    '''\n",
    "\n",
    "    T = len(Psi)\n",
    "    J = np.array([Psi[t].shape[1] for t in np.arange(T)])\n",
    "    \n",
    "    Gamma = {}\n",
    "    lambda0 = np.array([1 - sum(Lambda)])\n",
    "    Lambda_full = np.concatenate((lambda0, Lambda)) # create vector (1- sum(lambda), lambda_1, ..., lambda_G)\n",
    "    D = len(Lambda_full)\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Lambda_long = np.einsum('d,dj->dj', Lambda_full, np.ones((D,J[t]))).reshape((D*J[t],))\n",
    "        Gamma[t] = Lambda_long[:,None]*Psi[t]\n",
    "\n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda0 = np.ones((G,))/(2*(G+1))\n",
    "Gamma = Create_Gamma(lambda0, Psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "Suppose we are evaluating the choice probability function $p_t(\\theta)$ at some parameter vector $\\theta$. While it is possible to solve for the choice probabilities explicitly by numerical maximization, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Let $u_t=X_t\\beta$ and let $q_t^0$ be an initial guess of the choice probabilities, e.g. $q_t^0\\propto \\exp(X_t\\beta)$. Define further\n",
    "$$\n",
    "a=\\sum_{g:\\lambda_g\\geq 0} \\lambda_g   \\qquad b=\\sum_{g:\\lambda_g<0} |\\lambda_g|.\n",
    "$$\n",
    "\n",
    "The choice probabilities are then updated iteratively as\n",
    "$$\n",
    "q_t^{r} = \\frac{e^{v_t^{r}}}{\\sum_{j\\in \\mathcal J_t} e^{v_{tj}^{r}}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "v_t^{r} =\\ln q_t^{r-1}+\\left(u_t-\\nabla_q \\Omega_t(q^{r-1}_t|\\lambda)\\right)/(1+b).\n",
    "$$\n",
    "Using the definition of $Z_{gt}$ above, this becomes\n",
    "$$\n",
    "v^r_t=\\ln q_t^{r-1}+\\left(u_t+Z_{t}(q^{r-1})\\lambda-\\ln q_t^{r-1}  \\right)/(1+b) =  \\left( u_t+ b\\ln q^{r-1}_t+Z_{t}(q^{r-1})\\lambda\\right)/(1+b)\n",
    "$$\n",
    "\n",
    "\n",
    "For numerical stability, it can be a good idea to also do max-rescaling of $v^r_t$ at every iteration. The Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p_t(\\theta)||q_t^{r})\\leq \\frac{a+b}{1+b}D_{KL}(p_t(\\theta)||q^{r-1}_t).\n",
    "$$\n",
    "This is implemeneted in the function \"Similarity_ccp\" below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMEMBER $\\delta$ in Similarity gradient $\\nabla_q \\Omega$ !!!\n",
    "\n",
    "Note that $\\delta_j = \\sum_d \\eta_d (\\psi^d e_j)'\\ln(\\psi^d e_j)$. Hence if $\\varphi^d \\in \\mathbb{R}^J$ has $\\varphi^d_j = \\sum_k \\psi^d_{kj}\\ln(\\psi^d_{kj}) = {\\psi^d_{(j)}}'\\ln(\\psi^d_{(j)})$ such that we have ${\\varphi^d} = (\\psi^d \\circ \\ln(\\psi^d))'\\iota$ and set $\\varphi = (\\varphi^1 \\ldots \\varphi^D)\\in\\mathbb{R}^{J \\times D}$ then we have $\\delta = \\varphi \\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_matrix(psi):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(psi)\n",
    "    J = np.array([psi[t].shape[1] for t in np.arange(T)])\n",
    "    G = np.int32(psi[0].shape[0] / J[0] - 1)\n",
    "\n",
    "    phi = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        phi_t = np.empty((J[t], G))\n",
    "        psi_t = psi[t]\n",
    "\n",
    "        for d in np.arange(1,G+1):\n",
    "            psi_d = psi_t[d*J[t]:(d+1)*J[t],:]\n",
    "            phi_t[:,d-1] = (psi_d*np.log(psi_d, out = np.zeros_like(psi_d), where = (psi_d > 0))).sum(axis=0)\n",
    "        \n",
    "        phi[t] = phi_t\n",
    "\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_ccp(Theta, x, psi, tol = 1.0e-15, maximum_iterations = 1000):\n",
    "    '''\n",
    "    This function finds approximations to the true conditional choice probabilities given parameters.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        tol: tolerated approximation error\n",
    "        maximum_iterations: a no. of maximum iterations which if reached will stop the algorithm\n",
    "\n",
    "    Output\n",
    "        q_1: a dictionary of T numpy arrays (J[t],) of Similarity choice probabilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x) # Number of markets\n",
    "    K = x[0].shape[1] # Number of car characteristics\n",
    "\n",
    "    # Parameters\n",
    "    Beta = Theta[:K]\n",
    "    Lambda = Theta[K:]\n",
    "    G = len(Lambda)  # Number of groups\n",
    "\n",
    "    # Calculate small beta\n",
    "    C_minus = np.array([True if Lambda[g] < 0 else False for g in np.arange(G)])\n",
    "    #print(C_minus) # Find the categories g with negative a negative parameter lambda_g\n",
    "    if C_minus.all() == False:\n",
    "        b = 0\n",
    "    else:    \n",
    "        b = np.abs(Lambda[C_minus]).sum() # sum of absolute value of negative lambda parameters.\n",
    "\n",
    "    # Find the Gamma matrix and \\phi\n",
    "    Gamma = Create_Gamma(Lambda, psi)\n",
    "    Phi = phi_matrix(psi)\n",
    "\n",
    "    u = {t: np.einsum('jk,k->j', x[t], Beta) for t in np.arange(T)} # Calculate linear utilities\n",
    "    q = {t: np.exp(u[t]) / np.exp(u[t]).sum() for t in np.arange(T)}\n",
    "    q0 = q\n",
    "    Epsilon = 1.0e-10\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        q1 = {}\n",
    "        for t in np.arange(T):\n",
    "            # Calculate v\n",
    "            psi_q = np.einsum('cj,j->c', psi[t], q0[t]) # Compute matrix product\n",
    "            log_psiq =  np.log(np.abs(psi_q) + Epsilon) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "            delta = Phi[t]@Lambda\n",
    "            Grad = np.einsum('cj,c->j', Gamma[t], log_psiq) - delta # Compute matrix product\n",
    "            v = np.log(q0[t] + Epsilon) + (u[t] - Grad)/(1 + b) # Calculate v = log(q) + (u - (Gamma^T %o% log(Gamma %o% q) %o% Gamma) - delta)/(1 + b)\n",
    "            v -= v.max(keepdims = True) # Do max rescaling wrt. alternatives\n",
    "\n",
    "            # Calculate iterated ccp q^k\n",
    "            numerator = np.exp(v)\n",
    "            denom = numerator.sum()\n",
    "            q1[t] = numerator/denom\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.array([np.sum((q1[t]-q0[t])**2/q[t]) for t in np.arange(T)])) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "        \n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001526\n",
      "         Iterations: 25\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 29\n"
     ]
    }
   ],
   "source": [
    "beta0 = logit.estimate_logit(logit.q_logit, np.zeros((K,)), y, x, pop_share)['beta']\n",
    "theta0 = np.append(beta0, lambda0)\n",
    "q0 = Similarity_ccp(theta0, x, Psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q1_star = {t: np.ones((J[t],))/J[t] for t in np.arange(T)} # Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numgrad = {}\n",
    "epsilon = 1.0e-10\n",
    "Phi = phi_matrix(Psi)\n",
    "Gamma = Create_Gamma(theta0[K:], Psi)\n",
    "\n",
    "for t in np.arange(T):\n",
    "    numgrad_t = np.empty((J[t],))\n",
    "    Gamma_t = Gamma[t]\n",
    "    Psi_t = Psi[t]\n",
    "    delta = Phi[t]@theta0[K:]\n",
    "    q0_t = q1_star[t]\n",
    "    Omega0 = (Gamma_t@q0_t)@np.log(Psi_t@q0_t) - q0_t@delta\n",
    "\n",
    "    for j in np.arange(J[t]):\n",
    "        vec = np.zeros((J[t],))\n",
    "        vec[j] = 1\n",
    "        q1_t = q0_t + epsilon*vec\n",
    "\n",
    "        Omega1 = (Gamma_t@q1_t)@np.log(Psi_t@q1_t) - q1_t@delta\n",
    "\n",
    "        numgrad_t[j] = (Omega1 - Omega0) / epsilon\n",
    "    \n",
    "    numgrad[t] = numgrad_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numgrad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "angrad = {}\n",
    "for t in np.arange(T):\n",
    "    psi_q = np.einsum('cj,j->c', Psi[t], q1_star[t]) # Compute matrix product\n",
    "    log_psiq =  np.log(np.abs(psi_q) + epsilon) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "    delta = Phi[t]@theta0[K:]\n",
    "    angrad[t] = np.einsum('cj,c->j', Gamma[t], log_psiq) - delta + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "angrad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([la.norm(angrad[t] - numgrad[t]) for t in np.arange(T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute cross-derivative numerically from analytic gradient, and analytically by below formulas respectively\n",
    "\n",
    "$\\nabla_q \\Omega(q) = \\Gamma \\ln(\\psi q) - \\delta + \\iota$ and $Z_g = \\nabla_{q,\\lambda} \\Omega(q)_d = \\ln(q) - {\\psi^d}'\\ln(\\psi^d q) - \\varphi^d$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epsilon = 1.0e-10\n",
    "numcross = {}\n",
    "for t in np.arange(T):\n",
    "    q0_t = q1_star[t]\n",
    "    Phi_t = Phi[t]\n",
    "    Psi_t = Psi[t]\n",
    "    Gamma_t = Gamma[t]\n",
    "    delta = Phi_t@theta0[K:]\n",
    "    log_psiq = np.log(np.einsum('cj,j->c', Psi_t, q0_t))\n",
    "    cross0 = np.einsum('cj,c->j', Gamma_t, log_psiq) - delta\n",
    "    cross1 = np.empty((J[t],G))\n",
    "\n",
    "    for d in np.arange(G):\n",
    "        vec = np.zeros((G,))\n",
    "        vec[d] = 1\n",
    "        lambda1 = theta0[K:] + epsilon*vec\n",
    "        Gamma1 = Create_Gamma(lambda1, Psi)\n",
    "        delta1 = Phi_t @ lambda1\n",
    "        cross1[:,d] = np.einsum('cj,c->j', Gamma1[t], log_psiq) - delta1\n",
    "\n",
    "    numcross[t] = (cross1 - cross0[:,None])/epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ancross = {}\n",
    "J = np.array([x[t].shape[0] for t in np.arange(T)])\n",
    "for t in np.arange(T):\n",
    "    ancross_t = np.empty((J[t],G))\n",
    "    q0_t = q1_star[t]\n",
    "    Phi_t = Phi[t]\n",
    "    Psi_t = Psi[t]\n",
    "    for d in np.arange(1,G+1):\n",
    "        Psi_d = Psi_t[d*J[t]:(d+1)*J[t],:]\n",
    "\n",
    "        ancross_t[:,d-1] = -np.log(q0_t) + Psi_d.T @ np.log(Psi_d @ q0_t) - Phi_t[:,d-1]\n",
    "\n",
    "    ancross[t] = ancross_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(numcross[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(ancross[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(ancross[0]-numcross[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([la.norm(ancross[t] - numcross[t]) for t in np.arange(T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_grad_pertubation(q1_star, Psi)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand derivatives and price Elasticity\n",
    "\n",
    "While the demand derivatives in the Similarity model are not quite as simple as in the logit model, they are still easy to compute. \n",
    "Let $q=P(u|\\lambda)$, then\n",
    "$$\n",
    "\\nabla_u P(u|\\lambda)=\\left(\\nabla^2_{qq}\\Omega(q|\\lambda)\\right)^{-1}-qq'\n",
    "$$\n",
    "where the $()^{-1}$ denotes the matrix inverse. The derivatives with respect to any $x_{ij\\ell}$ can now easily be computed by the chain rule,\n",
    "$$\n",
    "    \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{\\partial u_{ik}}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell,\n",
    "$$\n",
    "\n",
    "Finally, moving to price elasticity is the same as in the logit model, if $x_{ik\\ell}$ is the log price of product $k$ for individual $i$, then\n",
    "$$\n",
    "    \\mathcal{E}_{jk}= \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}\\frac{1}{P_j(u_i|\\lambda)}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{1}{P_j(u_i|\\lambda)}\\beta_\\ell=\\frac{\\partial \\ln P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell$$\n",
    "we can also write this compactly as\n",
    "$$\n",
    "\\nabla_u \\ln P(u|\\lambda)=\\mathrm{diag}(P(u|\\lambda))^{-1}\\nabla_u P(u|\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pertubation_hessian(q, x, Theta, psi):\n",
    "    '''\n",
    "    This function calucates the hessian of the pertubation function \\Omega\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Hess: a dictionary of T numpy arrays (J[t],J[t]) of second partial derivatives of the pertubation function \\Omega for each market t\n",
    "    '''\n",
    "    \n",
    "    T = len(q.keys())\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    Gamma = Create_Gamma(Theta[K:], psi) # Find the \\Gamma matrices \n",
    "    \n",
    "    Hess={}\n",
    "    for t in np.arange(T):\n",
    "        psi_q = np.einsum('cj,j->c', psi[t], q[t]) # Compute a matrix product\n",
    "        Hess[t] = np.einsum('cj,c,cl->jl', Gamma[t], 1/psi_q, psi[t]) # Computes the product \\Gamma' diag(\\psi q)^{-1} \\psi (but faster)\n",
    "\n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_gradient(q, x, Theta, psi_stack):\n",
    "    \n",
    "    '''\n",
    "    This function calucates the gradient of the choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K) of partial derivatives of the choice proabilities wrt. utilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Grad = {}\n",
    "    Hess = compute_pertubation_hessian(q, x, Theta, psi_stack) # Compute the hessian of the pertubation function\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        inv_omega_hess = la.inv(Hess[t]) # (J,J) for each t=1,...,T , computes the inverse of the Hessian\n",
    "        qqT = q[t][:,None]*q[t][None,:] # (J,J) outerproduct of ccp's for each market t\n",
    "        Grad[t] = inv_omega_hess - qqT  # Compute Similarity gradient of ccp's wrt. utilities\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_u_grad_Log_ccp(q, x, Theta, psi_stack):\n",
    "    '''\n",
    "    This function calucates the gradient of the log choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Epsilon: a dictionary of T numpy arrays (J[t],J[t]) of partial derivatives of the log choice proabilities of products j wrt. utilites of products k for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack) # Find the gradient of ccp's wrt. utilities\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        #ccp_grad = Grad[t]\n",
    "        #inv_diagq = np.divide(1, q[t], out = np.inf*np.ones_like(q[t]), where = (q[t] > 0)) # Find the inverse of the ccp's and assign infinity to any entry if that entry has q = 0\n",
    "        Epsilon[t] = Grad[t]/q[t][:,None] # Computes diag(q)^{-1}Grad[t]\n",
    "        #np.einsum('j,jk->jk', inv_diagq, ccp_grad) # Computes a Hadamard product. Is equivalent to:   diag(q)^-1 %o% ccp_grad\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_elasticity(q, x, Theta, psi_stack, char_number = K-1):\n",
    "    ''' \n",
    "    This function calculates the elasticity of choice probabilities wrt. any characteristic or nest grouping of products\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        char_number: an integer which is an index of the parameter in theta wrt. which we wish calculate the elasticity. Default is the index for the parameter of 'pr'.\n",
    "\n",
    "    Returns\n",
    "        a dictionary of T numpy arrays (J[t],J[t]) of choice probability semi-elasticities for each market t\n",
    "    '''\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = Similarity_u_grad_Log_ccp(q, x, Theta, psi_stack) # Find the gradient of log ccp's wrt. utilities\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = Grad[t]*Theta[char_number] # Calculate semi-elasticities\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of Similarity\n",
    "\n",
    "The log-likelihood contribution is\n",
    "$$\n",
    "\\ell_t(\\theta)=y_t'\\ln p(\\mathbf{X}_t,\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_t$ and $\\theta$ - calculates $u_t=\\mathbf{X}_t\\beta$ and constructs $\\Gamma$, and then calls the fixed point routine described above. That routine will return $p(\\mathbf{X}_t,\\theta)$, and we can then evaluate $\\ell_t(\\theta)$. Using our above defined functions we now construct precisely such an estimation procedure.\n",
    "\n",
    "For maximizing the likelihood, we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_t=p(\\mathbf{X}_t,\\theta)$, then we have\n",
    "$$\n",
    "\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)=\\mathrm{diag}(q_t)^{-1}\\left(\\nabla_{qq}^2\\Omega(q_t|\\lambda)^{-1}-q_tq_t' \\right)\\left[\\mathbf{X}_t,-\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P(u|\\lambda)$ and the last term is a block matrix of size $J\\times dim(\\theta)$. Note that the latter cross derivative $\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)$ is given by $\\nabla_{q,\\lambda} \\Omega(q_t|\\lambda)_g = \\ln(q) - (\\Psi^g)' \\ln(\\Psi^g q) - \\varphi^d$ for each row $g=1,\\ldots,G$. The derivative of the log-likelihood function can be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_t(\\theta)=\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)' y_t \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_loglikelihood(Theta, y, x, sample_share, psi_stack):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (T,) of Similarity loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "    ccp_hat = Similarity_ccp(Theta, x, psi_stack)\n",
    "    sum_lambdaplus = np.array([theta for theta in Theta[K:] if theta >0]).sum()\n",
    "\n",
    "    '''if sum_lambdaplus >= 1:\n",
    "        ll = np.NINF*np.ones((T,))'''\n",
    "\n",
    "    \n",
    "    ll=np.empty((T,))\n",
    "    for t in np.arange(T):\n",
    "        ll[t] = sample_share[t]*(y[t].T@np.log(ccp_hat[t])) #np.einsum('j,j', y[t], np.log(ccp_hat[t], out = -np.inf*np.ones_like(ccp_hat[t]), where = (ccp_hat[t] > 0)))\n",
    "\n",
    "    print([sum_lambdaplus, -ll.mean()])\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_Similarity(Theta, y, x, sample_share, psi_stack):\n",
    "    ''' The negative loglikelihood criterion to minimize\n",
    "    '''\n",
    "    Q = -Similarity_loglikelihood(Theta, y, x, sample_share, psi_stack)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the derivative of the loglikehood wrt. parameters $\\nabla_\\theta \\ell_t(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation(q, psi_stack):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    J = np.array([q[t].shape[0] for t in np.arange(T)])\n",
    "    G = np.int32((psi_stack[0].shape[0] / J[0]) - 1)\n",
    "\n",
    "    Phi = phi_matrix(psi_stack)\n",
    "    \n",
    "    Z = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        \n",
    "        log_q = np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0))\n",
    "        Psi_t = Psi[t]\n",
    "        Z_t = np.empty((J[t], G))\n",
    "        for d in np.arange(1,G+1):\n",
    "            Psi_d = Psi_t[d*J[t]:(d+1)*J[t],:]\n",
    "            Psiq = np.einsum('cj,j->c', Psi_d, q[t])\n",
    "            log_psiq = np.log(Psiq, out = -np.inf*np.ones_like(Psiq), where = (Psiq > 0))\n",
    "            Z_t[:,d-1] = -log_q + np.einsum('cj,c->j', Psi_d, log_psiq) - Phi[t][:,d-1]\n",
    "\n",
    "        Z[t] = Z_t\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi = phi_matrix(Psi)\n",
    "Phi[0][:,len(nest_contvars)+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_theta_grad_log_ccp(Theta, x, psi_stack):\n",
    "    '''\n",
    "    This function calculates the derivative of the Similarity log ccp's wrt. parameters theta\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K+G) of derivatives of the Similarity log ccp's wrt. parameters theta for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "\n",
    "    q = Similarity_ccp(Theta, x, psi_stack) # Find choice probabilities\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack) # Find cross differentials of the pertubation function\n",
    "    u_grad = Similarity_u_grad_Log_ccp(q, x, Theta, psi_stack)  # Find the gradient of log ccp's wrt. utilities\n",
    "    Grad={}\n",
    "\n",
    "    for t in range(T):\n",
    "        G = np.concatenate((x[t], -Z[t]), axis=1)\n",
    "        Grad[t] = u_grad[t] @ G\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_score(Theta, y, x, sample_share, psi_stack):\n",
    "    '''\n",
    "    This function calculates the score of the Similarity loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of Similarity scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = Similarity_theta_grad_log_ccp(Theta, x, psi_stack) # Find derivatives of Similarity log ccp's wrt. parameters theta\n",
    "    D = log_ccp_grad[0].shape[1] # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] =sample_share[t]*(log_ccp_grad[t].T@y[t]) #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_Similarity_score(Theta, y, x, sample_share, psi_stack):\n",
    "    ''' The derivative of the negative loglikelihood criterion\n",
    "    '''\n",
    "    return -Similarity_score(Theta, y, x, sample_share, psi_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analyticgrad(y, x, theta, sample_share, Psi, delta = 1.0e-8):\n",
    "\n",
    "    numgrad = np.empty((T, K+G))\n",
    "\n",
    "    for i in np.arange(K+G):\n",
    "        vec = np.zeros((K+G,))\n",
    "        vec[i] = 1\n",
    "        numgrad[:,i] = (Similarity_loglikelihood(theta + delta*vec, y, x, sample_share, Psi) - Similarity_loglikelihood(theta, y, x, sample_share, Psi)) / delta\n",
    "\n",
    "    angrad = Similarity_score(theta, y, x, sample_share, Psi)\n",
    "\n",
    "    normdiff = la.norm(angrad - numgrad)\n",
    "    \n",
    "    return normdiff, angrad, normdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.0016471517159650625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517168120111]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517168152778]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517165527138]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517161612715]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517160877222]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517161166438]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151716662896]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517163931682]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517168221155]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171641808]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172009919]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517169902993]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172058573]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.00164715171714377]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171391214]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171970846]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171984494]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172058842]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517169999565]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517170834288]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171869339]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171983633]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172403404]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172006915]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.00164715171722091]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171836407]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717109342]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171967325]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171710278]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517170800639]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517170807688]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717048378]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717172831]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171953347]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171830626]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717198561]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171947729]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172003307]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717202232]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717199706]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171956244]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171996167]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171940387]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171792861]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171734941]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171999163]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517167023454]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517167772377]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517168301704]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717051745]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171016104]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.001647151723259925]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.001647151722056139]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517219190611]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517218235336]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517219798465]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517223013074]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517227211608]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517221485254]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517221158254]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517220643076]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517192025145]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.00164715172199389]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517210732668]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n"
     ]
    }
   ],
   "source": [
    "diff, an, num = test_analyticgrad(y, x, theta0, pop_share, Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1796307345394429e-07"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.08555430e-05,  6.11999636e-06,  4.53420716e-06, ...,\n",
       "        -2.67603355e-05, -8.29359021e-05, -4.95895540e-05],\n",
       "       [ 4.31652052e-05,  1.31785373e-05,  1.04930932e-05, ...,\n",
       "        -4.98394016e-05, -1.18049042e-04, -8.29727509e-05],\n",
       "       [ 1.55935401e-04,  5.80433137e-05,  3.90703502e-05, ...,\n",
       "        -1.70545030e-04, -6.07621693e-04, -6.65908324e-04],\n",
       "       ...,\n",
       "       [ 2.95786191e-04,  1.05478171e-04,  1.23873258e-04, ...,\n",
       "        -5.13041766e-04, -1.35766669e-03, -9.83906420e-04],\n",
       "       [ 2.02652241e-04,  5.73021459e-05,  6.34255856e-05, ...,\n",
       "        -2.94564533e-04, -8.68285959e-04, -7.86481381e-04],\n",
       "       [ 1.87161499e-04,  5.97493288e-05,  7.06001950e-05, ...,\n",
       "        -2.78533800e-04, -9.05125962e-04, -8.70216939e-04]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an - num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors in Maximum Likelihood estimation\n",
    "\n",
    "As usual we may consistently estimate the Covariance Matrix  of the Similarity maximum likelihood estimator for some estimate $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'\\in \\mathbb{R}^{K+G}$ as:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\left( \\sum_{i=1}^N \\nabla_\\theta \\ell_i (\\hat \\theta) \\nabla_\\theta \\ell_i (\\hat \\theta)' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Thereby we may find the estimated standard error of parameter $d$ as the squareroot of the d'th diagonal entry of $\\hat \\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_d = \\sqrt{\\hat \\Sigma_{dd}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_se(score, N):\n",
    "    '''\n",
    "    This function computes the asymptotic standard errors of the MLE.\n",
    "\n",
    "    Args.\n",
    "        score: a numpy array (T,K+G) of Similarity scores\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns\n",
    "        SE: a numpy array (K+G,) of asymptotic Similarity MLE standard errors\n",
    "    '''\n",
    "\n",
    "    SE = np.sqrt(np.diag(la.inv(np.einsum('td,tm->dm', score, score))) / N)\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        SE: a numpy array (K+G,) of asymptotic Similarity MLE standard errors\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        N: an integer giving the number of observations\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated asymptotic p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE\n",
    "    p = 2*scstat.t.sf(T, df = N-1)\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_Similarity(f, Theta0, y, x, sample_share, psi_stack, N, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given starting values and variables necessary in the Similarity model specification.\n",
    "\n",
    "    Args:\n",
    "        f: a function to minimize,\n",
    "        Theta0 : a numpy array (K+G,) of initial guess parameters (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests', \n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t,\n",
    "        N: an integer giving the number of observations,\n",
    "        Analytic_jac: a boolean. Default value is 'True'. If 'True' the analytic jacobian of the Similarity loglikelihood function is used in estimation. Else the numerical jacobian is used.\n",
    "        options: dictionary with options for the optimizer (e.g. disp=True which tells it to display information at termination.)\n",
    "    \n",
    "    Returns:\n",
    "        res: a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(f(Theta, y, x, sample_share, psi_stack))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_Similarity_score(Theta, y, x, sample_share, psi_stack), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Theta0.tolist(), options=options, jac=Grad, **kwargs) # optimize.minimize takes a list of parameters Theta0 (not a numpy array) as initial guess.\n",
    "    se = Similarity_se(Similarity_score(result.x, y, x, sample_share, psi_stack), N)\n",
    "    T,p = Similarity_t_p(se, result.x, N)\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'theta': result.x,\n",
    "        'se': se,\n",
    "        't': T,\n",
    "        'p': p,\n",
    "        'success':  result.success, # bool, whether convergence was succesful 1\n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001529\n",
      "         Iterations: 26\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.ones((K,))\n",
    "\n",
    "# Estimate the model\n",
    "Logit_beta = logit.estimate_logit(logit.q_logit, beta_0, y, x, sample_share=pop_share, Analytic_jac=True)['beta']\n",
    "Logit_SE = logit.logit_se(logit.logit_score(Logit_beta, y, x, pop_share), N)\n",
    "Logit_t, Logit_p = logit.logit_t_p(Logit_beta, logit.logit_score(Logit_beta, y, x, pop_share), N)\n",
    "\n",
    "# Initialize \\theta^0\n",
    "theta0 = np.append(Logit_beta,lambda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.001651690244391444]\n",
      "[0.45807927070581517, 0.0016485611056047584]\n",
      "[0.4332534963862184, 0.001636459192830168]\n",
      "[0.3339503991078314, 0.0015941905340232736]\n",
      "[0.03881379022735937, 0.0015285970161815269]\n",
      "[0.03279339648803796, 0.0015285650405973353]\n",
      "[0.03430005055597402, 0.0015285573566613637]\n",
      "[0.03452447012201116, 0.0015285552522675612]\n",
      "[0.035422148386159695, 0.0015285490170700072]\n",
      "[0.0361327554350772, 0.0015285333984966008]\n",
      "[0.036842904570052674, 0.0015285032542644776]\n",
      "[0.041749067794569925, 0.0015284106106405507]\n",
      "[0.05136920020809256, 0.001528203668060995]\n",
      "[0.06943459772837621, 0.0015278361604085062]\n",
      "[0.10260316015540435, 0.0015271755704751061]\n",
      "[0.15999128515574787, 0.0015260137508878876]\n",
      "[0.2640469359883095, 0.001524127500650879]\n",
      "[0.43416156207891726, 0.001522044988125294]\n",
      "[0.47328708436917266, 0.0015213441480257362]\n",
      "[0.4636624201081757, 0.0015208775251473465]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001521\n",
      "         Iterations: 15\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 20\n"
     ]
    }
   ],
   "source": [
    "resMLE = estimate_Similarity(q_Similarity, theta0, y, x, pop_share, Psi, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_table(theta,se,N,x_vars,nest_vars):\n",
    "    Similarity_t, Similarity_p = Similarity_t_p(se, theta, N)\n",
    "\n",
    "    if OO:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "    else:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "\n",
    "    table  = pd.DataFrame({'theta': [ str(np.round(theta[i], decimals = 4)) + '***' if Similarity_p[i] <0.01 else str(np.round(theta[i], decimals = 3)) + '**' if Similarity_p[i] <0.05 else str(np.round(theta[i], decimals = 3)) + '*' if Similarity_p[i] <0.1 else np.round(theta[i], decimals = 3) for i in range(len(theta))], \n",
    "                'se' : np.round(se, decimals = 5),\n",
    "                't (theta == 0)': np.round(Similarity_t, decimals = 3),\n",
    "                'p': np.round(Similarity_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.545</td>\n",
       "      <td>5.43585</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.318</td>\n",
       "      <td>2.79781</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.457</td>\n",
       "      <td>3.15701</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.945</td>\n",
       "      <td>2.89147</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.953</td>\n",
       "      <td>4.01604</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.093</td>\n",
       "      <td>5.26651</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.081</td>\n",
       "      <td>3.92949</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.742</td>\n",
       "      <td>1.60313</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.39</td>\n",
       "      <td>3.13216</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>-0.054</td>\n",
       "      <td>1.33363</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.844</td>\n",
       "      <td>1.09137</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>0.994</td>\n",
       "      <td>2.79930</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>1.05</td>\n",
       "      <td>1.61546</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>0.798</td>\n",
       "      <td>1.41541</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>0.822</td>\n",
       "      <td>1.68888</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>0.725</td>\n",
       "      <td>1.59168</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>0.952</td>\n",
       "      <td>2.24413</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>0.971</td>\n",
       "      <td>3.34418</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>0.909</td>\n",
       "      <td>1.88158</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.825</td>\n",
       "      <td>1.53840</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>1.052</td>\n",
       "      <td>1.45670</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>0.866</td>\n",
       "      <td>1.92154</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>0.919</td>\n",
       "      <td>2.96360</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>0.926</td>\n",
       "      <td>1.50023</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>0.96</td>\n",
       "      <td>2.64242</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>0.636</td>\n",
       "      <td>1.49284</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>0.834</td>\n",
       "      <td>2.44943</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.947</td>\n",
       "      <td>1.96351</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.74563</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>0.906</td>\n",
       "      <td>2.33524</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>0.933</td>\n",
       "      <td>1.51614</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.858</td>\n",
       "      <td>1.41034</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>1.031</td>\n",
       "      <td>1.45805</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>0.525</td>\n",
       "      <td>1.49643</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>0.872</td>\n",
       "      <td>2.82575</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>0.806</td>\n",
       "      <td>1.86495</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>0.999</td>\n",
       "      <td>11.98206</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>0.972</td>\n",
       "      <td>4.49771</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>0.989</td>\n",
       "      <td>3.19239</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>0.947</td>\n",
       "      <td>1.74859</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>0.993</td>\n",
       "      <td>2.28668</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>0.967</td>\n",
       "      <td>1.59364</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>0.998</td>\n",
       "      <td>3.58772</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>0.872</td>\n",
       "      <td>1.31756</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>0.875</td>\n",
       "      <td>2.03354</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>0.803</td>\n",
       "      <td>1.76517</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>0.971</td>\n",
       "      <td>3.04500</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.5991***</td>\n",
       "      <td>0.45231</td>\n",
       "      <td>3.535</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.309</td>\n",
       "      <td>0.34347</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.349</td>\n",
       "      <td>0.69657</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.86379</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.451</td>\n",
       "      <td>1.58629</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.109</td>\n",
       "      <td>0.47982</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.27073</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.068</td>\n",
       "      <td>0.34245</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.041</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.38956</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.28188</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.20183</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.15439</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.25448</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.16537</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.058</td>\n",
       "      <td>0.22784</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.22976</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>0.157</td>\n",
       "      <td>0.40932</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables         theta        se  t (theta == 0)      p\n",
       "in_out           -2.545   5.43585           0.468  0.640\n",
       "cy               -0.318   2.79781           0.114  0.910\n",
       "hp               -0.457   3.15701           0.145  0.885\n",
       "we               -0.945   2.89147           0.327  0.744\n",
       "le               -1.953   4.01604           0.486  0.627\n",
       "wi               -2.093   5.26651           0.397  0.691\n",
       "he               -2.081   3.92949           0.529  0.596\n",
       "li               -0.742   1.60313           0.463  0.643\n",
       "sp                -1.39   3.13216           0.444  0.657\n",
       "ac               -0.054   1.33363           0.040  0.968\n",
       "pr                0.844   1.09137           0.773  0.439\n",
       "brand_2           0.994   2.79930           0.355  0.723\n",
       "brand_3            1.05   1.61546           0.650  0.516\n",
       "brand_4           0.798   1.41541           0.564  0.573\n",
       "brand_5           0.822   1.68888           0.487  0.627\n",
       "brand_6           0.725   1.59168           0.456  0.649\n",
       "brand_7           0.952   2.24413           0.424  0.671\n",
       "brand_8           0.971   3.34418           0.290  0.772\n",
       "brand_9           0.909   1.88158           0.483  0.629\n",
       "brand_10          0.825   1.53840           0.536  0.592\n",
       "brand_11          1.052   1.45670           0.722  0.470\n",
       "brand_12          0.866   1.92154           0.451  0.652\n",
       "brand_13          0.919   2.96360           0.310  0.757\n",
       "brand_14          0.926   1.50023           0.617  0.537\n",
       "brand_15           0.96   2.64242           0.363  0.716\n",
       "brand_16          0.636   1.49284           0.426  0.670\n",
       "brand_17          0.834   2.44943           0.341  0.733\n",
       "brand_18          0.947   1.96351           0.482  0.630\n",
       "brand_19          0.885   1.74563           0.507  0.612\n",
       "brand_20          0.906   2.33524           0.388  0.698\n",
       "brand_21          0.933   1.51614           0.616  0.538\n",
       "brand_22          0.858   1.41034           0.608  0.543\n",
       "brand_23          1.031   1.45805           0.707  0.480\n",
       "brand_24          0.525   1.49643           0.351  0.726\n",
       "brand_25          0.872   2.82575           0.309  0.758\n",
       "brand_26          0.806   1.86495           0.432  0.666\n",
       "brand_27          0.999  11.98206           0.083  0.934\n",
       "brand_28          0.972   4.49771           0.216  0.829\n",
       "brand_29          0.989   3.19239           0.310  0.757\n",
       "brand_30          0.947   1.74859           0.542  0.588\n",
       "brand_31          0.993   2.28668           0.434  0.664\n",
       "brand_32          0.967   1.59364           0.607  0.544\n",
       "brand_33          0.998   3.58772           0.278  0.781\n",
       "brand_34          0.872   1.31756           0.662  0.508\n",
       "brand_35          0.875   2.03354           0.430  0.667\n",
       "brand_36          0.803   1.76517           0.455  0.649\n",
       "brand_37          0.971   3.04500           0.319  0.750\n",
       "home_2        1.5991***   0.45231           3.535  0.000\n",
       "cla_2             0.309   0.34347           0.899  0.369\n",
       "cla_3             0.349   0.69657           0.500  0.617\n",
       "cla_4             0.094   0.86379           0.109  0.913\n",
       "cla_5             0.451   1.58629           0.284  0.776\n",
       "group_in_out      0.109   0.47982           0.228  0.820\n",
       "group_cy         -0.009   0.27073           0.032  0.974\n",
       "group_hp          0.068   0.34245           0.200  0.842\n",
       "group_we          0.041   0.30303           0.136  0.892\n",
       "group_le         -0.082   0.38956           0.211  0.833\n",
       "group_wi         -0.131   0.28188           0.464  0.643\n",
       "group_he         -0.025   0.20183           0.126  0.899\n",
       "group_li         -0.003   0.15439           0.020  0.984\n",
       "group_sp         -0.073   0.25448           0.287  0.774\n",
       "group_ac         -0.198   0.16537           1.200  0.230\n",
       "group_brand       0.058   0.22784           0.255  0.799\n",
       "group_home         0.03   0.22976           0.131  0.896\n",
       "group_cla         0.157   0.40932           0.383  0.702"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Similarity_theta = resMLE['theta']\n",
    "Similarity_SE = resMLE['se']\n",
    "Similarity_t, Similarity_p = Similarity_t_p(Similarity_SE, Similarity_theta, N)\n",
    "reg_table(Similarity_theta, Similarity_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4636624201081757"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in Similarity_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative approach\n",
    "\n",
    "The log-likelihood function is not globally concave, and finding the global optimum can be difficult. Using the estimation procedure of Fosgerau et. al. (2023 working paper), we can instead fit the parameters using the first-order conditions for optimality. The estimator takes the form\n",
    "\n",
    "$$\n",
    "\\hat \\theta^0=\\arg \\min_{\\theta} \\sum_t s_t \\hat \\varepsilon^0_t(\\theta)'\\hat W^0_t\\hat \\varepsilon^0 _t(\\theta),\n",
    "$$\n",
    "where $\\hat W^0_t$ is a positive semidefinite weight matrix, $s_t$ is market $t$'s share of the total population and \n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t(u(X_t,\\beta)- \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)) ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat D^0_t=\\textrm{diag}(\\hat q^0_t)-\\hat q^0_t (\\hat q^0_t)'.\n",
    "$$\n",
    "Using equation (...) above, we have that $\\hat \\epsilon_t$ is a linear function of $\\theta$,\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t \\left(\\hat G^0_t\\theta- \\ln \\hat q^0_t\\right)\\equiv \\hat A^0_t\\theta-\\hat r^0_t.\n",
    "$$\n",
    "Using linearity, the weighted least squares criterion has a unique closed form solution,\n",
    "$$\n",
    "\\hat \\theta^0 =\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat A^0_t \\right)^{-1}\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat r_t^0 \\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_array(q, x, psi_stack):\n",
    "    ''' \n",
    "    This function calculates the G block matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        G: a dictionary  of T numpy arrays (J[t],K+G): a G matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack) # Find the cross derivative of the pertubation function \\Omega wrt. lambda and ccp's q\n",
    "    G = {t: np.concatenate((x[t],-Z[t]), axis=1) for t in np.arange(T)} # Join block matrices along 2nd dimensions  s.t. last dimension is K+G (same dimension as theta)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_array(q):\n",
    "    '''\n",
    "    This function calculates the D matrix - the logit derivative of ccp's wrt. utilities\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "\n",
    "    Returns\n",
    "        D: a dictionary of T numpy arrays (J[t],J[t]) of logit derivatives of ccp's wrt. utilities for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = {t: np.diag(q[t]) - np.einsum('j,k->jk', q[t], q[t]) for t in np.arange(T)}\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_array(q, x, psi_stack):\n",
    "    '''\n",
    "    This function calculates the A matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        A: a dictionary  of T numpy arrays (J[t],K+G): an A matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    D = D_array(q)\n",
    "    G = G_array(q, x, psi_stack)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_array(q):\n",
    "    '''\n",
    "    This function calculates 'r'; the logarithm of observed or nonparametrically estimated market shares\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "    \n",
    "    Returns\n",
    "        r: a dictionary of T numpy arrays (J[t],) of the log of ccp's for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = D_array(q) \n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) for t in np.arange(T)}\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS_init(q, x, sample_share, psi_stack, N):\n",
    "    ''' \n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the initial FKN parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "\n",
    "    #W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "    A = A_array(q, x, psi_stack)\n",
    "    r = r_array(q)\n",
    "\n",
    "    d = A[0].shape[1]\n",
    "    \n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1/q[t], A[t], optimize = True) # Fast product using that the weights are diagonal.\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1/q[t], r[t], optimize = True)\n",
    "    \n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    #se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "    \n",
    "    return theta_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the observed market shares we may thus find initial parameter estimates $\\hat \\theta^0$ as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFKN0 = WLS_init(y, x, pop_share, Psi, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for parameter bounds\n",
    "\n",
    "As we see above, the least squares estimator is not guaranteed to respect the parameter bounds $\\sum_g \\hat \\lambda_g<1$. We can use that if we replace $\\hat q^0_t$ with the choice probabilities from the maximum likelihood estimator of the logit model, $\\hat q^{logit}_t\\propto \\exp\\{X_t\\hat \\beta^{logit}\\}$, and plug these choice probabilities into the WLS estimator described above, it will return $\\hat \\theta=(\\hat \\beta^{logit},0,\\ldots,0)$ as the parameter estimate. Let $\\hat q_t(\\alpha)$ denote the weighted average of the logit probabilites and the market shares,\n",
    "$$\n",
    "\\hat q_t(\\alpha) =(1-\\alpha) \\hat q^{logit}_t+\\alpha \\hat q^0_t.\n",
    "$$\n",
    " Let $\\hat \\theta^0(\\alpha)$ denote the resulting parameter vector. We perform a line search for values of $\\alpha$, $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\ldots)$ until $\\hat \\theta^0(\\alpha)$ yields a feasible parameter vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogL(Theta, y, x, sample_share, psi_stack):\n",
    "    ''' A function giving the mean Similarity loglikehood evaluated at data and an array of parameters 'Theta'\n",
    "    '''\n",
    "    return np.mean(Similarity_loglikelihood(Theta, y, x, sample_share, psi_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, N):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta0.shape[0]\n",
    "    K = x[0].shape[1]\n",
    "    G = d-K\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit.logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    #alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    alpha0=0.5\n",
    "    #LogL_alpha = np.empty((num_alpha,))\n",
    "    #theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in range(1,100):\n",
    "\n",
    "        alpha = alpha0**k\n",
    "\n",
    "      \n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha = WLS_init(q_alpha, x, sample_share, psi_stack, N)\n",
    "\n",
    "        lambda_alpha = theta_alpha[K:]\n",
    "        \n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() <1:\n",
    "            break\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = len(Theta0)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit.logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    LogL_alpha = np.empty((num_alpha,))\n",
    "    theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in np.arange(len(alpha_line)):\n",
    "\n",
    "        alpha = alpha_line[k]\n",
    "\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha[k,:] = WLS_init(q_alpha, x, sample_share, psi_stack, N)\n",
    "\n",
    "        lambda_alpha = theta_alpha[k,K:]\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() >= 1:\n",
    "            LogL_alpha[k] = np.NINF\n",
    "        else:\n",
    "            LogL_alpha[k] = LogL(theta_alpha[k,:], y, x, sample_share, psi_stack)\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "    alpha_star = np.argmax(LogL_alpha)\n",
    "    theta_hat_star = theta_alpha[alpha_star,:]\n",
    "\n",
    "    return theta_hat_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the grid search method we find corressponding parameters $\\hat \\theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7.935050105401431e-13, 0.0015463632191568025]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.555485598820552, 0.0015273704055656172]\n",
      "[0.9258507060311653, 0.0015170844523477358]\n"
     ]
    }
   ],
   "source": [
    "theta_alpha = GridSearch(thetaFKN0, Logit_beta, y, x, pop_share, Psi, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9258507060311653, 0.0015170844523477358]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0015170844523477358"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_Similarity(theta_alpha, y, x, pop_share, Psi).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated FKN estimator\n",
    "\n",
    "The iterated estimator is as the initial one, except there is an additional term on $\\hat \\varepsilon$. First, we update the choice probabilities,\n",
    "$$\n",
    "\\hat q^k_i=p(\\mathbf X_i,\\hat \\theta^{k-1})\\\\\n",
    "$$\n",
    "Then we assign\n",
    "$$\n",
    "\\hat D^k_i=\\nabla^2_{qq}\\Omega(\\hat q_i^k|\\hat \\lambda^{k-1})^{-1}-(\\hat q^k_i \\hat q^k_i)'\n",
    "$$\n",
    "and then construct the residual\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)=\\hat D^k_i\\left( u(x_i,\\beta)-\\nabla_q \\Omega(\\hat q_i^k|\\lambda)\\right) -y_i+\\hat q_i^k,\n",
    "$$\n",
    "Which can once again be simplified as\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)= \\hat A_i^k \\theta-\\hat r^k_i,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat A^k_i=\\hat D_i^k\\hat G^k_i, \\hat r_i^k =\\hat D^k_i\\ln \\hat q_i^k-y_i\n",
    "$$\n",
    "and where $\\hat G^k_i$ is constructed as in the initial estimator. Using the weighted least squares estimator with weights $\\hat W_i^k=\\textrm{diag}(\\hat q^k_i)^{-1}$, we get the estimator\n",
    "$$\n",
    "\\hat \\theta^k = \\arg \\min_{\\theta}\\frac{1}{n}\\sum_i \\hat \\varepsilon^k_i(\\theta)'\\hat W_i^k \\hat \\varepsilon^k_i(\\theta).\n",
    "$$\n",
    "We can once again solve it in closed form as\n",
    "$$\n",
    "\\hat \\theta^k =\\left( \\frac{1}{n}\\sum_i \\hat (A^k_i)'\\hat W_i^k \\hat A^k_i)\\right)^{-1}\\left( \\frac{1}{n}\\sum_i (\\hat A_i^k)'\\hat W_i^k \\hat r_i^k\\right)\n",
    "$$\n",
    "Now we implement this procedure and iterate starting from our initial guess $\\hat \\theta^{*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS(Theta, y, x, sample_share, psi_stack, N):\n",
    "    '''\n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the iterated parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta.shape[0]\n",
    "    \n",
    "    # Get ccp's\n",
    "    q = Similarity_ccp(Theta, x, psi_stack)\n",
    "\n",
    "    # Construct A\n",
    "    D = ccp_gradient(q, x, Theta, psi_stack) # A is here constructed using the Similarity derivative of ccp's wrt. utilities instead of teh Logit derivative\n",
    "    G = G_array(q, x, psi_stack)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "    W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "\n",
    "    # Construct r\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where=(q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) + y[t] for t in np.arange(T)}\n",
    "\n",
    "    # Estimate parameters\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,jk,kp->dp', A[t], W[t], A[t], optimize = True)\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,jk,k->d', A[t], W[t], r[t], optimize = True)\n",
    "\n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "\n",
    "    return theta_hat,se_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKN_estimator(logit_beta, q_obs, x, sample_share, psi_stack, N, tol = 1.0e-15, max_iters = 1000):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    theta_init = WLS_init(q_obs, x, sample_share, psi_stack, N) #WLS_init(q_obs, x, sample_share, psi_stack, nest_count,  N)\n",
    "    \n",
    "    if np.array([p for p in theta_init[K:] if p>0]).sum() >= 1:\n",
    "        theta_hat_star = LineSearch(theta_init, logit_beta, q_obs, x, sample_share, psi_stack, N)\n",
    "        theta0 = theta_hat_star\n",
    "    else:\n",
    "        theta0 = theta_init\n",
    "\n",
    "    #logl0 = LogL(theta0, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    for k in np.arange(max_iters):\n",
    "        theta1, se1 = WLS(theta0, q_obs, x, sample_share, psi_stack, N)\n",
    "\n",
    "        '''logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "        \n",
    "        for m in range(10):\n",
    "            if logl1<logl0:\n",
    "                theta1=0.5*theta0+0.5*theta1\n",
    "                logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "            else:\n",
    "                break'''\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.abs(theta1 - theta0))\n",
    "\n",
    "        if dist<tol:\n",
    "            succes = True\n",
    "            iter = k\n",
    "            break\n",
    "        elif k==max_iters:\n",
    "            succes = False\n",
    "            iter = max_iters\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        theta0 = theta1\n",
    "\n",
    "    res = {'theta': theta1,\n",
    "           'se': se1,\n",
    "           'fun': -LogL(theta1, y, x, sample_share, psi_stack),\n",
    "           'iter': iter,\n",
    "           'succes': succes}\n",
    "    \n",
    "    return res \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1490931690055368, 0.0014929212140089557]\n"
     ]
    }
   ],
   "source": [
    "res = FKN_estimator(Logit_beta, y, x, pop_share, Psi, N, tol=1.0e-8, max_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-10.3489***</td>\n",
       "      <td>0.00344</td>\n",
       "      <td>3009.114</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.4444***</td>\n",
       "      <td>0.00175</td>\n",
       "      <td>254.692</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-3.4942***</td>\n",
       "      <td>0.00247</td>\n",
       "      <td>1412.005</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.0536***</td>\n",
       "      <td>0.00162</td>\n",
       "      <td>33.043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.108***</td>\n",
       "      <td>0.00173</td>\n",
       "      <td>1217.149</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.5361***</td>\n",
       "      <td>0.00324</td>\n",
       "      <td>1708.302</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.2605***</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>128.037</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-1.0173***</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>889.207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>3.0194***</td>\n",
       "      <td>0.00231</td>\n",
       "      <td>1305.769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.7988***</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>956.243</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.0205***</td>\n",
       "      <td>0.00018</td>\n",
       "      <td>114.954</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.7505***</td>\n",
       "      <td>0.00276</td>\n",
       "      <td>271.882</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>0.1587***</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>310.861</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.6177***</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>924.864</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.287***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>588.894</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.2367***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>478.439</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.691***</td>\n",
       "      <td>0.00144</td>\n",
       "      <td>480.670</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.5225***</td>\n",
       "      <td>0.00170</td>\n",
       "      <td>307.713</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7544***</td>\n",
       "      <td>0.00227</td>\n",
       "      <td>771.796</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.2602***</td>\n",
       "      <td>0.00058</td>\n",
       "      <td>448.632</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>0.0207***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>42.324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.3376***</td>\n",
       "      <td>0.00079</td>\n",
       "      <td>425.214</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-0.9302***</td>\n",
       "      <td>0.00137</td>\n",
       "      <td>677.699</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-1.64***</td>\n",
       "      <td>0.00138</td>\n",
       "      <td>1185.239</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.7494***</td>\n",
       "      <td>0.00267</td>\n",
       "      <td>655.372</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-0.7249***</td>\n",
       "      <td>0.00057</td>\n",
       "      <td>1274.395</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.5701***</td>\n",
       "      <td>0.00075</td>\n",
       "      <td>756.722</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.3425***</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>711.421</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-0.9539***</td>\n",
       "      <td>0.00114</td>\n",
       "      <td>836.739</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.0862***</td>\n",
       "      <td>0.00066</td>\n",
       "      <td>131.250</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.0218***</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>45.817</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.072***</td>\n",
       "      <td>0.00050</td>\n",
       "      <td>144.532</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>0.1319***</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>250.675</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.2895***</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>565.183</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.8324***</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>654.744</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.6325***</td>\n",
       "      <td>0.00074</td>\n",
       "      <td>850.258</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.7298***</td>\n",
       "      <td>0.03555</td>\n",
       "      <td>76.796</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.7499***</td>\n",
       "      <td>0.00127</td>\n",
       "      <td>588.593</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.4165***</td>\n",
       "      <td>0.01411</td>\n",
       "      <td>171.290</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.356***</td>\n",
       "      <td>0.00254</td>\n",
       "      <td>534.759</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-0.9813***</td>\n",
       "      <td>0.00239</td>\n",
       "      <td>410.886</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.3742***</td>\n",
       "      <td>0.00084</td>\n",
       "      <td>444.323</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-2.4848***</td>\n",
       "      <td>0.01093</td>\n",
       "      <td>227.366</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.701***</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>1042.047</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.2846***</td>\n",
       "      <td>0.00071</td>\n",
       "      <td>402.320</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1586***</td>\n",
       "      <td>0.00067</td>\n",
       "      <td>238.194</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.6074***</td>\n",
       "      <td>0.00297</td>\n",
       "      <td>541.647</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.1366***</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>2506.484</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>-0.0459***</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>293.866</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.0453***</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>191.682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.0492***</td>\n",
       "      <td>0.00037</td>\n",
       "      <td>132.733</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.1304***</td>\n",
       "      <td>0.00053</td>\n",
       "      <td>247.467</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.73***</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>2867.113</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.1119***</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>743.676</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.0704***</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>345.522</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.0379***</td>\n",
       "      <td>0.00017</td>\n",
       "      <td>222.942</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0858***</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>550.904</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0587***</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>451.903</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0788***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>865.965</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.046***</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>434.394</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0237***</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>147.395</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0343***</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>313.411</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.2648***</td>\n",
       "      <td>0.00024</td>\n",
       "      <td>1119.822</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.2295***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>994.633</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.072***</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>551.256</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)    p\n",
       "in_out        -10.3489***  0.00344        3009.114  0.0\n",
       "cy             -0.4444***  0.00175         254.692  0.0\n",
       "hp             -3.4942***  0.00247        1412.005  0.0\n",
       "we             -0.0536***  0.00162          33.043  0.0\n",
       "le              -2.108***  0.00173        1217.149  0.0\n",
       "wi              5.5361***  0.00324        1708.302  0.0\n",
       "he              0.2605***  0.00203         128.037  0.0\n",
       "li             -1.0173***  0.00114         889.207  0.0\n",
       "sp              3.0194***  0.00231        1305.769  0.0\n",
       "ac              0.7988***  0.00084         956.243  0.0\n",
       "pr             -0.0205***  0.00018         114.954  0.0\n",
       "brand_2        -0.7505***  0.00276         271.882  0.0\n",
       "brand_3         0.1587***  0.00051         310.861  0.0\n",
       "brand_4        -0.6177***  0.00067         924.864  0.0\n",
       "brand_5         -0.287***  0.00049         588.894  0.0\n",
       "brand_6        -0.2367***  0.00049         478.439  0.0\n",
       "brand_7         -0.691***  0.00144         480.670  0.0\n",
       "brand_8        -0.5225***  0.00170         307.713  0.0\n",
       "brand_9        -1.7544***  0.00227         771.796  0.0\n",
       "brand_10        0.2602***  0.00058         448.632  0.0\n",
       "brand_11        0.0207***  0.00049          42.324  0.0\n",
       "brand_12       -0.3376***  0.00079         425.214  0.0\n",
       "brand_13       -0.9302***  0.00137         677.699  0.0\n",
       "brand_14         -1.64***  0.00138        1185.239  0.0\n",
       "brand_15       -1.7494***  0.00267         655.372  0.0\n",
       "brand_16       -0.7249***  0.00057        1274.395  0.0\n",
       "brand_17       -0.5701***  0.00075         756.722  0.0\n",
       "brand_18        0.3425***  0.00048         711.421  0.0\n",
       "brand_19       -0.9539***  0.00114         836.739  0.0\n",
       "brand_20       -0.0862***  0.00066         131.250  0.0\n",
       "brand_21       -0.0218***  0.00048          45.817  0.0\n",
       "brand_22         0.072***  0.00050         144.532  0.0\n",
       "brand_23        0.1319***  0.00053         250.675  0.0\n",
       "brand_24       -0.2895***  0.00051         565.183  0.0\n",
       "brand_25       -0.8324***  0.00127         654.744  0.0\n",
       "brand_26       -0.6325***  0.00074         850.258  0.0\n",
       "brand_27       -2.7298***  0.03555          76.796  0.0\n",
       "brand_28       -0.7499***  0.00127         588.593  0.0\n",
       "brand_29       -2.4165***  0.01411         171.290  0.0\n",
       "brand_30        -1.356***  0.00254         534.759  0.0\n",
       "brand_31       -0.9813***  0.00239         410.886  0.0\n",
       "brand_32       -0.3742***  0.00084         444.323  0.0\n",
       "brand_33       -2.4848***  0.01093         227.366  0.0\n",
       "brand_34        -0.701***  0.00067        1042.047  0.0\n",
       "brand_35       -0.2846***  0.00071         402.320  0.0\n",
       "brand_36       -0.1586***  0.00067         238.194  0.0\n",
       "brand_37       -1.6074***  0.00297         541.647  0.0\n",
       "home_2          1.1366***  0.00045        2506.484  0.0\n",
       "cla_2          -0.0459***  0.00016         293.866  0.0\n",
       "cla_3           0.0453***  0.00024         191.682  0.0\n",
       "cla_4           0.0492***  0.00037         132.733  0.0\n",
       "cla_5           0.1304***  0.00053         247.467  0.0\n",
       "group_in_out      0.73***  0.00025        2867.113  0.0\n",
       "group_cy       -0.1119***  0.00015         743.676  0.0\n",
       "group_hp        0.0704***  0.00020         345.522  0.0\n",
       "group_we        0.0379***  0.00017         222.942  0.0\n",
       "group_le       -0.0858***  0.00016         550.904  0.0\n",
       "group_wi       -0.0587***  0.00013         451.903  0.0\n",
       "group_he       -0.0788***  0.00009         865.965  0.0\n",
       "group_li         0.046***  0.00011         434.394  0.0\n",
       "group_sp       -0.0237***  0.00016         147.395  0.0\n",
       "group_ac       -0.0343***  0.00011         313.411  0.0\n",
       "group_brand     0.2648***  0.00024        1119.822  0.0\n",
       "group_home     -0.2295***  0.00023         994.633  0.0\n",
       "group_cla       -0.072***  0.00013         551.256  0.0"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKN_theta = res['theta']\n",
    "FKN_SE = res['se']\n",
    "FKN_t, FKN_p = Similarity_t_p(FKN_SE, FKN_theta, N)\n",
    "reg_table(FKN_theta, FKN_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP Estimation and instruments\n",
    "\n",
    "The setting is now a bit different. Instead of the noise coming from random sampling of individuals, we now have an additional source of uncertainty, stemming frm the random sampling of the fixed effects $\\xi_{tj}$ for each market and each product. The number of ”observations” is therefore\n",
    "\n",
    "$$\n",
    "S = T \\cdot \\sum_t J_t\n",
    "$$\n",
    "\n",
    "Note that while random sampling of individuals choices (number of observations\n",
    "in the hundreds of millions) still has an effect on the estimated parameters in\n",
    "principle, this effect is completely drowned out by the sampling variance of the\n",
    "fixed effects (number of observations T ≈ 15000?), so we choose to ignore it\n",
    "here. When estimating random coefficients models, there is also a third source\n",
    "of uncertainty stemming from approximation of numerical integrals. This is not\n",
    "an issue in Similarity, as we have the inverse demand in closed form.\n",
    "\n",
    "The principles are pretty similar to what we have been doing already. When\n",
    "applicable, I will use the same notation as in the FKN section. Define the\n",
    "residual,\n",
    "\n",
    "$$\\xi_m(\\theta) = u(X_m, \\beta) − \\nabla_q \\Omega(q^0|\\lambda)$$\n",
    "\n",
    "In the Similarity model, this residual is a linear function of $\\theta$ which has the form\n",
    "\n",
    "$$\\xi_m(\\theta) =  G^0_m \\theta − r_m^0$$\n",
    "\n",
    "where $ G^0_m=[X_m, Z_m^0]$, where $Z_m^0 = \\nabla_{q,\\lambda}\\Omega(q_m^0|\\lambda)$ and $r^0_m = \\ln q^0_m$ as in the FKN section with $q^0_m$ being e.g. the observed market shares in market $m$. For the BLP estimator, we set this residual orthogonal to a matrix of instruments $\\hat Z_m$ of size $J_m \\times d$, and find the estimator $ \\hat \\theta^{IV}$ which solves the moment conditions\n",
    "\n",
    "$$\\frac{1}{T} \\sum_m \\hat Z_m' \\xi(\\hat \\theta^{IV}) = 0$$\n",
    "\n",
    "Since $\\hat \\xi$ is linear, the moment equations have a unique solution,\n",
    "\n",
    "$$\\hat \\theta^{IV} = \\left(\\frac{1}{T}\\sum_m \\hat Z_m' G^0_m \\right)^{-1}\\left(\\frac{1}{T}\\sum_m \\hat Z_m' r^0_m \\right)$$\n",
    "\n",
    "We require an instrument for the price of the goods. This is something which is correlated with the price, but uncorrelated with the error term $\\xi_m$ (in the BLP model, $\\xi_{mj}$ represents unobserved components of car quality). A standard instrument in this case would be a measure of marginal cost (or something which is correlated with marginal cost, like a production price index). For everything other than price, we can simply use the regressor itself as the instrument i.e. $ \\hat Z^{mjd} = G^0_{mjd}$, for all other dimensions than price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct our instruments $\\hat Z$. We'll use the average exchange rate of the destination country relative to average exchange rate of the origin country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "xexr = {t: dat[dat['market'] == t][z_vars[0]].values for t in np.arange(T)}\n",
    "G0 = G_array(y, x, Psi)\n",
    "pr_index = len(x_contvars)\n",
    "for t in np.arange(T):\n",
    "    G0[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z = G0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the moment estimator $\\hat \\theta^{IV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_estimator(y, z, x, sample_share, psi_stack):\n",
    "    '''\n",
    "    Args.\n",
    "        y: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        z: a dictionary of T numpy arrays (J[t],K+G) of instruments for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a numpy array (K+G,) of BLP parameter estimates\n",
    "    '''\n",
    "    T = len(z)\n",
    "\n",
    "    G = G_array(y, x, psi_stack)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t], out = np.NINF*np.ones_like((y[t])), where = (y[t] > 0)) for t in np.arange(T)}\n",
    "    \n",
    "    sZG = np.empty((T,d,d))\n",
    "    sZr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sZG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', z[t], G[t])\n",
    "        sZr[t,:] = sample_share[t]*np.einsum('jd,j->d', z[t], r[t])\n",
    "\n",
    "    theta_hat = la.solve(sZG.sum(axis=0), sZr.sum(axis=0))\n",
    "    \n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLP_theta = BLP_estimator(y, z, x, np.ones((T,)), Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1550832732622858"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in BLP_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Logit model we get the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_logit = x\n",
    "for t in np.arange(T):\n",
    "    G_logit[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z_logit = G_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.92920752,  -2.3589754 ,  -6.76421995,   0.02963003,\n",
       "        -2.05176127,  10.84731336,  -1.04140126,  -0.58331478,\n",
       "         5.15289118,   0.51808091,  -0.17336342,  -2.037768  ,\n",
       "        -0.81720168,  -1.44357757,  -1.04059281,  -1.16245013,\n",
       "        -1.74530433,  -0.85123531,  -2.72300281,  -1.08758839,\n",
       "        -0.68958989,  -0.95909482,  -2.11727698,  -2.93039275,\n",
       "        -2.90655875,  -2.05527142,  -1.82107985,   0.51974428,\n",
       "        -2.02980519,  -0.79701277,  -0.86356478,  -0.86816254,\n",
       "        -0.81661044,  -1.48858878,  -0.9378501 ,  -1.87621854,\n",
       "        -3.7657435 ,  -1.52567201,  -3.14936663,  -2.07998398,\n",
       "        -1.85954898,  -0.7631942 ,  -1.94891051,  -1.60837966,\n",
       "        -1.15784827,  -0.48973547,  -2.57588437,   1.56903974,\n",
       "         0.0275757 ,   0.04982496,  -0.30342661,  -0.3829885 ])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta = logit.LogitBLP_estimator(y, z_logit, x, np.ones((T,)))\n",
    "LogitBLP_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLP approximation to optimal instruments\n",
    "\n",
    "BLP propose an algorithm for constructing an approximation to the optimal instruments. It is described in simple terms in Reynaert & Verboven (2014), and it has the following steps.\n",
    "It requires an initial parameter estimator $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'$, here we can just usethe MLE we have already computed. Let $W_m$ denote the matrix of instruments (this is the matrix $X_m$ with the price replaced by the exchange rate). The steps are then as follows:\n",
    "\n",
    "First we form the regression equation of the covariates on the instruments:\n",
    "$$\n",
    "X_m = W_m \\Pi + E_m\n",
    "$$\n",
    "\n",
    "The OLS estimate is then given as:\n",
    "$$\n",
    "\\hat \\Pi = \\left( \\frac{1}{T}\\sum_m W_m' W_m \\right)^{-1}\\left( \\frac{1}{T}\\sum_m W_m' X_m\\right)\n",
    "$$\n",
    "\n",
    "Thus the predicted covariates given the instruments $W$ are:\n",
    "$$\n",
    "\\hat X_m = W_m \\hat \\Pi\n",
    "$$\n",
    "\n",
    "Having constructed $\\hat X_m$ (which consists of the exogenous regressors, and the predicted price given $W_m$), we compute the predicted mean utility:\n",
    "\n",
    "$$\n",
    "\\hat u_m = \\hat X_m \\hat \\beta\n",
    "$$\n",
    "\n",
    "and then the predicted market shares at the mean utility:\n",
    "\n",
    "$$\n",
    "\\hat q_m^{*} = P(\\hat u_m | \\hat \\lambda)\n",
    "$$\n",
    "\n",
    "Computationally, here we just use $\\hat X_m$ in place of $X_m$ in the CCP function.\n",
    "Given the predicted market shares, we compute\n",
    "\n",
    "$$\n",
    "\\hat G_m^{*} = \\left[\\hat X_m, \\nabla_{q,\\lambda} \\Omega (\\hat q_m^{*} | \\hat \\lambda)\\right]\n",
    "$$\n",
    "\n",
    "which is the same as the function $\\hat G_m^0$ we already have constructed, except we evaluate it at the\n",
    "predictions $\\hat X_m$ and $\\hat q_m^{*}$ instead of at $X_m$ and $\\hat q_m^0$.\n",
    "\n",
    "The procedure above gives an approximation to the optimal instruments. We also require a weight matrix. The optimal weight matrix is the (generalized) inverse of the conditional (on the instruments) covariance of the fixed effects. Assuming $\\xi_{jm}$ is independetly and identically distributed over j and m, the conditional covariance simplifies to a scalar $\\sigma^2$ times an identity matrix (of size $J_m$).\n",
    "This means that all fixed effects are weighted equally, and the weights therefore drop out of the IV regression. The optimal IV estimator is therefore\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} = \\left(\\frac{1}{T}\\sum_m (\\hat G_m^*)'\\hat G_m^0\\right)^{-1}\\left( \\frac{1}{T}\\sum_m (\\hat G_m^*)'\\hat r_m^0 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat \\xi^*$ denote the estimated residual evaluated at the new parameter estimates,\n",
    "\n",
    "$$\n",
    "\\hat \\xi_{mj}^* = \\hat \\xi_{mj}(\\hat \\theta^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "We may estimate the constant $\\sigma^2$ by\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{T}\\sum_{m}\\sum_{j = 1}^{J_m} \\left(\\hat \\xi_{mj}^*\\right)^2 \n",
    "$$\n",
    "\n",
    "The distribution of the estimator $\\hat \\theta^{\\text{IV}}$ is then\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} \\sim \\mathcal{N}(\\theta_0, \\Sigma^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "which can be consistently estimated by\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma^{\\text{IV}} = \\hat \\sigma^2 \\left( \\sum_m (\\hat G_m^*)'\\hat G_m^0 \\right)^{-1}\n",
    "$$\n",
    "\n",
    "and the standard errors are then the square root of the diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_x(x, w, sample_share):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(w)\n",
    "    K = w[0].shape[1]\n",
    "\n",
    "    sWW = np.empty((T,K,K))\n",
    "    sWX = np.empty((T,K,K))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sWW[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], w[t])\n",
    "        sWX[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], x[t])\n",
    "\n",
    "    Pi_hat = la.solve(sWW.sum(axis=0), sWX.sum(axis=0))\n",
    "    X_hat = {t: np.einsum('jl,lk->jk', w[t], Pi_hat) for t in np.arange(T)}\n",
    "\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_se(Theta, y, x, psi_stack):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    S = T * np.array([x[t].shape[0] for t in np.arange(T)]).sum()\n",
    "\n",
    "    G = G_array(y, x, psi_stack)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t]) for t in np.arange(T)}\n",
    "    \n",
    "    # We calculate \\sigma^2\n",
    "    xi = {t: np.einsum('jd,d->j', G[t], Theta) - r[t] for t in np.arange(T)}\n",
    "    sum_xij2 = np.empty((T,))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sum_xij2[t] = (xi[t]**2).sum()\n",
    "    \n",
    "    sigma2 = np.sum(sum_xij2) / S\n",
    "\n",
    "    # We calculate GG for each market t\n",
    "    GG = np.empty((T,d,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        GG[t,:,:] = np.einsum('jd,jp->dp', G[t], G[t])\n",
    "\n",
    "    # Finally we compute \\Sigma and the standard errors\n",
    "    Sigma = sigma2*la.inv(GG.sum(axis=0))\n",
    "    SE = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalBLP_estimator(Theta0, q_obs, w, x, sample_share, psi_stack):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    beta0 = Theta0[:K]\n",
    "    lambda0 = Theta0[K:]\n",
    "    \n",
    "    X_hat = predict_x(x, w, sample_share)\n",
    "    q0 = Similarity_ccp(Theta0, X_hat, psi_stack)\n",
    "    G_star = G_array(q0, X_hat, psi_stack)\n",
    "    G0 = G_array(q_obs, x, psi_stack)\n",
    "    \n",
    "    r = {t: np.log(q_obs[t]) for t in np.arange(T)}\n",
    "\n",
    "    d = G0[0].shape[1]\n",
    "\n",
    "    sGG = np.empty((T,d,d))\n",
    "    sGr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sGG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', G_star[t], G0[t])\n",
    "        sGr[t,:] = sample_share[t]*np.einsum('jd,j->d', G_star[t], r[t])\n",
    "\n",
    "    Theta_IV = la.solve(sGG.sum(axis=0), sGr.sum(axis=0))\n",
    "    SE_IV = BLP_se(Theta_IV, q_obs, x, psi_stack)\n",
    "\n",
    "    return Theta_IV, SE_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThetaOptBLP, SEOptBLP = OptimalBLP_estimator(FKN_theta, y, z_logit, x, np.ones((T,)), Psi)\n",
    "OptBLP_t, OptBLP_p = Similarity_t_p(SEOptBLP, ThetaOptBLP, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2208776689182779"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in ThetaOptBLP[K:]  if p > 0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-11.5932***</td>\n",
       "      <td>0.03487</td>\n",
       "      <td>332.472</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.7933***</td>\n",
       "      <td>0.02008</td>\n",
       "      <td>39.510</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-5.2246***</td>\n",
       "      <td>0.02453</td>\n",
       "      <td>212.977</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.0872***</td>\n",
       "      <td>0.02038</td>\n",
       "      <td>4.276</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.284***</td>\n",
       "      <td>0.02333</td>\n",
       "      <td>97.879</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.8413***</td>\n",
       "      <td>0.03421</td>\n",
       "      <td>170.727</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.8169***</td>\n",
       "      <td>0.02778</td>\n",
       "      <td>29.403</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.8253***</td>\n",
       "      <td>0.01238</td>\n",
       "      <td>66.655</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>4.6289***</td>\n",
       "      <td>0.02632</td>\n",
       "      <td>175.903</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>1.2716***</td>\n",
       "      <td>0.01320</td>\n",
       "      <td>96.361</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.1536***</td>\n",
       "      <td>0.00238</td>\n",
       "      <td>64.462</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.9483***</td>\n",
       "      <td>0.03191</td>\n",
       "      <td>29.719</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>-0.1602***</td>\n",
       "      <td>0.00451</td>\n",
       "      <td>35.533</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.9661***</td>\n",
       "      <td>0.00483</td>\n",
       "      <td>199.901</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.4698***</td>\n",
       "      <td>0.00447</td>\n",
       "      <td>105.052</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.3691***</td>\n",
       "      <td>0.00439</td>\n",
       "      <td>84.132</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-1.0353***</td>\n",
       "      <td>0.00765</td>\n",
       "      <td>135.272</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.3679***</td>\n",
       "      <td>0.01109</td>\n",
       "      <td>33.172</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7969***</td>\n",
       "      <td>0.00724</td>\n",
       "      <td>248.189</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>-0.2626***</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>60.802</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.0546***</td>\n",
       "      <td>0.00441</td>\n",
       "      <td>12.379</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.507***</td>\n",
       "      <td>0.00529</td>\n",
       "      <td>95.853</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-1.3158***</td>\n",
       "      <td>0.00663</td>\n",
       "      <td>198.342</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-2.1334***</td>\n",
       "      <td>0.00925</td>\n",
       "      <td>230.736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-2.0143***</td>\n",
       "      <td>0.00881</td>\n",
       "      <td>228.693</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-1.2838***</td>\n",
       "      <td>0.00429</td>\n",
       "      <td>299.058</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-1.0088***</td>\n",
       "      <td>0.00510</td>\n",
       "      <td>197.849</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.4086***</td>\n",
       "      <td>0.00476</td>\n",
       "      <td>85.927</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-1.3282***</td>\n",
       "      <td>0.00588</td>\n",
       "      <td>225.884</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.3111***</td>\n",
       "      <td>0.00535</td>\n",
       "      <td>58.164</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.2624***</td>\n",
       "      <td>0.00415</td>\n",
       "      <td>63.222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>-0.2126***</td>\n",
       "      <td>0.00424</td>\n",
       "      <td>50.128</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>-0.1598***</td>\n",
       "      <td>0.00460</td>\n",
       "      <td>34.718</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.716***</td>\n",
       "      <td>0.00433</td>\n",
       "      <td>165.239</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.8499***</td>\n",
       "      <td>0.00514</td>\n",
       "      <td>165.207</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-1.0749***</td>\n",
       "      <td>0.00512</td>\n",
       "      <td>210.012</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.4221***</td>\n",
       "      <td>0.05297</td>\n",
       "      <td>45.721</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-1.0237***</td>\n",
       "      <td>0.00872</td>\n",
       "      <td>117.327</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.0694***</td>\n",
       "      <td>0.01778</td>\n",
       "      <td>116.359</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.2462***</td>\n",
       "      <td>0.00901</td>\n",
       "      <td>138.275</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-1.6062***</td>\n",
       "      <td>0.01819</td>\n",
       "      <td>88.284</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.4082***</td>\n",
       "      <td>0.00818</td>\n",
       "      <td>49.928</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-1.6853***</td>\n",
       "      <td>0.03079</td>\n",
       "      <td>54.738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.8271***</td>\n",
       "      <td>0.00551</td>\n",
       "      <td>150.144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.5353***</td>\n",
       "      <td>0.00516</td>\n",
       "      <td>103.736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1642***</td>\n",
       "      <td>0.00451</td>\n",
       "      <td>36.437</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.8653***</td>\n",
       "      <td>0.01099</td>\n",
       "      <td>169.751</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.0784***</td>\n",
       "      <td>0.00195</td>\n",
       "      <td>553.959</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.0372***</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>15.141</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.1463***</td>\n",
       "      <td>0.00330</td>\n",
       "      <td>44.353</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.1527***</td>\n",
       "      <td>0.00463</td>\n",
       "      <td>32.995</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.1811***</td>\n",
       "      <td>0.00589</td>\n",
       "      <td>30.757</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.838***</td>\n",
       "      <td>0.00203</td>\n",
       "      <td>412.202</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.0786***</td>\n",
       "      <td>0.00134</td>\n",
       "      <td>58.890</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.1593***</td>\n",
       "      <td>0.00181</td>\n",
       "      <td>87.899</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.0083***</td>\n",
       "      <td>0.00158</td>\n",
       "      <td>5.223</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0391***</td>\n",
       "      <td>0.00155</td>\n",
       "      <td>25.220</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0789***</td>\n",
       "      <td>0.00119</td>\n",
       "      <td>66.381</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0733***</td>\n",
       "      <td>0.00083</td>\n",
       "      <td>87.911</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.0243***</td>\n",
       "      <td>0.00093</td>\n",
       "      <td>26.247</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0386***</td>\n",
       "      <td>0.00155</td>\n",
       "      <td>24.909</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0363***</td>\n",
       "      <td>0.00112</td>\n",
       "      <td>32.467</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.191***</td>\n",
       "      <td>0.00098</td>\n",
       "      <td>195.324</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.3742***</td>\n",
       "      <td>0.00120</td>\n",
       "      <td>312.298</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.1054***</td>\n",
       "      <td>0.00143</td>\n",
       "      <td>73.868</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)    p\n",
       "in_out        -11.5932***  0.03487         332.472  0.0\n",
       "cy             -0.7933***  0.02008          39.510  0.0\n",
       "hp             -5.2246***  0.02453         212.977  0.0\n",
       "we              0.0872***  0.02038           4.276  0.0\n",
       "le              -2.284***  0.02333          97.879  0.0\n",
       "wi              5.8413***  0.03421         170.727  0.0\n",
       "he              0.8169***  0.02778          29.403  0.0\n",
       "li             -0.8253***  0.01238          66.655  0.0\n",
       "sp              4.6289***  0.02632         175.903  0.0\n",
       "ac              1.2716***  0.01320          96.361  0.0\n",
       "pr             -0.1536***  0.00238          64.462  0.0\n",
       "brand_2        -0.9483***  0.03191          29.719  0.0\n",
       "brand_3        -0.1602***  0.00451          35.533  0.0\n",
       "brand_4        -0.9661***  0.00483         199.901  0.0\n",
       "brand_5        -0.4698***  0.00447         105.052  0.0\n",
       "brand_6        -0.3691***  0.00439          84.132  0.0\n",
       "brand_7        -1.0353***  0.00765         135.272  0.0\n",
       "brand_8        -0.3679***  0.01109          33.172  0.0\n",
       "brand_9        -1.7969***  0.00724         248.189  0.0\n",
       "brand_10       -0.2626***  0.00432          60.802  0.0\n",
       "brand_11       -0.0546***  0.00441          12.379  0.0\n",
       "brand_12        -0.507***  0.00529          95.853  0.0\n",
       "brand_13       -1.3158***  0.00663         198.342  0.0\n",
       "brand_14       -2.1334***  0.00925         230.736  0.0\n",
       "brand_15       -2.0143***  0.00881         228.693  0.0\n",
       "brand_16       -1.2838***  0.00429         299.058  0.0\n",
       "brand_17       -1.0088***  0.00510         197.849  0.0\n",
       "brand_18        0.4086***  0.00476          85.927  0.0\n",
       "brand_19       -1.3282***  0.00588         225.884  0.0\n",
       "brand_20       -0.3111***  0.00535          58.164  0.0\n",
       "brand_21       -0.2624***  0.00415          63.222  0.0\n",
       "brand_22       -0.2126***  0.00424          50.128  0.0\n",
       "brand_23       -0.1598***  0.00460          34.718  0.0\n",
       "brand_24        -0.716***  0.00433         165.239  0.0\n",
       "brand_25       -0.8499***  0.00514         165.207  0.0\n",
       "brand_26       -1.0749***  0.00512         210.012  0.0\n",
       "brand_27       -2.4221***  0.05297          45.721  0.0\n",
       "brand_28       -1.0237***  0.00872         117.327  0.0\n",
       "brand_29       -2.0694***  0.01778         116.359  0.0\n",
       "brand_30       -1.2462***  0.00901         138.275  0.0\n",
       "brand_31       -1.6062***  0.01819          88.284  0.0\n",
       "brand_32       -0.4082***  0.00818          49.928  0.0\n",
       "brand_33       -1.6853***  0.03079          54.738  0.0\n",
       "brand_34       -0.8271***  0.00551         150.144  0.0\n",
       "brand_35       -0.5353***  0.00516         103.736  0.0\n",
       "brand_36       -0.1642***  0.00451          36.437  0.0\n",
       "brand_37       -1.8653***  0.01099         169.751  0.0\n",
       "home_2          1.0784***  0.00195         553.959  0.0\n",
       "cla_2           0.0372***  0.00245          15.141  0.0\n",
       "cla_3           0.1463***  0.00330          44.353  0.0\n",
       "cla_4           0.1527***  0.00463          32.995  0.0\n",
       "cla_5           0.1811***  0.00589          30.757  0.0\n",
       "group_in_out     0.838***  0.00203         412.202  0.0\n",
       "group_cy       -0.0786***  0.00134          58.890  0.0\n",
       "group_hp        0.1593***  0.00181          87.899  0.0\n",
       "group_we        0.0083***  0.00158           5.223  0.0\n",
       "group_le       -0.0391***  0.00155          25.220  0.0\n",
       "group_wi       -0.0789***  0.00119          66.381  0.0\n",
       "group_he       -0.0733***  0.00083          87.911  0.0\n",
       "group_li        0.0243***  0.00093          26.247  0.0\n",
       "group_sp       -0.0386***  0.00155          24.909  0.0\n",
       "group_ac       -0.0363***  0.00112          32.467  0.0\n",
       "group_brand      0.191***  0.00098         195.324  0.0\n",
       "group_home     -0.3742***  0.00120         312.298  0.0\n",
       "group_cla      -0.1054***  0.00143          73.868  0.0"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_table(ThetaOptBLP, SEOptBLP, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.83799025, -0.07862352,  0.15932883,  0.00827759, -0.03905791,\n",
       "       -0.07890525, -0.07325578,  0.0243136 , -0.038573  , -0.03633816,\n",
       "        0.19096739, -0.37421501, -0.10543107])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOptBLP[K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17336341520624832"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15359331774371004"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOptBLP[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.020491991765463364"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKN_theta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8471166690149027"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logit_beta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "qOpt = Similarity_ccp(ThetaOptBLP, z_logit, Psi, tol = 1.0e-30)\n",
    "HessOpt = compute_pertubation_hessian(qOpt, z_logit, ThetaOptBLP, Psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([qOpt[t].min() for t in np.arange(T)]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([la.eigvals(HessOpt[t]).min() for t in np.arange(T)]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market $t=1$ the price elasticities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Semi-elasticity wrt. product</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semi-elasticity of product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002001</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000080</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.000062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.149963</td>\n",
       "      <td>-0.250969</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.009968</td>\n",
       "      <td>0.004430</td>\n",
       "      <td>-0.004808</td>\n",
       "      <td>0.005798</td>\n",
       "      <td>-0.028612</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>-0.006033</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004012</td>\n",
       "      <td>0.003825</td>\n",
       "      <td>0.004869</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>-0.000231</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>0.002137</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>-0.012407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.151617</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>-0.245125</td>\n",
       "      <td>-0.000564</td>\n",
       "      <td>0.003668</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.000406</td>\n",
       "      <td>0.009407</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.002555</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>-0.003819</td>\n",
       "      <td>-0.001105</td>\n",
       "      <td>-0.000614</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.003014</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.005827</td>\n",
       "      <td>0.005804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.153529</td>\n",
       "      <td>0.002846</td>\n",
       "      <td>-0.000332</td>\n",
       "      <td>-0.253296</td>\n",
       "      <td>0.015510</td>\n",
       "      <td>0.006303</td>\n",
       "      <td>-0.007316</td>\n",
       "      <td>0.023663</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000799</td>\n",
       "      <td>-0.004218</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>-0.000202</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>-0.002005</td>\n",
       "      <td>0.020199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.150957</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.002986</td>\n",
       "      <td>0.021465</td>\n",
       "      <td>-0.249166</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002500</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>-0.002585</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>0.019353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.150962</td>\n",
       "      <td>-0.001591</td>\n",
       "      <td>0.002525</td>\n",
       "      <td>0.007304</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>-0.220405</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>-0.007111</td>\n",
       "      <td>-0.000946</td>\n",
       "      <td>-0.001197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.003631</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.003473</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>-0.005073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.151841</td>\n",
       "      <td>0.003173</td>\n",
       "      <td>0.000458</td>\n",
       "      <td>-0.014023</td>\n",
       "      <td>0.002885</td>\n",
       "      <td>0.006309</td>\n",
       "      <td>-0.251610</td>\n",
       "      <td>0.055590</td>\n",
       "      <td>0.003933</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007776</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000199</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.002094</td>\n",
       "      <td>-0.003429</td>\n",
       "      <td>0.010391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.152489</td>\n",
       "      <td>-0.005347</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.015487</td>\n",
       "      <td>0.004081</td>\n",
       "      <td>-0.004016</td>\n",
       "      <td>0.018981</td>\n",
       "      <td>-0.249928</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>-0.007556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012176</td>\n",
       "      <td>0.010295</td>\n",
       "      <td>0.006478</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>-0.000178</td>\n",
       "      <td>0.005913</td>\n",
       "      <td>-0.000394</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>-0.015616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.150766</td>\n",
       "      <td>-0.005642</td>\n",
       "      <td>0.003141</td>\n",
       "      <td>0.009376</td>\n",
       "      <td>0.004422</td>\n",
       "      <td>-0.006415</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>0.009745</td>\n",
       "      <td>-0.247644</td>\n",
       "      <td>-0.004985</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011585</td>\n",
       "      <td>0.010031</td>\n",
       "      <td>0.004212</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.001156</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.003969</td>\n",
       "      <td>0.000228</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>-0.010405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.150978</td>\n",
       "      <td>-0.003213</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>-0.001927</td>\n",
       "      <td>0.005591</td>\n",
       "      <td>-0.021532</td>\n",
       "      <td>-0.001183</td>\n",
       "      <td>-0.229295</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.003384</td>\n",
       "      <td>0.004827</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>-0.000098</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>-0.009897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.151311</td>\n",
       "      <td>0.001541</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>-0.009448</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.003524</td>\n",
       "      <td>-0.004460</td>\n",
       "      <td>0.013753</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.031238</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>-0.001443</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>-0.002000</td>\n",
       "      <td>0.009791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.150807</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.004197</td>\n",
       "      <td>-0.003380</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>-0.001164</td>\n",
       "      <td>-0.000275</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003336</td>\n",
       "      <td>-0.000800</td>\n",
       "      <td>0.024118</td>\n",
       "      <td>0.001964</td>\n",
       "      <td>0.001175</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>-0.007051</td>\n",
       "      <td>0.002491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.151598</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>-0.000727</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>0.001567</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.000419</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.018104</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>-0.000129</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.004115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.158391</td>\n",
       "      <td>0.002673</td>\n",
       "      <td>-0.000267</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.005598</td>\n",
       "      <td>-0.000390</td>\n",
       "      <td>0.020929</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>-0.000661</td>\n",
       "      <td>-0.002727</td>\n",
       "      <td>0.023075</td>\n",
       "      <td>0.016878</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>-0.005373</td>\n",
       "      <td>0.002255</td>\n",
       "      <td>0.004655</td>\n",
       "      <td>0.005378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.151025</td>\n",
       "      <td>-0.001465</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.007117</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.001813</td>\n",
       "      <td>0.004059</td>\n",
       "      <td>-0.015322</td>\n",
       "      <td>-0.000936</td>\n",
       "      <td>-0.003696</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002576</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.004077</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>-0.000156</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>-0.000576</td>\n",
       "      <td>-0.005950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.157544</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>-0.002916</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.002366</td>\n",
       "      <td>0.004563</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.014374</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.002647</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000240</td>\n",
       "      <td>0.000446</td>\n",
       "      <td>-0.001188</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>-0.001085</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>-0.000313</td>\n",
       "      <td>0.010180</td>\n",
       "      <td>0.027328</td>\n",
       "      <td>0.002037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.151365</td>\n",
       "      <td>0.004560</td>\n",
       "      <td>-0.000189</td>\n",
       "      <td>-0.007776</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.005923</td>\n",
       "      <td>-0.004122</td>\n",
       "      <td>0.009768</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>0.008095</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>-0.002230</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000243</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>-0.003334</td>\n",
       "      <td>0.005689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.150629</td>\n",
       "      <td>0.009433</td>\n",
       "      <td>0.003323</td>\n",
       "      <td>-0.010145</td>\n",
       "      <td>0.004878</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>-0.004449</td>\n",
       "      <td>0.012863</td>\n",
       "      <td>0.000381</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>-0.001288</td>\n",
       "      <td>-0.001414</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>-0.000485</td>\n",
       "      <td>0.009115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.149811</td>\n",
       "      <td>0.008455</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>0.008016</td>\n",
       "      <td>-0.005186</td>\n",
       "      <td>0.001988</td>\n",
       "      <td>0.004258</td>\n",
       "      <td>-0.002967</td>\n",
       "      <td>-0.000062</td>\n",
       "      <td>-0.000836</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001502</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>0.002448</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>-0.000019</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>-0.004704</td>\n",
       "      <td>-0.000587</td>\n",
       "      <td>0.002877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.153787</td>\n",
       "      <td>0.010970</td>\n",
       "      <td>-0.004853</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.005766</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.020013</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.004475</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000608</td>\n",
       "      <td>-0.001793</td>\n",
       "      <td>-0.005981</td>\n",
       "      <td>-0.003015</td>\n",
       "      <td>-0.001972</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>-0.006817</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.008473</td>\n",
       "      <td>0.005370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.162160</td>\n",
       "      <td>0.012039</td>\n",
       "      <td>-0.002335</td>\n",
       "      <td>-0.000809</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.006444</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.023695</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.004924</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>-0.001053</td>\n",
       "      <td>-0.003901</td>\n",
       "      <td>-0.002773</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>-0.005361</td>\n",
       "      <td>0.002564</td>\n",
       "      <td>0.006788</td>\n",
       "      <td>0.006933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.150434</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.004527</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>-0.000244</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>-0.019998</td>\n",
       "      <td>0.012041</td>\n",
       "      <td>-0.003443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.001764</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>-0.000361</td>\n",
       "      <td>0.001565</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.002927</td>\n",
       "      <td>-0.010901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.151265</td>\n",
       "      <td>0.003693</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>-0.008823</td>\n",
       "      <td>0.002042</td>\n",
       "      <td>0.009992</td>\n",
       "      <td>-0.004711</td>\n",
       "      <td>0.009337</td>\n",
       "      <td>0.001025</td>\n",
       "      <td>0.006679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.000355</td>\n",
       "      <td>-0.000069</td>\n",
       "      <td>0.002015</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>-0.006599</td>\n",
       "      <td>0.005249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.150854</td>\n",
       "      <td>-0.001071</td>\n",
       "      <td>0.003509</td>\n",
       "      <td>0.008491</td>\n",
       "      <td>-0.005005</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>-0.004036</td>\n",
       "      <td>-0.000145</td>\n",
       "      <td>-0.002344</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001792</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.003879</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.003686</td>\n",
       "      <td>-0.002703</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>-0.000844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.151177</td>\n",
       "      <td>-0.002828</td>\n",
       "      <td>0.000951</td>\n",
       "      <td>0.003523</td>\n",
       "      <td>0.003261</td>\n",
       "      <td>0.007434</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>-0.006914</td>\n",
       "      <td>-0.001207</td>\n",
       "      <td>-0.005595</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.001966</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>-0.000415</td>\n",
       "      <td>0.002347</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>-0.003068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.150893</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>0.002990</td>\n",
       "      <td>0.008637</td>\n",
       "      <td>0.005150</td>\n",
       "      <td>-0.033727</td>\n",
       "      <td>0.005300</td>\n",
       "      <td>-0.006813</td>\n",
       "      <td>-0.017959</td>\n",
       "      <td>-0.000562</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.003911</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.005481</td>\n",
       "      <td>0.003753</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>-0.006631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.161834</td>\n",
       "      <td>0.002365</td>\n",
       "      <td>-0.002071</td>\n",
       "      <td>0.004222</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.017294</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.003352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000192</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>-0.002864</td>\n",
       "      <td>-0.001497</td>\n",
       "      <td>-0.000531</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.005587</td>\n",
       "      <td>0.001807</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.004619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.150613</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.015630</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>-0.005192</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.002110</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002310</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>-0.002387</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>0.007242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.150561</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.015980</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>-0.005535</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.005758</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002486</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.002558</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>-0.001019</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.005212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.150599</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.014885</td>\n",
       "      <td>-0.007195</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.012378</td>\n",
       "      <td>-0.003855</td>\n",
       "      <td>0.004335</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>0.002591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001282</td>\n",
       "      <td>-0.002290</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.003853</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>-0.008809</td>\n",
       "      <td>0.002433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.159225</td>\n",
       "      <td>0.002391</td>\n",
       "      <td>0.010741</td>\n",
       "      <td>0.004953</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.005208</td>\n",
       "      <td>0.001508</td>\n",
       "      <td>0.021716</td>\n",
       "      <td>0.000849</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>-0.003583</td>\n",
       "      <td>-0.001793</td>\n",
       "      <td>-0.000478</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>-0.006953</td>\n",
       "      <td>0.002914</td>\n",
       "      <td>0.008449</td>\n",
       "      <td>0.004927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.150948</td>\n",
       "      <td>0.001663</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>0.021843</td>\n",
       "      <td>0.006248</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.007993</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.002762</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003303</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>-0.002581</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.019018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.150848</td>\n",
       "      <td>-0.004150</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.030212</td>\n",
       "      <td>0.020049</td>\n",
       "      <td>-0.009283</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>-0.029346</td>\n",
       "      <td>-0.002243</td>\n",
       "      <td>-0.007854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004723</td>\n",
       "      <td>0.004004</td>\n",
       "      <td>0.004920</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>-0.000175</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>-0.004209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.151449</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>-0.007543</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>-0.002694</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.007784</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>-0.003721</td>\n",
       "      <td>0.002511</td>\n",
       "      <td>-0.000466</td>\n",
       "      <td>-0.000786</td>\n",
       "      <td>0.000041</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>0.003485</td>\n",
       "      <td>-0.003961</td>\n",
       "      <td>0.005372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.151417</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>-0.000193</td>\n",
       "      <td>-0.008499</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>0.009622</td>\n",
       "      <td>-0.004507</td>\n",
       "      <td>0.011523</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.007177</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000818</td>\n",
       "      <td>-0.002336</td>\n",
       "      <td>0.000031</td>\n",
       "      <td>0.000155</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>-0.002263</td>\n",
       "      <td>0.006082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.151075</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>-0.002040</td>\n",
       "      <td>-0.004611</td>\n",
       "      <td>0.004960</td>\n",
       "      <td>0.010357</td>\n",
       "      <td>0.047499</td>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.004560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.250348</td>\n",
       "      <td>0.007721</td>\n",
       "      <td>-0.001211</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>-0.002002</td>\n",
       "      <td>0.005241</td>\n",
       "      <td>0.012162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.151288</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>-0.013242</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.049379</td>\n",
       "      <td>0.004007</td>\n",
       "      <td>0.005695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009493</td>\n",
       "      <td>-0.252355</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>-0.000694</td>\n",
       "      <td>-0.000947</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>-0.002101</td>\n",
       "      <td>0.010674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.153179</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>0.002242</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.004761</td>\n",
       "      <td>0.000134</td>\n",
       "      <td>0.015038</td>\n",
       "      <td>0.000814</td>\n",
       "      <td>0.003932</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000721</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>-0.242994</td>\n",
       "      <td>-0.001421</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>0.000105</td>\n",
       "      <td>-0.005608</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.006892</td>\n",
       "      <td>0.005142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.152055</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>-0.002852</td>\n",
       "      <td>-0.000889</td>\n",
       "      <td>0.003063</td>\n",
       "      <td>0.005270</td>\n",
       "      <td>0.000457</td>\n",
       "      <td>0.014037</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000521</td>\n",
       "      <td>-0.000970</td>\n",
       "      <td>-0.004105</td>\n",
       "      <td>-0.234798</td>\n",
       "      <td>0.015040</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>-0.003978</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.006205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.151618</td>\n",
       "      <td>0.003040</td>\n",
       "      <td>-0.002152</td>\n",
       "      <td>-0.002447</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.008795</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.004305</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>-0.001797</td>\n",
       "      <td>-0.003387</td>\n",
       "      <td>0.020402</td>\n",
       "      <td>-0.239948</td>\n",
       "      <td>0.001476</td>\n",
       "      <td>-0.001658</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.007314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.149978</td>\n",
       "      <td>-0.005352</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.009074</td>\n",
       "      <td>0.003814</td>\n",
       "      <td>-0.022138</td>\n",
       "      <td>0.000759</td>\n",
       "      <td>-0.004283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002268</td>\n",
       "      <td>0.002918</td>\n",
       "      <td>0.005635</td>\n",
       "      <td>0.027421</td>\n",
       "      <td>0.020147</td>\n",
       "      <td>-0.280402</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>-0.000265</td>\n",
       "      <td>-0.000762</td>\n",
       "      <td>-0.009468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.152897</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.003357</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.013788</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000229</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>-0.005633</td>\n",
       "      <td>-0.001383</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>-0.199770</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.005805</td>\n",
       "      <td>0.005239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.150041</td>\n",
       "      <td>0.001924</td>\n",
       "      <td>0.003280</td>\n",
       "      <td>0.006841</td>\n",
       "      <td>-0.005891</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>-0.001899</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002473</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>-0.000010</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>-0.247587</td>\n",
       "      <td>0.020085</td>\n",
       "      <td>0.000067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.150696</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>-0.002694</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.002936</td>\n",
       "      <td>-0.002404</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.000404</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002758</td>\n",
       "      <td>-0.000899</td>\n",
       "      <td>0.006095</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>-0.000013</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>-0.242477</td>\n",
       "      <td>0.001324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.150894</td>\n",
       "      <td>-0.005298</td>\n",
       "      <td>0.005106</td>\n",
       "      <td>0.030207</td>\n",
       "      <td>0.020912</td>\n",
       "      <td>-0.006548</td>\n",
       "      <td>0.008107</td>\n",
       "      <td>-0.035682</td>\n",
       "      <td>-0.001980</td>\n",
       "      <td>-0.007935</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007124</td>\n",
       "      <td>0.005085</td>\n",
       "      <td>0.005061</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>-0.000174</td>\n",
       "      <td>0.005134</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.001473</td>\n",
       "      <td>-0.259122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Semi-elasticity wrt. product        0         1         2         3   \\\n",
       "Semi-elasticity of product                                             \n",
       "0                            -0.002001  0.000026  0.000055  0.000094   \n",
       "1                             0.149963 -0.250969  0.003533  0.009968   \n",
       "2                             0.151617  0.001715 -0.245125 -0.000564   \n",
       "3                             0.153529  0.002846 -0.000332 -0.253296   \n",
       "4                             0.150957  0.001751  0.002986  0.021465   \n",
       "5                             0.150962 -0.001591  0.002525  0.007304   \n",
       "6                             0.151841  0.003173  0.000458 -0.014023   \n",
       "7                             0.152489 -0.005347  0.003622  0.015487   \n",
       "8                             0.150766 -0.005642  0.003141  0.009376   \n",
       "9                             0.150978 -0.003213  0.002803  0.010258   \n",
       "10                            0.151311  0.001541  0.002445 -0.009448   \n",
       "11                            0.150807  0.000588  0.004197 -0.003380   \n",
       "12                            0.151598  0.001002 -0.000727  0.005236   \n",
       "13                            0.158391  0.002673 -0.000267  0.002311   \n",
       "14                            0.151025 -0.001465  0.002465  0.007117   \n",
       "15                            0.157544  0.002078 -0.002916  0.004218   \n",
       "16                            0.151365  0.004560 -0.000189 -0.007776   \n",
       "17                            0.150629  0.009433  0.003323 -0.010145   \n",
       "18                            0.149811  0.008455  0.005252  0.008016   \n",
       "19                            0.153787  0.010970 -0.004853  0.001704   \n",
       "20                            0.162160  0.012039 -0.002335 -0.000809   \n",
       "21                            0.150434  0.002294  0.001484  0.004527   \n",
       "22                            0.151265  0.003693  0.000832 -0.008823   \n",
       "23                            0.150854 -0.001071  0.003509  0.008491   \n",
       "24                            0.151177 -0.002828  0.000951  0.003523   \n",
       "25                            0.150893 -0.002514  0.002990  0.008637   \n",
       "26                            0.161834  0.002365 -0.002071  0.004222   \n",
       "27                            0.150613  0.001578  0.015630  0.001468   \n",
       "28                            0.150561  0.003139  0.015980  0.002897   \n",
       "29                            0.150599  0.002191  0.014885 -0.007195   \n",
       "30                            0.159225  0.002391  0.010741  0.004953   \n",
       "31                            0.150948  0.001663  0.003532  0.021843   \n",
       "32                            0.150848 -0.004150  0.003284  0.030212   \n",
       "33                            0.151449  0.004057  0.001795 -0.007543   \n",
       "34                            0.151417  0.004406 -0.000193 -0.008499   \n",
       "35                            0.151075  0.002925  0.001839 -0.002040   \n",
       "36                            0.151288  0.003429  0.001766 -0.013242   \n",
       "37                            0.153179  0.002112 -0.003413  0.002242   \n",
       "38                            0.152055  0.002519 -0.002852 -0.000889   \n",
       "39                            0.151618  0.003040 -0.002152 -0.002447   \n",
       "40                            0.149978 -0.005352  0.003533  0.007514   \n",
       "41                            0.152897  0.002055 -0.002706  0.001880   \n",
       "42                            0.150041  0.001924  0.003280  0.006841   \n",
       "43                            0.150696  0.000600  0.004606 -0.002694   \n",
       "44                            0.150894 -0.005298  0.005106  0.030207   \n",
       "\n",
       "Semi-elasticity wrt. product        4         5         6         7   \\\n",
       "Semi-elasticity of product                                             \n",
       "0                             0.000067  0.000080  0.000048  0.000142   \n",
       "1                             0.004430 -0.004808  0.005798 -0.028612   \n",
       "2                             0.003668  0.003705  0.000406  0.009407   \n",
       "3                             0.015510  0.006303 -0.007316  0.023663   \n",
       "4                            -0.249166  0.004783  0.002083  0.008629   \n",
       "5                             0.004005 -0.220405  0.003814 -0.007111   \n",
       "6                             0.002885  0.006309 -0.251610  0.055590   \n",
       "7                             0.004081 -0.004016  0.018981 -0.249928   \n",
       "8                             0.004422 -0.006415  0.016125  0.009745   \n",
       "9                             0.002654 -0.001927  0.005591 -0.021532   \n",
       "10                            0.003992  0.003524 -0.004460  0.013753   \n",
       "11                            0.002660  0.003325 -0.001164 -0.000275   \n",
       "12                            0.001567  0.002121  0.002914  0.004271   \n",
       "13                            0.003255  0.005598 -0.000390  0.020929   \n",
       "14                            0.002042  0.001813  0.004059 -0.015322   \n",
       "15                            0.002366  0.004563  0.001025  0.014374   \n",
       "16                            0.001351  0.005923 -0.004122  0.009768   \n",
       "17                            0.004878  0.003490 -0.004449  0.012863   \n",
       "18                           -0.005186  0.001988  0.004258 -0.002967   \n",
       "19                            0.001559  0.005766  0.000992  0.020013   \n",
       "20                            0.003209  0.006444  0.001008  0.023695   \n",
       "21                            0.003536 -0.000244  0.001769 -0.019998   \n",
       "22                            0.002042  0.009992 -0.004711  0.009337   \n",
       "23                           -0.005005  0.005346  0.004119 -0.004036   \n",
       "24                            0.003261  0.007434  0.001829 -0.006914   \n",
       "25                            0.005150 -0.033727  0.005300 -0.006813   \n",
       "26                            0.002245  0.004358  0.001469  0.017294   \n",
       "27                           -0.005192  0.003545  0.001182  0.005050   \n",
       "28                           -0.005535  0.002798  0.001502  0.001947   \n",
       "29                            0.002173  0.012378 -0.003855  0.004335   \n",
       "30                            0.003743  0.005208  0.001508  0.021716   \n",
       "31                            0.006248  0.004809  0.000978  0.007993   \n",
       "32                            0.020049 -0.009283  0.007215 -0.029346   \n",
       "33                            0.001699  0.004873 -0.002694  0.007908   \n",
       "34                            0.000498  0.009622 -0.004507  0.011523   \n",
       "35                           -0.004611  0.004960  0.010357  0.047499   \n",
       "36                            0.001832  0.006737  0.005642  0.049379   \n",
       "37                            0.002150  0.004761  0.000134  0.015038   \n",
       "38                            0.003063  0.005270  0.000457  0.014037   \n",
       "39                            0.003017  0.004991  0.000976  0.008795   \n",
       "40                            0.000631  0.009074  0.003814 -0.022138   \n",
       "41                            0.003357  0.004574  0.000313  0.013788   \n",
       "42                           -0.005891  0.004528  0.003445 -0.001899   \n",
       "43                            0.002631  0.002936 -0.002404  0.002470   \n",
       "44                            0.020912 -0.006548  0.008107 -0.035682   \n",
       "\n",
       "Semi-elasticity wrt. product        8         9   ...        35        36  \\\n",
       "Semi-elasticity of product                        ...                       \n",
       "0                             0.000012  0.000049  ...  0.000036  0.000029   \n",
       "1                            -0.002514 -0.006033  ...  0.004012  0.003825   \n",
       "2                             0.000679  0.002555  ...  0.001224  0.000957   \n",
       "3                             0.001193  0.005500  ... -0.000799 -0.004218   \n",
       "4                             0.000779  0.001969  ... -0.002500  0.000808   \n",
       "5                            -0.000946 -0.001197  ...  0.002251  0.002487   \n",
       "6                             0.003933  0.005746  ...  0.007776  0.003445   \n",
       "7                             0.000812 -0.007556  ...  0.012176  0.010295   \n",
       "8                            -0.247644 -0.004985  ...  0.011585  0.010031   \n",
       "9                            -0.001183 -0.229295  ...  0.003331  0.003384   \n",
       "10                            0.000845  0.031238  ...  0.001158 -0.001443   \n",
       "11                            0.000418  0.000355  ...  0.003336 -0.000800   \n",
       "12                            0.000341  0.000419  ...  0.001180  0.001932   \n",
       "13                            0.000915  0.003908  ...  0.000749 -0.000661   \n",
       "14                           -0.000936 -0.003696  ...  0.002576  0.002296   \n",
       "15                            0.000676  0.002647  ... -0.000240  0.000446   \n",
       "16                            0.000782  0.008095  ...  0.000024 -0.002230   \n",
       "17                            0.000381  0.003791  ...  0.001188 -0.001288   \n",
       "18                           -0.000062 -0.000836  ... -0.001502  0.002834   \n",
       "19                            0.000670  0.004475  ... -0.000608 -0.001793   \n",
       "20                            0.000638  0.004924  ...  0.000690 -0.001053   \n",
       "21                            0.012041 -0.003443  ...  0.001264  0.001764   \n",
       "22                            0.001025  0.006679  ...  0.000818 -0.002514   \n",
       "23                           -0.000145 -0.002344  ... -0.001792  0.002653   \n",
       "24                           -0.001207 -0.005595  ...  0.001585  0.001367   \n",
       "25                           -0.017959 -0.000562  ...  0.003854  0.003383   \n",
       "26                            0.000774  0.003352  ...  0.000192  0.000407   \n",
       "27                            0.000650  0.002110  ... -0.002310  0.001193   \n",
       "28                            0.000607  0.005758  ... -0.002486  0.001508   \n",
       "29                            0.000477  0.002591  ...  0.001282 -0.002290   \n",
       "30                            0.000849  0.003811  ...  0.000667  0.000370   \n",
       "31                            0.000808  0.002762  ... -0.003303  0.001525   \n",
       "32                           -0.002243 -0.007854  ...  0.004723  0.004004   \n",
       "33                            0.000570  0.007784  ...  0.001707 -0.003721   \n",
       "34                            0.000842  0.007177  ... -0.000818 -0.002336   \n",
       "35                            0.003764  0.004560  ... -0.250348  0.007721   \n",
       "36                            0.004007  0.005695  ...  0.009493 -0.252355   \n",
       "37                            0.000814  0.003932  ... -0.000721  0.000492   \n",
       "38                            0.000879  0.003719  ...  0.000521 -0.000970   \n",
       "39                            0.000876  0.004305  ...  0.001959 -0.001797   \n",
       "40                            0.000759 -0.004283  ...  0.002268  0.002918   \n",
       "41                            0.000771  0.003385  ...  0.000229  0.000924   \n",
       "42                            0.000092  0.002561  ... -0.002473  0.002040   \n",
       "43                            0.000404 -0.000011  ...  0.002758 -0.000899   \n",
       "44                           -0.001980 -0.007935  ...  0.007124  0.005085   \n",
       "\n",
       "Semi-elasticity wrt. product        37        38        39        40  \\\n",
       "Semi-elasticity of product                                             \n",
       "0                             0.000062  0.000021  0.000016  0.000001   \n",
       "1                             0.004869  0.002009  0.001788 -0.000231   \n",
       "2                            -0.003819 -0.001105 -0.000614  0.000074   \n",
       "3                             0.001476 -0.000202 -0.000411  0.000092   \n",
       "4                             0.001959  0.000966  0.000701  0.000011   \n",
       "5                             0.003631  0.001391  0.000971  0.000129   \n",
       "6                             0.000169  0.000199  0.000314  0.000090   \n",
       "7                             0.006478  0.002093  0.000967 -0.000178   \n",
       "8                             0.004212  0.001574  0.001156  0.000073   \n",
       "9                             0.004827  0.001580  0.001348 -0.000098   \n",
       "10                            0.001137  0.000831  0.000671  0.000052   \n",
       "11                            0.024118  0.001964  0.001175 -0.000014   \n",
       "12                            0.018104  0.000341 -0.000129  0.000022   \n",
       "13                           -0.002727  0.023075  0.016878  0.001611   \n",
       "14                            0.004077  0.001422  0.001162 -0.000156   \n",
       "15                           -0.001188 -0.000922 -0.001085  0.000101   \n",
       "16                            0.000798  0.000243  0.000180  0.000059   \n",
       "17                           -0.001414  0.000705  0.000694  0.000064   \n",
       "18                            0.005656  0.002448  0.001450 -0.000019   \n",
       "19                           -0.005981 -0.003015 -0.001972  0.000163   \n",
       "20                           -0.003901 -0.002773 -0.001514  0.000180   \n",
       "21                            0.001873  0.001033  0.001084 -0.000361   \n",
       "22                            0.002232  0.000621  0.000355 -0.000069   \n",
       "23                            0.003879  0.001487  0.001125 -0.000153   \n",
       "24                            0.001966  0.000828  0.000592 -0.000415   \n",
       "25                            0.003911  0.000998  0.000739  0.005481   \n",
       "26                           -0.002864 -0.001497 -0.000531  0.000128   \n",
       "27                            0.000947  0.001062  0.001023  0.000007   \n",
       "28                            0.002558  0.001287  0.001030  0.000011   \n",
       "29                            0.004324  0.001247  0.000514  0.000013   \n",
       "30                           -0.003583 -0.001793 -0.000478  0.000151   \n",
       "31                            0.001497  0.001033  0.000600  0.000027   \n",
       "32                            0.004920  0.001709  0.001419 -0.000175   \n",
       "33                            0.002511 -0.000466 -0.000786  0.000041   \n",
       "34                            0.000031  0.000155  0.000120  0.000064   \n",
       "35                           -0.001211  0.000303  0.000840  0.000071   \n",
       "36                            0.001017 -0.000694 -0.000947  0.000113   \n",
       "37                           -0.242994 -0.001421 -0.000864  0.000105   \n",
       "38                           -0.004105 -0.234798  0.015040  0.001481   \n",
       "39                           -0.003387  0.020402 -0.239948  0.001476   \n",
       "40                            0.005635  0.027421  0.020147 -0.280402   \n",
       "41                           -0.005633 -0.001383 -0.000425  0.000101   \n",
       "42                            0.004549  0.001468  0.001128 -0.000010   \n",
       "43                            0.006095  0.002037  0.001123 -0.000013   \n",
       "44                            0.005061  0.002114  0.001837 -0.000174   \n",
       "\n",
       "Semi-elasticity wrt. product        41        42        43        44  \n",
       "Semi-elasticity of product                                            \n",
       "0                             0.000061  0.000029  0.000069  0.000062  \n",
       "1                             0.004716  0.002137  0.001564 -0.012407  \n",
       "2                            -0.003014  0.001768  0.005827  0.005804  \n",
       "3                             0.001232  0.002169 -0.002005  0.020199  \n",
       "4                             0.003045 -0.002585  0.002710  0.019353  \n",
       "5                             0.003473  0.001663  0.002532 -0.005073  \n",
       "6                             0.000393  0.002094 -0.003429  0.010391  \n",
       "7                             0.005913 -0.000394  0.001203 -0.015616  \n",
       "8                             0.003969  0.000228  0.002365 -0.010405  \n",
       "9                             0.004137  0.001514 -0.000015 -0.009897  \n",
       "10                            0.002277  0.001048 -0.002000  0.009791  \n",
       "11                            0.005312  0.000057 -0.007051  0.002491  \n",
       "12                            0.000436  0.000227  0.003749  0.004115  \n",
       "13                           -0.005373  0.002255  0.004655  0.005378  \n",
       "14                            0.003669  0.001386 -0.000576 -0.005950  \n",
       "15                           -0.000313  0.010180  0.027328  0.002037  \n",
       "16                            0.001367  0.004293 -0.003334  0.005689  \n",
       "17                            0.000150  0.001617 -0.000485  0.009115  \n",
       "18                            0.005987 -0.004704 -0.000587  0.002877  \n",
       "19                           -0.006817  0.002212  0.008473  0.005370  \n",
       "20                           -0.005361  0.002564  0.006788  0.006933  \n",
       "21                            0.001565  0.001280  0.002927 -0.010901  \n",
       "22                            0.002015  0.003486 -0.006599  0.005249  \n",
       "23                            0.003686 -0.002703  0.001196 -0.000844  \n",
       "24                            0.002347  0.001742  0.002711 -0.003068  \n",
       "25                            0.003753  0.001084  0.003101 -0.006631  \n",
       "26                           -0.005587  0.001807  0.006264  0.004619  \n",
       "27                            0.002711 -0.002387  0.003274  0.007242  \n",
       "28                            0.003378 -0.001019  0.000421  0.005212  \n",
       "29                            0.003853  0.002010 -0.008809  0.002433  \n",
       "30                           -0.006953  0.002914  0.008449  0.004927  \n",
       "31                            0.002675 -0.002581  0.001990  0.019018  \n",
       "32                            0.004458  0.002093  0.002966 -0.004209  \n",
       "33                            0.002310  0.003485 -0.003961  0.005372  \n",
       "34                            0.000754  0.003980 -0.002263  0.006082  \n",
       "35                            0.000384 -0.002002  0.005241  0.012162  \n",
       "36                            0.001901  0.002031 -0.002101  0.010674  \n",
       "37                           -0.005608  0.002191  0.006892  0.005142  \n",
       "38                           -0.003978  0.002043  0.006655  0.006205  \n",
       "39                           -0.001658  0.002129  0.004978  0.007314  \n",
       "40                            0.005403 -0.000265 -0.000762 -0.009468  \n",
       "41                           -0.199770  0.001978  0.005805  0.005239  \n",
       "42                            0.004087 -0.247587  0.020085  0.000067  \n",
       "43                            0.005111  0.008556 -0.242477  0.001324  \n",
       "44                            0.005134  0.000032  0.001473 -0.259122  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_hat = Similarity_elasticity(qOpt, z_logit, ThetaOptBLP, Psi, char_number = pr_index)\n",
    "pd.DataFrame(E_hat[0]).rename_axis(index = 'Semi-elasticity of product', columns = 'Semi-elasticity wrt. product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios for the Similarity model\n",
    "\n",
    "The diversion ratio to product j from product k is the fraction of consumers leaving product k and switching to product j following a one percent increase in the price of product k. Hence we have:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{jk}^i = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial x_{ik\\ell}}{\\partial P_k(u_i|\\lambda) / \\partial x_{ik\\ell}} = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial u_{ik}}{\\partial P_k(u_i|\\lambda) / \\partial u_{ik}}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{D}^i = \\left( \\mathcal{D}_{jk}^i \\right)_{j,k \\in \\{0,1,\\ldots ,5\\}}$ is the matrix of diversion ratios for individual i. This can be written more compactly as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}^i = -100 \\cdot  (\\nabla_u P(u|\\lambda) \\circ I_J)^{-1}\\nabla_u P(u|\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Similarity_diversion_ratio(q, x, Theta, psi_stack):\n",
    "    '''\n",
    "    This function calculates diversion ratios from the Similarity model\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Diversion_ratio: a dictionary of T numpy arrays (J,J) of diversion ratios from product j to product k for each individual i\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack) # Find the derivatives of ccp's wrt. utilities\n",
    "    inv_diaggrad = {t: np.divide(1, np.diag(Grad[t]), out = np.zeros_like(np.diag(Grad[t])), where = (np.diag(Grad[t]) != 0)) for t in np.arange(T)}  # Compute the inverse of the 'own'-derivatives of ccp's\n",
    "    DR = {t: np.multiply(-100, np.einsum('j,jk->jk', inv_diaggrad[t], Grad[t])) for t in np.arange(T)} # Compute diversion ratios as a hadamard product.\n",
    "    \n",
    "    return DR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the implied diversion ratios $\\mathcal{ D}^i$ from our estimates $\\hat \\theta^{\\text{Similarity}}$, we find for market $t=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Diversion ratio wrt. product</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diversion ratio of product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.307574</td>\n",
       "      <td>2.723282</td>\n",
       "      <td>4.688007</td>\n",
       "      <td>3.330634</td>\n",
       "      <td>3.978255</td>\n",
       "      <td>2.418855</td>\n",
       "      <td>7.114428</td>\n",
       "      <td>0.585777</td>\n",
       "      <td>2.471648</td>\n",
       "      <td>...</td>\n",
       "      <td>1.806840</td>\n",
       "      <td>1.471651</td>\n",
       "      <td>3.078596</td>\n",
       "      <td>1.057676</td>\n",
       "      <td>0.777501</td>\n",
       "      <td>0.056353</td>\n",
       "      <td>3.059413</td>\n",
       "      <td>1.452561</td>\n",
       "      <td>3.424609</td>\n",
       "      <td>3.081005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59.753702</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.407780</td>\n",
       "      <td>3.971815</td>\n",
       "      <td>1.765288</td>\n",
       "      <td>-1.915900</td>\n",
       "      <td>2.310176</td>\n",
       "      <td>-11.400449</td>\n",
       "      <td>-1.001716</td>\n",
       "      <td>-2.403690</td>\n",
       "      <td>...</td>\n",
       "      <td>1.598733</td>\n",
       "      <td>1.524083</td>\n",
       "      <td>1.939929</td>\n",
       "      <td>0.800616</td>\n",
       "      <td>0.712439</td>\n",
       "      <td>-0.091904</td>\n",
       "      <td>1.879247</td>\n",
       "      <td>0.851388</td>\n",
       "      <td>0.623112</td>\n",
       "      <td>-4.943474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61.852762</td>\n",
       "      <td>0.699685</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-0.230059</td>\n",
       "      <td>1.496433</td>\n",
       "      <td>1.511280</td>\n",
       "      <td>0.165823</td>\n",
       "      <td>3.837705</td>\n",
       "      <td>0.277151</td>\n",
       "      <td>1.042219</td>\n",
       "      <td>...</td>\n",
       "      <td>0.499441</td>\n",
       "      <td>0.390229</td>\n",
       "      <td>-1.558047</td>\n",
       "      <td>-0.450632</td>\n",
       "      <td>-0.250630</td>\n",
       "      <td>0.030153</td>\n",
       "      <td>-1.229623</td>\n",
       "      <td>0.721270</td>\n",
       "      <td>2.377183</td>\n",
       "      <td>2.367821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60.612358</td>\n",
       "      <td>1.123734</td>\n",
       "      <td>-0.130962</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>6.123156</td>\n",
       "      <td>2.488534</td>\n",
       "      <td>-2.888239</td>\n",
       "      <td>9.342014</td>\n",
       "      <td>0.471011</td>\n",
       "      <td>2.171276</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.315435</td>\n",
       "      <td>-1.665383</td>\n",
       "      <td>0.582643</td>\n",
       "      <td>-0.079936</td>\n",
       "      <td>-0.162272</td>\n",
       "      <td>0.036501</td>\n",
       "      <td>0.486446</td>\n",
       "      <td>0.856248</td>\n",
       "      <td>-0.791584</td>\n",
       "      <td>7.974383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60.584936</td>\n",
       "      <td>0.702675</td>\n",
       "      <td>1.198474</td>\n",
       "      <td>8.614700</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.919672</td>\n",
       "      <td>0.835956</td>\n",
       "      <td>3.463076</td>\n",
       "      <td>0.312539</td>\n",
       "      <td>0.790256</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.003199</td>\n",
       "      <td>0.324105</td>\n",
       "      <td>0.786177</td>\n",
       "      <td>0.387511</td>\n",
       "      <td>0.281390</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>1.221932</td>\n",
       "      <td>-1.037354</td>\n",
       "      <td>1.087608</td>\n",
       "      <td>7.766907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>68.492913</td>\n",
       "      <td>-0.721817</td>\n",
       "      <td>1.145596</td>\n",
       "      <td>3.313780</td>\n",
       "      <td>1.816947</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.730338</td>\n",
       "      <td>-3.226153</td>\n",
       "      <td>-0.429099</td>\n",
       "      <td>-0.543120</td>\n",
       "      <td>...</td>\n",
       "      <td>1.021258</td>\n",
       "      <td>1.128283</td>\n",
       "      <td>1.647316</td>\n",
       "      <td>0.631165</td>\n",
       "      <td>0.440621</td>\n",
       "      <td>0.058702</td>\n",
       "      <td>1.575611</td>\n",
       "      <td>0.754708</td>\n",
       "      <td>1.148661</td>\n",
       "      <td>-2.301801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>60.347764</td>\n",
       "      <td>1.261240</td>\n",
       "      <td>0.182150</td>\n",
       "      <td>-5.573290</td>\n",
       "      <td>1.146561</td>\n",
       "      <td>2.507432</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>22.093681</td>\n",
       "      <td>1.563061</td>\n",
       "      <td>2.283746</td>\n",
       "      <td>...</td>\n",
       "      <td>3.090436</td>\n",
       "      <td>1.369371</td>\n",
       "      <td>0.067007</td>\n",
       "      <td>0.079279</td>\n",
       "      <td>0.124887</td>\n",
       "      <td>0.035755</td>\n",
       "      <td>0.156182</td>\n",
       "      <td>0.832195</td>\n",
       "      <td>-1.362733</td>\n",
       "      <td>4.129654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>61.013306</td>\n",
       "      <td>-2.139479</td>\n",
       "      <td>1.449071</td>\n",
       "      <td>6.196582</td>\n",
       "      <td>1.632709</td>\n",
       "      <td>-1.607002</td>\n",
       "      <td>7.594537</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.324712</td>\n",
       "      <td>-3.023080</td>\n",
       "      <td>...</td>\n",
       "      <td>4.871817</td>\n",
       "      <td>4.119325</td>\n",
       "      <td>2.591954</td>\n",
       "      <td>0.837357</td>\n",
       "      <td>0.386801</td>\n",
       "      <td>-0.071338</td>\n",
       "      <td>2.366054</td>\n",
       "      <td>-0.157675</td>\n",
       "      <td>0.481346</td>\n",
       "      <td>-6.248169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>60.880222</td>\n",
       "      <td>-2.278189</td>\n",
       "      <td>1.268219</td>\n",
       "      <td>3.786182</td>\n",
       "      <td>1.785708</td>\n",
       "      <td>-2.590289</td>\n",
       "      <td>6.511305</td>\n",
       "      <td>3.935118</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-2.013101</td>\n",
       "      <td>...</td>\n",
       "      <td>4.678081</td>\n",
       "      <td>4.050586</td>\n",
       "      <td>1.700734</td>\n",
       "      <td>0.635783</td>\n",
       "      <td>0.466801</td>\n",
       "      <td>0.029635</td>\n",
       "      <td>1.602749</td>\n",
       "      <td>0.092118</td>\n",
       "      <td>0.954872</td>\n",
       "      <td>-4.201635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65.844556</td>\n",
       "      <td>-1.401242</td>\n",
       "      <td>1.222436</td>\n",
       "      <td>4.473787</td>\n",
       "      <td>1.157347</td>\n",
       "      <td>-0.840381</td>\n",
       "      <td>2.438539</td>\n",
       "      <td>-9.390706</td>\n",
       "      <td>-0.516007</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.452784</td>\n",
       "      <td>1.475755</td>\n",
       "      <td>2.105206</td>\n",
       "      <td>0.689144</td>\n",
       "      <td>0.588098</td>\n",
       "      <td>-0.042876</td>\n",
       "      <td>1.804333</td>\n",
       "      <td>0.660459</td>\n",
       "      <td>-0.006694</td>\n",
       "      <td>-4.316460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>67.209822</td>\n",
       "      <td>0.684456</td>\n",
       "      <td>1.085990</td>\n",
       "      <td>-4.196717</td>\n",
       "      <td>1.773312</td>\n",
       "      <td>1.565510</td>\n",
       "      <td>-1.981098</td>\n",
       "      <td>6.108854</td>\n",
       "      <td>0.375374</td>\n",
       "      <td>13.875570</td>\n",
       "      <td>...</td>\n",
       "      <td>0.514499</td>\n",
       "      <td>-0.641121</td>\n",
       "      <td>0.504904</td>\n",
       "      <td>0.369225</td>\n",
       "      <td>0.298178</td>\n",
       "      <td>0.022914</td>\n",
       "      <td>1.011457</td>\n",
       "      <td>0.465532</td>\n",
       "      <td>-0.888198</td>\n",
       "      <td>4.348960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>63.122804</td>\n",
       "      <td>0.245930</td>\n",
       "      <td>1.756871</td>\n",
       "      <td>-1.414565</td>\n",
       "      <td>1.113438</td>\n",
       "      <td>1.391678</td>\n",
       "      <td>-0.487103</td>\n",
       "      <td>-0.115179</td>\n",
       "      <td>0.175101</td>\n",
       "      <td>0.148452</td>\n",
       "      <td>...</td>\n",
       "      <td>1.396441</td>\n",
       "      <td>-0.334963</td>\n",
       "      <td>10.094989</td>\n",
       "      <td>0.822013</td>\n",
       "      <td>0.491928</td>\n",
       "      <td>-0.005738</td>\n",
       "      <td>2.223618</td>\n",
       "      <td>0.023664</td>\n",
       "      <td>-2.951181</td>\n",
       "      <td>1.042815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>63.751945</td>\n",
       "      <td>0.421170</td>\n",
       "      <td>-0.305782</td>\n",
       "      <td>2.202055</td>\n",
       "      <td>0.658817</td>\n",
       "      <td>0.892046</td>\n",
       "      <td>1.225229</td>\n",
       "      <td>1.796113</td>\n",
       "      <td>0.143316</td>\n",
       "      <td>0.176375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496377</td>\n",
       "      <td>0.812443</td>\n",
       "      <td>7.613177</td>\n",
       "      <td>0.143395</td>\n",
       "      <td>-0.054341</td>\n",
       "      <td>0.009309</td>\n",
       "      <td>0.183309</td>\n",
       "      <td>0.095478</td>\n",
       "      <td>1.576495</td>\n",
       "      <td>1.730673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>61.941815</td>\n",
       "      <td>1.045372</td>\n",
       "      <td>-0.104545</td>\n",
       "      <td>0.903703</td>\n",
       "      <td>1.273015</td>\n",
       "      <td>2.189393</td>\n",
       "      <td>-0.152577</td>\n",
       "      <td>8.184640</td>\n",
       "      <td>0.357958</td>\n",
       "      <td>1.528222</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292988</td>\n",
       "      <td>-0.258691</td>\n",
       "      <td>-1.066396</td>\n",
       "      <td>9.023778</td>\n",
       "      <td>6.600394</td>\n",
       "      <td>0.629833</td>\n",
       "      <td>-2.101337</td>\n",
       "      <td>0.881681</td>\n",
       "      <td>1.820387</td>\n",
       "      <td>2.103009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>77.591907</td>\n",
       "      <td>-0.752650</td>\n",
       "      <td>1.266537</td>\n",
       "      <td>3.656234</td>\n",
       "      <td>1.049076</td>\n",
       "      <td>0.931345</td>\n",
       "      <td>2.085216</td>\n",
       "      <td>-7.872016</td>\n",
       "      <td>-0.480964</td>\n",
       "      <td>-1.899089</td>\n",
       "      <td>...</td>\n",
       "      <td>1.323684</td>\n",
       "      <td>1.179444</td>\n",
       "      <td>2.094613</td>\n",
       "      <td>0.730329</td>\n",
       "      <td>0.596893</td>\n",
       "      <td>-0.080382</td>\n",
       "      <td>1.885045</td>\n",
       "      <td>0.712292</td>\n",
       "      <td>-0.295886</td>\n",
       "      <td>-3.056907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>65.112260</td>\n",
       "      <td>0.858738</td>\n",
       "      <td>-1.205096</td>\n",
       "      <td>1.743319</td>\n",
       "      <td>0.977933</td>\n",
       "      <td>1.885918</td>\n",
       "      <td>0.423566</td>\n",
       "      <td>5.940557</td>\n",
       "      <td>0.279306</td>\n",
       "      <td>1.093897</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.099025</td>\n",
       "      <td>0.184194</td>\n",
       "      <td>-0.491141</td>\n",
       "      <td>-0.380912</td>\n",
       "      <td>-0.448276</td>\n",
       "      <td>0.041940</td>\n",
       "      <td>-0.129267</td>\n",
       "      <td>4.207322</td>\n",
       "      <td>11.294398</td>\n",
       "      <td>0.841841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>78.314255</td>\n",
       "      <td>2.359145</td>\n",
       "      <td>-0.097846</td>\n",
       "      <td>-4.022942</td>\n",
       "      <td>0.698778</td>\n",
       "      <td>3.064278</td>\n",
       "      <td>-2.132677</td>\n",
       "      <td>5.053614</td>\n",
       "      <td>0.404751</td>\n",
       "      <td>4.188492</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012547</td>\n",
       "      <td>-1.153888</td>\n",
       "      <td>0.412969</td>\n",
       "      <td>0.125584</td>\n",
       "      <td>0.092933</td>\n",
       "      <td>0.030558</td>\n",
       "      <td>0.707315</td>\n",
       "      <td>2.221386</td>\n",
       "      <td>-1.724811</td>\n",
       "      <td>2.943277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>60.130527</td>\n",
       "      <td>3.765556</td>\n",
       "      <td>1.326423</td>\n",
       "      <td>-4.049923</td>\n",
       "      <td>1.947161</td>\n",
       "      <td>1.393137</td>\n",
       "      <td>-1.775899</td>\n",
       "      <td>5.134715</td>\n",
       "      <td>0.152197</td>\n",
       "      <td>1.513511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474430</td>\n",
       "      <td>-0.514001</td>\n",
       "      <td>-0.564358</td>\n",
       "      <td>0.281373</td>\n",
       "      <td>0.276885</td>\n",
       "      <td>0.025600</td>\n",
       "      <td>0.059889</td>\n",
       "      <td>0.645333</td>\n",
       "      <td>-0.193806</td>\n",
       "      <td>3.638627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>60.180238</td>\n",
       "      <td>3.396359</td>\n",
       "      <td>2.109711</td>\n",
       "      <td>3.220067</td>\n",
       "      <td>-2.083402</td>\n",
       "      <td>0.798514</td>\n",
       "      <td>1.710408</td>\n",
       "      <td>-1.191866</td>\n",
       "      <td>-0.024990</td>\n",
       "      <td>-0.335746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.603257</td>\n",
       "      <td>1.138482</td>\n",
       "      <td>2.272206</td>\n",
       "      <td>0.983532</td>\n",
       "      <td>0.582591</td>\n",
       "      <td>-0.007761</td>\n",
       "      <td>2.404876</td>\n",
       "      <td>-1.889455</td>\n",
       "      <td>-0.235847</td>\n",
       "      <td>1.155558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>61.296852</td>\n",
       "      <td>4.372408</td>\n",
       "      <td>-1.934338</td>\n",
       "      <td>0.679340</td>\n",
       "      <td>0.621235</td>\n",
       "      <td>2.298105</td>\n",
       "      <td>0.395208</td>\n",
       "      <td>7.976903</td>\n",
       "      <td>0.267242</td>\n",
       "      <td>1.783708</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.242225</td>\n",
       "      <td>-0.714840</td>\n",
       "      <td>-2.383924</td>\n",
       "      <td>-1.201805</td>\n",
       "      <td>-0.785907</td>\n",
       "      <td>0.064784</td>\n",
       "      <td>-2.717111</td>\n",
       "      <td>0.881851</td>\n",
       "      <td>3.377378</td>\n",
       "      <td>2.140506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>59.823880</td>\n",
       "      <td>4.441284</td>\n",
       "      <td>-0.861434</td>\n",
       "      <td>-0.298303</td>\n",
       "      <td>1.183777</td>\n",
       "      <td>2.377157</td>\n",
       "      <td>0.371844</td>\n",
       "      <td>8.741505</td>\n",
       "      <td>0.235342</td>\n",
       "      <td>1.816396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.254505</td>\n",
       "      <td>-0.388624</td>\n",
       "      <td>-1.439099</td>\n",
       "      <td>-1.023191</td>\n",
       "      <td>-0.558664</td>\n",
       "      <td>0.066515</td>\n",
       "      <td>-1.977710</td>\n",
       "      <td>0.946007</td>\n",
       "      <td>2.504094</td>\n",
       "      <td>2.557890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>66.395028</td>\n",
       "      <td>1.012327</td>\n",
       "      <td>0.655070</td>\n",
       "      <td>1.998164</td>\n",
       "      <td>1.560715</td>\n",
       "      <td>-0.107553</td>\n",
       "      <td>0.780708</td>\n",
       "      <td>-8.826340</td>\n",
       "      <td>5.314287</td>\n",
       "      <td>-1.519507</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557655</td>\n",
       "      <td>0.778760</td>\n",
       "      <td>0.826453</td>\n",
       "      <td>0.455944</td>\n",
       "      <td>0.478625</td>\n",
       "      <td>-0.159230</td>\n",
       "      <td>0.690684</td>\n",
       "      <td>0.564797</td>\n",
       "      <td>1.291818</td>\n",
       "      <td>-4.811362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>63.789920</td>\n",
       "      <td>1.557412</td>\n",
       "      <td>0.350843</td>\n",
       "      <td>-3.720712</td>\n",
       "      <td>0.861119</td>\n",
       "      <td>4.213561</td>\n",
       "      <td>-1.986598</td>\n",
       "      <td>3.937499</td>\n",
       "      <td>0.432176</td>\n",
       "      <td>2.816622</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345017</td>\n",
       "      <td>-1.060358</td>\n",
       "      <td>0.941289</td>\n",
       "      <td>0.262022</td>\n",
       "      <td>0.149665</td>\n",
       "      <td>-0.029093</td>\n",
       "      <td>0.849948</td>\n",
       "      <td>1.470179</td>\n",
       "      <td>-2.782702</td>\n",
       "      <td>2.213420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>63.859950</td>\n",
       "      <td>-0.453555</td>\n",
       "      <td>1.485339</td>\n",
       "      <td>3.594251</td>\n",
       "      <td>-2.118838</td>\n",
       "      <td>2.263104</td>\n",
       "      <td>1.743785</td>\n",
       "      <td>-1.708345</td>\n",
       "      <td>-0.061421</td>\n",
       "      <td>-0.992407</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.758752</td>\n",
       "      <td>1.122955</td>\n",
       "      <td>1.642286</td>\n",
       "      <td>0.629607</td>\n",
       "      <td>0.476044</td>\n",
       "      <td>-0.064976</td>\n",
       "      <td>1.560303</td>\n",
       "      <td>-1.144215</td>\n",
       "      <td>0.506105</td>\n",
       "      <td>-0.357233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>61.697304</td>\n",
       "      <td>-1.154006</td>\n",
       "      <td>0.388293</td>\n",
       "      <td>1.437706</td>\n",
       "      <td>1.330801</td>\n",
       "      <td>3.033880</td>\n",
       "      <td>0.746384</td>\n",
       "      <td>-2.821548</td>\n",
       "      <td>-0.492762</td>\n",
       "      <td>-2.283554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.646765</td>\n",
       "      <td>0.557696</td>\n",
       "      <td>0.802261</td>\n",
       "      <td>0.337925</td>\n",
       "      <td>0.241433</td>\n",
       "      <td>-0.169342</td>\n",
       "      <td>0.958020</td>\n",
       "      <td>0.710840</td>\n",
       "      <td>1.106255</td>\n",
       "      <td>-1.252241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>67.136595</td>\n",
       "      <td>-1.118702</td>\n",
       "      <td>1.330561</td>\n",
       "      <td>3.842962</td>\n",
       "      <td>2.291499</td>\n",
       "      <td>-15.005947</td>\n",
       "      <td>2.358185</td>\n",
       "      <td>-3.031221</td>\n",
       "      <td>-7.990543</td>\n",
       "      <td>-0.250007</td>\n",
       "      <td>...</td>\n",
       "      <td>1.714581</td>\n",
       "      <td>1.505041</td>\n",
       "      <td>1.740007</td>\n",
       "      <td>0.444166</td>\n",
       "      <td>0.328976</td>\n",
       "      <td>2.438647</td>\n",
       "      <td>1.669779</td>\n",
       "      <td>0.482396</td>\n",
       "      <td>1.379902</td>\n",
       "      <td>-2.950235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>79.975867</td>\n",
       "      <td>1.168884</td>\n",
       "      <td>-1.023580</td>\n",
       "      <td>2.086674</td>\n",
       "      <td>1.109237</td>\n",
       "      <td>2.153528</td>\n",
       "      <td>0.726188</td>\n",
       "      <td>8.546445</td>\n",
       "      <td>0.382361</td>\n",
       "      <td>1.656654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094871</td>\n",
       "      <td>0.200914</td>\n",
       "      <td>-1.415397</td>\n",
       "      <td>-0.739957</td>\n",
       "      <td>-0.262547</td>\n",
       "      <td>0.063330</td>\n",
       "      <td>-2.761260</td>\n",
       "      <td>0.892934</td>\n",
       "      <td>3.095423</td>\n",
       "      <td>2.282624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>61.799687</td>\n",
       "      <td>0.647432</td>\n",
       "      <td>6.413435</td>\n",
       "      <td>0.602209</td>\n",
       "      <td>-2.130189</td>\n",
       "      <td>1.454462</td>\n",
       "      <td>0.484987</td>\n",
       "      <td>2.072111</td>\n",
       "      <td>0.266826</td>\n",
       "      <td>0.865823</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.947959</td>\n",
       "      <td>0.489677</td>\n",
       "      <td>0.388713</td>\n",
       "      <td>0.435858</td>\n",
       "      <td>0.419616</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>1.112549</td>\n",
       "      <td>-0.979275</td>\n",
       "      <td>1.343311</td>\n",
       "      <td>2.971380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>61.609299</td>\n",
       "      <td>1.284370</td>\n",
       "      <td>6.538975</td>\n",
       "      <td>1.185544</td>\n",
       "      <td>-2.265007</td>\n",
       "      <td>1.144782</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.796756</td>\n",
       "      <td>0.248331</td>\n",
       "      <td>2.356234</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.017086</td>\n",
       "      <td>0.617157</td>\n",
       "      <td>1.046739</td>\n",
       "      <td>0.526506</td>\n",
       "      <td>0.421325</td>\n",
       "      <td>0.004299</td>\n",
       "      <td>1.382458</td>\n",
       "      <td>-0.417046</td>\n",
       "      <td>0.172398</td>\n",
       "      <td>2.132565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>60.264098</td>\n",
       "      <td>0.876701</td>\n",
       "      <td>5.956487</td>\n",
       "      <td>-2.879211</td>\n",
       "      <td>0.869427</td>\n",
       "      <td>4.953270</td>\n",
       "      <td>-1.542667</td>\n",
       "      <td>1.734806</td>\n",
       "      <td>0.190690</td>\n",
       "      <td>1.036695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.513104</td>\n",
       "      <td>-0.916531</td>\n",
       "      <td>1.730413</td>\n",
       "      <td>0.499103</td>\n",
       "      <td>0.205805</td>\n",
       "      <td>0.005272</td>\n",
       "      <td>1.541926</td>\n",
       "      <td>0.804420</td>\n",
       "      <td>-3.524984</td>\n",
       "      <td>0.973559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>60.237194</td>\n",
       "      <td>0.904687</td>\n",
       "      <td>4.063510</td>\n",
       "      <td>1.873679</td>\n",
       "      <td>1.416105</td>\n",
       "      <td>1.970374</td>\n",
       "      <td>0.570347</td>\n",
       "      <td>8.215525</td>\n",
       "      <td>0.321236</td>\n",
       "      <td>1.441897</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252153</td>\n",
       "      <td>0.139968</td>\n",
       "      <td>-1.355449</td>\n",
       "      <td>-0.678144</td>\n",
       "      <td>-0.180696</td>\n",
       "      <td>0.057265</td>\n",
       "      <td>-2.630605</td>\n",
       "      <td>1.102513</td>\n",
       "      <td>3.196428</td>\n",
       "      <td>1.864120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>60.506495</td>\n",
       "      <td>0.666420</td>\n",
       "      <td>1.415737</td>\n",
       "      <td>8.755561</td>\n",
       "      <td>2.504421</td>\n",
       "      <td>1.927772</td>\n",
       "      <td>0.391876</td>\n",
       "      <td>3.203974</td>\n",
       "      <td>0.323985</td>\n",
       "      <td>1.107071</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.323956</td>\n",
       "      <td>0.611351</td>\n",
       "      <td>0.599879</td>\n",
       "      <td>0.413875</td>\n",
       "      <td>0.240610</td>\n",
       "      <td>0.010822</td>\n",
       "      <td>1.072239</td>\n",
       "      <td>-1.034575</td>\n",
       "      <td>0.797824</td>\n",
       "      <td>7.623407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>58.436032</td>\n",
       "      <td>-1.607503</td>\n",
       "      <td>1.272197</td>\n",
       "      <td>11.703659</td>\n",
       "      <td>7.766799</td>\n",
       "      <td>-3.596251</td>\n",
       "      <td>2.794854</td>\n",
       "      <td>-11.368122</td>\n",
       "      <td>-0.868887</td>\n",
       "      <td>-3.042385</td>\n",
       "      <td>...</td>\n",
       "      <td>1.829504</td>\n",
       "      <td>1.551225</td>\n",
       "      <td>1.905965</td>\n",
       "      <td>0.662122</td>\n",
       "      <td>0.549528</td>\n",
       "      <td>-0.067942</td>\n",
       "      <td>1.727016</td>\n",
       "      <td>0.810672</td>\n",
       "      <td>1.148838</td>\n",
       "      <td>-1.630530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>68.072812</td>\n",
       "      <td>1.823351</td>\n",
       "      <td>0.806759</td>\n",
       "      <td>-3.390472</td>\n",
       "      <td>0.763652</td>\n",
       "      <td>2.190507</td>\n",
       "      <td>-1.210698</td>\n",
       "      <td>3.554418</td>\n",
       "      <td>0.256103</td>\n",
       "      <td>3.498835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.767256</td>\n",
       "      <td>-1.672344</td>\n",
       "      <td>1.128526</td>\n",
       "      <td>-0.209471</td>\n",
       "      <td>-0.353102</td>\n",
       "      <td>0.018653</td>\n",
       "      <td>1.038476</td>\n",
       "      <td>1.566222</td>\n",
       "      <td>-1.780344</td>\n",
       "      <td>2.414779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>66.874399</td>\n",
       "      <td>1.946124</td>\n",
       "      <td>-0.085228</td>\n",
       "      <td>-3.753682</td>\n",
       "      <td>0.219795</td>\n",
       "      <td>4.249494</td>\n",
       "      <td>-1.990439</td>\n",
       "      <td>5.089087</td>\n",
       "      <td>0.371998</td>\n",
       "      <td>3.169879</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.361079</td>\n",
       "      <td>-1.031558</td>\n",
       "      <td>0.013500</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.053070</td>\n",
       "      <td>0.028336</td>\n",
       "      <td>0.333219</td>\n",
       "      <td>1.757802</td>\n",
       "      <td>-0.999262</td>\n",
       "      <td>2.686081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>60.346196</td>\n",
       "      <td>1.168443</td>\n",
       "      <td>0.734426</td>\n",
       "      <td>-0.814831</td>\n",
       "      <td>-1.841957</td>\n",
       "      <td>1.981129</td>\n",
       "      <td>4.137124</td>\n",
       "      <td>18.973026</td>\n",
       "      <td>1.503330</td>\n",
       "      <td>1.821369</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>3.084201</td>\n",
       "      <td>-0.483713</td>\n",
       "      <td>0.120954</td>\n",
       "      <td>0.335526</td>\n",
       "      <td>0.028457</td>\n",
       "      <td>0.153316</td>\n",
       "      <td>-0.799507</td>\n",
       "      <td>2.093355</td>\n",
       "      <td>4.858240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>59.950389</td>\n",
       "      <td>1.358618</td>\n",
       "      <td>0.699906</td>\n",
       "      <td>-5.247210</td>\n",
       "      <td>0.725831</td>\n",
       "      <td>2.669638</td>\n",
       "      <td>2.235924</td>\n",
       "      <td>19.567199</td>\n",
       "      <td>1.587675</td>\n",
       "      <td>2.256671</td>\n",
       "      <td>...</td>\n",
       "      <td>3.761833</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.403076</td>\n",
       "      <td>-0.274871</td>\n",
       "      <td>-0.375332</td>\n",
       "      <td>0.044664</td>\n",
       "      <td>0.753435</td>\n",
       "      <td>0.804621</td>\n",
       "      <td>-0.832677</td>\n",
       "      <td>4.229861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>63.038379</td>\n",
       "      <td>0.869240</td>\n",
       "      <td>-1.404645</td>\n",
       "      <td>0.922745</td>\n",
       "      <td>0.884984</td>\n",
       "      <td>1.959189</td>\n",
       "      <td>0.054995</td>\n",
       "      <td>6.188639</td>\n",
       "      <td>0.335077</td>\n",
       "      <td>1.618132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296558</td>\n",
       "      <td>0.202606</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-0.584631</td>\n",
       "      <td>-0.355646</td>\n",
       "      <td>0.043352</td>\n",
       "      <td>-2.307994</td>\n",
       "      <td>0.901696</td>\n",
       "      <td>2.836361</td>\n",
       "      <td>2.115903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>64.759849</td>\n",
       "      <td>1.072703</td>\n",
       "      <td>-1.214811</td>\n",
       "      <td>-0.378549</td>\n",
       "      <td>1.304366</td>\n",
       "      <td>2.244621</td>\n",
       "      <td>0.194562</td>\n",
       "      <td>5.978327</td>\n",
       "      <td>0.374557</td>\n",
       "      <td>1.583908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221740</td>\n",
       "      <td>-0.413138</td>\n",
       "      <td>-1.748166</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>6.405689</td>\n",
       "      <td>0.630852</td>\n",
       "      <td>-1.694380</td>\n",
       "      <td>0.870250</td>\n",
       "      <td>2.834361</td>\n",
       "      <td>2.642501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>63.188048</td>\n",
       "      <td>1.267021</td>\n",
       "      <td>-0.896809</td>\n",
       "      <td>-1.020009</td>\n",
       "      <td>1.257204</td>\n",
       "      <td>2.079919</td>\n",
       "      <td>0.406818</td>\n",
       "      <td>3.665531</td>\n",
       "      <td>0.365025</td>\n",
       "      <td>1.794117</td>\n",
       "      <td>...</td>\n",
       "      <td>0.816451</td>\n",
       "      <td>-0.748794</td>\n",
       "      <td>-1.411562</td>\n",
       "      <td>8.502502</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.615221</td>\n",
       "      <td>-0.691136</td>\n",
       "      <td>0.887230</td>\n",
       "      <td>2.074443</td>\n",
       "      <td>3.047953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>53.486828</td>\n",
       "      <td>-1.908805</td>\n",
       "      <td>1.260072</td>\n",
       "      <td>2.679559</td>\n",
       "      <td>0.225158</td>\n",
       "      <td>3.236129</td>\n",
       "      <td>1.360223</td>\n",
       "      <td>-7.895229</td>\n",
       "      <td>0.270641</td>\n",
       "      <td>-1.527603</td>\n",
       "      <td>...</td>\n",
       "      <td>0.808692</td>\n",
       "      <td>1.040626</td>\n",
       "      <td>2.009466</td>\n",
       "      <td>9.779153</td>\n",
       "      <td>7.184954</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.927028</td>\n",
       "      <td>-0.094525</td>\n",
       "      <td>-0.271655</td>\n",
       "      <td>-3.376743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>76.536614</td>\n",
       "      <td>1.028766</td>\n",
       "      <td>-1.354368</td>\n",
       "      <td>0.941223</td>\n",
       "      <td>1.680508</td>\n",
       "      <td>2.289429</td>\n",
       "      <td>0.156607</td>\n",
       "      <td>6.901941</td>\n",
       "      <td>0.385792</td>\n",
       "      <td>1.694395</td>\n",
       "      <td>...</td>\n",
       "      <td>0.114838</td>\n",
       "      <td>0.462690</td>\n",
       "      <td>-2.819768</td>\n",
       "      <td>-0.692291</td>\n",
       "      <td>-0.212746</td>\n",
       "      <td>0.050792</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.989939</td>\n",
       "      <td>2.905899</td>\n",
       "      <td>2.622612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>60.601307</td>\n",
       "      <td>0.777279</td>\n",
       "      <td>1.324888</td>\n",
       "      <td>2.762957</td>\n",
       "      <td>-2.379232</td>\n",
       "      <td>1.828830</td>\n",
       "      <td>1.391622</td>\n",
       "      <td>-0.767053</td>\n",
       "      <td>0.036978</td>\n",
       "      <td>1.034334</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.998710</td>\n",
       "      <td>0.824047</td>\n",
       "      <td>1.837195</td>\n",
       "      <td>0.592978</td>\n",
       "      <td>0.455459</td>\n",
       "      <td>-0.004155</td>\n",
       "      <td>1.650916</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>8.112358</td>\n",
       "      <td>0.027043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>62.148757</td>\n",
       "      <td>0.247451</td>\n",
       "      <td>1.899406</td>\n",
       "      <td>-1.111080</td>\n",
       "      <td>1.085065</td>\n",
       "      <td>1.210767</td>\n",
       "      <td>-0.991245</td>\n",
       "      <td>1.018578</td>\n",
       "      <td>0.166733</td>\n",
       "      <td>-0.004560</td>\n",
       "      <td>...</td>\n",
       "      <td>1.137455</td>\n",
       "      <td>-0.370946</td>\n",
       "      <td>2.513799</td>\n",
       "      <td>0.840085</td>\n",
       "      <td>0.463221</td>\n",
       "      <td>-0.005194</td>\n",
       "      <td>2.108001</td>\n",
       "      <td>3.528750</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.545863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>58.232673</td>\n",
       "      <td>-2.044599</td>\n",
       "      <td>1.970411</td>\n",
       "      <td>11.657314</td>\n",
       "      <td>8.070207</td>\n",
       "      <td>-2.526907</td>\n",
       "      <td>3.128503</td>\n",
       "      <td>-13.770272</td>\n",
       "      <td>-0.764098</td>\n",
       "      <td>-3.062448</td>\n",
       "      <td>...</td>\n",
       "      <td>2.749306</td>\n",
       "      <td>1.962516</td>\n",
       "      <td>1.953070</td>\n",
       "      <td>0.815711</td>\n",
       "      <td>0.708840</td>\n",
       "      <td>-0.067243</td>\n",
       "      <td>1.981423</td>\n",
       "      <td>0.012251</td>\n",
       "      <td>0.568508</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Diversion ratio wrt. product          0           1           2           3   \\\n",
       "Diversion ratio of product                                                     \n",
       "0                            -100.000000    1.307574    2.723282    4.688007   \n",
       "1                              59.753702 -100.000000    1.407780    3.971815   \n",
       "2                              61.852762    0.699685 -100.000000   -0.230059   \n",
       "3                              60.612358    1.123734   -0.130962 -100.000000   \n",
       "4                              60.584936    0.702675    1.198474    8.614700   \n",
       "5                              68.492913   -0.721817    1.145596    3.313780   \n",
       "6                              60.347764    1.261240    0.182150   -5.573290   \n",
       "7                              61.013306   -2.139479    1.449071    6.196582   \n",
       "8                              60.880222   -2.278189    1.268219    3.786182   \n",
       "9                              65.844556   -1.401242    1.222436    4.473787   \n",
       "10                             67.209822    0.684456    1.085990   -4.196717   \n",
       "11                             63.122804    0.245930    1.756871   -1.414565   \n",
       "12                             63.751945    0.421170   -0.305782    2.202055   \n",
       "13                             61.941815    1.045372   -0.104545    0.903703   \n",
       "14                             77.591907   -0.752650    1.266537    3.656234   \n",
       "15                             65.112260    0.858738   -1.205096    1.743319   \n",
       "16                             78.314255    2.359145   -0.097846   -4.022942   \n",
       "17                             60.130527    3.765556    1.326423   -4.049923   \n",
       "18                             60.180238    3.396359    2.109711    3.220067   \n",
       "19                             61.296852    4.372408   -1.934338    0.679340   \n",
       "20                             59.823880    4.441284   -0.861434   -0.298303   \n",
       "21                             66.395028    1.012327    0.655070    1.998164   \n",
       "22                             63.789920    1.557412    0.350843   -3.720712   \n",
       "23                             63.859950   -0.453555    1.485339    3.594251   \n",
       "24                             61.697304   -1.154006    0.388293    1.437706   \n",
       "25                             67.136595   -1.118702    1.330561    3.842962   \n",
       "26                             79.975867    1.168884   -1.023580    2.086674   \n",
       "27                             61.799687    0.647432    6.413435    0.602209   \n",
       "28                             61.609299    1.284370    6.538975    1.185544   \n",
       "29                             60.264098    0.876701    5.956487   -2.879211   \n",
       "30                             60.237194    0.904687    4.063510    1.873679   \n",
       "31                             60.506495    0.666420    1.415737    8.755561   \n",
       "32                             58.436032   -1.607503    1.272197   11.703659   \n",
       "33                             68.072812    1.823351    0.806759   -3.390472   \n",
       "34                             66.874399    1.946124   -0.085228   -3.753682   \n",
       "35                             60.346196    1.168443    0.734426   -0.814831   \n",
       "36                             59.950389    1.358618    0.699906   -5.247210   \n",
       "37                             63.038379    0.869240   -1.404645    0.922745   \n",
       "38                             64.759849    1.072703   -1.214811   -0.378549   \n",
       "39                             63.188048    1.267021   -0.896809   -1.020009   \n",
       "40                             53.486828   -1.908805    1.260072    2.679559   \n",
       "41                             76.536614    1.028766   -1.354368    0.941223   \n",
       "42                             60.601307    0.777279    1.324888    2.762957   \n",
       "43                             62.148757    0.247451    1.899406   -1.111080   \n",
       "44                             58.232673   -2.044599    1.970411   11.657314   \n",
       "\n",
       "Diversion ratio wrt. product          4           5           6           7   \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               3.330634    3.978255    2.418855    7.114428   \n",
       "1                               1.765288   -1.915900    2.310176  -11.400449   \n",
       "2                               1.496433    1.511280    0.165823    3.837705   \n",
       "3                               6.123156    2.488534   -2.888239    9.342014   \n",
       "4                            -100.000000    1.919672    0.835956    3.463076   \n",
       "5                               1.816947 -100.000000    1.730338   -3.226153   \n",
       "6                               1.146561    2.507432 -100.000000   22.093681   \n",
       "7                               1.632709   -1.607002    7.594537 -100.000000   \n",
       "8                               1.785708   -2.590289    6.511305    3.935118   \n",
       "9                               1.157347   -0.840381    2.438539   -9.390706   \n",
       "10                              1.773312    1.565510   -1.981098    6.108854   \n",
       "11                              1.113438    1.391678   -0.487103   -0.115179   \n",
       "12                              0.658817    0.892046    1.225229    1.796113   \n",
       "13                              1.273015    2.189393   -0.152577    8.184640   \n",
       "14                              1.049076    0.931345    2.085216   -7.872016   \n",
       "15                              0.977933    1.885918    0.423566    5.940557   \n",
       "16                              0.698778    3.064278   -2.132677    5.053614   \n",
       "17                              1.947161    1.393137   -1.775899    5.134715   \n",
       "18                             -2.083402    0.798514    1.710408   -1.191866   \n",
       "19                              0.621235    2.298105    0.395208    7.976903   \n",
       "20                              1.183777    2.377157    0.371844    8.741505   \n",
       "21                              1.560715   -0.107553    0.780708   -8.826340   \n",
       "22                              0.861119    4.213561   -1.986598    3.937499   \n",
       "23                             -2.118838    2.263104    1.743785   -1.708345   \n",
       "24                              1.330801    3.033880    0.746384   -2.821548   \n",
       "25                              2.291499  -15.005947    2.358185   -3.031221   \n",
       "26                              1.109237    2.153528    0.726188    8.546445   \n",
       "27                             -2.130189    1.454462    0.484987    2.072111   \n",
       "28                             -2.265007    1.144782    0.614441    0.796756   \n",
       "29                              0.869427    4.953270   -1.542667    1.734806   \n",
       "30                              1.416105    1.970374    0.570347    8.215525   \n",
       "31                              2.504421    1.927772    0.391876    3.203974   \n",
       "32                              7.766799   -3.596251    2.794854  -11.368122   \n",
       "33                              0.763652    2.190507   -1.210698    3.554418   \n",
       "34                              0.219795    4.249494   -1.990439    5.089087   \n",
       "35                             -1.841957    1.981129    4.137124   18.973026   \n",
       "36                              0.725831    2.669638    2.235924   19.567199   \n",
       "37                              0.884984    1.959189    0.054995    6.188639   \n",
       "38                              1.304366    2.244621    0.194562    5.978327   \n",
       "39                              1.257204    2.079919    0.406818    3.665531   \n",
       "40                              0.225158    3.236129    1.360223   -7.895229   \n",
       "41                              1.680508    2.289429    0.156607    6.901941   \n",
       "42                             -2.379232    1.828830    1.391622   -0.767053   \n",
       "43                              1.085065    1.210767   -0.991245    1.018578   \n",
       "44                              8.070207   -2.526907    3.128503  -13.770272   \n",
       "\n",
       "Diversion ratio wrt. product          8           9   ...          35  \\\n",
       "Diversion ratio of product                            ...               \n",
       "0                               0.585777    2.471648  ...    1.806840   \n",
       "1                              -1.001716   -2.403690  ...    1.598733   \n",
       "2                               0.277151    1.042219  ...    0.499441   \n",
       "3                               0.471011    2.171276  ...   -0.315435   \n",
       "4                               0.312539    0.790256  ...   -1.003199   \n",
       "5                              -0.429099   -0.543120  ...    1.021258   \n",
       "6                               1.563061    2.283746  ...    3.090436   \n",
       "7                               0.324712   -3.023080  ...    4.871817   \n",
       "8                            -100.000000   -2.013101  ...    4.678081   \n",
       "9                              -0.516007 -100.000000  ...    1.452784   \n",
       "10                              0.375374   13.875570  ...    0.514499   \n",
       "11                              0.175101    0.148452  ...    1.396441   \n",
       "12                              0.143316    0.176375  ...    0.496377   \n",
       "13                              0.357958    1.528222  ...    0.292988   \n",
       "14                             -0.480964   -1.899089  ...    1.323684   \n",
       "15                              0.279306    1.093897  ...   -0.099025   \n",
       "16                              0.404751    4.188492  ...    0.012547   \n",
       "17                              0.152197    1.513511  ...    0.474430   \n",
       "18                             -0.024990   -0.335746  ...   -0.603257   \n",
       "19                              0.267242    1.783708  ...   -0.242225   \n",
       "20                              0.235342    1.816396  ...    0.254505   \n",
       "21                              5.314287   -1.519507  ...    0.557655   \n",
       "22                              0.432176    2.816622  ...    0.345017   \n",
       "23                             -0.061421   -0.992407  ...   -0.758752   \n",
       "24                             -0.492762   -2.283554  ...    0.646765   \n",
       "25                             -7.990543   -0.250007  ...    1.714581   \n",
       "26                              0.382361    1.656654  ...    0.094871   \n",
       "27                              0.266826    0.865823  ...   -0.947959   \n",
       "28                              0.248331    2.356234  ...   -1.017086   \n",
       "29                              0.190690    1.036695  ...    0.513104   \n",
       "30                              0.321236    1.441897  ...    0.252153   \n",
       "31                              0.323985    1.107071  ...   -1.323956   \n",
       "32                             -0.868887   -3.042385  ...    1.829504   \n",
       "33                              0.256103    3.498835  ...    0.767256   \n",
       "34                              0.371998    3.169879  ...   -0.361079   \n",
       "35                              1.503330    1.821369  ... -100.000000   \n",
       "36                              1.587675    2.256671  ...    3.761833   \n",
       "37                              0.335077    1.618132  ...   -0.296558   \n",
       "38                              0.374557    1.583908  ...    0.221740   \n",
       "39                              0.365025    1.794117  ...    0.816451   \n",
       "40                              0.270641   -1.527603  ...    0.808692   \n",
       "41                              0.385792    1.694395  ...    0.114838   \n",
       "42                              0.036978    1.034334  ...   -0.998710   \n",
       "43                              0.166733   -0.004560  ...    1.137455   \n",
       "44                             -0.764098   -3.062448  ...    2.749306   \n",
       "\n",
       "Diversion ratio wrt. product          36          37          38          39  \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               1.471651    3.078596    1.057676    0.777501   \n",
       "1                               1.524083    1.939929    0.800616    0.712439   \n",
       "2                               0.390229   -1.558047   -0.450632   -0.250630   \n",
       "3                              -1.665383    0.582643   -0.079936   -0.162272   \n",
       "4                               0.324105    0.786177    0.387511    0.281390   \n",
       "5                               1.128283    1.647316    0.631165    0.440621   \n",
       "6                               1.369371    0.067007    0.079279    0.124887   \n",
       "7                               4.119325    2.591954    0.837357    0.386801   \n",
       "8                               4.050586    1.700734    0.635783    0.466801   \n",
       "9                               1.475755    2.105206    0.689144    0.588098   \n",
       "10                             -0.641121    0.504904    0.369225    0.298178   \n",
       "11                             -0.334963   10.094989    0.822013    0.491928   \n",
       "12                              0.812443    7.613177    0.143395   -0.054341   \n",
       "13                             -0.258691   -1.066396    9.023778    6.600394   \n",
       "14                              1.179444    2.094613    0.730329    0.596893   \n",
       "15                              0.184194   -0.491141   -0.380912   -0.448276   \n",
       "16                             -1.153888    0.412969    0.125584    0.092933   \n",
       "17                             -0.514001   -0.564358    0.281373    0.276885   \n",
       "18                              1.138482    2.272206    0.983532    0.582591   \n",
       "19                             -0.714840   -2.383924   -1.201805   -0.785907   \n",
       "20                             -0.388624   -1.439099   -1.023191   -0.558664   \n",
       "21                              0.778760    0.826453    0.455944    0.478625   \n",
       "22                             -1.060358    0.941289    0.262022    0.149665   \n",
       "23                              1.122955    1.642286    0.629607    0.476044   \n",
       "24                              0.557696    0.802261    0.337925    0.241433   \n",
       "25                              1.505041    1.740007    0.444166    0.328976   \n",
       "26                              0.200914   -1.415397   -0.739957   -0.262547   \n",
       "27                              0.489677    0.388713    0.435858    0.419616   \n",
       "28                              0.617157    1.046739    0.526506    0.421325   \n",
       "29                             -0.916531    1.730413    0.499103    0.205805   \n",
       "30                              0.139968   -1.355449   -0.678144   -0.180696   \n",
       "31                              0.611351    0.599879    0.413875    0.240610   \n",
       "32                              1.551225    1.905965    0.662122    0.549528   \n",
       "33                             -1.672344    1.128526   -0.209471   -0.353102   \n",
       "34                             -1.031558    0.013500    0.068315    0.053070   \n",
       "35                              3.084201   -0.483713    0.120954    0.335526   \n",
       "36                           -100.000000    0.403076   -0.274871   -0.375332   \n",
       "37                              0.202606 -100.000000   -0.584631   -0.355646   \n",
       "38                             -0.413138   -1.748166 -100.000000    6.405689   \n",
       "39                             -0.748794   -1.411562    8.502502 -100.000000   \n",
       "40                              1.040626    2.009466    9.779153    7.184954   \n",
       "41                              0.462690   -2.819768   -0.692291   -0.212746   \n",
       "42                              0.824047    1.837195    0.592978    0.455459   \n",
       "43                             -0.370946    2.513799    0.840085    0.463221   \n",
       "44                              1.962516    1.953070    0.815711    0.708840   \n",
       "\n",
       "Diversion ratio wrt. product          40          41          42          43  \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               0.056353    3.059413    1.452561    3.424609   \n",
       "1                              -0.091904    1.879247    0.851388    0.623112   \n",
       "2                               0.030153   -1.229623    0.721270    2.377183   \n",
       "3                               0.036501    0.486446    0.856248   -0.791584   \n",
       "4                               0.004315    1.221932   -1.037354    1.087608   \n",
       "5                               0.058702    1.575611    0.754708    1.148661   \n",
       "6                               0.035755    0.156182    0.832195   -1.362733   \n",
       "7                              -0.071338    2.366054   -0.157675    0.481346   \n",
       "8                               0.029635    1.602749    0.092118    0.954872   \n",
       "9                              -0.042876    1.804333    0.660459   -0.006694   \n",
       "10                              0.022914    1.011457    0.465532   -0.888198   \n",
       "11                             -0.005738    2.223618    0.023664   -2.951181   \n",
       "12                              0.009309    0.183309    0.095478    1.576495   \n",
       "13                              0.629833   -2.101337    0.881681    1.820387   \n",
       "14                             -0.080382    1.885045    0.712292   -0.295886   \n",
       "15                              0.041940   -0.129267    4.207322   11.294398   \n",
       "16                              0.030558    0.707315    2.221386   -1.724811   \n",
       "17                              0.025600    0.059889    0.645333   -0.193806   \n",
       "18                             -0.007761    2.404876   -1.889455   -0.235847   \n",
       "19                              0.064784   -2.717111    0.881851    3.377378   \n",
       "20                              0.066515   -1.977710    0.946007    2.504094   \n",
       "21                             -0.159230    0.690684    0.564797    1.291818   \n",
       "22                             -0.029093    0.849948    1.470179   -2.782702   \n",
       "23                             -0.064976    1.560303   -1.144215    0.506105   \n",
       "24                             -0.169342    0.958020    0.710840    1.106255   \n",
       "25                              2.438647    1.669779    0.482396    1.379902   \n",
       "26                              0.063330   -2.761260    0.892934    3.095423   \n",
       "27                              0.002839    1.112549   -0.979275    1.343311   \n",
       "28                              0.004299    1.382458   -0.417046    0.172398   \n",
       "29                              0.005272    1.541926    0.804420   -3.524984   \n",
       "30                              0.057265   -2.630605    1.102513    3.196428   \n",
       "31                              0.010822    1.072239   -1.034575    0.797824   \n",
       "32                             -0.067942    1.727016    0.810672    1.148838   \n",
       "33                              0.018653    1.038476    1.566222   -1.780344   \n",
       "34                              0.028336    0.333219    1.757802   -0.999262   \n",
       "35                              0.028457    0.153316   -0.799507    2.093355   \n",
       "36                              0.044664    0.753435    0.804621   -0.832677   \n",
       "37                              0.043352   -2.307994    0.901696    2.836361   \n",
       "38                              0.630852   -1.694380    0.870250    2.834361   \n",
       "39                              0.615221   -0.691136    0.887230    2.074443   \n",
       "40                           -100.000000    1.927028   -0.094525   -0.271655   \n",
       "41                              0.050792 -100.000000    0.989939    2.905899   \n",
       "42                             -0.004155    1.650916 -100.000000    8.112358   \n",
       "43                             -0.005194    2.108001    3.528750 -100.000000   \n",
       "44                             -0.067243    1.981423    0.012251    0.568508   \n",
       "\n",
       "Diversion ratio wrt. product          44  \n",
       "Diversion ratio of product                \n",
       "0                               3.081005  \n",
       "1                              -4.943474  \n",
       "2                               2.367821  \n",
       "3                               7.974383  \n",
       "4                               7.766907  \n",
       "5                              -2.301801  \n",
       "6                               4.129654  \n",
       "7                              -6.248169  \n",
       "8                              -4.201635  \n",
       "9                              -4.316460  \n",
       "10                              4.348960  \n",
       "11                              1.042815  \n",
       "12                              1.730673  \n",
       "13                              2.103009  \n",
       "14                             -3.056907  \n",
       "15                              0.841841  \n",
       "16                              2.943277  \n",
       "17                              3.638627  \n",
       "18                              1.155558  \n",
       "19                              2.140506  \n",
       "20                              2.557890  \n",
       "21                             -4.811362  \n",
       "22                              2.213420  \n",
       "23                             -0.357233  \n",
       "24                             -1.252241  \n",
       "25                             -2.950235  \n",
       "26                              2.282624  \n",
       "27                              2.971380  \n",
       "28                              2.132565  \n",
       "29                              0.973559  \n",
       "30                              1.864120  \n",
       "31                              7.623407  \n",
       "32                             -1.630530  \n",
       "33                              2.414779  \n",
       "34                              2.686081  \n",
       "35                              4.858240  \n",
       "36                              4.229861  \n",
       "37                              2.115903  \n",
       "38                              2.642501  \n",
       "39                              3.047953  \n",
       "40                             -3.376743  \n",
       "41                              2.622612  \n",
       "42                              0.027043  \n",
       "43                              0.545863  \n",
       "44                           -100.000000  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DR_hat = Similarity_diversion_ratio(qOpt, z_logit, ThetaOptBLP, Psi)\n",
    "pd.DataFrame(DR_hat[0]).rename_axis(index = 'Diversion ratio of product', columns = 'Diversion ratio wrt. product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticities and Diversion Ratios visualization\n",
    "\n",
    "Finally we advance our exposition towards visualing the Similarity semi-elasticities $\\mathcal{E}$ and diversion ratios $\\mathcal{D}$ compared to those implied by a multinomial Logit model. Since the number of products varies across markets $t$ we aggregate our results according to the categorical variable `cla` describing the class or segment of each vehicle $j$. This variable takes values 'subcompact', 'compact', 'intermediate', 'standard', and 'luxury' encoded as the integers $0,1,\\ldots, 5$ in our dataset. To this end we consider the 'pooled' elasticities and diversion ratios calculated using the directional derivative $\\frac{\\partial q_c}{\\partial u_{\\ell}} = \\sum_{j: x_{j,\\text{cla}} = c} \\sum_{k: x_{k,\\text{cla}} = \\ell} \\frac{\\partial q_j}{\\partial u_k}$ of class $c$ wrt. the utility of class $\\ell$, where $q_c = \\sum_{j: x_{j,\\text{cla}} = c} q_j$ denotes the within-group choice proabbilitity of choosing a car of class $c$. \n",
    "\n",
    "The pooled choice probability semi-elasticity $\\mathcal{E}_{c\\ell}$ of class $c$ wrt. the prices of cars of class $\\ell$ can then be computed as $\\mathcal{E}_{c\\ell} = \\frac{\\partial q_c}{\\partial u_{\\ell}}\\frac{1}{q_c}\\theta^{\\text{price}}$. \n",
    "\n",
    "Similarly, we may compute the pooled diversion ratio $\\mathcal{D}_{c\\ell}$ following a unit increase in the price of cars of class $\\ell$ from class $\\ell$ to cars of class $c$ as $\\mathcal{D}_{c\\ell} = -100\\cdot\\frac{\\partial q_c / \\partial u_{\\ell}}{\\partial q_{\\ell} / \\partial u_{\\ell}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_directionalgrad(data, Theta, q, x, psi, direction_var, market_id = 'market', product_id = 'co', model = 'Similarity', outside_option = True):\n",
    "    '''\n",
    "    '''\n",
    "    T = len(q)\n",
    "    J = np.array([q[t].shape[0] for t in np.arange(T)])\n",
    "    data = data.sort_values([market_id, product_id])\n",
    "\n",
    "    vec = {}\n",
    "    q_agg = {}\n",
    "    dq_du_agg = {}\n",
    "\n",
    "    if model == 'Similarity':\n",
    "        Grad = ccp_gradient(q, x, Theta, psi)\n",
    "    else:\n",
    "        Grad = {t: (np.diag(q[t]) - q[t][:,None]*q[t][None,:]) for t in np.arange(T)} \n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        G_t = data[data[market_id] == t][direction_var].nunique()\n",
    "        vec[t] = pd.get_dummies(data[data[market_id] == t][direction_var], columns = direction_var).values.reshape((J[t], G_t)).transpose()\n",
    "        \n",
    "        # Calculate the sum of within-group probabilities\n",
    "        q_agg[t] = vec[t]@q[t]\n",
    "\n",
    "        # Calculate directional derivatives\n",
    "        dq_du_agg[t] = np.einsum('cj,jk,lk->cl', vec[t], Grad[t], vec[t])\n",
    "    \n",
    "    return q_agg, dq_du_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elasticity_agg(data, Theta, q, x, psi, direction_var, market_id = 'market', product_id = 'co', char_number = K-1, model = 'Similarity', outside_option = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    q_agg, dq_du_agg = ccp_directionalgrad(data, Theta, q, x, psi, direction_var, market_id, product_id, model, outside_option)\n",
    "    E_agg = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        E_agg[t] = Theta[char_number]*np.einsum('cl,c->cl', dq_du_agg[t], 1./q_agg[t])\n",
    "\n",
    "    return E_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_Similarityagg = Elasticity_agg(dat, ThetaOptBLP, qOpt, z_logit, Psi, 'cla', char_number = pr_index)\n",
    "E_Logitagg = Elasticity_agg(dat, LogitBLP_beta, logit.logit_ccp(LogitBLP_beta, z_logit), z_logit, Psi, 'cla', char_number = pr_index, model = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_agg = E_Similarityagg[0].shape[0]\n",
    "\n",
    "E0, E1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    E0[t,:,:] = E_Logitagg[t]\n",
    "    E1[t,:,:] = E_Similarityagg[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot histograms of our results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E0p = {j : (E0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E0p[j], num_bins, range = (np.quantile(E0p[j], 0.10), np.quantile(E0p[j], 0.90)), color = 'r', alpha = 1) # Logit is blue\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E1p = {j : (E1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E1p[j], num_bins, range = (np.quantile(E1p[j], 0.10), np.quantile(E1p[j], 0.90)), color = 'b', alpha = 1) # Similarity is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Similarity price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mean elasticities for the logit model are given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean elasticity wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean elasticity of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>-0.172311</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>-0.172365</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>-0.172652</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>-0.173053</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>-0.173175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean elasticity wrt. class         0         1         2         3         4  \\\n",
       "Mean elasticity of class                                                       \n",
       "0                          -0.003261  0.001052  0.000998  0.000711  0.000311   \n",
       "1                           0.170102 -0.172311  0.000998  0.000711  0.000311   \n",
       "2                           0.170102  0.001052 -0.172365  0.000711  0.000311   \n",
       "3                           0.170102  0.001052  0.000998 -0.172652  0.000311   \n",
       "4                           0.170102  0.001052  0.000998  0.000711 -0.173053   \n",
       "5                           0.170102  0.001052  0.000998  0.000711  0.000311   \n",
       "\n",
       "Mean elasticity wrt. class         5  \n",
       "Mean elasticity of class              \n",
       "0                           0.000189  \n",
       "1                           0.000189  \n",
       "2                           0.000189  \n",
       "3                           0.000189  \n",
       "4                           0.000189  \n",
       "5                          -0.173175  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(E0.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. class', index = 'Mean elasticity of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Similarity the mean elasticities are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean elasticity wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean elasticity of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003379</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000324</td>\n",
       "      <td>0.000204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153150</td>\n",
       "      <td>-0.276852</td>\n",
       "      <td>0.057406</td>\n",
       "      <td>0.040848</td>\n",
       "      <td>0.017559</td>\n",
       "      <td>0.007890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.149749</td>\n",
       "      <td>0.062559</td>\n",
       "      <td>-0.261905</td>\n",
       "      <td>0.029687</td>\n",
       "      <td>0.015086</td>\n",
       "      <td>0.004824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.149443</td>\n",
       "      <td>0.064762</td>\n",
       "      <td>0.040639</td>\n",
       "      <td>-0.269544</td>\n",
       "      <td>0.012626</td>\n",
       "      <td>0.002074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.149392</td>\n",
       "      <td>0.063261</td>\n",
       "      <td>0.043447</td>\n",
       "      <td>0.026326</td>\n",
       "      <td>-0.288088</td>\n",
       "      <td>0.005663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.149614</td>\n",
       "      <td>0.047400</td>\n",
       "      <td>0.029296</td>\n",
       "      <td>0.009562</td>\n",
       "      <td>0.007964</td>\n",
       "      <td>-0.243836</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean elasticity wrt. class         0         1         2         3         4  \\\n",
       "Mean elasticity of class                                                       \n",
       "0                          -0.003379  0.001125  0.001002  0.000725  0.000324   \n",
       "1                           0.153150 -0.276852  0.057406  0.040848  0.017559   \n",
       "2                           0.149749  0.062559 -0.261905  0.029687  0.015086   \n",
       "3                           0.149443  0.064762  0.040639 -0.269544  0.012626   \n",
       "4                           0.149392  0.063261  0.043447  0.026326 -0.288088   \n",
       "5                           0.149614  0.047400  0.029296  0.009562  0.007964   \n",
       "\n",
       "Mean elasticity wrt. class         5  \n",
       "Mean elasticity of class              \n",
       "0                           0.000204  \n",
       "1                           0.007890  \n",
       "2                           0.004824  \n",
       "3                           0.002074  \n",
       "4                           0.005663  \n",
       "5                          -0.243836  "
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(E1.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. class', index = 'Mean elasticity of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios\n",
    "\n",
    "We now visualize the implied diversion ratios $\\mathcal{D}$. If $\\bar D_{c\\ell}$ denotes the sum of choice probability weigthed diversion ratios, then we have as above that $\\bar D_{c\\ell} = \\sum_{j}\\sum_{k} \\mathrm{1}_{\\{j\\in c\\}} \\mathrm{1}_{\\{k\\in \\ell\\}} q_j q_k \\mathcal{D}_{jk}$ i.e. more generally $\\bar D = (\\psi^{\\text{class}} \\circ q) \\mathcal{D} (\\psi^{\\text{class}} \\circ q).'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiversionRatio_agg(data, Theta, q, x, psi, direction_var, market_id = 'market', product_id = 'co', char_number = K-1, model = 'Similarity', outside_option = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    dq_du_agg = ccp_directionalgrad(data, Theta, q, x, psi, direction_var, market_id, product_id, model, outside_option)[1]\n",
    "    D_agg = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        D_agg[t] = -100*np.einsum('cl,c->cl', dq_du_agg[t], 1./np.diag(dq_du_agg[t]))\n",
    "\n",
    "    return D_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_Similarityagg = DiversionRatio_agg(dat, ThetaOptBLP, qOpt, z_logit, Psi, 'cla', char_number = pr_index)\n",
    "D_Logitagg = DiversionRatio_agg(dat, LogitBLP_beta, logit.logit_ccp(LogitBLP_beta, z_logit), z_logit, Psi, 'cla', char_number = pr_index, model = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0, D1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    D0[t,:,:] = D_Logitagg[t]\n",
    "    D1[t,:,:] = D_Similarityagg[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D0p = {j : (D0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D0p[j], num_bins, range = (np.quantile(D0p[j], 0.10), np.quantile(D0p[j], 0.90)), color = 'r', alpha = 1) # Logit is red\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit diversion ratios by class')\n",
    "fig.supxlabel('Diversion ratio from class')\n",
    "fig.supylabel('Diversion ratio to class')\n",
    "\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D1p = {j : (D1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg, sharex=False, sharey=False)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D1p[j], num_bins, range = (np.quantile(D1p[j], 0.10), np.quantile(D1p[j], 0.90)), color = 'b', alpha = 1) # Similarity is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Similarity diversion ratios by class')\n",
    "fig.supxlabel('Diversion ratio from class')\n",
    "fig.supylabel('Diversion ratio to class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also calculate the mean diversion ratios within each class. For the Logit model these are given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean diversion ratio wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean diversion ratio of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>32.977441</td>\n",
       "      <td>30.037606</td>\n",
       "      <td>21.805955</td>\n",
       "      <td>9.610813</td>\n",
       "      <td>5.568185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.716873</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.579788</td>\n",
       "      <td>0.413296</td>\n",
       "      <td>0.180329</td>\n",
       "      <td>0.109713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.685594</td>\n",
       "      <td>0.611145</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.413182</td>\n",
       "      <td>0.180340</td>\n",
       "      <td>0.109739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.522081</td>\n",
       "      <td>0.609950</td>\n",
       "      <td>0.578479</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.179993</td>\n",
       "      <td>0.109496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.294337</td>\n",
       "      <td>0.608266</td>\n",
       "      <td>0.576919</td>\n",
       "      <td>0.411271</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.109208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98.225371</td>\n",
       "      <td>0.607820</td>\n",
       "      <td>0.576488</td>\n",
       "      <td>0.410944</td>\n",
       "      <td>0.179377</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean diversion ratio wrt. class           0           1           2  \\\n",
       "Mean diversion ratio of class                                         \n",
       "0                               -100.000000   32.977441   30.037606   \n",
       "1                                 98.716873 -100.000000    0.579788   \n",
       "2                                 98.685594    0.611145 -100.000000   \n",
       "3                                 98.522081    0.609950    0.578479   \n",
       "4                                 98.294337    0.608266    0.576919   \n",
       "5                                 98.225371    0.607820    0.576488   \n",
       "\n",
       "Mean diversion ratio wrt. class           3           4           5  \n",
       "Mean diversion ratio of class                                        \n",
       "0                                 21.805955    9.610813    5.568185  \n",
       "1                                  0.413296    0.180329    0.109713  \n",
       "2                                  0.413182    0.180340    0.109739  \n",
       "3                               -100.000000    0.179993    0.109496  \n",
       "4                                  0.411271 -100.000000    0.109208  \n",
       "5                                  0.410944    0.179377 -100.000000  "
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D0.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. class', index = 'Mean diversion ratio of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the Similarity model the mean diversion ratios are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean diversion ratio wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean diversion ratio of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>34.338534</td>\n",
       "      <td>29.303129</td>\n",
       "      <td>21.249561</td>\n",
       "      <td>9.533781</td>\n",
       "      <td>5.574995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>56.070165</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>20.249637</td>\n",
       "      <td>14.685870</td>\n",
       "      <td>6.222308</td>\n",
       "      <td>2.772021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>57.319588</td>\n",
       "      <td>23.769937</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>11.316729</td>\n",
       "      <td>5.739513</td>\n",
       "      <td>1.854233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.555597</td>\n",
       "      <td>23.848287</td>\n",
       "      <td>15.103754</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>4.709254</td>\n",
       "      <td>0.783108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.023365</td>\n",
       "      <td>21.663408</td>\n",
       "      <td>15.037037</td>\n",
       "      <td>9.166066</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>2.110124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>61.578837</td>\n",
       "      <td>19.446999</td>\n",
       "      <td>11.778062</td>\n",
       "      <td>3.899938</td>\n",
       "      <td>3.296164</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean diversion ratio wrt. class           0           1           2  \\\n",
       "Mean diversion ratio of class                                         \n",
       "0                               -100.000000   34.338534   29.303129   \n",
       "1                                 56.070165 -100.000000   20.249637   \n",
       "2                                 57.319588   23.769937 -100.000000   \n",
       "3                                 55.555597   23.848287   15.103754   \n",
       "4                                 52.023365   21.663408   15.037037   \n",
       "5                                 61.578837   19.446999   11.778062   \n",
       "\n",
       "Mean diversion ratio wrt. class           3           4           5  \n",
       "Mean diversion ratio of class                                        \n",
       "0                                 21.249561    9.533781    5.574995  \n",
       "1                                 14.685870    6.222308    2.772021  \n",
       "2                                 11.316729    5.739513    1.854233  \n",
       "3                               -100.000000    4.709254    0.783108  \n",
       "4                                  9.166066 -100.000000    2.110124  \n",
       "5                                  3.899938    3.296164 -100.000000  "
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D1.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. class', index = 'Mean diversion ratio of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "To conclude we compare parameter estimates from the above models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit_MLE</th>\n",
       "      <th>Similarity_MLE</th>\n",
       "      <th>FKN</th>\n",
       "      <th>Similarity_BLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.468*** (4e-05)</td>\n",
       "      <td>-2.545 (5.43585)</td>\n",
       "      <td>-10.349*** (0.00344)</td>\n",
       "      <td>-11.593*** (0.03487)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.318*** (2e-05)</td>\n",
       "      <td>-0.318 (2.79781)</td>\n",
       "      <td>-0.444*** (0.00175)</td>\n",
       "      <td>-0.793*** (0.02008)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.457*** (2e-05)</td>\n",
       "      <td>-0.457 (3.15701)</td>\n",
       "      <td>-3.494*** (0.00247)</td>\n",
       "      <td>-5.225*** (0.02453)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.948*** (2e-05)</td>\n",
       "      <td>-0.945 (2.89147)</td>\n",
       "      <td>-0.054*** (0.00162)</td>\n",
       "      <td>0.087*** (0.02038)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.928*** (3e-05)</td>\n",
       "      <td>-1.953 (4.01604)</td>\n",
       "      <td>-2.108*** (0.00173)</td>\n",
       "      <td>-2.284*** (0.02333)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.056*** (3e-05)</td>\n",
       "      <td>-2.093 (5.26651)</td>\n",
       "      <td>5.536*** (0.00324)</td>\n",
       "      <td>5.841*** (0.03421)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.021*** (3e-05)</td>\n",
       "      <td>-2.081 (3.92949)</td>\n",
       "      <td>0.26*** (0.00203)</td>\n",
       "      <td>0.817*** (0.02778)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.715*** (1e-05)</td>\n",
       "      <td>-0.742 (1.60313)</td>\n",
       "      <td>-1.017*** (0.00114)</td>\n",
       "      <td>-0.825*** (0.01238)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.373*** (3e-05)</td>\n",
       "      <td>-1.39 (3.13216)</td>\n",
       "      <td>3.019*** (0.00231)</td>\n",
       "      <td>4.629*** (0.02632)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.01*** (1e-05)</td>\n",
       "      <td>-0.054 (1.33363)</td>\n",
       "      <td>0.799*** (0.00084)</td>\n",
       "      <td>1.272*** (0.0132)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.847*** (1e-05)</td>\n",
       "      <td>0.844 (1.09137)</td>\n",
       "      <td>-0.02*** (0.00018)</td>\n",
       "      <td>-0.154*** (0.00238)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.109 (0.47982)</td>\n",
       "      <td>0.73*** (0.00025)</td>\n",
       "      <td>0.838*** (0.00203)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.009 (0.27073)</td>\n",
       "      <td>-0.112*** (0.00015)</td>\n",
       "      <td>-0.079*** (0.00134)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.068 (0.34245)</td>\n",
       "      <td>0.07*** (0.0002)</td>\n",
       "      <td>0.159*** (0.00181)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.041 (0.30303)</td>\n",
       "      <td>0.038*** (0.00017)</td>\n",
       "      <td>0.008*** (0.00158)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.082 (0.38956)</td>\n",
       "      <td>-0.086*** (0.00016)</td>\n",
       "      <td>-0.039*** (0.00155)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.131 (0.28188)</td>\n",
       "      <td>-0.059*** (0.00013)</td>\n",
       "      <td>-0.079*** (0.00119)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.025 (0.20183)</td>\n",
       "      <td>-0.079*** (9e-05)</td>\n",
       "      <td>-0.073*** (0.00083)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.003 (0.15439)</td>\n",
       "      <td>0.046*** (0.00011)</td>\n",
       "      <td>0.024*** (0.00093)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.073 (0.25448)</td>\n",
       "      <td>-0.024*** (0.00016)</td>\n",
       "      <td>-0.039*** (0.00155)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.198 (0.16537)</td>\n",
       "      <td>-0.034*** (0.00011)</td>\n",
       "      <td>-0.036*** (0.00112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.058 (0.22784)</td>\n",
       "      <td>0.265*** (0.00024)</td>\n",
       "      <td>0.191*** (0.00098)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.03 (0.22976)</td>\n",
       "      <td>-0.23*** (0.00023)</td>\n",
       "      <td>-0.374*** (0.0012)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.157 (0.40932)</td>\n",
       "      <td>-0.072*** (0.00013)</td>\n",
       "      <td>-0.105*** (0.00143)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Logit_MLE    Similarity_MLE                   FKN  \\\n",
       "in_out        -2.468*** (4e-05)  -2.545 (5.43585)  -10.349*** (0.00344)   \n",
       "cy            -0.318*** (2e-05)  -0.318 (2.79781)   -0.444*** (0.00175)   \n",
       "hp            -0.457*** (2e-05)  -0.457 (3.15701)   -3.494*** (0.00247)   \n",
       "we            -0.948*** (2e-05)  -0.945 (2.89147)   -0.054*** (0.00162)   \n",
       "le            -1.928*** (3e-05)  -1.953 (4.01604)   -2.108*** (0.00173)   \n",
       "wi            -2.056*** (3e-05)  -2.093 (5.26651)    5.536*** (0.00324)   \n",
       "he            -2.021*** (3e-05)  -2.081 (3.92949)     0.26*** (0.00203)   \n",
       "li            -0.715*** (1e-05)  -0.742 (1.60313)   -1.017*** (0.00114)   \n",
       "sp            -1.373*** (3e-05)   -1.39 (3.13216)    3.019*** (0.00231)   \n",
       "ac              0.01*** (1e-05)  -0.054 (1.33363)    0.799*** (0.00084)   \n",
       "pr             0.847*** (1e-05)   0.844 (1.09137)    -0.02*** (0.00018)   \n",
       "group_in_out              - (-)   0.109 (0.47982)     0.73*** (0.00025)   \n",
       "group_cy                  - (-)  -0.009 (0.27073)   -0.112*** (0.00015)   \n",
       "group_hp                  - (-)   0.068 (0.34245)      0.07*** (0.0002)   \n",
       "group_we                  - (-)   0.041 (0.30303)    0.038*** (0.00017)   \n",
       "group_le                  - (-)  -0.082 (0.38956)   -0.086*** (0.00016)   \n",
       "group_wi                  - (-)  -0.131 (0.28188)   -0.059*** (0.00013)   \n",
       "group_he                  - (-)  -0.025 (0.20183)     -0.079*** (9e-05)   \n",
       "group_li                  - (-)  -0.003 (0.15439)    0.046*** (0.00011)   \n",
       "group_sp                  - (-)  -0.073 (0.25448)   -0.024*** (0.00016)   \n",
       "group_ac                  - (-)  -0.198 (0.16537)   -0.034*** (0.00011)   \n",
       "group_brand               - (-)   0.058 (0.22784)    0.265*** (0.00024)   \n",
       "group_home                - (-)    0.03 (0.22976)    -0.23*** (0.00023)   \n",
       "group_cla                 - (-)   0.157 (0.40932)   -0.072*** (0.00013)   \n",
       "\n",
       "                    Similarity_BLP  \n",
       "in_out        -11.593*** (0.03487)  \n",
       "cy             -0.793*** (0.02008)  \n",
       "hp             -5.225*** (0.02453)  \n",
       "we              0.087*** (0.02038)  \n",
       "le             -2.284*** (0.02333)  \n",
       "wi              5.841*** (0.03421)  \n",
       "he              0.817*** (0.02778)  \n",
       "li             -0.825*** (0.01238)  \n",
       "sp              4.629*** (0.02632)  \n",
       "ac               1.272*** (0.0132)  \n",
       "pr             -0.154*** (0.00238)  \n",
       "group_in_out    0.838*** (0.00203)  \n",
       "group_cy       -0.079*** (0.00134)  \n",
       "group_hp        0.159*** (0.00181)  \n",
       "group_we        0.008*** (0.00158)  \n",
       "group_le       -0.039*** (0.00155)  \n",
       "group_wi       -0.079*** (0.00119)  \n",
       "group_he       -0.073*** (0.00083)  \n",
       "group_li        0.024*** (0.00093)  \n",
       "group_sp       -0.039*** (0.00155)  \n",
       "group_ac       -0.036*** (0.00112)  \n",
       "group_brand     0.191*** (0.00098)  \n",
       "group_home      -0.374*** (0.0012)  \n",
       "group_cla      -0.105*** (0.00143)  "
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_index = pr_index + 1\n",
    "Logit_nest = ['-' for i in np.arange(G)]\n",
    "Logitbeta_show = [*(np.round(Logit_beta[:beta_index], decimals = 3).astype('str')), *Logit_nest]\n",
    "Logitse_show = [*(np.round(Logit_SE[:beta_index], decimals=5).astype('str')), *Logit_nest]\n",
    "Logitp_show = [*Logit_p[:beta_index], *[1 for i in np.arange(G)]]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Logit_MLE' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip(Logitp_show, Logitbeta_show, Logitse_show)],\n",
    "    'Similarity_MLE': [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*Similarity_p[:beta_index], *Similarity_p[K:]], [*(np.round(Similarity_theta[:beta_index], decimals = 3).astype('str')), *(np.round(Similarity_theta[K:], decimals = 3).astype('str'))], [*(np.round(Similarity_SE[:beta_index], decimals=5).astype('str')), *(np.round(Similarity_SE[K:], decimals=5).astype('str'))])],\n",
    "    'FKN' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*FKN_p[:beta_index], *FKN_p[K:]], [*(np.round(FKN_theta[:beta_index], decimals = 3).astype('str')), *(np.round(FKN_theta[K:], decimals = 3).astype('str'))], [*(np.round(FKN_SE[:beta_index], decimals=5).astype('str')), *(np.round(FKN_SE[K:], decimals=5).astype('str'))])],\n",
    "    'Similarity_BLP' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*OptBLP_p[:beta_index], *OptBLP_p[K:]], [*(np.round(ThetaOptBLP[:beta_index], decimals = 3).astype('str')), *(np.round(ThetaOptBLP[K:], decimals = 3).astype('str'))], [*(np.round(SEOptBLP[:beta_index], decimals=5).astype('str')), *(np.round(SEOptBLP[K:], decimals=5).astype('str'))])]\n",
    "}, \n",
    "index = [*x_vars[:beta_index], *['group_' + par for par in nest_vars]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
