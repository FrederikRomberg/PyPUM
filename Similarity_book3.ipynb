{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling Demand for Cars with the IPDL model\n",
    "\n",
    "In this notebook, we will introduce and estimate the Inverse Product Differentiation Logit (IPDL) model of Fosgerau et al. (2023) using publically available data on the European car market from Frank Verboven's website at https://sites.google.com/site/frankverbo/data-and-software/data-set-on-the-european-car-market. We begin by introducing the data set. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data\n",
    "====\n",
    "\n",
    "The dataset consists of approximately 110 vehicle makes per year in the period 1970-1999 in five European markets (Belgium, France, Germany, Italy, and the United Kingdom). The data set includes 47 variables in total. The first four columns are market and product codes for the year, country, and make as well as quantity sold (No. of new registrations) which will be used in computing observed market shares. The remaining variables consist of car characteristics such as prices, horse power, weight and other physical car characteristics as well as macroeconomic variables such as GDP per capita which have been used to construct estimates of the average wage income and purchasing power.\n",
    "\n",
    "We have in total 30 years and 5 countries, totalling $T=150$ year-country combinations, indexed by $t$, and we refer to each simply as market $t$. In market $t$, the choice set is $\\mathcal{J}_t$ which includes the set of available makes as well as an outside option. Let $\\mathcal{J} := \\bigcup_{t=1}^T \\mathcal{J}_t$ be the full choice set and \n",
    " $J:=\\#\\mathcal{J}$ the number of choices which were available in at least one market, for this data set there are $J=357$ choices.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the dataset `eurocars.csv` we thus have a dataframe of $\\sum_{t=1}^T \\#\\mathcal{J}_t = 11459$ rows and $47$ columns. The `ye` column runs through $y=70,\\ldots,99$, the `ma` column runs through $m=1,\\ldots,M$, and the ``co`` column takes values $j\\in \\mathcal{J}$. \n",
    "\n",
    "Because we consider a country-year pair as the level of observation, we construct a `market` column taking values $t=1,\\ldots,T$. In Python, this variable will take values $t=0,\\ldots,T-1$. We construct an outside option $j=0$ in each market $t$ by letting the 'sales' of $j=0$ be determined as \n",
    "\n",
    "$$\\mathrm{sales}_{0t} = \\mathrm{pop}_t - \\sum_{j=1}^J \\mathrm{sales}_{jt}$$\n",
    "\n",
    "where $\\mathrm{pop}_t$ is the total population in market $t$, and the car characteristics of the outside option is set to zero. The market shares of each product in market $t$ can then be found as\n",
    "$$\n",
    "\\textrm{market share}_{jt}=\\frac{\\mathrm{sales_{jt}}}{\\mathrm{pop}_t}.\n",
    "$$\n",
    "We also read in the variable description of the dataset contained in `eurocars.dta`. We will use the list `x_vars` throughout to work with our explanatory variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import os\n",
    "from numpy import linalg as la\n",
    "from scipy import optimize\n",
    "import scipy.stats as scstat\n",
    "from matplotlib import pyplot as plt\n",
    "import itertools as iter\n",
    "\n",
    "# Files\n",
    "import Logit_file as logit\n",
    "import Eurocarsdata_file as eurocarsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset and variable names\n",
    "# os.chdir('../GREENCAR_notebooks/') # Assigns work directory\n",
    "\n",
    "input_path = os.getcwd() # Assigns input path as current working directory (cwd)\n",
    "descr = (pd.read_stata('eurocars.dta', iterator = True)).variable_labels() # Obtain variable descriptions\n",
    "dat_file = pd.read_csv(os.path.join(input_path, 'eurocars.csv')) # reads in the data set as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ye</td>\n",
       "      <td>year (=first dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ma</td>\n",
       "      <td>market (=second dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>co</td>\n",
       "      <td>model code (=third dimension of panel)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zcode</td>\n",
       "      <td>alternative model code (predecessors and succe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>brd</td>\n",
       "      <td>brand code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>type</td>\n",
       "      <td>name of brand and model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>model</td>\n",
       "      <td>name of model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>org</td>\n",
       "      <td>origin code (demand side, country with which c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>loc</td>\n",
       "      <td>location code (production side, country where ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>frm</td>\n",
       "      <td>firm code</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>qu</td>\n",
       "      <td>sales (number of new car registrations)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>pl</td>\n",
       "      <td>places (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>do</td>\n",
       "      <td>doors (number, not reliable variable)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>li1</td>\n",
       "      <td>measure 1 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>li2</td>\n",
       "      <td>measure 2 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>li3</td>\n",
       "      <td>measure 3 for fuel efficiency (liter per km, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>princ</td>\n",
       "      <td>=pr/(ngdp/pop): price relative to per capita i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>eurpr</td>\n",
       "      <td>=pr/avdexr: price in common currency (in SDR t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>exppr</td>\n",
       "      <td>=pr/avexr: price in exporter currency</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>avexr</td>\n",
       "      <td>av. exchange rate of exporter country (exporte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>avdexr</td>\n",
       "      <td>av. exchange rate of destination country (dest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>avcpr</td>\n",
       "      <td>av. consumer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>avppr</td>\n",
       "      <td>av. producer price index of exporter country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>avdcpr</td>\n",
       "      <td>av. consumer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>avdppr</td>\n",
       "      <td>av. producer price index of destination country</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>xexr</td>\n",
       "      <td>avdexr/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>tax</td>\n",
       "      <td>percentage VAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>pop</td>\n",
       "      <td>population</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>ngdp</td>\n",
       "      <td>nominal gross domestic product of destination ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>rgdp</td>\n",
       "      <td>real gross domestic product</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>engdp</td>\n",
       "      <td>=ngdp/avdexr: nominal gdp in common currency (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>ergdp</td>\n",
       "      <td>=rgdp/avexr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>engdpc</td>\n",
       "      <td>=engdp/pop: nominal gdp per capita in common c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>ergdpc</td>\n",
       "      <td>=ergdp/pop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              ye                   year (=first dimension of panel)\n",
       "1              ma                market (=second dimension of panel)\n",
       "2              co             model code (=third dimension of panel)\n",
       "3           zcode  alternative model code (predecessors and succe...\n",
       "4             brd                                         brand code\n",
       "5            type                            name of brand and model\n",
       "6           brand                                      name of brand\n",
       "7           model                                      name of model\n",
       "8             org  origin code (demand side, country with which c...\n",
       "9             loc  location code (production side, country where ...\n",
       "10            cla                              class or segment code\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            frm                                          firm code\n",
       "13             qu            sales (number of new car registrations)\n",
       "14             cy            cylinder volume or displacement (in cc)\n",
       "15             hp                                 horsepower (in kW)\n",
       "16             we                                     weight (in kg)\n",
       "17             pl             places (number, not reliable variable)\n",
       "18             do              doors (number, not reliable variable)\n",
       "19             le                                     length (in cm)\n",
       "20             wi                                      width (in cm)\n",
       "21             he                                     height (in cm)\n",
       "22            li1  measure 1 for fuel efficiency (liter per km, a...\n",
       "23            li2  measure 2 for fuel efficiency (liter per km, a...\n",
       "24            li3  measure 3 for fuel efficiency (liter per km, a...\n",
       "25             li          average of li1, li2, li3 (used in papers)\n",
       "26             sp                            maximum speed (km/hour)\n",
       "27             ac  time to acceleration (in seconds from 0 to 100...\n",
       "28             pr   price (in destination currency including V.A.T.)\n",
       "29          princ  =pr/(ngdp/pop): price relative to per capita i...\n",
       "30          eurpr  =pr/avdexr: price in common currency (in SDR t...\n",
       "31          exppr              =pr/avexr: price in exporter currency\n",
       "32          avexr  av. exchange rate of exporter country (exporte...\n",
       "33         avdexr  av. exchange rate of destination country (dest...\n",
       "34          avcpr       av. consumer price index of exporter country\n",
       "35          avppr       av. producer price index of exporter country\n",
       "36         avdcpr    av. consumer price index of destination country\n",
       "37         avdppr    av. producer price index of destination country\n",
       "38           xexr                                       avdexr/avexr\n",
       "39            tax                                     percentage VAT\n",
       "40            pop                                         population\n",
       "41           ngdp  nominal gross domestic product of destination ...\n",
       "42           rgdp                        real gross domestic product\n",
       "43          engdp  =ngdp/avdexr: nominal gdp in common currency (...\n",
       "44          ergdp                                        =rgdp/avexr\n",
       "45         engdpc  =engdp/pop: nominal gdp per capita in common c...\n",
       "46         ergdpc                                         =ergdp/pop"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(descr, index=['description']).transpose().reset_index().rename(columns={'index' : 'variable names'}) # Prints data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable names</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cy</td>\n",
       "      <td>cylinder volume or displacement (in cc)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hp</td>\n",
       "      <td>horsepower (in kW)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>we</td>\n",
       "      <td>weight (in kg)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>le</td>\n",
       "      <td>length (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wi</td>\n",
       "      <td>width (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>he</td>\n",
       "      <td>height (in cm)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>li</td>\n",
       "      <td>average of li1, li2, li3 (used in papers)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sp</td>\n",
       "      <td>maximum speed (km/hour)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ac</td>\n",
       "      <td>time to acceleration (in seconds from 0 to 100...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pr</td>\n",
       "      <td>price (in destination currency including V.A.T.)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>brand</td>\n",
       "      <td>name of brand</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>home</td>\n",
       "      <td>domestic car dummy (appropriate interaction of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cla</td>\n",
       "      <td>class or segment code</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   variable names                                        description\n",
       "0              cy            cylinder volume or displacement (in cc)\n",
       "1              hp                                 horsepower (in kW)\n",
       "2              we                                     weight (in kg)\n",
       "3              le                                     length (in cm)\n",
       "4              wi                                      width (in cm)\n",
       "5              he                                     height (in cm)\n",
       "6              li          average of li1, li2, li3 (used in papers)\n",
       "7              sp                            maximum speed (km/hour)\n",
       "8              ac  time to acceleration (in seconds from 0 to 100...\n",
       "9              pr   price (in destination currency including V.A.T.)\n",
       "10          brand                                      name of brand\n",
       "11           home  domestic car dummy (appropriate interaction of...\n",
       "12            cla                              class or segment code"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outside option is included if OO == True, otherwise analysis is done on the inside options only.\n",
    "OO = True\n",
    "\n",
    "# Choose which variables to include in the analysis, and assign them either as discrete variables or continuous.\n",
    "\n",
    "x_discretevars = [ 'brand', 'home', 'cla']\n",
    "x_contvars = ['cy', 'hp', 'we', 'le', 'wi', 'he', 'li', 'sp', 'ac', 'pr']\n",
    "z_IV_contvars = ['xexr']\n",
    "z_IV_discretevars = []\n",
    "x_allvars =  [*x_contvars, *x_discretevars]\n",
    "z_allvars = [*z_IV_contvars, *z_IV_discretevars]\n",
    "\n",
    "if OO:\n",
    "    nest_contvars = [var for var in x_contvars if var != 'pr'] # We nest over all variables other than price, but an alternative list can be specified here if desired.\n",
    "    nest_discvars = ['in_out', *x_discretevars]\n",
    "    nest_vars = ['in_out', *nest_contvars, *x_discretevars]\n",
    "else:\n",
    "    nest_contvars = [var for var in x_contvars if (var != 'pr')]\n",
    "    nest_discvars = x_discretevars # See above\n",
    "    nest_vars = [*nest_contvars, *nest_discvars]\n",
    "\n",
    "G = len(nest_vars)\n",
    "\n",
    "# Print list of chosen variables as a dataframe\n",
    "pd.DataFrame(descr, index=['description'])[x_allvars].transpose().reset_index().rename(columns={'index' : 'variable names'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now clean the data to fit our setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "dat, dat_org, x_vars, z_vars, N, pop_share, T, J, K = eurocarsdata.Eurocars_cleandata(dat_file, x_contvars, x_discretevars, z_IV_contvars, z_IV_discretevars, outside_option=OO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of numpy arrays for each market. This allows the size of the data set to vary over markets.\n",
    "\n",
    "dat = dat.reset_index(drop = True).sort_values(by = ['market', 'co']) # Sort data so that reshape is successfull\n",
    "\n",
    "x = {t: dat[dat['market'] == t][x_vars].values.reshape((J[t],K)) for t in np.arange(T)} # Dict of explanatory variables\n",
    "y = {t: dat[dat['market'] == t]['ms'].to_numpy().reshape((J[t])) for t in np.arange(T)} # Dict of market shares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function tests whether the utility parameters are identified, by looking at the rank of the stacked matrix of explanatory variables.\n",
    "\n",
    "def rank_test(x):\n",
    "    x_stacked = np.concatenate([x[t] for t in np.arange(T)], axis = 0)\n",
    "    eigs=la.eig(x_stacked.T@x_stacked)[0]\n",
    "\n",
    "    if np.min(eigs)<1.0e-8:\n",
    "        print('x does not have full rank')\n",
    "    else:\n",
    "        print('x has full rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x has full rank\n"
     ]
    }
   ],
   "source": [
    "rank_test(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perturbed utility, logit and nested logit\n",
    "\n",
    "In the following, a vector $z\\in \\mathbb R^d$ is always a column vector. The IPDL model is a discrete choice model, where the probability vector over the alternatives is given by the solution to a utility maximization problem of the form\n",
    "$$\n",
    "p=\\arg\\max_{q\\in \\Delta} q'u-\\Omega(q)\n",
    "$$\n",
    "where $\\Delta$ is the probability simplex over the set of discrete choices, $u$ is a vector of payoffs for each option, $\\Omega$ is a convex function and $q'$ denotes the transpose of $q$. All additive random utility models can be represented in this way (Fosgerau and Sørensen (2021)). For example, the logit choice probabilities result from the perturbation function $\\Omega(q)=q'\\ln q$ where $\\ln q$ is the elementwise logarithm.\n",
    "\n",
    "In the nested logit model, the choice set is divided into a partition $\\mathcal C=\\left\\{C_1,\\ldots,C_L\\right\\}$, and the perturbation function is given by\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\sum_{\\ell =1}^L \\left( \\sum_{j\\in C_\\ell}q_j\\right)\\ln \\left( \\sum_{j\\in C}q_j\\right),\n",
    "$$\n",
    "where $\\lambda\\in [0,1)$ is a parameter. This function can be written equivalently as\n",
    "$$\n",
    "\\Omega(q|\\lambda)=(1-\\lambda)q'\\ln q+\\lambda \\left(\\psi q\\right)'\\ln \\left( \\psi q\\right),\n",
    "$$\n",
    "where $\\psi$ is a $J \\times L$ matrix, where $\\psi_{j\\ell}=1$ if option $j$ belongs to nest $C_\\ell$ and zero otherwise.\n",
    " This specification generates nested logit choice probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Similarity Model\n",
    "\n",
    "Kernel denisity + Silverman's rule of thumb bla bla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_d = dat[dat['market'] == 0]['cla'].nunique()\n",
    "Indicator = pd.get_dummies(dat[dat['market'] == 0]['cla']).values.reshape((J[0], C_d))\n",
    "K_disc = Indicator@(Indicator.T)\n",
    "Psidisc_t = np.einsum('jk,k->jk', K_disc, 1./(K_disc.sum(axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.389381</td>\n",
       "      <td>-0.246903</td>\n",
       "      <td>-0.242920</td>\n",
       "      <td>-0.287168</td>\n",
       "      <td>-0.439823</td>\n",
       "      <td>-0.238496</td>\n",
       "      <td>-0.330088</td>\n",
       "      <td>-0.550885</td>\n",
       "      <td>-0.397345</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.243363</td>\n",
       "      <td>-0.256416</td>\n",
       "      <td>-0.245133</td>\n",
       "      <td>-0.242920</td>\n",
       "      <td>-0.286726</td>\n",
       "      <td>-0.780531</td>\n",
       "      <td>-0.208850</td>\n",
       "      <td>-0.371460</td>\n",
       "      <td>-0.330310</td>\n",
       "      <td>-0.331416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.389381</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142478</td>\n",
       "      <td>0.146460</td>\n",
       "      <td>0.102212</td>\n",
       "      <td>-0.050442</td>\n",
       "      <td>0.150885</td>\n",
       "      <td>0.059292</td>\n",
       "      <td>-0.161504</td>\n",
       "      <td>-0.007965</td>\n",
       "      <td>...</td>\n",
       "      <td>0.146018</td>\n",
       "      <td>0.132965</td>\n",
       "      <td>0.144248</td>\n",
       "      <td>0.146460</td>\n",
       "      <td>0.102655</td>\n",
       "      <td>-0.391150</td>\n",
       "      <td>0.180531</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>0.059071</td>\n",
       "      <td>0.057965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.246903</td>\n",
       "      <td>-0.142478</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>-0.040265</td>\n",
       "      <td>-0.192920</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>-0.083186</td>\n",
       "      <td>-0.303982</td>\n",
       "      <td>-0.150442</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>-0.009513</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>-0.039823</td>\n",
       "      <td>-0.533628</td>\n",
       "      <td>0.038053</td>\n",
       "      <td>-0.124558</td>\n",
       "      <td>-0.083407</td>\n",
       "      <td>-0.084513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.242920</td>\n",
       "      <td>-0.146460</td>\n",
       "      <td>-0.003982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.044248</td>\n",
       "      <td>-0.196903</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>-0.087168</td>\n",
       "      <td>-0.307965</td>\n",
       "      <td>-0.154425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>-0.013496</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.043805</td>\n",
       "      <td>-0.537611</td>\n",
       "      <td>0.034071</td>\n",
       "      <td>-0.128540</td>\n",
       "      <td>-0.087389</td>\n",
       "      <td>-0.088496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.287168</td>\n",
       "      <td>-0.102212</td>\n",
       "      <td>0.040265</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.152655</td>\n",
       "      <td>0.048673</td>\n",
       "      <td>-0.042920</td>\n",
       "      <td>-0.263717</td>\n",
       "      <td>-0.110177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>0.030752</td>\n",
       "      <td>0.042035</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-0.493363</td>\n",
       "      <td>0.078319</td>\n",
       "      <td>-0.084292</td>\n",
       "      <td>-0.043142</td>\n",
       "      <td>-0.044248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.439823</td>\n",
       "      <td>0.050442</td>\n",
       "      <td>0.192920</td>\n",
       "      <td>0.196903</td>\n",
       "      <td>0.152655</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.201327</td>\n",
       "      <td>0.109735</td>\n",
       "      <td>-0.111062</td>\n",
       "      <td>0.042478</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196460</td>\n",
       "      <td>0.183407</td>\n",
       "      <td>0.194690</td>\n",
       "      <td>0.196903</td>\n",
       "      <td>0.153097</td>\n",
       "      <td>-0.340708</td>\n",
       "      <td>0.230973</td>\n",
       "      <td>0.068363</td>\n",
       "      <td>0.109513</td>\n",
       "      <td>0.108407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.238496</td>\n",
       "      <td>-0.150885</td>\n",
       "      <td>-0.008407</td>\n",
       "      <td>-0.004425</td>\n",
       "      <td>-0.048673</td>\n",
       "      <td>-0.201327</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.091593</td>\n",
       "      <td>-0.312389</td>\n",
       "      <td>-0.158850</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004867</td>\n",
       "      <td>-0.017920</td>\n",
       "      <td>-0.006637</td>\n",
       "      <td>-0.004425</td>\n",
       "      <td>-0.048230</td>\n",
       "      <td>-0.542035</td>\n",
       "      <td>0.029646</td>\n",
       "      <td>-0.132965</td>\n",
       "      <td>-0.091814</td>\n",
       "      <td>-0.092920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.330088</td>\n",
       "      <td>-0.059292</td>\n",
       "      <td>0.083186</td>\n",
       "      <td>0.087168</td>\n",
       "      <td>0.042920</td>\n",
       "      <td>-0.109735</td>\n",
       "      <td>0.091593</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.220796</td>\n",
       "      <td>-0.067257</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086726</td>\n",
       "      <td>0.073673</td>\n",
       "      <td>0.084956</td>\n",
       "      <td>0.087168</td>\n",
       "      <td>0.043363</td>\n",
       "      <td>-0.450442</td>\n",
       "      <td>0.121239</td>\n",
       "      <td>-0.041372</td>\n",
       "      <td>-0.000221</td>\n",
       "      <td>-0.001327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.550885</td>\n",
       "      <td>0.161504</td>\n",
       "      <td>0.303982</td>\n",
       "      <td>0.307965</td>\n",
       "      <td>0.263717</td>\n",
       "      <td>0.111062</td>\n",
       "      <td>0.312389</td>\n",
       "      <td>0.220796</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.153540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.307522</td>\n",
       "      <td>0.294469</td>\n",
       "      <td>0.305752</td>\n",
       "      <td>0.307965</td>\n",
       "      <td>0.264159</td>\n",
       "      <td>-0.229646</td>\n",
       "      <td>0.342035</td>\n",
       "      <td>0.179425</td>\n",
       "      <td>0.220575</td>\n",
       "      <td>0.219469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.397345</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>0.150442</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.110177</td>\n",
       "      <td>-0.042478</td>\n",
       "      <td>0.158850</td>\n",
       "      <td>0.067257</td>\n",
       "      <td>-0.153540</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.153982</td>\n",
       "      <td>0.140929</td>\n",
       "      <td>0.152212</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>0.110619</td>\n",
       "      <td>-0.383186</td>\n",
       "      <td>0.188496</td>\n",
       "      <td>0.025885</td>\n",
       "      <td>0.067035</td>\n",
       "      <td>0.065929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.284956</td>\n",
       "      <td>-0.104425</td>\n",
       "      <td>0.038053</td>\n",
       "      <td>0.042035</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>-0.154867</td>\n",
       "      <td>0.046460</td>\n",
       "      <td>-0.045133</td>\n",
       "      <td>-0.265929</td>\n",
       "      <td>-0.112389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041593</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>0.039823</td>\n",
       "      <td>0.042035</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>-0.495575</td>\n",
       "      <td>0.076106</td>\n",
       "      <td>-0.086504</td>\n",
       "      <td>-0.045354</td>\n",
       "      <td>-0.046460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.325221</td>\n",
       "      <td>-0.064159</td>\n",
       "      <td>0.078319</td>\n",
       "      <td>0.082301</td>\n",
       "      <td>0.038053</td>\n",
       "      <td>-0.114602</td>\n",
       "      <td>0.086726</td>\n",
       "      <td>-0.004867</td>\n",
       "      <td>-0.225664</td>\n",
       "      <td>-0.072124</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081858</td>\n",
       "      <td>0.068805</td>\n",
       "      <td>0.080088</td>\n",
       "      <td>0.082301</td>\n",
       "      <td>0.038496</td>\n",
       "      <td>-0.455310</td>\n",
       "      <td>0.116372</td>\n",
       "      <td>-0.046239</td>\n",
       "      <td>-0.005088</td>\n",
       "      <td>-0.006195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.285177</td>\n",
       "      <td>-0.104204</td>\n",
       "      <td>0.038274</td>\n",
       "      <td>0.042257</td>\n",
       "      <td>-0.001991</td>\n",
       "      <td>-0.154646</td>\n",
       "      <td>0.046681</td>\n",
       "      <td>-0.044912</td>\n",
       "      <td>-0.265708</td>\n",
       "      <td>-0.112168</td>\n",
       "      <td>...</td>\n",
       "      <td>0.041814</td>\n",
       "      <td>0.028761</td>\n",
       "      <td>0.040044</td>\n",
       "      <td>0.042257</td>\n",
       "      <td>-0.001549</td>\n",
       "      <td>-0.495354</td>\n",
       "      <td>0.076327</td>\n",
       "      <td>-0.086283</td>\n",
       "      <td>-0.045133</td>\n",
       "      <td>-0.046239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.187611</td>\n",
       "      <td>-0.201770</td>\n",
       "      <td>-0.059292</td>\n",
       "      <td>-0.055310</td>\n",
       "      <td>-0.099558</td>\n",
       "      <td>-0.252212</td>\n",
       "      <td>-0.050885</td>\n",
       "      <td>-0.142478</td>\n",
       "      <td>-0.363274</td>\n",
       "      <td>-0.209735</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.055752</td>\n",
       "      <td>-0.068805</td>\n",
       "      <td>-0.057522</td>\n",
       "      <td>-0.055310</td>\n",
       "      <td>-0.099115</td>\n",
       "      <td>-0.592920</td>\n",
       "      <td>-0.021239</td>\n",
       "      <td>-0.183850</td>\n",
       "      <td>-0.142699</td>\n",
       "      <td>-0.143805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.378097</td>\n",
       "      <td>-0.011283</td>\n",
       "      <td>0.131195</td>\n",
       "      <td>0.135177</td>\n",
       "      <td>0.090929</td>\n",
       "      <td>-0.061726</td>\n",
       "      <td>0.139602</td>\n",
       "      <td>0.048009</td>\n",
       "      <td>-0.172788</td>\n",
       "      <td>-0.019248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.134735</td>\n",
       "      <td>0.121681</td>\n",
       "      <td>0.132965</td>\n",
       "      <td>0.135177</td>\n",
       "      <td>0.091372</td>\n",
       "      <td>-0.402434</td>\n",
       "      <td>0.169248</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>0.047788</td>\n",
       "      <td>0.046681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.263717</td>\n",
       "      <td>-0.125664</td>\n",
       "      <td>0.016814</td>\n",
       "      <td>0.020796</td>\n",
       "      <td>-0.023451</td>\n",
       "      <td>-0.176106</td>\n",
       "      <td>0.025221</td>\n",
       "      <td>-0.066372</td>\n",
       "      <td>-0.287168</td>\n",
       "      <td>-0.133628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020354</td>\n",
       "      <td>0.007301</td>\n",
       "      <td>0.018584</td>\n",
       "      <td>0.020796</td>\n",
       "      <td>-0.023009</td>\n",
       "      <td>-0.516814</td>\n",
       "      <td>0.054867</td>\n",
       "      <td>-0.107743</td>\n",
       "      <td>-0.066593</td>\n",
       "      <td>-0.067699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.285398</td>\n",
       "      <td>-0.103982</td>\n",
       "      <td>0.038496</td>\n",
       "      <td>0.042478</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>-0.154425</td>\n",
       "      <td>0.046903</td>\n",
       "      <td>-0.044690</td>\n",
       "      <td>-0.265487</td>\n",
       "      <td>-0.111947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.042035</td>\n",
       "      <td>0.028982</td>\n",
       "      <td>0.040265</td>\n",
       "      <td>0.042478</td>\n",
       "      <td>-0.001327</td>\n",
       "      <td>-0.495133</td>\n",
       "      <td>0.076549</td>\n",
       "      <td>-0.086062</td>\n",
       "      <td>-0.044912</td>\n",
       "      <td>-0.046018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.260398</td>\n",
       "      <td>-0.128982</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.017478</td>\n",
       "      <td>-0.026770</td>\n",
       "      <td>-0.179425</td>\n",
       "      <td>0.021903</td>\n",
       "      <td>-0.069690</td>\n",
       "      <td>-0.290487</td>\n",
       "      <td>-0.136947</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017035</td>\n",
       "      <td>0.003982</td>\n",
       "      <td>0.015265</td>\n",
       "      <td>0.017478</td>\n",
       "      <td>-0.026327</td>\n",
       "      <td>-0.520133</td>\n",
       "      <td>0.051549</td>\n",
       "      <td>-0.111062</td>\n",
       "      <td>-0.069912</td>\n",
       "      <td>-0.071018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.330973</td>\n",
       "      <td>-0.058407</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.088053</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>-0.108850</td>\n",
       "      <td>0.092478</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>-0.219912</td>\n",
       "      <td>-0.066372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.087611</td>\n",
       "      <td>0.074558</td>\n",
       "      <td>0.085841</td>\n",
       "      <td>0.088053</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>-0.449558</td>\n",
       "      <td>0.122124</td>\n",
       "      <td>-0.040487</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>-0.000442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.220354</td>\n",
       "      <td>-0.169027</td>\n",
       "      <td>-0.026549</td>\n",
       "      <td>-0.022566</td>\n",
       "      <td>-0.066814</td>\n",
       "      <td>-0.219469</td>\n",
       "      <td>-0.018142</td>\n",
       "      <td>-0.109735</td>\n",
       "      <td>-0.330531</td>\n",
       "      <td>-0.176991</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023009</td>\n",
       "      <td>-0.036062</td>\n",
       "      <td>-0.024779</td>\n",
       "      <td>-0.022566</td>\n",
       "      <td>-0.066372</td>\n",
       "      <td>-0.560177</td>\n",
       "      <td>0.011504</td>\n",
       "      <td>-0.151106</td>\n",
       "      <td>-0.109956</td>\n",
       "      <td>-0.111062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.132301</td>\n",
       "      <td>-0.257080</td>\n",
       "      <td>-0.114602</td>\n",
       "      <td>-0.110619</td>\n",
       "      <td>-0.154867</td>\n",
       "      <td>-0.307522</td>\n",
       "      <td>-0.106195</td>\n",
       "      <td>-0.197788</td>\n",
       "      <td>-0.418584</td>\n",
       "      <td>-0.265044</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.111062</td>\n",
       "      <td>-0.124115</td>\n",
       "      <td>-0.112832</td>\n",
       "      <td>-0.110619</td>\n",
       "      <td>-0.154425</td>\n",
       "      <td>-0.648230</td>\n",
       "      <td>-0.076549</td>\n",
       "      <td>-0.239159</td>\n",
       "      <td>-0.198009</td>\n",
       "      <td>-0.199115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.219912</td>\n",
       "      <td>-0.169469</td>\n",
       "      <td>-0.026991</td>\n",
       "      <td>-0.023009</td>\n",
       "      <td>-0.067257</td>\n",
       "      <td>-0.219912</td>\n",
       "      <td>-0.018584</td>\n",
       "      <td>-0.110177</td>\n",
       "      <td>-0.330973</td>\n",
       "      <td>-0.177434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.023451</td>\n",
       "      <td>-0.036504</td>\n",
       "      <td>-0.025221</td>\n",
       "      <td>-0.023009</td>\n",
       "      <td>-0.066814</td>\n",
       "      <td>-0.560619</td>\n",
       "      <td>0.011062</td>\n",
       "      <td>-0.151549</td>\n",
       "      <td>-0.110398</td>\n",
       "      <td>-0.111504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.348009</td>\n",
       "      <td>-0.041372</td>\n",
       "      <td>0.101106</td>\n",
       "      <td>0.105088</td>\n",
       "      <td>0.060841</td>\n",
       "      <td>-0.091814</td>\n",
       "      <td>0.109513</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>-0.202876</td>\n",
       "      <td>-0.049336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104646</td>\n",
       "      <td>0.091593</td>\n",
       "      <td>0.102876</td>\n",
       "      <td>0.105088</td>\n",
       "      <td>0.061283</td>\n",
       "      <td>-0.432522</td>\n",
       "      <td>0.139159</td>\n",
       "      <td>-0.023451</td>\n",
       "      <td>0.017699</td>\n",
       "      <td>0.016593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.390708</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.143805</td>\n",
       "      <td>0.147788</td>\n",
       "      <td>0.103540</td>\n",
       "      <td>-0.049115</td>\n",
       "      <td>0.152212</td>\n",
       "      <td>0.060619</td>\n",
       "      <td>-0.160177</td>\n",
       "      <td>-0.006637</td>\n",
       "      <td>...</td>\n",
       "      <td>0.147345</td>\n",
       "      <td>0.134292</td>\n",
       "      <td>0.145575</td>\n",
       "      <td>0.147788</td>\n",
       "      <td>0.103982</td>\n",
       "      <td>-0.389823</td>\n",
       "      <td>0.181858</td>\n",
       "      <td>0.019248</td>\n",
       "      <td>0.060398</td>\n",
       "      <td>0.059292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.440265</td>\n",
       "      <td>0.050885</td>\n",
       "      <td>0.193363</td>\n",
       "      <td>0.197345</td>\n",
       "      <td>0.153097</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>0.201770</td>\n",
       "      <td>0.110177</td>\n",
       "      <td>-0.110619</td>\n",
       "      <td>0.042920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196903</td>\n",
       "      <td>0.183850</td>\n",
       "      <td>0.195133</td>\n",
       "      <td>0.197345</td>\n",
       "      <td>0.153540</td>\n",
       "      <td>-0.340265</td>\n",
       "      <td>0.231416</td>\n",
       "      <td>0.068805</td>\n",
       "      <td>0.109956</td>\n",
       "      <td>0.108850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.551770</td>\n",
       "      <td>0.162389</td>\n",
       "      <td>0.304867</td>\n",
       "      <td>0.308850</td>\n",
       "      <td>0.264602</td>\n",
       "      <td>0.111947</td>\n",
       "      <td>0.313274</td>\n",
       "      <td>0.221681</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.154425</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308407</td>\n",
       "      <td>0.295354</td>\n",
       "      <td>0.306637</td>\n",
       "      <td>0.308850</td>\n",
       "      <td>0.265044</td>\n",
       "      <td>-0.228761</td>\n",
       "      <td>0.342920</td>\n",
       "      <td>0.180310</td>\n",
       "      <td>0.221460</td>\n",
       "      <td>0.220354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.165044</td>\n",
       "      <td>-0.224336</td>\n",
       "      <td>-0.081858</td>\n",
       "      <td>-0.077876</td>\n",
       "      <td>-0.122124</td>\n",
       "      <td>-0.274779</td>\n",
       "      <td>-0.073451</td>\n",
       "      <td>-0.165044</td>\n",
       "      <td>-0.385841</td>\n",
       "      <td>-0.232301</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.078319</td>\n",
       "      <td>-0.091372</td>\n",
       "      <td>-0.080088</td>\n",
       "      <td>-0.077876</td>\n",
       "      <td>-0.121681</td>\n",
       "      <td>-0.615487</td>\n",
       "      <td>-0.043805</td>\n",
       "      <td>-0.206416</td>\n",
       "      <td>-0.165265</td>\n",
       "      <td>-0.166372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.264823</td>\n",
       "      <td>-0.124558</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>0.021903</td>\n",
       "      <td>-0.022345</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.026327</td>\n",
       "      <td>-0.065265</td>\n",
       "      <td>-0.286062</td>\n",
       "      <td>-0.132522</td>\n",
       "      <td>...</td>\n",
       "      <td>0.021460</td>\n",
       "      <td>0.008407</td>\n",
       "      <td>0.019690</td>\n",
       "      <td>0.021903</td>\n",
       "      <td>-0.021903</td>\n",
       "      <td>-0.515708</td>\n",
       "      <td>0.055973</td>\n",
       "      <td>-0.106637</td>\n",
       "      <td>-0.065487</td>\n",
       "      <td>-0.066593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.318142</td>\n",
       "      <td>-0.071239</td>\n",
       "      <td>0.071239</td>\n",
       "      <td>0.075221</td>\n",
       "      <td>0.030973</td>\n",
       "      <td>-0.121681</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>-0.011947</td>\n",
       "      <td>-0.232743</td>\n",
       "      <td>-0.079204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074779</td>\n",
       "      <td>0.061726</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.075221</td>\n",
       "      <td>0.031416</td>\n",
       "      <td>-0.462389</td>\n",
       "      <td>0.109292</td>\n",
       "      <td>-0.053319</td>\n",
       "      <td>-0.012168</td>\n",
       "      <td>-0.013274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.355752</td>\n",
       "      <td>-0.033628</td>\n",
       "      <td>0.108850</td>\n",
       "      <td>0.112832</td>\n",
       "      <td>0.068584</td>\n",
       "      <td>-0.084071</td>\n",
       "      <td>0.117257</td>\n",
       "      <td>0.025664</td>\n",
       "      <td>-0.195133</td>\n",
       "      <td>-0.041593</td>\n",
       "      <td>...</td>\n",
       "      <td>0.112389</td>\n",
       "      <td>0.099336</td>\n",
       "      <td>0.110619</td>\n",
       "      <td>0.112832</td>\n",
       "      <td>0.069027</td>\n",
       "      <td>-0.424779</td>\n",
       "      <td>0.146903</td>\n",
       "      <td>-0.015708</td>\n",
       "      <td>0.025442</td>\n",
       "      <td>0.024336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.186504</td>\n",
       "      <td>-0.202876</td>\n",
       "      <td>-0.060398</td>\n",
       "      <td>-0.056416</td>\n",
       "      <td>-0.100664</td>\n",
       "      <td>-0.253319</td>\n",
       "      <td>-0.051991</td>\n",
       "      <td>-0.143584</td>\n",
       "      <td>-0.364381</td>\n",
       "      <td>-0.210841</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.056858</td>\n",
       "      <td>-0.069912</td>\n",
       "      <td>-0.058628</td>\n",
       "      <td>-0.056416</td>\n",
       "      <td>-0.100221</td>\n",
       "      <td>-0.594027</td>\n",
       "      <td>-0.022345</td>\n",
       "      <td>-0.184956</td>\n",
       "      <td>-0.143805</td>\n",
       "      <td>-0.144912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.288717</td>\n",
       "      <td>-0.100664</td>\n",
       "      <td>0.041814</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>-0.151106</td>\n",
       "      <td>0.050221</td>\n",
       "      <td>-0.041372</td>\n",
       "      <td>-0.262168</td>\n",
       "      <td>-0.108628</td>\n",
       "      <td>...</td>\n",
       "      <td>0.045354</td>\n",
       "      <td>0.032301</td>\n",
       "      <td>0.043584</td>\n",
       "      <td>0.045796</td>\n",
       "      <td>0.001991</td>\n",
       "      <td>-0.491814</td>\n",
       "      <td>0.079867</td>\n",
       "      <td>-0.082743</td>\n",
       "      <td>-0.041593</td>\n",
       "      <td>-0.042699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.442035</td>\n",
       "      <td>0.052655</td>\n",
       "      <td>0.195133</td>\n",
       "      <td>0.199115</td>\n",
       "      <td>0.154867</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>0.203540</td>\n",
       "      <td>0.111947</td>\n",
       "      <td>-0.108850</td>\n",
       "      <td>0.044690</td>\n",
       "      <td>...</td>\n",
       "      <td>0.198673</td>\n",
       "      <td>0.185619</td>\n",
       "      <td>0.196903</td>\n",
       "      <td>0.199115</td>\n",
       "      <td>0.155310</td>\n",
       "      <td>-0.338496</td>\n",
       "      <td>0.233186</td>\n",
       "      <td>0.070575</td>\n",
       "      <td>0.111726</td>\n",
       "      <td>0.110619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.318142</td>\n",
       "      <td>-0.071239</td>\n",
       "      <td>0.071239</td>\n",
       "      <td>0.075221</td>\n",
       "      <td>0.030973</td>\n",
       "      <td>-0.121681</td>\n",
       "      <td>0.079646</td>\n",
       "      <td>-0.011947</td>\n",
       "      <td>-0.232743</td>\n",
       "      <td>-0.079204</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074779</td>\n",
       "      <td>0.061726</td>\n",
       "      <td>0.073009</td>\n",
       "      <td>0.075221</td>\n",
       "      <td>0.031416</td>\n",
       "      <td>-0.462389</td>\n",
       "      <td>0.109292</td>\n",
       "      <td>-0.053319</td>\n",
       "      <td>-0.012168</td>\n",
       "      <td>-0.013274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.287168</td>\n",
       "      <td>-0.102212</td>\n",
       "      <td>0.040265</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.152655</td>\n",
       "      <td>0.048673</td>\n",
       "      <td>-0.042920</td>\n",
       "      <td>-0.263717</td>\n",
       "      <td>-0.110177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>0.030752</td>\n",
       "      <td>0.042035</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-0.493363</td>\n",
       "      <td>0.078319</td>\n",
       "      <td>-0.084292</td>\n",
       "      <td>-0.043142</td>\n",
       "      <td>-0.044248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.243363</td>\n",
       "      <td>-0.146018</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-0.043805</td>\n",
       "      <td>-0.196460</td>\n",
       "      <td>0.004867</td>\n",
       "      <td>-0.086726</td>\n",
       "      <td>-0.307522</td>\n",
       "      <td>-0.153982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.013053</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>0.000442</td>\n",
       "      <td>-0.043363</td>\n",
       "      <td>-0.537168</td>\n",
       "      <td>0.034513</td>\n",
       "      <td>-0.128097</td>\n",
       "      <td>-0.086947</td>\n",
       "      <td>-0.088053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.256416</td>\n",
       "      <td>-0.132965</td>\n",
       "      <td>0.009513</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>-0.030752</td>\n",
       "      <td>-0.183407</td>\n",
       "      <td>0.017920</td>\n",
       "      <td>-0.073673</td>\n",
       "      <td>-0.294469</td>\n",
       "      <td>-0.140929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011283</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>-0.030310</td>\n",
       "      <td>-0.524115</td>\n",
       "      <td>0.047566</td>\n",
       "      <td>-0.115044</td>\n",
       "      <td>-0.073894</td>\n",
       "      <td>-0.075000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.245133</td>\n",
       "      <td>-0.144248</td>\n",
       "      <td>-0.001770</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>-0.042035</td>\n",
       "      <td>-0.194690</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>-0.084956</td>\n",
       "      <td>-0.305752</td>\n",
       "      <td>-0.152212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001770</td>\n",
       "      <td>-0.011283</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002212</td>\n",
       "      <td>-0.041593</td>\n",
       "      <td>-0.535398</td>\n",
       "      <td>0.036283</td>\n",
       "      <td>-0.126327</td>\n",
       "      <td>-0.085177</td>\n",
       "      <td>-0.086283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.242920</td>\n",
       "      <td>-0.146460</td>\n",
       "      <td>-0.003982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.044248</td>\n",
       "      <td>-0.196903</td>\n",
       "      <td>0.004425</td>\n",
       "      <td>-0.087168</td>\n",
       "      <td>-0.307965</td>\n",
       "      <td>-0.154425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>-0.013496</td>\n",
       "      <td>-0.002212</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.043805</td>\n",
       "      <td>-0.537611</td>\n",
       "      <td>0.034071</td>\n",
       "      <td>-0.128540</td>\n",
       "      <td>-0.087389</td>\n",
       "      <td>-0.088496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.286726</td>\n",
       "      <td>-0.102655</td>\n",
       "      <td>0.039823</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>-0.153097</td>\n",
       "      <td>0.048230</td>\n",
       "      <td>-0.043363</td>\n",
       "      <td>-0.264159</td>\n",
       "      <td>-0.110619</td>\n",
       "      <td>...</td>\n",
       "      <td>0.043363</td>\n",
       "      <td>0.030310</td>\n",
       "      <td>0.041593</td>\n",
       "      <td>0.043805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.493805</td>\n",
       "      <td>0.077876</td>\n",
       "      <td>-0.084735</td>\n",
       "      <td>-0.043584</td>\n",
       "      <td>-0.044690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.780531</td>\n",
       "      <td>0.391150</td>\n",
       "      <td>0.533628</td>\n",
       "      <td>0.537611</td>\n",
       "      <td>0.493363</td>\n",
       "      <td>0.340708</td>\n",
       "      <td>0.542035</td>\n",
       "      <td>0.450442</td>\n",
       "      <td>0.229646</td>\n",
       "      <td>0.383186</td>\n",
       "      <td>...</td>\n",
       "      <td>0.537168</td>\n",
       "      <td>0.524115</td>\n",
       "      <td>0.535398</td>\n",
       "      <td>0.537611</td>\n",
       "      <td>0.493805</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571681</td>\n",
       "      <td>0.409071</td>\n",
       "      <td>0.450221</td>\n",
       "      <td>0.449115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.208850</td>\n",
       "      <td>-0.180531</td>\n",
       "      <td>-0.038053</td>\n",
       "      <td>-0.034071</td>\n",
       "      <td>-0.078319</td>\n",
       "      <td>-0.230973</td>\n",
       "      <td>-0.029646</td>\n",
       "      <td>-0.121239</td>\n",
       "      <td>-0.342035</td>\n",
       "      <td>-0.188496</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.034513</td>\n",
       "      <td>-0.047566</td>\n",
       "      <td>-0.036283</td>\n",
       "      <td>-0.034071</td>\n",
       "      <td>-0.077876</td>\n",
       "      <td>-0.571681</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.162611</td>\n",
       "      <td>-0.121460</td>\n",
       "      <td>-0.122566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.371460</td>\n",
       "      <td>-0.017920</td>\n",
       "      <td>0.124558</td>\n",
       "      <td>0.128540</td>\n",
       "      <td>0.084292</td>\n",
       "      <td>-0.068363</td>\n",
       "      <td>0.132965</td>\n",
       "      <td>0.041372</td>\n",
       "      <td>-0.179425</td>\n",
       "      <td>-0.025885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.128097</td>\n",
       "      <td>0.115044</td>\n",
       "      <td>0.126327</td>\n",
       "      <td>0.128540</td>\n",
       "      <td>0.084735</td>\n",
       "      <td>-0.409071</td>\n",
       "      <td>0.162611</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.041150</td>\n",
       "      <td>0.040044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.330310</td>\n",
       "      <td>-0.059071</td>\n",
       "      <td>0.083407</td>\n",
       "      <td>0.087389</td>\n",
       "      <td>0.043142</td>\n",
       "      <td>-0.109513</td>\n",
       "      <td>0.091814</td>\n",
       "      <td>0.000221</td>\n",
       "      <td>-0.220575</td>\n",
       "      <td>-0.067035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086947</td>\n",
       "      <td>0.073894</td>\n",
       "      <td>0.085177</td>\n",
       "      <td>0.087389</td>\n",
       "      <td>0.043584</td>\n",
       "      <td>-0.450221</td>\n",
       "      <td>0.121460</td>\n",
       "      <td>-0.041150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.331416</td>\n",
       "      <td>-0.057965</td>\n",
       "      <td>0.084513</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.044248</td>\n",
       "      <td>-0.108407</td>\n",
       "      <td>0.092920</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>-0.219469</td>\n",
       "      <td>-0.065929</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088053</td>\n",
       "      <td>0.075000</td>\n",
       "      <td>0.086283</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.044690</td>\n",
       "      <td>-0.449115</td>\n",
       "      <td>0.122566</td>\n",
       "      <td>-0.040044</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "0   0.000000 -0.389381 -0.246903 -0.242920 -0.287168 -0.439823 -0.238496   \n",
       "1   0.389381  0.000000  0.142478  0.146460  0.102212 -0.050442  0.150885   \n",
       "2   0.246903 -0.142478  0.000000  0.003982 -0.040265 -0.192920  0.008407   \n",
       "3   0.242920 -0.146460 -0.003982  0.000000 -0.044248 -0.196903  0.004425   \n",
       "4   0.287168 -0.102212  0.040265  0.044248  0.000000 -0.152655  0.048673   \n",
       "5   0.439823  0.050442  0.192920  0.196903  0.152655  0.000000  0.201327   \n",
       "6   0.238496 -0.150885 -0.008407 -0.004425 -0.048673 -0.201327  0.000000   \n",
       "7   0.330088 -0.059292  0.083186  0.087168  0.042920 -0.109735  0.091593   \n",
       "8   0.550885  0.161504  0.303982  0.307965  0.263717  0.111062  0.312389   \n",
       "9   0.397345  0.007965  0.150442  0.154425  0.110177 -0.042478  0.158850   \n",
       "10  0.284956 -0.104425  0.038053  0.042035 -0.002212 -0.154867  0.046460   \n",
       "11  0.325221 -0.064159  0.078319  0.082301  0.038053 -0.114602  0.086726   \n",
       "12  0.285177 -0.104204  0.038274  0.042257 -0.001991 -0.154646  0.046681   \n",
       "13  0.187611 -0.201770 -0.059292 -0.055310 -0.099558 -0.252212 -0.050885   \n",
       "14  0.378097 -0.011283  0.131195  0.135177  0.090929 -0.061726  0.139602   \n",
       "15  0.263717 -0.125664  0.016814  0.020796 -0.023451 -0.176106  0.025221   \n",
       "16  0.285398 -0.103982  0.038496  0.042478 -0.001770 -0.154425  0.046903   \n",
       "17  0.260398 -0.128982  0.013496  0.017478 -0.026770 -0.179425  0.021903   \n",
       "18  0.330973 -0.058407  0.084071  0.088053  0.043805 -0.108850  0.092478   \n",
       "19  0.220354 -0.169027 -0.026549 -0.022566 -0.066814 -0.219469 -0.018142   \n",
       "20  0.132301 -0.257080 -0.114602 -0.110619 -0.154867 -0.307522 -0.106195   \n",
       "21  0.219912 -0.169469 -0.026991 -0.023009 -0.067257 -0.219912 -0.018584   \n",
       "22  0.348009 -0.041372  0.101106  0.105088  0.060841 -0.091814  0.109513   \n",
       "23  0.390708  0.001327  0.143805  0.147788  0.103540 -0.049115  0.152212   \n",
       "24  0.440265  0.050885  0.193363  0.197345  0.153097  0.000442  0.201770   \n",
       "25  0.551770  0.162389  0.304867  0.308850  0.264602  0.111947  0.313274   \n",
       "26  0.165044 -0.224336 -0.081858 -0.077876 -0.122124 -0.274779 -0.073451   \n",
       "27  0.264823 -0.124558  0.017920  0.021903 -0.022345 -0.175000  0.026327   \n",
       "28  0.318142 -0.071239  0.071239  0.075221  0.030973 -0.121681  0.079646   \n",
       "29  0.355752 -0.033628  0.108850  0.112832  0.068584 -0.084071  0.117257   \n",
       "30  0.186504 -0.202876 -0.060398 -0.056416 -0.100664 -0.253319 -0.051991   \n",
       "31  0.288717 -0.100664  0.041814  0.045796  0.001549 -0.151106  0.050221   \n",
       "32  0.442035  0.052655  0.195133  0.199115  0.154867  0.002212  0.203540   \n",
       "33  0.318142 -0.071239  0.071239  0.075221  0.030973 -0.121681  0.079646   \n",
       "34  0.287168 -0.102212  0.040265  0.044248  0.000000 -0.152655  0.048673   \n",
       "35  0.243363 -0.146018 -0.003540  0.000442 -0.043805 -0.196460  0.004867   \n",
       "36  0.256416 -0.132965  0.009513  0.013496 -0.030752 -0.183407  0.017920   \n",
       "37  0.245133 -0.144248 -0.001770  0.002212 -0.042035 -0.194690  0.006637   \n",
       "38  0.242920 -0.146460 -0.003982  0.000000 -0.044248 -0.196903  0.004425   \n",
       "39  0.286726 -0.102655  0.039823  0.043805 -0.000442 -0.153097  0.048230   \n",
       "40  0.780531  0.391150  0.533628  0.537611  0.493363  0.340708  0.542035   \n",
       "41  0.208850 -0.180531 -0.038053 -0.034071 -0.078319 -0.230973 -0.029646   \n",
       "42  0.371460 -0.017920  0.124558  0.128540  0.084292 -0.068363  0.132965   \n",
       "43  0.330310 -0.059071  0.083407  0.087389  0.043142 -0.109513  0.091814   \n",
       "44  0.331416 -0.057965  0.084513  0.088496  0.044248 -0.108407  0.092920   \n",
       "\n",
       "          7         8         9   ...        35        36        37        38  \\\n",
       "0  -0.330088 -0.550885 -0.397345  ... -0.243363 -0.256416 -0.245133 -0.242920   \n",
       "1   0.059292 -0.161504 -0.007965  ...  0.146018  0.132965  0.144248  0.146460   \n",
       "2  -0.083186 -0.303982 -0.150442  ...  0.003540 -0.009513  0.001770  0.003982   \n",
       "3  -0.087168 -0.307965 -0.154425  ... -0.000442 -0.013496 -0.002212  0.000000   \n",
       "4  -0.042920 -0.263717 -0.110177  ...  0.043805  0.030752  0.042035  0.044248   \n",
       "5   0.109735 -0.111062  0.042478  ...  0.196460  0.183407  0.194690  0.196903   \n",
       "6  -0.091593 -0.312389 -0.158850  ... -0.004867 -0.017920 -0.006637 -0.004425   \n",
       "7   0.000000 -0.220796 -0.067257  ...  0.086726  0.073673  0.084956  0.087168   \n",
       "8   0.220796  0.000000  0.153540  ...  0.307522  0.294469  0.305752  0.307965   \n",
       "9   0.067257 -0.153540  0.000000  ...  0.153982  0.140929  0.152212  0.154425   \n",
       "10 -0.045133 -0.265929 -0.112389  ...  0.041593  0.028540  0.039823  0.042035   \n",
       "11 -0.004867 -0.225664 -0.072124  ...  0.081858  0.068805  0.080088  0.082301   \n",
       "12 -0.044912 -0.265708 -0.112168  ...  0.041814  0.028761  0.040044  0.042257   \n",
       "13 -0.142478 -0.363274 -0.209735  ... -0.055752 -0.068805 -0.057522 -0.055310   \n",
       "14  0.048009 -0.172788 -0.019248  ...  0.134735  0.121681  0.132965  0.135177   \n",
       "15 -0.066372 -0.287168 -0.133628  ...  0.020354  0.007301  0.018584  0.020796   \n",
       "16 -0.044690 -0.265487 -0.111947  ...  0.042035  0.028982  0.040265  0.042478   \n",
       "17 -0.069690 -0.290487 -0.136947  ...  0.017035  0.003982  0.015265  0.017478   \n",
       "18  0.000885 -0.219912 -0.066372  ...  0.087611  0.074558  0.085841  0.088053   \n",
       "19 -0.109735 -0.330531 -0.176991  ... -0.023009 -0.036062 -0.024779 -0.022566   \n",
       "20 -0.197788 -0.418584 -0.265044  ... -0.111062 -0.124115 -0.112832 -0.110619   \n",
       "21 -0.110177 -0.330973 -0.177434  ... -0.023451 -0.036504 -0.025221 -0.023009   \n",
       "22  0.017920 -0.202876 -0.049336  ...  0.104646  0.091593  0.102876  0.105088   \n",
       "23  0.060619 -0.160177 -0.006637  ...  0.147345  0.134292  0.145575  0.147788   \n",
       "24  0.110177 -0.110619  0.042920  ...  0.196903  0.183850  0.195133  0.197345   \n",
       "25  0.221681  0.000885  0.154425  ...  0.308407  0.295354  0.306637  0.308850   \n",
       "26 -0.165044 -0.385841 -0.232301  ... -0.078319 -0.091372 -0.080088 -0.077876   \n",
       "27 -0.065265 -0.286062 -0.132522  ...  0.021460  0.008407  0.019690  0.021903   \n",
       "28 -0.011947 -0.232743 -0.079204  ...  0.074779  0.061726  0.073009  0.075221   \n",
       "29  0.025664 -0.195133 -0.041593  ...  0.112389  0.099336  0.110619  0.112832   \n",
       "30 -0.143584 -0.364381 -0.210841  ... -0.056858 -0.069912 -0.058628 -0.056416   \n",
       "31 -0.041372 -0.262168 -0.108628  ...  0.045354  0.032301  0.043584  0.045796   \n",
       "32  0.111947 -0.108850  0.044690  ...  0.198673  0.185619  0.196903  0.199115   \n",
       "33 -0.011947 -0.232743 -0.079204  ...  0.074779  0.061726  0.073009  0.075221   \n",
       "34 -0.042920 -0.263717 -0.110177  ...  0.043805  0.030752  0.042035  0.044248   \n",
       "35 -0.086726 -0.307522 -0.153982  ...  0.000000 -0.013053 -0.001770  0.000442   \n",
       "36 -0.073673 -0.294469 -0.140929  ...  0.013053  0.000000  0.011283  0.013496   \n",
       "37 -0.084956 -0.305752 -0.152212  ...  0.001770 -0.011283  0.000000  0.002212   \n",
       "38 -0.087168 -0.307965 -0.154425  ... -0.000442 -0.013496 -0.002212  0.000000   \n",
       "39 -0.043363 -0.264159 -0.110619  ...  0.043363  0.030310  0.041593  0.043805   \n",
       "40  0.450442  0.229646  0.383186  ...  0.537168  0.524115  0.535398  0.537611   \n",
       "41 -0.121239 -0.342035 -0.188496  ... -0.034513 -0.047566 -0.036283 -0.034071   \n",
       "42  0.041372 -0.179425 -0.025885  ...  0.128097  0.115044  0.126327  0.128540   \n",
       "43  0.000221 -0.220575 -0.067035  ...  0.086947  0.073894  0.085177  0.087389   \n",
       "44  0.001327 -0.219469 -0.065929  ...  0.088053  0.075000  0.086283  0.088496   \n",
       "\n",
       "          39        40        41        42        43        44  \n",
       "0  -0.286726 -0.780531 -0.208850 -0.371460 -0.330310 -0.331416  \n",
       "1   0.102655 -0.391150  0.180531  0.017920  0.059071  0.057965  \n",
       "2  -0.039823 -0.533628  0.038053 -0.124558 -0.083407 -0.084513  \n",
       "3  -0.043805 -0.537611  0.034071 -0.128540 -0.087389 -0.088496  \n",
       "4   0.000442 -0.493363  0.078319 -0.084292 -0.043142 -0.044248  \n",
       "5   0.153097 -0.340708  0.230973  0.068363  0.109513  0.108407  \n",
       "6  -0.048230 -0.542035  0.029646 -0.132965 -0.091814 -0.092920  \n",
       "7   0.043363 -0.450442  0.121239 -0.041372 -0.000221 -0.001327  \n",
       "8   0.264159 -0.229646  0.342035  0.179425  0.220575  0.219469  \n",
       "9   0.110619 -0.383186  0.188496  0.025885  0.067035  0.065929  \n",
       "10 -0.001770 -0.495575  0.076106 -0.086504 -0.045354 -0.046460  \n",
       "11  0.038496 -0.455310  0.116372 -0.046239 -0.005088 -0.006195  \n",
       "12 -0.001549 -0.495354  0.076327 -0.086283 -0.045133 -0.046239  \n",
       "13 -0.099115 -0.592920 -0.021239 -0.183850 -0.142699 -0.143805  \n",
       "14  0.091372 -0.402434  0.169248  0.006637  0.047788  0.046681  \n",
       "15 -0.023009 -0.516814  0.054867 -0.107743 -0.066593 -0.067699  \n",
       "16 -0.001327 -0.495133  0.076549 -0.086062 -0.044912 -0.046018  \n",
       "17 -0.026327 -0.520133  0.051549 -0.111062 -0.069912 -0.071018  \n",
       "18  0.044248 -0.449558  0.122124 -0.040487  0.000664 -0.000442  \n",
       "19 -0.066372 -0.560177  0.011504 -0.151106 -0.109956 -0.111062  \n",
       "20 -0.154425 -0.648230 -0.076549 -0.239159 -0.198009 -0.199115  \n",
       "21 -0.066814 -0.560619  0.011062 -0.151549 -0.110398 -0.111504  \n",
       "22  0.061283 -0.432522  0.139159 -0.023451  0.017699  0.016593  \n",
       "23  0.103982 -0.389823  0.181858  0.019248  0.060398  0.059292  \n",
       "24  0.153540 -0.340265  0.231416  0.068805  0.109956  0.108850  \n",
       "25  0.265044 -0.228761  0.342920  0.180310  0.221460  0.220354  \n",
       "26 -0.121681 -0.615487 -0.043805 -0.206416 -0.165265 -0.166372  \n",
       "27 -0.021903 -0.515708  0.055973 -0.106637 -0.065487 -0.066593  \n",
       "28  0.031416 -0.462389  0.109292 -0.053319 -0.012168 -0.013274  \n",
       "29  0.069027 -0.424779  0.146903 -0.015708  0.025442  0.024336  \n",
       "30 -0.100221 -0.594027 -0.022345 -0.184956 -0.143805 -0.144912  \n",
       "31  0.001991 -0.491814  0.079867 -0.082743 -0.041593 -0.042699  \n",
       "32  0.155310 -0.338496  0.233186  0.070575  0.111726  0.110619  \n",
       "33  0.031416 -0.462389  0.109292 -0.053319 -0.012168 -0.013274  \n",
       "34  0.000442 -0.493363  0.078319 -0.084292 -0.043142 -0.044248  \n",
       "35 -0.043363 -0.537168  0.034513 -0.128097 -0.086947 -0.088053  \n",
       "36 -0.030310 -0.524115  0.047566 -0.115044 -0.073894 -0.075000  \n",
       "37 -0.041593 -0.535398  0.036283 -0.126327 -0.085177 -0.086283  \n",
       "38 -0.043805 -0.537611  0.034071 -0.128540 -0.087389 -0.088496  \n",
       "39  0.000000 -0.493805  0.077876 -0.084735 -0.043584 -0.044690  \n",
       "40  0.493805  0.000000  0.571681  0.409071  0.450221  0.449115  \n",
       "41 -0.077876 -0.571681  0.000000 -0.162611 -0.121460 -0.122566  \n",
       "42  0.084735 -0.409071  0.162611  0.000000  0.041150  0.040044  \n",
       "43  0.043584 -0.450221  0.121460 -0.041150  0.000000 -0.001106  \n",
       "44  0.044690 -0.449115  0.122566 -0.040044  0.001106  0.000000  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_cont = len(x_contvars)\n",
    "z = dat[dat['market'] == 0][x_contvars].values.transpose()\n",
    "pd.DataFrame((z[:,:,None]*np.ones((D_cont, J[0], J[0]))  - z[:,None,:])[0,:,:]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_nests(data, markets_id, products_id, in_out_id, cont_var, disc_var, N, outside_option = True):\n",
    "    '''\n",
    "    This function creates the nest matrices \\Psi^{gt}, and stack them over g for each t.\n",
    "\n",
    "    Args.\n",
    "        data: a pandas DataFrame\n",
    "        markets_id: a string denoting the column of 'data' containing an enumeration t=0,1,...,T-1 of markets\n",
    "        products_id: a string denoting the column of 'data' containing product codes which uniquely identifies products\n",
    "        columns: a list containing the column names of columns in 'data' from which nest groupings g=0,1,...,G-1 for each market t are to be generated\n",
    "        cont_var: a list of the continuous variables in 'columns'\n",
    "        cont_var_bins: a list containing the number of bins to make for each continuous variable in 'columns'\n",
    "        outside_option: a boolean indicating whether the model is estimated with or without an outside option. Default is set to 'True' i.e. with an outside option.\n",
    "\n",
    "    Returns\n",
    "        Psi: a dictionary of length T of the J[t] by J[t] identity stacked on top of the Psi_g matrices for each market t and each gropuing g\n",
    "        nest_dict: a dictionary of length T of pandas series describing the structure of each nest for each market t and each grouping g\n",
    "        nest_count: a dictionary of length T of (G,) numpy arrays containing the amount of nests in each category g\n",
    "    '''\n",
    "\n",
    "    T = data[markets_id].nunique()\n",
    "    J = np.array([data[data[markets_id] == t][products_id].nunique() for t in np.arange(T)])\n",
    "    \n",
    "    # We include nest on outside vs. inside options. The amount of categories varies if the outside option is included in the analysis.\n",
    "    dat = data.sort_values(by = [markets_id, products_id]) # We sort the data in ascending, first according to market and then according to the product id\n",
    "    \n",
    "    Psi = {}\n",
    "    if OO:\n",
    "        in_out_index = [n for n in np.arange(len(disc_var)) if disc_var[n] == in_out_id][0]\n",
    "        non_in_out_indices = np.array([n for n in np.arange(len(disc_var)) if disc_var[n] != in_out_id])\n",
    "\n",
    "    # Assign nests for products in each market t\n",
    "    for t in np.arange(T):\n",
    "        data_t = dat[dat[markets_id] == t] # Subset data on market t\n",
    "\n",
    "        # Estimate discrete kernels\n",
    "        D_disc = len(disc_var)\n",
    "        K_disc = np.empty((D_disc, J[t], J[t]))\n",
    "        C = np.array(data_t[disc_var].nunique())\n",
    "\n",
    "        for d in np.arange(D_disc):\n",
    "            Indicator = pd.get_dummies(data_t[disc_var[d]]).values.reshape((J[t], C[d]))\n",
    "            K_disc[d,:,:] = Indicator@(Indicator.T)\n",
    "\n",
    "        Psidisc_t = np.einsum('djk,dk->djk', K_disc, 1./(K_disc.sum(axis=1)))\n",
    "            \n",
    "        # Estimate continuous kernels\n",
    "        D_cont = len(cont_var)\n",
    "        IQR = scstat.iqr(data_t[cont_var].values, axis = 0) # Compute interquartile range of each continuous variable\n",
    "        sd = np.std(data_t[cont_var].values, axis = 0) # Compute empirical standard deviation of each continuous variable\n",
    "        h = 0.9*np.fmin(sd, IQR/1.34)/(N**(1/5)) # Use Silverman's rule of thumb for bandwidth estimation for each continuous variable\n",
    "        z = data_t[cont_var].values.transpose()\n",
    "        diff = z[:,:,None]*np.ones((D_cont, J[t], J[t])) - z[:,None,:]\n",
    "        K_cont = np.exp(-(diff**2)/(2*h[:,None,None]))\n",
    "\n",
    "        Psicont_t = np.einsum('djk,dk->djk', K_cont, 1./K_cont.sum(axis=1))\n",
    "\n",
    "        # Stack Psi\n",
    "        D = len([*cont_var, *disc_var]) + 1\n",
    "\n",
    "        if outside_option:\n",
    "            Psi[t] = np.concatenate((np.eye(J[t]).reshape((1,J[t],J[t])), Psidisc_t[in_out_index,:,:].reshape((1,J[t],J[t])), Psicont_t, Psidisc_t[non_in_out_indices,:,:]), axis = 0).reshape((D*J[t], J[t]))\n",
    "        else:\n",
    "            Psi[t] = np.concatenate((np.eye(J[t]).reshape((1,J[t],J[t])), Psicont_t, Psidisc_t), axis = 0).reshape((D*J[t], J[t]))\n",
    "\n",
    "    return Psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "Psi = Create_nests(dat, 'market', 'co', 'in_out', nest_contvars, nest_discvars, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Create_Gamma(Lambda, Psi):\n",
    "    '''\n",
    "    This function \n",
    "    '''\n",
    "\n",
    "    T = len(Psi)\n",
    "    J = np.array([Psi[t].shape[1] for t in np.arange(T)])\n",
    "    \n",
    "    Gamma = {}\n",
    "    lambda0 = np.array([1 - sum(Lambda)])\n",
    "    Lambda_full = np.concatenate((lambda0, Lambda)) # create vector (1- sum(lambda), lambda_1, ..., lambda_G)\n",
    "    D = len(Lambda_full)\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Lambda_long = np.einsum('d,dj->dj', Lambda_full, np.ones((D,J[t]))).reshape((D*J[t],))\n",
    "        Gamma[t] = Lambda_long[:,None]*Psi[t]\n",
    "\n",
    "    return Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda0 = np.ones((G,))/(2*(G+1))\n",
    "Gamma = Create_Gamma(lambda0, Psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model solution\n",
    "\n",
    "Suppose we are evaluating the choice probability function $p_t(\\theta)$ at some parameter vector $\\theta$. While it is possible to solve for the choice probabilities explicitly by numerical maximization, Fosgerau and Nielsen (2021) suggest a contraction mapping approach which is conceptually simpler. Let $u_t=X_t\\beta$ and let $q_t^0$ be an initial guess of the choice probabilities, e.g. $q_t^0\\propto \\exp(X_t\\beta)$. Define further\n",
    "$$\n",
    "a=\\sum_{g:\\lambda_g\\geq 0} \\lambda_g   \\qquad b=\\sum_{g:\\lambda_g<0} |\\lambda_g|.\n",
    "$$\n",
    "\n",
    "The choice probabilities are then updated iteratively as\n",
    "$$\n",
    "q_t^{r} = \\frac{e^{v_t^{r}}}{\\sum_{j\\in \\mathcal J_t} e^{v_{tj}^{r}}},\n",
    "$$\n",
    "where\n",
    "$$\n",
    "v_t^{r} =\\ln q_t^{r-1}+\\left(u_t-\\nabla_q \\Omega_t(q^{r-1}_t|\\lambda)\\right)/(1+b).\n",
    "$$\n",
    "Using the definition of $Z_{gt}$ above, this becomes\n",
    "$$\n",
    "v^r_t=\\ln q_t^{r-1}+\\left(u_t+Z_{t}(q^{r-1})\\lambda-\\ln q_t^{r-1}  \\right)/(1+b) =  \\left( u_t+ b\\ln q^{r-1}_t+Z_{t}(q^{r-1})\\lambda\\right)/(1+b)\n",
    "$$\n",
    "\n",
    "\n",
    "For numerical stability, it can be a good idea to also do max-rescaling of $v^r_t$ at every iteration. The Kullback-Leibler divergence $D_{KL}(p||q)=p'\\ln \\frac{p}{q}$ decays linearly with each iteration,\n",
    "$$\n",
    "D_{KL}(p_t(\\theta)||q_t^{r})\\leq \\frac{a+b}{1+b}D_{KL}(p_t(\\theta)||q^{r-1}_t).\n",
    "$$\n",
    "This is implemeneted in the function \"IPDL_ccp\" below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMEMBER $\\delta$ in Similarity gradient $\\nabla_q \\Omega$ !!!\n",
    "\n",
    "Note that $\\delta_j = \\sum_d \\eta_d (\\psi^d e_j)'\\ln(\\psi^d e_j)$. Hence if $\\varphi^d \\in \\mathbb{R}^J$ has $\\varphi^d_j = \\sum_k \\psi^d_{kj}\\ln(\\psi^d_{kj}) = {\\psi^d_{(j)}}'\\ln(\\psi^d_{(j)})$ such that we have ${\\varphi^d} = (\\psi^d \\circ \\ln(\\psi^d))'\\iota$ and set $\\varphi = (\\varphi^1 \\ldots \\varphi^D)\\in\\mathbb{R}^{J \\times D}$ then we have $\\delta = \\varphi \\eta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def phi_matrix(psi):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(psi)\n",
    "    J = np.array([psi[t].shape[1] for t in np.arange(T)])\n",
    "    G = np.int32(psi[0].shape[0] / J[0] - 1)\n",
    "\n",
    "    phi = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        phi_t = np.empty((J[t], G))\n",
    "        psi_t = psi[t]\n",
    "\n",
    "        for d in np.arange(1,G+1):\n",
    "            psi_d = psi_t[d*J[t]:(d+1)*J[t],:]\n",
    "            phi_t[:,d-1] = (psi_d*np.log(psi_d, out = np.zeros_like(psi_d), where = (psi_d > 0))).sum(axis=0)\n",
    "        \n",
    "        phi[t] = phi_t\n",
    "\n",
    "    return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_ccp(Theta, x, psi, tol = 1.0e-15, maximum_iterations = 1000):\n",
    "    '''\n",
    "    This function finds approximations to the true conditional choice probabilities given parameters.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        tol: tolerated approximation error\n",
    "        maximum_iterations: a no. of maximum iterations which if reached will stop the algorithm\n",
    "\n",
    "    Output\n",
    "        q_1: a dictionary of T numpy arrays (J[t],) of IPDL choice probabilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x) # Number of markets\n",
    "    K = x[0].shape[1] # Number of car characteristics\n",
    "\n",
    "    # Parameters\n",
    "    Beta = Theta[:K]\n",
    "    Lambda = Theta[K:]\n",
    "    G = len(Lambda)  # Number of groups\n",
    "\n",
    "    # Calculate small beta\n",
    "    C_minus = np.array([True if Lambda[g] < 0 else False for g in np.arange(G)])\n",
    "    #print(C_minus) # Find the categories g with negative a negative parameter lambda_g\n",
    "    if C_minus.all() == False:\n",
    "        b = 0\n",
    "    else:    \n",
    "        b = np.abs(Lambda[C_minus]).sum() # sum of absolute value of negative lambda parameters.\n",
    "\n",
    "    # Find the Gamma matrix and \\phi\n",
    "    Gamma = Create_Gamma(Lambda, psi)\n",
    "    Phi = phi_matrix(psi)\n",
    "\n",
    "    u = {t: np.einsum('jk,k->j', x[t], Beta) for t in np.arange(T)} # Calculate linear utilities\n",
    "    q = {t: np.exp(u[t]) / np.exp(u[t]).sum() for t in np.arange(T)}\n",
    "    q0 = q\n",
    "    Epsilon = 1.0e-10\n",
    "\n",
    "    for k in range(maximum_iterations):\n",
    "        q1 = {}\n",
    "        for t in np.arange(T):\n",
    "            # Calculate v\n",
    "            psi_q = np.einsum('cj,j->c', psi[t], q0[t]) # Compute matrix product\n",
    "            log_psiq =  np.log(np.abs(psi_q) + Epsilon) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "            delta = Phi[t]@Lambda\n",
    "            Grad = np.einsum('cj,c->j', Gamma[t], log_psiq) - delta # Compute matrix product\n",
    "            v = np.log(q0[t] + Epsilon) + (u[t] - Grad)/(1 + b) # Calculate v = log(q) + (u - (Gamma^T %o% log(Gamma %o% q) %o% Gamma) - delta)/(1 + b)\n",
    "            v -= v.max(keepdims = True) # Do max rescaling wrt. alternatives\n",
    "\n",
    "            # Calculate iterated ccp q^k\n",
    "            numerator = np.exp(v)\n",
    "            denom = numerator.sum()\n",
    "            q1[t] = numerator/denom\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.array([np.sum((q1[t]-q0[t])**2/q[t]) for t in np.arange(T)])) # Uses logit weights. This avoids precision issues when q1~q0~0.\n",
    "        \n",
    "        if dist<tol:\n",
    "            break\n",
    "        elif k==maximum_iterations:\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        q0 = q1\n",
    "\n",
    "    return q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001526\n",
      "         Iterations: 25\n",
      "         Function evaluations: 29\n",
      "         Gradient evaluations: 29\n"
     ]
    }
   ],
   "source": [
    "beta0 = logit.estimate_logit(logit.q_logit, np.zeros((K,)), y, x, pop_share)['beta']\n",
    "theta0 = np.append(beta0, lambda0)\n",
    "q0 = IPDL_ccp(theta0, x, Psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q1_star = {t: np.ones((J[t],))/J[t] for t in np.arange(T)} # Uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numgrad = {}\n",
    "epsilon = 1.0e-10\n",
    "Phi = phi_matrix(Psi)\n",
    "Gamma = Create_Gamma(theta0[K:], Psi)\n",
    "\n",
    "for t in np.arange(T):\n",
    "    numgrad_t = np.empty((J[t],))\n",
    "    Gamma_t = Gamma[t]\n",
    "    Psi_t = Psi[t]\n",
    "    delta = Phi[t]@theta0[K:]\n",
    "    q0_t = q1_star[t]\n",
    "    Omega0 = (Gamma_t@q0_t)@np.log(Psi_t@q0_t) - q0_t@delta\n",
    "\n",
    "    for j in np.arange(J[t]):\n",
    "        vec = np.zeros((J[t],))\n",
    "        vec[j] = 1\n",
    "        q1_t = q0_t + epsilon*vec\n",
    "\n",
    "        Omega1 = (Gamma_t@q1_t)@np.log(Psi_t@q1_t) - q1_t@delta\n",
    "\n",
    "        numgrad_t[j] = (Omega1 - Omega0) / epsilon\n",
    "    \n",
    "    numgrad[t] = numgrad_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numgrad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "angrad = {}\n",
    "for t in np.arange(T):\n",
    "    psi_q = np.einsum('cj,j->c', Psi[t], q1_star[t]) # Compute matrix product\n",
    "    log_psiq =  np.log(np.abs(psi_q) + epsilon) # Add Epsilon? to avoid zeros in log np.log(np.abs(gamma_q), out = np.NINF*np.ones_like(gamma_q), where = (np.abs(gamma_q) > 0))\n",
    "    delta = Phi[t]@theta0[K:]\n",
    "    angrad[t] = np.einsum('cj,c->j', Gamma[t], log_psiq) - delta + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "angrad[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([la.norm(angrad[t] - numgrad[t]) for t in np.arange(T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute cross-derivative numerically from analytic gradient, and analytically by below formulas respectively\n",
    "\n",
    "$\\nabla_q \\Omega(q) = \\Gamma \\ln(\\psi q) - \\delta + \\iota$ and $Z_g = \\nabla_{q,\\lambda} \\Omega(q)_d = \\ln(q) - {\\psi^d}'\\ln(\\psi^d q) - \\varphi^d$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "epsilon = 1.0e-10\n",
    "numcross = {}\n",
    "for t in np.arange(T):\n",
    "    q0_t = q1_star[t]\n",
    "    Phi_t = Phi[t]\n",
    "    Psi_t = Psi[t]\n",
    "    Gamma_t = Gamma[t]\n",
    "    delta = Phi_t@theta0[K:]\n",
    "    log_psiq = np.log(np.einsum('cj,j->c', Psi_t, q0_t))\n",
    "    cross0 = np.einsum('cj,c->j', Gamma_t, log_psiq) - delta\n",
    "    cross1 = np.empty((J[t],G))\n",
    "\n",
    "    for d in np.arange(G):\n",
    "        vec = np.zeros((G,))\n",
    "        vec[d] = 1\n",
    "        lambda1 = theta0[K:] + epsilon*vec\n",
    "        Gamma1 = Create_Gamma(lambda1, Psi)\n",
    "        delta1 = Phi_t @ lambda1\n",
    "        cross1[:,d] = np.einsum('cj,c->j', Gamma1[t], log_psiq) - delta1\n",
    "\n",
    "    numcross[t] = (cross1 - cross0[:,None])/epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ancross = {}\n",
    "J = np.array([x[t].shape[0] for t in np.arange(T)])\n",
    "for t in np.arange(T):\n",
    "    ancross_t = np.empty((J[t],G))\n",
    "    q0_t = q1_star[t]\n",
    "    Phi_t = Phi[t]\n",
    "    Psi_t = Psi[t]\n",
    "    for d in np.arange(1,G+1):\n",
    "        Psi_d = Psi_t[d*J[t]:(d+1)*J[t],:]\n",
    "\n",
    "        ancross_t[:,d-1] = -np.log(q0_t) + Psi_d.T @ np.log(Psi_d @ q0_t) - Phi_t[:,d-1]\n",
    "\n",
    "    ancross[t] = ancross_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(numcross[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(ancross[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pd.DataFrame(ancross[0]-numcross[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([la.norm(ancross[t] - numcross[t]) for t in np.arange(T)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross_grad_pertubation(q1_star, Psi)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand derivatives and price Elasticity\n",
    "\n",
    "While the demand derivatives in the IPDL model are not quite as simple as in the logit model, they are still easy to compute. \n",
    "Let $q=P(u|\\lambda)$, then\n",
    "$$\n",
    "\\nabla_u P(u|\\lambda)=\\left(\\nabla^2_{qq}\\Omega(q|\\lambda)\\right)^{-1}-qq'\n",
    "$$\n",
    "where the $()^{-1}$ denotes the matrix inverse. The derivatives with respect to any $x_{ij\\ell}$ can now easily be computed by the chain rule,\n",
    "$$\n",
    "    \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{\\partial u_{ik}}{\\partial x_{ik\\ell}}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell,\n",
    "$$\n",
    "\n",
    "Finally, moving to price elasticity is the same as in the logit model, if $x_{ik\\ell}$ is the log price of product $k$ for individual $i$, then\n",
    "$$\n",
    "    \\mathcal{E}_{jk}= \\frac{\\partial P_j(u_i|\\lambda)}{\\partial x_{ik\\ell}}\\frac{1}{P_j(u_i|\\lambda)}=\\frac{\\partial P_j(u_i|\\lambda)}{\\partial u_{ik}}\\frac{1}{P_j(u_i|\\lambda)}\\beta_\\ell=\\frac{\\partial \\ln P_j(u_i|\\lambda)}{\\partial u_{ik}}\\beta_\\ell$$\n",
    "we can also write this compactly as\n",
    "$$\n",
    "\\nabla_u \\ln P(u|\\lambda)=\\mathrm{diag}(P(u|\\lambda))^{-1}\\nabla_u P(u|\\lambda).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pertubation_hessian(q, x, Theta, psi):\n",
    "    '''\n",
    "    This function calucates the hessian of the pertubation function \\Omega\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Hess: a dictionary of T numpy arrays (J[t],J[t]) of second partial derivatives of the pertubation function \\Omega for each market t\n",
    "    '''\n",
    "    \n",
    "    T = len(q.keys())\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    Gamma = Create_Gamma(Theta[K:], psi) # Find the \\Gamma matrices \n",
    "    \n",
    "    Hess={}\n",
    "    for t in np.arange(T):\n",
    "        psi_q = np.einsum('cj,j->c', psi[t], q[t]) # Compute a matrix product\n",
    "        Hess[t] = np.einsum('cj,c,cl->jl', Gamma[t], 1/psi_q, psi[t]) # Computes the product \\Gamma' diag(\\psi q)^{-1} \\psi (but faster)\n",
    "\n",
    "    return Hess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_gradient(q, x, Theta, psi_stack):\n",
    "    \n",
    "    '''\n",
    "    This function calucates the gradient of the choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K) of partial derivatives of the choice proabilities wrt. utilities for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Grad = {}\n",
    "    Hess = compute_pertubation_hessian(q, x, Theta, psi_stack) # Compute the hessian of the pertubation function\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        inv_omega_hess = la.inv(Hess[t]) # (J,J) for each t=1,...,T , computes the inverse of the Hessian\n",
    "        qqT = q[t][:,None]*q[t][None,:] # (J,J) outerproduct of ccp's for each market t\n",
    "        Grad[t] = inv_omega_hess - qqT  # Compute IPDL gradient of ccp's wrt. utilities\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack):\n",
    "    '''\n",
    "    This function calucates the gradient of the log choice proabilities wrt. characteristics\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Epsilon: a dictionary of T numpy arrays (J[t],J[t]) of partial derivatives of the log choice proabilities of products j wrt. utilites of products k for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack) # Find the gradient of ccp's wrt. utilities\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        #ccp_grad = Grad[t]\n",
    "        #inv_diagq = np.divide(1, q[t], out = np.inf*np.ones_like(q[t]), where = (q[t] > 0)) # Find the inverse of the ccp's and assign infinity to any entry if that entry has q = 0\n",
    "        Epsilon[t] = Grad[t]/q[t][:,None] # Computes diag(q)^{-1}Grad[t]\n",
    "        #np.einsum('j,jk->jk', inv_diagq, ccp_grad) # Computes a Hadamard product. Is equivalent to:   diag(q)^-1 %o% ccp_grad\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_elasticity(q, x, Theta, psi_stack, char_number = K-1):\n",
    "    ''' \n",
    "    This function calculates the elasticity of choice probabilities wrt. any characteristic or nest grouping of products\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        char_number: an integer which is an index of the parameter in theta wrt. which we wish calculate the elasticity. Default is the index for the parameter of 'pr'.\n",
    "\n",
    "    Returns\n",
    "        a dictionary of T numpy arrays (J[t],J[t]) of choice probability semi-elasticities for each market t\n",
    "    '''\n",
    "    T = len(q.keys())\n",
    "    Epsilon = {}\n",
    "    Grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack) # Find the gradient of log ccp's wrt. utilities\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        Epsilon[t] = Grad[t]*Theta[char_number] # Calculate semi-elasticities\n",
    "\n",
    "    return Epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum likelihood estimation of IPDL\n",
    "\n",
    "The log-likelihood contribution is\n",
    "$$\n",
    "\\ell_t(\\theta)=y_t'\\ln p(\\mathbf{X}_t,\\theta),\n",
    "$$\n",
    "and an estimation routine must therefore have a function that - given $\\mathbf{X}_t$ and $\\theta$ - calculates $u_t=\\mathbf{X}_t\\beta$ and constructs $\\Gamma$, and then calls the fixed point routine described above. That routine will return $p(\\mathbf{X}_t,\\theta)$, and we can then evaluate $\\ell_t(\\theta)$. Using our above defined functions we now construct precisely such an estimation procedure.\n",
    "\n",
    "For maximizing the likelihood, we want the derivates at some $\\theta=(\\beta',\\lambda')$. Let $q_t=p(\\mathbf{X}_t,\\theta)$, then we have\n",
    "$$\n",
    "\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)=\\mathrm{diag}(q_t)^{-1}\\left(\\nabla_{qq}^2\\Omega(q_t|\\lambda)^{-1}-q_tq_t' \\right)\\left[\\mathbf{X}_t,-\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)\\right]\n",
    "$$\n",
    "Note that the first two components is the elasticity $\\nabla_u \\ln P(u|\\lambda)$ and the last term is a block matrix of size $J\\times dim(\\theta)$. Note that the latter cross derivative $\\nabla_{q,\\lambda}^2 \\Omega(q_t|\\lambda)$ is given by $\\nabla_{q,\\lambda} \\Omega(q_t|\\lambda)_g = \\ln(q) - (\\Psi^g)' \\ln(\\Psi^g q) - \\varphi^d$ for each row $g=1,\\ldots,G$. The derivative of the log-likelihood function can be obtained from this as\n",
    "$$\n",
    "\\nabla_\\theta \\ell_t(\\theta)=\\nabla_\\theta \\ln p(\\mathbf{X}_t,\\theta)' y_t \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack):\n",
    "    ''' \n",
    "    This function computes the loglikehood contribution for each individual i.\n",
    "    \n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Output\n",
    "        ll: a numpy array (T,) of IPDL loglikelihood contributions\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "    K = x[0].shape[1]\n",
    "    ccp_hat = IPDL_ccp(Theta, x, psi_stack)\n",
    "    sum_lambdaplus = np.array([theta for theta in Theta[K:] if theta >0]).sum()\n",
    "\n",
    "    '''if sum_lambdaplus >= 1:\n",
    "        ll = np.NINF*np.ones((T,))'''\n",
    "\n",
    "    \n",
    "    ll=np.empty((T,))\n",
    "    for t in np.arange(T):\n",
    "        ll[t] = sample_share[t]*(y[t].T@np.log(ccp_hat[t])) #np.einsum('j,j', y[t], np.log(ccp_hat[t], out = -np.inf*np.ones_like(ccp_hat[t]), where = (ccp_hat[t] > 0)))\n",
    "\n",
    "    print([sum_lambdaplus, -ll.mean()])\n",
    "\n",
    "    return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL(Theta, y, x, sample_share, psi_stack):\n",
    "    ''' The negative loglikelihood criterion to minimize\n",
    "    '''\n",
    "    Q = -IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack)\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also implement the derivative of the loglikehood wrt. parameters $\\nabla_\\theta \\ell_t(\\theta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_grad_pertubation(q, psi_stack):\n",
    "    ''' \n",
    "    This function calculates the cross diffential of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    \n",
    "    Returns\n",
    "        Z: a dictionary of T numpy arrays (J[t],G) of cross diffentials of the pertubation function \\Omega wrt. first ccp's and then the lambda parameters\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "    J = np.array([q[t].shape[0] for t in np.arange(T)])\n",
    "    G = np.int32((psi_stack[0].shape[0] / J[0]) - 1)\n",
    "\n",
    "    Phi = phi_matrix(psi_stack)\n",
    "    \n",
    "    Z = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        \n",
    "        log_q = np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0))\n",
    "        Psi_t = Psi[t]\n",
    "        Z_t = np.empty((J[t], G))\n",
    "        for d in np.arange(1,G+1):\n",
    "            Psi_d = Psi_t[d*J[t]:(d+1)*J[t],:]\n",
    "            Psiq = np.einsum('cj,j->c', Psi_d, q[t])\n",
    "            log_psiq = np.log(Psiq, out = -np.inf*np.ones_like(Psiq), where = (Psiq > 0))\n",
    "            Z_t[:,d-1] = -log_q + np.einsum('cj,c->j', Psi_d, log_psiq) - Phi[t][:,d-1]\n",
    "\n",
    "        Z[t] = Z_t\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        , -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963,\n",
       "       -3.78418963, -3.78418963, -3.78418963, -3.78418963, -3.78418963])"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Phi = phi_matrix(Psi)\n",
    "Phi[0][:,len(nest_contvars)+2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_theta_grad_log_ccp(Theta, x, psi_stack):\n",
    "    '''\n",
    "    This function calculates the derivative of the IPDL log ccp's wrt. parameters theta\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "    Returns\n",
    "        Grad: a dictionary of T numpy arrays (J[t],K+G) of derivatives of the IPDL log ccp's wrt. parameters theta for each market t\n",
    "    '''\n",
    "\n",
    "    T = len(x.keys())\n",
    "\n",
    "    q = IPDL_ccp(Theta, x, psi_stack) # Find choice probabilities\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack) # Find cross differentials of the pertubation function\n",
    "    u_grad = IPDL_u_grad_Log_ccp(q, x, Theta, psi_stack)  # Find the gradient of log ccp's wrt. utilities\n",
    "    Grad={}\n",
    "\n",
    "    for t in range(T):\n",
    "        G = np.concatenate((x[t], -Z[t]), axis=1)\n",
    "        Grad[t] = u_grad[t] @ G\n",
    "\n",
    "    return Grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_score(Theta, y, x, sample_share, psi_stack):\n",
    "    '''\n",
    "    This function calculates the score of the IPDL loglikelihood.\n",
    "\n",
    "    Args.\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Score: a numpy array (T,K+G) of IPDL scores\n",
    "    '''\n",
    "    T = len(x.keys())\n",
    "\n",
    "    log_ccp_grad = IPDL_theta_grad_log_ccp(Theta, x, psi_stack) # Find derivatives of the IPDL log ccp's wrt. parameters theta\n",
    "    D = log_ccp_grad[0].shape[1] # equal to K+G\n",
    "    Score = np.empty((T,D))\n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        Score[t,:] =sample_share[t]*(log_ccp_grad[t].T@y[t]) #np.einsum('j,jd->d', y[t], log_ccp_grad[t]) # Computes a matrix product\n",
    "\n",
    "    return Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_IPDL_score(Theta, y, x, sample_share, psi_stack):\n",
    "    ''' The derivative of the negative loglikelihood criterion\n",
    "    '''\n",
    "    return -IPDL_score(Theta, y, x, sample_share, psi_stack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_analyticgrad(y, x, theta, sample_share, Psi, delta = 1.0e-8):\n",
    "\n",
    "    numgrad = np.empty((T, K+G))\n",
    "\n",
    "    for i in np.arange(K+G):\n",
    "        vec = np.zeros((K+G,))\n",
    "        vec[i] = 1\n",
    "        numgrad[:,i] = (IPDL_loglikelihood(theta + delta*vec, y, x, sample_share, Psi) - IPDL_loglikelihood(theta, y, x, sample_share, Psi)) / delta\n",
    "\n",
    "    angrad = IPDL_score(theta, y, x, sample_share, Psi)\n",
    "\n",
    "    normdiff = la.norm(angrad - numgrad)\n",
    "    \n",
    "    return normdiff, angrad, normdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.0016471517159650625]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517168120111]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517168152778]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517165527138]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517161612715]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517160877222]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517161166438]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151716662896]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517163931682]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517168221155]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171641808]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172009919]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517169902993]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172058573]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.00164715171714377]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171391214]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171970846]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171984494]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172058842]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517169999565]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517170834288]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171869339]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171983633]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172403404]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172006915]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.00164715171722091]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171836407]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717109342]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171967325]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171710278]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517170800639]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517170807688]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717048378]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717172831]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171953347]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171830626]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717198561]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171947729]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517172003307]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717202232]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717199706]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171956244]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171996167]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171940387]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171792861]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171734941]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171999163]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517167023454]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517167772377]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517168301704]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.001647151717051745]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857142857142, 0.0016471517171016104]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.001647151723259925]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.001647151722056139]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517219190611]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517218235336]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517219798465]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517223013074]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517227211608]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517221485254]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517221158254]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517220643076]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517192025145]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.00164715172199389]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n",
      "[0.4642857242857142, 0.0016471517210732668]\n",
      "[0.4642857142857142, 0.0016471517171985396]\n"
     ]
    }
   ],
   "source": [
    "diff, an, num = test_analyticgrad(y, x, theta0, pop_share, Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1796307345394429e-07"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.08555430e-05,  6.11999636e-06,  4.53420716e-06, ...,\n",
       "        -2.67603355e-05, -8.29359021e-05, -4.95895540e-05],\n",
       "       [ 4.31652052e-05,  1.31785373e-05,  1.04930932e-05, ...,\n",
       "        -4.98394016e-05, -1.18049042e-04, -8.29727509e-05],\n",
       "       [ 1.55935401e-04,  5.80433137e-05,  3.90703502e-05, ...,\n",
       "        -1.70545030e-04, -6.07621693e-04, -6.65908324e-04],\n",
       "       ...,\n",
       "       [ 2.95786191e-04,  1.05478171e-04,  1.23873258e-04, ...,\n",
       "        -5.13041766e-04, -1.35766669e-03, -9.83906420e-04],\n",
       "       [ 2.02652241e-04,  5.73021459e-05,  6.34255856e-05, ...,\n",
       "        -2.94564533e-04, -8.68285959e-04, -7.86481381e-04],\n",
       "       [ 1.87161499e-04,  5.97493288e-05,  7.06001950e-05, ...,\n",
       "        -2.78533800e-04, -9.05125962e-04, -8.70216939e-04]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "an - num"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard errors in Maximum Likelihood estimation\n",
    "\n",
    "As usual we may consistently estimate the Covariance Matrix  of the IPDL maximum likelihood estimator for some estimate $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'\\in \\mathbb{R}^{K+G}$ as:\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma = \\left( \\sum_{i=1}^N \\nabla_\\theta \\ell_i (\\hat \\theta) \\nabla_\\theta \\ell_i (\\hat \\theta)' \\right)^{-1}\n",
    "$$\n",
    "\n",
    "Thereby we may find the estimated standard error of parameter $d$ as the squareroot of the d'th diagonal entry of $\\hat \\Sigma$:\n",
    "\n",
    "$$\n",
    "\\hat \\sigma_d = \\sqrt{\\hat \\Sigma_{dd}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_se(score, N):\n",
    "    '''\n",
    "    This function computes the asymptotic standard errors of the MLE.\n",
    "\n",
    "    Args.\n",
    "        score: a numpy array (T,K+G) of IPDL scores\n",
    "        N: an integer giving the number of observations\n",
    "\n",
    "    Returns\n",
    "        SE: a numpy array (K+G,) of asymptotic IPDL MLE standard errors\n",
    "    '''\n",
    "\n",
    "    SE = np.sqrt(np.diag(la.inv(np.einsum('td,tm->dm', score, score))) / N)\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_t_p(SE, Theta, N, Theta_hypothesis = 0):\n",
    "    ''' \n",
    "    This function calculates t statistics and p values for characteristic and nest grouping parameters\n",
    "\n",
    "    Args.\n",
    "        SE: a numpy array (K+G,) of asymptotic IPDL MLE standard errors\n",
    "        Theta: a numpy array (K+G,) of parameters of (\\beta', \\lambda')',\n",
    "        N: an integer giving the number of observations\n",
    "        Theta_hypothesis: a (K+G,) array or integer of parameter values to test in t-test. Default value is 0.\n",
    "    \n",
    "    Returns\n",
    "        T: a (K+G,) array of estimated t tests\n",
    "        p: a (K+G,) array of estimated asymptotic p values computed using the above t-tests\n",
    "    '''\n",
    "\n",
    "    T = np.abs(Theta - Theta_hypothesis) / SE\n",
    "    p = 2*scstat.t.sf(T, df = N-1)\n",
    "\n",
    "    return T,p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now estimate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_IPDL(f, Theta0, y, x, sample_share, psi_stack, N, Analytic_jac:bool = True, options = {'disp': True}, **kwargs):\n",
    "    ''' \n",
    "    Takes a function and returns the minimum, given starting values and variables necessary in the IPDL model specification.\n",
    "\n",
    "    Args:\n",
    "        f: a function to minimize,\n",
    "        Theta0 : a numpy array (K+G,) of initial guess parameters (\\beta', \\lambda')',\n",
    "        y: a dictionary of T numpy arrays (J[t],) of observed market shares in onehot encoding for each market t,\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t,\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests', \n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t,\n",
    "        N: an integer giving the number of observations,\n",
    "        Analytic_jac: a boolean. Default value is 'True'. If 'True' the analytic jacobian of the IPDL loglikelihood function is used in estimation. Else the numerical jacobian is used.\n",
    "        options: dictionary with options for the optimizer (e.g. disp=True which tells it to display information at termination.)\n",
    "    \n",
    "    Returns:\n",
    "        res: a dictionary with results from the estimation.\n",
    "    '''\n",
    "\n",
    "    # The objective function is the average of q(), \n",
    "    # but Q is only a function of one variable, theta, \n",
    "    # which is what minimize() will expect\n",
    "    Q = lambda Theta: np.mean(f(Theta, y, x, sample_share, psi_stack))\n",
    "\n",
    "    if Analytic_jac == True:\n",
    "        Grad = lambda Theta: np.mean(q_IPDL_score(Theta, y, x, sample_share, psi_stack), axis=0) # Finds the Jacobian of Q. Takes mean of criterion q derivatives along axis=0, i.e. the mean across individuals.\n",
    "    else:\n",
    "        Grad = None\n",
    "\n",
    "    # call optimizer\n",
    "    result = optimize.minimize(Q, Theta0.tolist(), options=options, jac=Grad, **kwargs) # optimize.minimize takes a list of parameters Theta0 (not a numpy array) as initial guess.\n",
    "    se = IPDL_se(IPDL_score(result.x, y, x, sample_share, psi_stack), N)\n",
    "    T,p = IPDL_t_p(se, result.x, N)\n",
    "\n",
    "    # collect output in a dict \n",
    "    res = {\n",
    "        'theta': result.x,\n",
    "        'se': se,\n",
    "        't': T,\n",
    "        'p': p,\n",
    "        'success':  result.success, # bool, whether convergence was succesful 1\n",
    "        'nit':      result.nit, # no. algorithm iterations \n",
    "        'nfev':     result.nfev, # no. function evaluations \n",
    "        'fun':      result.fun # function value at termination \n",
    "    }\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001529\n",
      "         Iterations: 26\n",
      "         Function evaluations: 34\n",
      "         Gradient evaluations: 34\n"
     ]
    }
   ],
   "source": [
    "beta_0 = np.ones((K,))\n",
    "\n",
    "# Estimate the model\n",
    "Logit_beta = logit.estimate_logit(logit.q_logit, beta_0, y, x, sample_share=pop_share, Analytic_jac=True)['beta']\n",
    "Logit_SE = logit.logit_se(logit.logit_score(Logit_beta, y, x, pop_share), N)\n",
    "Logit_t, Logit_p = logit.logit_t_p(Logit_beta, logit.logit_score(Logit_beta, y, x, pop_share), N)\n",
    "\n",
    "# Initialize \\theta^0\n",
    "theta0 = np.append(Logit_beta,lambda0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4642857142857142, 0.001651690244391444]\n",
      "[0.45807927070581517, 0.0016485611056047584]\n",
      "[0.4332534963862184, 0.001636459192830168]\n",
      "[0.3339503991078314, 0.0015941905340232736]\n",
      "[0.03881379022735937, 0.0015285970161815269]\n",
      "[0.03279339648803796, 0.0015285650405973353]\n",
      "[0.03430005055597402, 0.0015285573566613637]\n",
      "[0.03452447012201116, 0.0015285552522675612]\n",
      "[0.035422148386159695, 0.0015285490170700072]\n",
      "[0.0361327554350772, 0.0015285333984966008]\n",
      "[0.036842904570052674, 0.0015285032542644776]\n",
      "[0.041749067794569925, 0.0015284106106405507]\n",
      "[0.05136920020809256, 0.001528203668060995]\n",
      "[0.06943459772837621, 0.0015278361604085062]\n",
      "[0.10260316015540435, 0.0015271755704751061]\n",
      "[0.15999128515574787, 0.0015260137508878876]\n",
      "[0.2640469359883095, 0.001524127500650879]\n",
      "[0.43416156207891726, 0.001522044988125294]\n",
      "[0.47328708436917266, 0.0015213441480257362]\n",
      "[0.4636624201081757, 0.0015208775251473465]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.001521\n",
      "         Iterations: 15\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 20\n"
     ]
    }
   ],
   "source": [
    "resMLE = estimate_IPDL(q_IPDL, theta0, y, x, pop_share, Psi, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_table(theta,se,N,x_vars,nest_vars):\n",
    "    IPDL_t, IPDL_p = IPDL_t_p(se, theta, N)\n",
    "\n",
    "    if OO:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "    else:\n",
    "        regdex = [*x_vars, *['group_' + var for var in nest_vars]]\n",
    "\n",
    "    table  = pd.DataFrame({'theta': [ str(np.round(theta[i], decimals = 4)) + '***' if IPDL_p[i] <0.01 else str(np.round(theta[i], decimals = 3)) + '**' if IPDL_p[i] <0.05 else str(np.round(theta[i], decimals = 3)) + '*' if IPDL_p[i] <0.1 else np.round(theta[i], decimals = 3) for i in range(len(theta))], \n",
    "                'se' : np.round(se, decimals = 5),\n",
    "                't (theta == 0)': np.round(IPDL_t, decimals = 3),\n",
    "                'p': np.round(IPDL_p, decimals = 3)}, index = regdex).rename_axis(columns = 'variables')\n",
    "    \n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.545</td>\n",
       "      <td>5.43585</td>\n",
       "      <td>0.468</td>\n",
       "      <td>0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.318</td>\n",
       "      <td>2.79781</td>\n",
       "      <td>0.114</td>\n",
       "      <td>0.910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.457</td>\n",
       "      <td>3.15701</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.945</td>\n",
       "      <td>2.89147</td>\n",
       "      <td>0.327</td>\n",
       "      <td>0.744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.953</td>\n",
       "      <td>4.01604</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.093</td>\n",
       "      <td>5.26651</td>\n",
       "      <td>0.397</td>\n",
       "      <td>0.691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.081</td>\n",
       "      <td>3.92949</td>\n",
       "      <td>0.529</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.742</td>\n",
       "      <td>1.60313</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.39</td>\n",
       "      <td>3.13216</td>\n",
       "      <td>0.444</td>\n",
       "      <td>0.657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>-0.054</td>\n",
       "      <td>1.33363</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.844</td>\n",
       "      <td>1.09137</td>\n",
       "      <td>0.773</td>\n",
       "      <td>0.439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>0.994</td>\n",
       "      <td>2.79930</td>\n",
       "      <td>0.355</td>\n",
       "      <td>0.723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>1.05</td>\n",
       "      <td>1.61546</td>\n",
       "      <td>0.650</td>\n",
       "      <td>0.516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>0.798</td>\n",
       "      <td>1.41541</td>\n",
       "      <td>0.564</td>\n",
       "      <td>0.573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>0.822</td>\n",
       "      <td>1.68888</td>\n",
       "      <td>0.487</td>\n",
       "      <td>0.627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>0.725</td>\n",
       "      <td>1.59168</td>\n",
       "      <td>0.456</td>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>0.952</td>\n",
       "      <td>2.24413</td>\n",
       "      <td>0.424</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>0.971</td>\n",
       "      <td>3.34418</td>\n",
       "      <td>0.290</td>\n",
       "      <td>0.772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>0.909</td>\n",
       "      <td>1.88158</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.825</td>\n",
       "      <td>1.53840</td>\n",
       "      <td>0.536</td>\n",
       "      <td>0.592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>1.052</td>\n",
       "      <td>1.45670</td>\n",
       "      <td>0.722</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>0.866</td>\n",
       "      <td>1.92154</td>\n",
       "      <td>0.451</td>\n",
       "      <td>0.652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>0.919</td>\n",
       "      <td>2.96360</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>0.926</td>\n",
       "      <td>1.50023</td>\n",
       "      <td>0.617</td>\n",
       "      <td>0.537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>0.96</td>\n",
       "      <td>2.64242</td>\n",
       "      <td>0.363</td>\n",
       "      <td>0.716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>0.636</td>\n",
       "      <td>1.49284</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>0.834</td>\n",
       "      <td>2.44943</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.947</td>\n",
       "      <td>1.96351</td>\n",
       "      <td>0.482</td>\n",
       "      <td>0.630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>0.885</td>\n",
       "      <td>1.74563</td>\n",
       "      <td>0.507</td>\n",
       "      <td>0.612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>0.906</td>\n",
       "      <td>2.33524</td>\n",
       "      <td>0.388</td>\n",
       "      <td>0.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>0.933</td>\n",
       "      <td>1.51614</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.858</td>\n",
       "      <td>1.41034</td>\n",
       "      <td>0.608</td>\n",
       "      <td>0.543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>1.031</td>\n",
       "      <td>1.45805</td>\n",
       "      <td>0.707</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>0.525</td>\n",
       "      <td>1.49643</td>\n",
       "      <td>0.351</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>0.872</td>\n",
       "      <td>2.82575</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>0.806</td>\n",
       "      <td>1.86495</td>\n",
       "      <td>0.432</td>\n",
       "      <td>0.666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>0.999</td>\n",
       "      <td>11.98206</td>\n",
       "      <td>0.083</td>\n",
       "      <td>0.934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>0.972</td>\n",
       "      <td>4.49771</td>\n",
       "      <td>0.216</td>\n",
       "      <td>0.829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>0.989</td>\n",
       "      <td>3.19239</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>0.947</td>\n",
       "      <td>1.74859</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>0.993</td>\n",
       "      <td>2.28668</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>0.967</td>\n",
       "      <td>1.59364</td>\n",
       "      <td>0.607</td>\n",
       "      <td>0.544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>0.998</td>\n",
       "      <td>3.58772</td>\n",
       "      <td>0.278</td>\n",
       "      <td>0.781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>0.872</td>\n",
       "      <td>1.31756</td>\n",
       "      <td>0.662</td>\n",
       "      <td>0.508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>0.875</td>\n",
       "      <td>2.03354</td>\n",
       "      <td>0.430</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>0.803</td>\n",
       "      <td>1.76517</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>0.971</td>\n",
       "      <td>3.04500</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.5991***</td>\n",
       "      <td>0.45231</td>\n",
       "      <td>3.535</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.309</td>\n",
       "      <td>0.34347</td>\n",
       "      <td>0.899</td>\n",
       "      <td>0.369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.349</td>\n",
       "      <td>0.69657</td>\n",
       "      <td>0.500</td>\n",
       "      <td>0.617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.094</td>\n",
       "      <td>0.86379</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.451</td>\n",
       "      <td>1.58629</td>\n",
       "      <td>0.284</td>\n",
       "      <td>0.776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.109</td>\n",
       "      <td>0.47982</td>\n",
       "      <td>0.228</td>\n",
       "      <td>0.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.27073</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.068</td>\n",
       "      <td>0.34245</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.041</td>\n",
       "      <td>0.30303</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.38956</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.131</td>\n",
       "      <td>0.28188</td>\n",
       "      <td>0.464</td>\n",
       "      <td>0.643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.025</td>\n",
       "      <td>0.20183</td>\n",
       "      <td>0.126</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>-0.003</td>\n",
       "      <td>0.15439</td>\n",
       "      <td>0.020</td>\n",
       "      <td>0.984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.25448</td>\n",
       "      <td>0.287</td>\n",
       "      <td>0.774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.198</td>\n",
       "      <td>0.16537</td>\n",
       "      <td>1.200</td>\n",
       "      <td>0.230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.058</td>\n",
       "      <td>0.22784</td>\n",
       "      <td>0.255</td>\n",
       "      <td>0.799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>0.03</td>\n",
       "      <td>0.22976</td>\n",
       "      <td>0.131</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>0.157</td>\n",
       "      <td>0.40932</td>\n",
       "      <td>0.383</td>\n",
       "      <td>0.702</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables         theta        se  t (theta == 0)      p\n",
       "in_out           -2.545   5.43585           0.468  0.640\n",
       "cy               -0.318   2.79781           0.114  0.910\n",
       "hp               -0.457   3.15701           0.145  0.885\n",
       "we               -0.945   2.89147           0.327  0.744\n",
       "le               -1.953   4.01604           0.486  0.627\n",
       "wi               -2.093   5.26651           0.397  0.691\n",
       "he               -2.081   3.92949           0.529  0.596\n",
       "li               -0.742   1.60313           0.463  0.643\n",
       "sp                -1.39   3.13216           0.444  0.657\n",
       "ac               -0.054   1.33363           0.040  0.968\n",
       "pr                0.844   1.09137           0.773  0.439\n",
       "brand_2           0.994   2.79930           0.355  0.723\n",
       "brand_3            1.05   1.61546           0.650  0.516\n",
       "brand_4           0.798   1.41541           0.564  0.573\n",
       "brand_5           0.822   1.68888           0.487  0.627\n",
       "brand_6           0.725   1.59168           0.456  0.649\n",
       "brand_7           0.952   2.24413           0.424  0.671\n",
       "brand_8           0.971   3.34418           0.290  0.772\n",
       "brand_9           0.909   1.88158           0.483  0.629\n",
       "brand_10          0.825   1.53840           0.536  0.592\n",
       "brand_11          1.052   1.45670           0.722  0.470\n",
       "brand_12          0.866   1.92154           0.451  0.652\n",
       "brand_13          0.919   2.96360           0.310  0.757\n",
       "brand_14          0.926   1.50023           0.617  0.537\n",
       "brand_15           0.96   2.64242           0.363  0.716\n",
       "brand_16          0.636   1.49284           0.426  0.670\n",
       "brand_17          0.834   2.44943           0.341  0.733\n",
       "brand_18          0.947   1.96351           0.482  0.630\n",
       "brand_19          0.885   1.74563           0.507  0.612\n",
       "brand_20          0.906   2.33524           0.388  0.698\n",
       "brand_21          0.933   1.51614           0.616  0.538\n",
       "brand_22          0.858   1.41034           0.608  0.543\n",
       "brand_23          1.031   1.45805           0.707  0.480\n",
       "brand_24          0.525   1.49643           0.351  0.726\n",
       "brand_25          0.872   2.82575           0.309  0.758\n",
       "brand_26          0.806   1.86495           0.432  0.666\n",
       "brand_27          0.999  11.98206           0.083  0.934\n",
       "brand_28          0.972   4.49771           0.216  0.829\n",
       "brand_29          0.989   3.19239           0.310  0.757\n",
       "brand_30          0.947   1.74859           0.542  0.588\n",
       "brand_31          0.993   2.28668           0.434  0.664\n",
       "brand_32          0.967   1.59364           0.607  0.544\n",
       "brand_33          0.998   3.58772           0.278  0.781\n",
       "brand_34          0.872   1.31756           0.662  0.508\n",
       "brand_35          0.875   2.03354           0.430  0.667\n",
       "brand_36          0.803   1.76517           0.455  0.649\n",
       "brand_37          0.971   3.04500           0.319  0.750\n",
       "home_2        1.5991***   0.45231           3.535  0.000\n",
       "cla_2             0.309   0.34347           0.899  0.369\n",
       "cla_3             0.349   0.69657           0.500  0.617\n",
       "cla_4             0.094   0.86379           0.109  0.913\n",
       "cla_5             0.451   1.58629           0.284  0.776\n",
       "group_in_out      0.109   0.47982           0.228  0.820\n",
       "group_cy         -0.009   0.27073           0.032  0.974\n",
       "group_hp          0.068   0.34245           0.200  0.842\n",
       "group_we          0.041   0.30303           0.136  0.892\n",
       "group_le         -0.082   0.38956           0.211  0.833\n",
       "group_wi         -0.131   0.28188           0.464  0.643\n",
       "group_he         -0.025   0.20183           0.126  0.899\n",
       "group_li         -0.003   0.15439           0.020  0.984\n",
       "group_sp         -0.073   0.25448           0.287  0.774\n",
       "group_ac         -0.198   0.16537           1.200  0.230\n",
       "group_brand       0.058   0.22784           0.255  0.799\n",
       "group_home         0.03   0.22976           0.131  0.896\n",
       "group_cla         0.157   0.40932           0.383  0.702"
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Similarity_theta = resMLE['theta']\n",
    "Similarity_SE = resMLE['se']\n",
    "Similarity_t, Similarity_p = IPDL_t_p(Similarity_SE, Similarity_theta, N)\n",
    "reg_table(Similarity_theta, Similarity_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4636624201081757"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in Similarity_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An alternative approach\n",
    "\n",
    "The log-likelihood function is not globally concave, and finding the global optimum can be difficult. Using the estimation procedure of Fosgerau et. al. (2023 working paper), we can instead fit the parameters using the first-order conditions for optimality. The estimator takes the form\n",
    "\n",
    "$$\n",
    "\\hat \\theta^0=\\arg \\min_{\\theta} \\sum_t s_t \\hat \\varepsilon^0_t(\\theta)'\\hat W^0_t\\hat \\varepsilon^0 _t(\\theta),\n",
    "$$\n",
    "where $\\hat W^0_t$ is a positive semidefinite weight matrix, $s_t$ is market $t$'s share of the total population and \n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t(u(X_t,\\beta)- \\nabla_q \\Omega_t(\\hat q_t^0|\\lambda)) ,\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\hat D^0_t=\\textrm{diag}(\\hat q^0_t)-\\hat q^0_t (\\hat q^0_t)'.\n",
    "$$\n",
    "Using equation (...) above, we have that $\\hat \\epsilon_t$ is a linear function of $\\theta$,\n",
    "$$\n",
    "\\hat \\varepsilon^0_t(\\theta)=\\hat D^0_t \\left(\\hat G^0_t\\theta- \\ln \\hat q^0_t\\right)\\equiv \\hat A^0_t\\theta-\\hat r^0_t.\n",
    "$$\n",
    "Using linearity, the weighted least squares criterion has a unique closed form solution,\n",
    "$$\n",
    "\\hat \\theta^0 =\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat A^0_t \\right)^{-1}\\left(\\sum_t s_t (\\hat A^0_t)'\\hat W^0_t \\hat r_t^0 \\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_array(q, x, psi_stack):\n",
    "    ''' \n",
    "    This function calculates the G block matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        G: a dictionary  of T numpy arrays (J[t],K+G): a G matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    Z = cross_grad_pertubation(q, psi_stack) # Find the cross derivative of the pertubation function \\Omega wrt. lambda and ccp's q\n",
    "    G = {t: np.concatenate((x[t],-Z[t]), axis=1) for t in np.arange(T)} # Join block matrices along 2nd dimensions  s.t. last dimension is K+G (same dimension as theta)\n",
    "\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "def D_array(q):\n",
    "    '''\n",
    "    This function calculates the D matrix - the logit derivative of ccp's wrt. utilities\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "\n",
    "    Returns\n",
    "        D: a dictionary of T numpy arrays (J[t],J[t]) of logit derivatives of ccp's wrt. utilities for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = {t: np.diag(q[t]) - np.einsum('j,k->jk', q[t], q[t]) for t in np.arange(T)}\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_array(q, x, psi_stack):\n",
    "    '''\n",
    "    This function calculates the A matrix\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        A: a dictionary  of T numpy arrays (J[t],K+G): an A matrix for each market t\n",
    "    '''\n",
    "    T = len(x)\n",
    "\n",
    "    D = D_array(q)\n",
    "    G = G_array(q, x, psi_stack)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_array(q):\n",
    "    '''\n",
    "    This function calculates 'r'; the logarithm of observed or nonparametrically estimated market shares\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "    \n",
    "    Returns\n",
    "        r: a dictionary of T numpy arrays (J[t],) of the log of ccp's for each market t\n",
    "    '''\n",
    "    T = len(q)\n",
    "\n",
    "    D = D_array(q) \n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where = (q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) for t in np.arange(T)}\n",
    "\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS_init(q, x, sample_share, psi_stack, N):\n",
    "    ''' \n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the initial FKN parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "\n",
    "    T = len(x)\n",
    "\n",
    "    #W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "    A = A_array(q, x, psi_stack)\n",
    "    r = r_array(q)\n",
    "\n",
    "    d = A[0].shape[1]\n",
    "    \n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,j,jp->dp', A[t], 1/q[t], A[t], optimize = True) # Fast product using that the weights are diagonal.\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,j,j->d', A[t], 1/q[t], r[t], optimize = True)\n",
    "    \n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    #se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "    \n",
    "    return theta_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the observed market shares we may thus find initial parameter estimates $\\hat \\theta^0$ as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFKN0 = WLS_init(y, x, pop_share, Psi, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1338629760910652"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in thetaFKN0[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.1338629760910652, 0.0019139941163568424]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.0019139941163568424"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogL(thetaFKN0, y, x, pop_share, Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization for parameter bounds\n",
    "\n",
    "As we see above, the least squares estimator is not guaranteed to respect the parameter bounds $\\sum_g \\hat \\lambda_g<1$. We can use that if we replace $\\hat q^0_t$ with the choice probabilities from the maximum likelihood estimator of the logit model, $\\hat q^{logit}_t\\propto \\exp\\{X_t\\hat \\beta^{logit}\\}$, and plug these choice probabilities into the WLS estimator described above, it will return $\\hat \\theta=(\\hat \\beta^{logit},0,\\ldots,0)$ as the parameter estimate. Let $\\hat q_t(\\alpha)$ denote the weighted average of the logit probabilites and the market shares,\n",
    "$$\n",
    "\\hat q_t(\\alpha) =(1-\\alpha) \\hat q^{logit}_t+\\alpha \\hat q^0_t.\n",
    "$$\n",
    " Let $\\hat \\theta^0(\\alpha)$ denote the resulting parameter vector. We perform a line search for values of $\\alpha$, $(\\frac{1}{2},\\frac{1}{4},\\frac{1}{8},\\ldots)$ until $\\hat \\theta^0(\\alpha)$ yields a feasible parameter vector.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogL(Theta, y, x, sample_share, psi_stack):\n",
    "    ''' A function giving the mean IPDL loglikehood evaluated at data and an array of parameters 'Theta'\n",
    "    '''\n",
    "    return np.mean(IPDL_loglikelihood(Theta, y, x, sample_share, psi_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LineSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta0.shape[0]\n",
    "    K = x[0].shape[1]\n",
    "    G = d-K\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit.logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    #alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    alpha0=0.5\n",
    "    #LogL_alpha = np.empty((num_alpha,))\n",
    "    #theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in range(1,100):\n",
    "\n",
    "        alpha = alpha0**k\n",
    "\n",
    "      \n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha = WLS_init(q_alpha, x, sample_share, psi_stack, N)[0]\n",
    "\n",
    "        lambda_alpha = theta_alpha[K:]\n",
    "        \n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() <1:\n",
    "            break\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "\n",
    "    return theta_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GridSearch(Theta0, Logit_Beta, y, x, sample_share, psi_stack, N, num_alpha = 5):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = len(Theta0)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    # Find probabilities\n",
    "    q_logit = logit.logit_ccp(Logit_Beta, x)\n",
    "    q_obs = y\n",
    "\n",
    "    # Search\n",
    "    alpha_line = np.linspace(0, 1, num_alpha)\n",
    "    LogL_alpha = np.empty((num_alpha,))\n",
    "    theta_alpha = np.empty((num_alpha, d))\n",
    "\n",
    "    for k in np.arange(len(alpha_line)):\n",
    "\n",
    "        alpha = alpha_line[k]\n",
    "\n",
    "        q_alpha = {t: (1 - alpha)*q_logit[t] + alpha*q_obs[t] for t in np.arange(T)}\n",
    "        theta_alpha[k,:] = WLS_init(q_alpha, x, sample_share, psi_stack, N)\n",
    "\n",
    "        lambda_alpha = theta_alpha[k,K:]\n",
    "        pos_pars = np.array([theta for theta in lambda_alpha if theta > 0])\n",
    "\n",
    "        if pos_pars.sum() >= 1:\n",
    "            LogL_alpha[k] = np.NINF\n",
    "        else:\n",
    "            LogL_alpha[k] = LogL(theta_alpha[k,:], y, x, sample_share, psi_stack)\n",
    "    \n",
    "    # Pick the best set of parameters\n",
    "    alpha_star = np.argmax(LogL_alpha)\n",
    "    theta_hat_star = theta_alpha[alpha_star,:]\n",
    "\n",
    "    return theta_hat_star"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the grid search method we find corressponding parameters $\\hat \\theta^*$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4442084420991392e-12, 0.0015285351341616218]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7047451502467542, 0.0015133909973959689]\n",
      "[0.9786616613288587, 0.0015107140039863107]\n"
     ]
    }
   ],
   "source": [
    "theta_alpha = GridSearch(thetaFKN0, Logit_beta, y, x, pop_share, Psi, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.47, -0.32, -0.46, -0.95, -1.93, -2.06, -2.02, -0.72, -1.37,\n",
       "        0.01,  0.85,  1.  ,  1.01,  0.82,  0.82,  0.75,  0.96,  0.97,\n",
       "        0.92,  0.82,  1.05,  0.87,  0.92,  0.96,  0.96,  0.68,  0.84,\n",
       "        0.91,  0.89,  0.9 ,  0.92,  0.83,  0.99,  0.58,  0.88,  0.82,\n",
       "        1.  ,  0.97,  0.99,  0.95,  1.  ,  0.97,  1.  ,  0.89,  0.88,\n",
       "        0.8 ,  0.97,  1.66,  0.32,  0.33,  0.06,  0.42, -0.  ,  0.  ,\n",
       "       -0.  , -0.  , -0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,  0.  ,\n",
       "        0.  ,  0.  ])"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(theta_alpha2[0,:],decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9786616613288587"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in theta_alpha[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9786616613288587, 0.0015107140039863107]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0015107140039863107"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_IPDL(theta_alpha, y, x, pop_share, Psi).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterated FKN estimator\n",
    "\n",
    "The iterated estimator is as the initial one, except there is an additional term on $\\hat \\varepsilon$. First, we update the choice probabilities,\n",
    "$$\n",
    "\\hat q^k_i=p(\\mathbf X_i,\\hat \\theta^{k-1})\\\\\n",
    "$$\n",
    "Then we assign\n",
    "$$\n",
    "\\hat D^k_i=\\nabla^2_{qq}\\Omega(\\hat q_i^k|\\hat \\lambda^{k-1})^{-1}-(\\hat q^k_i \\hat q^k_i)'\n",
    "$$\n",
    "and then construct the residual\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)=\\hat D^k_i\\left( u(x_i,\\beta)-\\nabla_q \\Omega(\\hat q_i^k|\\lambda)\\right) -y_i+\\hat q_i^k,\n",
    "$$\n",
    "Which can once again be simplified as\n",
    "$$\n",
    "\\hat \\varepsilon^k_i(\\theta)= \\hat A_i^k \\theta-\\hat r^k_i,\n",
    "$$\n",
    "where\n",
    "$$\n",
    "\\hat A^k_i=\\hat D_i^k\\hat G^k_i, \\hat r_i^k =\\hat D^k_i\\ln \\hat q_i^k-y_i\n",
    "$$\n",
    "and where $\\hat G^k_i$ is constructed as in the initial estimator. Using the weighted least squares estimator with weights $\\hat W_i^k=\\textrm{diag}(\\hat q^k_i)^{-1}$, we get the estimator\n",
    "$$\n",
    "\\hat \\theta^k = \\arg \\min_{\\theta}\\frac{1}{n}\\sum_i \\hat \\varepsilon^k_i(\\theta)'\\hat W_i^k \\hat \\varepsilon^k_i(\\theta).\n",
    "$$\n",
    "We can once again solve it in closed form as\n",
    "$$\n",
    "\\hat \\theta^k =\\left( \\frac{1}{n}\\sum_i \\hat (A^k_i)'\\hat W_i^k \\hat A^k_i)\\right)^{-1}\\left( \\frac{1}{n}\\sum_i (\\hat A_i^k)'\\hat W_i^k \\hat r_i^k\\right)\n",
    "$$\n",
    "Now we implement this procedure and iterate starting from our initial guess $\\hat \\theta^{*}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WLS(Theta, y, x, sample_share, psi_stack, N):\n",
    "    '''\n",
    "    This function calculates the weighted least squares estimator \\hat \\theta^k and its relevant estimated standard error for the iterated parameter estimates.\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "        N: An integer giving the total amount of observations\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a (K+G,) numpy array of initial FKN parameter estimates\n",
    "        se_hat: a (K+G,) numpy array of standard errors for initial FKN parameter estimates\n",
    "    '''\n",
    "    T = len(x)\n",
    "    d = Theta.shape[0]\n",
    "    \n",
    "    # Get ccp's\n",
    "    q = IPDL_ccp(Theta, x, psi_stack)\n",
    "\n",
    "    # Construct A\n",
    "    D = ccp_gradient(q, x, Theta, psi_stack) # A is here constructed using the IPDL derivative of ccp's wrt. utilities instead of teh Logit derivative\n",
    "    G = G_array(q, x, psi_stack)\n",
    "    A = {t: np.einsum('jk,kd->jd', D[t], G[t]) for t in np.arange(T)}\n",
    "    W = {t: la.inv(np.diag(q[t])) for t in np.arange(T)}\n",
    "\n",
    "    # Construct r\n",
    "    log_q = {t: np.log(q[t], out = -np.inf*np.ones_like(q[t]), where=(q[t] > 0)) for t in np.arange(T)}\n",
    "    r = {t: np.einsum('jk,k->j', D[t], log_q[t]) + y[t] for t in np.arange(T)}\n",
    "\n",
    "    # Estimate parameters\n",
    "    AWA = np.empty((T,d,d))\n",
    "    AWr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        AWA[t,:,:] = sample_share[t]*np.einsum('jd,jk,kp->dp', A[t], W[t], A[t], optimize = True)\n",
    "        AWr[t,:] = sample_share[t]*np.einsum('jd,jk,k->d', A[t], W[t], r[t], optimize = True)\n",
    "\n",
    "    theta_hat = la.solve(AWA.sum(axis = 0), AWr.sum(axis = 0))\n",
    "    se_hat = np.sqrt(np.diag(la.inv(AWA.sum(axis = 0))) / N)\n",
    "\n",
    "    return theta_hat,se_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FKN_estimator(logit_beta, q_obs, x, sample_share, psi_stack, N, tol = 1.0e-15, max_iters = 1000):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    theta_init = WLS_init(q_obs, x, sample_share, psi_stack, N) #WLS_init(q_obs, x, sample_share, psi_stack, nest_count,  N)\n",
    "    \n",
    "    if np.array([p for p in theta_init[K:] if p>0]).sum() >= 1:\n",
    "        theta_hat_star = GridSearch(theta_init, logit_beta, q_obs, x, sample_share, psi_stack, N)\n",
    "        theta0 = theta_hat_star\n",
    "    else:\n",
    "        theta0 = theta_init\n",
    "\n",
    "    #logl0 = LogL(theta0, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "    \n",
    "    for k in np.arange(max_iters):\n",
    "        theta1, se1 = WLS(theta0, q_obs, x, sample_share, psi_stack, N)\n",
    "\n",
    "        '''logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "        \n",
    "        for m in range(10):\n",
    "            if logl1<logl0:\n",
    "                theta1=0.5*theta0+0.5*theta1\n",
    "                logl1=LogL(theta1, q_obs, x, sample_share, psi_stack, nest_count)\n",
    "            else:\n",
    "                break'''\n",
    "\n",
    "        # Check convergence in an appropriate distance function\n",
    "        dist = np.max(np.abs(theta1 - theta0))\n",
    "\n",
    "        if dist<tol:\n",
    "            succes = True\n",
    "            iter = k\n",
    "            break\n",
    "        elif k==max_iters:\n",
    "            succes = False\n",
    "            iter = max_iters\n",
    "            break\n",
    "        else:\n",
    "            None\n",
    "            \n",
    "        # Iteration step\n",
    "        theta0 = theta1\n",
    "\n",
    "    res = {'theta': theta1,\n",
    "           'se': se1,\n",
    "           'fun': -LogL(theta1, y, x, sample_share, psi_stack),\n",
    "           'iter': iter,\n",
    "           'succes': succes}\n",
    "    \n",
    "    return res \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.4442084420991392e-12, 0.0015285351341616218]\n",
      "[0.7047451502467542, 0.0015133909973959689]\n",
      "[0.9786616613288587, 0.0015107140039863107]\n",
      "[1.138504330922137, 0.0014929062681705275]\n"
     ]
    }
   ],
   "source": [
    "res = FKN_estimator(Logit_beta, y, x, pop_share, Psi, N, tol=1.0e-8, max_iters=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-10.224***</td>\n",
       "      <td>0.00334</td>\n",
       "      <td>3057.742</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.4589***</td>\n",
       "      <td>0.00169</td>\n",
       "      <td>271.910</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-3.3221***</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>1385.043</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.0274***</td>\n",
       "      <td>0.00157</td>\n",
       "      <td>17.512</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.0917***</td>\n",
       "      <td>0.00168</td>\n",
       "      <td>1246.144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.4397***</td>\n",
       "      <td>0.00315</td>\n",
       "      <td>1729.395</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.2751***</td>\n",
       "      <td>0.00196</td>\n",
       "      <td>140.349</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-1.0254***</td>\n",
       "      <td>0.00111</td>\n",
       "      <td>920.024</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>2.9853***</td>\n",
       "      <td>0.00224</td>\n",
       "      <td>1330.819</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.7855***</td>\n",
       "      <td>0.00081</td>\n",
       "      <td>971.044</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.2279***</td>\n",
       "      <td>0.00104</td>\n",
       "      <td>218.538</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.7258***</td>\n",
       "      <td>0.00267</td>\n",
       "      <td>271.587</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>0.1599***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>323.985</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.5833***</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>898.714</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.2775***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>588.994</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.2222***</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>465.420</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-0.6548***</td>\n",
       "      <td>0.00139</td>\n",
       "      <td>471.249</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.503***</td>\n",
       "      <td>0.00164</td>\n",
       "      <td>306.660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.6922***</td>\n",
       "      <td>0.00220</td>\n",
       "      <td>769.372</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>0.2588***</td>\n",
       "      <td>0.00056</td>\n",
       "      <td>461.273</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>0.0232***</td>\n",
       "      <td>0.00047</td>\n",
       "      <td>49.117</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.3266***</td>\n",
       "      <td>0.00077</td>\n",
       "      <td>425.221</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-0.889***</td>\n",
       "      <td>0.00133</td>\n",
       "      <td>670.209</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-1.593***</td>\n",
       "      <td>0.00135</td>\n",
       "      <td>1181.167</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.6832***</td>\n",
       "      <td>0.00258</td>\n",
       "      <td>651.844</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-0.6916***</td>\n",
       "      <td>0.00055</td>\n",
       "      <td>1253.730</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.546***</td>\n",
       "      <td>0.00073</td>\n",
       "      <td>750.698</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.3273***</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>704.229</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-0.9209***</td>\n",
       "      <td>0.00110</td>\n",
       "      <td>834.769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.0862***</td>\n",
       "      <td>0.00063</td>\n",
       "      <td>136.189</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.0185***</td>\n",
       "      <td>0.00046</td>\n",
       "      <td>40.365</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>0.0763***</td>\n",
       "      <td>0.00048</td>\n",
       "      <td>158.536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>0.1341***</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>263.615</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.2854***</td>\n",
       "      <td>0.00049</td>\n",
       "      <td>582.928</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.7967***</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>647.917</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-0.6039***</td>\n",
       "      <td>0.00072</td>\n",
       "      <td>839.412</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.6391***</td>\n",
       "      <td>0.03422</td>\n",
       "      <td>77.113</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.7117***</td>\n",
       "      <td>0.00123</td>\n",
       "      <td>578.188</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-2.3273***</td>\n",
       "      <td>0.01372</td>\n",
       "      <td>169.645</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.3046***</td>\n",
       "      <td>0.00245</td>\n",
       "      <td>532.050</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-0.963***</td>\n",
       "      <td>0.00231</td>\n",
       "      <td>417.409</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.3637***</td>\n",
       "      <td>0.00081</td>\n",
       "      <td>446.755</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-2.4302***</td>\n",
       "      <td>0.01056</td>\n",
       "      <td>230.116</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.6795***</td>\n",
       "      <td>0.00065</td>\n",
       "      <td>1042.573</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.2739***</td>\n",
       "      <td>0.00068</td>\n",
       "      <td>400.581</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1458***</td>\n",
       "      <td>0.00064</td>\n",
       "      <td>226.369</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.57***</td>\n",
       "      <td>0.00287</td>\n",
       "      <td>546.914</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.0981***</td>\n",
       "      <td>0.00045</td>\n",
       "      <td>2457.543</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>-0.0486***</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>323.533</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.0395***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>172.814</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.038***</td>\n",
       "      <td>0.00036</td>\n",
       "      <td>106.125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.1191***</td>\n",
       "      <td>0.00051</td>\n",
       "      <td>234.131</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.7365***</td>\n",
       "      <td>0.00025</td>\n",
       "      <td>2969.502</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.1059***</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>729.738</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.065***</td>\n",
       "      <td>0.00020</td>\n",
       "      <td>331.450</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.0347***</td>\n",
       "      <td>0.00016</td>\n",
       "      <td>211.891</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.0846***</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>563.924</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0545***</td>\n",
       "      <td>0.00012</td>\n",
       "      <td>437.411</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0758***</td>\n",
       "      <td>0.00009</td>\n",
       "      <td>862.365</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.0445***</td>\n",
       "      <td>0.00010</td>\n",
       "      <td>435.553</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0238***</td>\n",
       "      <td>0.00015</td>\n",
       "      <td>153.843</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.0327***</td>\n",
       "      <td>0.00011</td>\n",
       "      <td>309.958</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.2577***</td>\n",
       "      <td>0.00023</td>\n",
       "      <td>1121.408</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.2198***</td>\n",
       "      <td>0.00022</td>\n",
       "      <td>985.773</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.0672***</td>\n",
       "      <td>0.00013</td>\n",
       "      <td>526.993</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables          theta       se  t (theta == 0)    p\n",
       "in_out        -10.224***  0.00334        3057.742  0.0\n",
       "cy            -0.4589***  0.00169         271.910  0.0\n",
       "hp            -3.3221***  0.00240        1385.043  0.0\n",
       "we            -0.0274***  0.00157          17.512  0.0\n",
       "le            -2.0917***  0.00168        1246.144  0.0\n",
       "wi             5.4397***  0.00315        1729.395  0.0\n",
       "he             0.2751***  0.00196         140.349  0.0\n",
       "li            -1.0254***  0.00111         920.024  0.0\n",
       "sp             2.9853***  0.00224        1330.819  0.0\n",
       "ac             0.7855***  0.00081         971.044  0.0\n",
       "pr            -0.2279***  0.00104         218.538  0.0\n",
       "brand_2       -0.7258***  0.00267         271.587  0.0\n",
       "brand_3        0.1599***  0.00049         323.985  0.0\n",
       "brand_4       -0.5833***  0.00065         898.714  0.0\n",
       "brand_5       -0.2775***  0.00047         588.994  0.0\n",
       "brand_6       -0.2222***  0.00048         465.420  0.0\n",
       "brand_7       -0.6548***  0.00139         471.249  0.0\n",
       "brand_8        -0.503***  0.00164         306.660  0.0\n",
       "brand_9       -1.6922***  0.00220         769.372  0.0\n",
       "brand_10       0.2588***  0.00056         461.273  0.0\n",
       "brand_11       0.0232***  0.00047          49.117  0.0\n",
       "brand_12      -0.3266***  0.00077         425.221  0.0\n",
       "brand_13       -0.889***  0.00133         670.209  0.0\n",
       "brand_14       -1.593***  0.00135        1181.167  0.0\n",
       "brand_15      -1.6832***  0.00258         651.844  0.0\n",
       "brand_16      -0.6916***  0.00055        1253.730  0.0\n",
       "brand_17       -0.546***  0.00073         750.698  0.0\n",
       "brand_18       0.3273***  0.00046         704.229  0.0\n",
       "brand_19      -0.9209***  0.00110         834.769  0.0\n",
       "brand_20      -0.0862***  0.00063         136.189  0.0\n",
       "brand_21      -0.0185***  0.00046          40.365  0.0\n",
       "brand_22       0.0763***  0.00048         158.536  0.0\n",
       "brand_23       0.1341***  0.00051         263.615  0.0\n",
       "brand_24      -0.2854***  0.00049         582.928  0.0\n",
       "brand_25      -0.7967***  0.00123         647.917  0.0\n",
       "brand_26      -0.6039***  0.00072         839.412  0.0\n",
       "brand_27      -2.6391***  0.03422          77.113  0.0\n",
       "brand_28      -0.7117***  0.00123         578.188  0.0\n",
       "brand_29      -2.3273***  0.01372         169.645  0.0\n",
       "brand_30      -1.3046***  0.00245         532.050  0.0\n",
       "brand_31       -0.963***  0.00231         417.409  0.0\n",
       "brand_32      -0.3637***  0.00081         446.755  0.0\n",
       "brand_33      -2.4302***  0.01056         230.116  0.0\n",
       "brand_34      -0.6795***  0.00065        1042.573  0.0\n",
       "brand_35      -0.2739***  0.00068         400.581  0.0\n",
       "brand_36      -0.1458***  0.00064         226.369  0.0\n",
       "brand_37        -1.57***  0.00287         546.914  0.0\n",
       "home_2         1.0981***  0.00045        2457.543  0.0\n",
       "cla_2         -0.0486***  0.00015         323.533  0.0\n",
       "cla_3          0.0395***  0.00023         172.814  0.0\n",
       "cla_4           0.038***  0.00036         106.125  0.0\n",
       "cla_5          0.1191***  0.00051         234.131  0.0\n",
       "group_in_out   0.7365***  0.00025        2969.502  0.0\n",
       "group_cy      -0.1059***  0.00015         729.738  0.0\n",
       "group_hp        0.065***  0.00020         331.450  0.0\n",
       "group_we       0.0347***  0.00016         211.891  0.0\n",
       "group_le      -0.0846***  0.00015         563.924  0.0\n",
       "group_wi      -0.0545***  0.00012         437.411  0.0\n",
       "group_he      -0.0758***  0.00009         862.365  0.0\n",
       "group_li       0.0445***  0.00010         435.553  0.0\n",
       "group_sp      -0.0238***  0.00015         153.843  0.0\n",
       "group_ac      -0.0327***  0.00011         309.958  0.0\n",
       "group_brand    0.2577***  0.00023        1121.408  0.0\n",
       "group_home    -0.2198***  0.00022         985.773  0.0\n",
       "group_cla     -0.0672***  0.00013         526.993  0.0"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKN_theta = res['theta']\n",
    "FKN_SE = res['se']\n",
    "FKN_t, FKN_p = IPDL_t_p(FKN_SE, FKN_theta, N)\n",
    "reg_table(FKN_theta, FKN_SE, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BLP Estimation and instruments\n",
    "\n",
    "The setting is now a bit different. Instead of the noise coming from random sampling of individuals, we now have an additional source of uncertainty, stemming frm the random sampling of the fixed effects $\\xi_{tj}$ for each market and each product. The number of ”observations” is therefore\n",
    "\n",
    "$$\n",
    "S = T \\cdot \\sum_t J_t\n",
    "$$\n",
    "\n",
    "Note that while random sampling of individuals choices (number of observations\n",
    "in the hundreds of millions) still has an effect on the estimated parameters in\n",
    "principle, this effect is completely drowned out by the sampling variance of the\n",
    "fixed effects (number of observations T ≈ 15000?), so we choose to ignore it\n",
    "here. When estimating random coefficients models, there is also a third source\n",
    "of uncertainty stemming from approximation of numerical integrals. This is not\n",
    "an issue in IPDL, as we have the inverse demand in closed form.\n",
    "\n",
    "The principles are pretty similar to what we have been doing already. When\n",
    "applicable, I will use the same notation as in the FKN section. Define the\n",
    "residual,\n",
    "\n",
    "$$\\xi_m(\\theta) = u(X_m, \\beta) − \\nabla_q \\Omega(q^0|\\lambda)$$\n",
    "\n",
    "In the IPDL model, this residual is a linear function of $\\theta$ which has the form\n",
    "\n",
    "$$\\xi_m(\\theta) =  G^0_m \\theta − r_m^0$$\n",
    "\n",
    "where $ G^0_m=[X_m, Z_m^0]$, where $Z_m^0 = \\nabla_{q,\\lambda}\\Omega(q_m^0|\\lambda)$ and $r^0_m = \\ln q^0_m$ as in the FKN section with $q^0_m$ being e.g. the observed market shares in market $m$. For the BLP estimator, we set this residual orthogonal to a matrix of instruments $\\hat Z_m$ of size $J_m \\times d$, and find the estimator $ \\hat \\theta^{IV}$ which solves the moment conditions\n",
    "\n",
    "$$\\frac{1}{T} \\sum_m \\hat Z_m' \\xi(\\hat \\theta^{IV}) = 0$$\n",
    "\n",
    "Since $\\hat \\xi$ is linear, the moment equations have a unique solution,\n",
    "\n",
    "$$\\hat \\theta^{IV} = \\left(\\frac{1}{T}\\sum_m \\hat Z_m' G^0_m \\right)^{-1}\\left(\\frac{1}{T}\\sum_m \\hat Z_m' r^0_m \\right)$$\n",
    "\n",
    "We require an instrument for the price of the goods. This is something which is correlated with the price, but uncorrelated with the error term $\\xi_m$ (in the BLP model, $\\xi_{mj}$ represents unobserved components of car quality). A standard instrument in this case would be a measure of marginal cost (or something which is correlated with marginal cost, like a production price index). For everything other than price, we can simply use the regressor itself as the instrument i.e. $ \\hat Z^{mjd} = G^0_{mjd}$, for all other dimensions than price."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we construct our instruments $\\hat Z$. We'll use the average exchange rate of the destination country relative to average exchange rate of the origin country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "xexr = {t: dat[dat['market'] == t][z_vars[0]].values for t in np.arange(T)}\n",
    "G0 = G_array(y, x, Psi)\n",
    "pr_index = len(x_contvars)\n",
    "for t in np.arange(T):\n",
    "    G0[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z = G0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then calculate the moment estimator $\\hat \\theta^{IV}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_estimator(y, z, x, sample_share, psi_stack):\n",
    "    '''\n",
    "    Args.\n",
    "        y: a dictionary of T numpy arrasy (J[t],) of observed or nonparametrically estimated market shares for each market t\n",
    "        z: a dictionary of T numpy arrays (J[t],K+G) of instruments for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        sample_share: A (T,) numpy array of the fraction of observations in each market t \n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        theta_hat: a numpy array (K+G,) of BLP parameter estimates\n",
    "    '''\n",
    "    T = len(z)\n",
    "\n",
    "    G = G_array(y, x, psi_stack)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t], out = np.NINF*np.ones_like((y[t])), where = (y[t] > 0)) for t in np.arange(T)}\n",
    "    \n",
    "    sZG = np.empty((T,d,d))\n",
    "    sZr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sZG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', z[t], G[t])\n",
    "        sZr[t,:] = sample_share[t]*np.einsum('jd,j->d', z[t], r[t])\n",
    "\n",
    "    theta_hat = la.solve(sZG.sum(axis=0), sZr.sum(axis=0))\n",
    "    \n",
    "    return theta_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "BLP_theta = BLP_estimator(y, z, x, np.ones((T,)), Psi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1181512088784706"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in BLP_theta[K:] if p>0]).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Logit model we get the parameter estimates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_logit = x\n",
    "for t in np.arange(T):\n",
    "    G_logit[t][:,pr_index] = xexr[t] / xexr[t].max()\n",
    "\n",
    "z_logit = G_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-14.92920752,  -2.3589754 ,  -6.76421995,   0.02963003,\n",
       "        -2.05176127,  10.84731336,  -1.04140126,  -0.58331478,\n",
       "         5.15289118,   0.51808091,  -0.17336342,  -2.037768  ,\n",
       "        -0.81720168,  -1.44357757,  -1.04059281,  -1.16245013,\n",
       "        -1.74530433,  -0.85123531,  -2.72300281,  -1.08758839,\n",
       "        -0.68958989,  -0.95909482,  -2.11727698,  -2.93039275,\n",
       "        -2.90655875,  -2.05527142,  -1.82107985,   0.51974428,\n",
       "        -2.02980519,  -0.79701277,  -0.86356478,  -0.86816254,\n",
       "        -0.81661044,  -1.48858878,  -0.9378501 ,  -1.87621854,\n",
       "        -3.7657435 ,  -1.52567201,  -3.14936663,  -2.07998398,\n",
       "        -1.85954898,  -0.7631942 ,  -1.94891051,  -1.60837966,\n",
       "        -1.15784827,  -0.48973547,  -2.57588437,   1.56903974,\n",
       "         0.0275757 ,   0.04982496,  -0.30342661,  -0.3829885 ])"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta = logit.LogitBLP_estimator(y, z_logit, x, np.ones((T,)))\n",
    "LogitBLP_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BLP approximation to optimal instruments\n",
    "\n",
    "BLP propose an algorithm for constructing an approximation to the optimal instruments. It is described in simple terms in Reynaert & Verboven (2014), and it has the following steps.\n",
    "It requires an initial parameter estimator $\\hat \\theta = (\\hat \\beta', \\hat \\lambda')'$, here we can just usethe MLE we have already computed. Let $W_m$ denote the matrix of instruments (this is the matrix $X_m$ with the price replaced by the exchange rate). The steps are then as follows:\n",
    "\n",
    "First we form the regression equation of the covariates on the instruments:\n",
    "$$\n",
    "X_m = W_m \\Pi + E_m\n",
    "$$\n",
    "\n",
    "The OLS estimate is then given as:\n",
    "$$\n",
    "\\hat \\Pi = \\left( \\frac{1}{T}\\sum_m W_m' W_m \\right)^{-1}\\left( \\frac{1}{T}\\sum_m W_m' X_m\\right)\n",
    "$$\n",
    "\n",
    "Thus the predicted covariates given the instruments $W$ are:\n",
    "$$\n",
    "\\hat X_m = W_m \\hat \\Pi\n",
    "$$\n",
    "\n",
    "Having constructed $\\hat X_m$ (which consists of the exogenous regressors, and the predicted price given $W_m$), we compute the predicted mean utility:\n",
    "\n",
    "$$\n",
    "\\hat u_m = \\hat X_m \\hat \\beta\n",
    "$$\n",
    "\n",
    "and then the predicted market shares at the mean utility:\n",
    "\n",
    "$$\n",
    "\\hat q_m^{*} = P(\\hat u_m | \\hat \\lambda)\n",
    "$$\n",
    "\n",
    "Computationally, here we just use $\\hat X_m$ in place of $X_m$ in the CCP function.\n",
    "Given the predicted market shares, we compute\n",
    "\n",
    "$$\n",
    "\\hat G_m^{*} = \\left[\\hat X_m, \\nabla_{q,\\lambda} \\Omega (\\hat q_m^{*} | \\hat \\lambda)\\right]\n",
    "$$\n",
    "\n",
    "which is the same as the function $\\hat G_m^0$ we already have constructed, except we evaluate it at the\n",
    "predictions $\\hat X_m$ and $\\hat q_m^{*}$ instead of at $X_m$ and $\\hat q_m^0$.\n",
    "\n",
    "The procedure above gives an approximation to the optimal instruments. We also require a weight matrix. The optimal weight matrix is the (generalized) inverse of the conditional (on the instruments) covariance of the fixed effects. Assuming $\\xi_{jm}$ is independetly and identically distributed over j and m, the conditional covariance simplifies to a scalar $\\sigma^2$ times an identity matrix (of size $J_m$).\n",
    "This means that all fixed effects are weighted equally, and the weights therefore drop out of the IV regression. The optimal IV estimator is therefore\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} = \\left(\\frac{1}{T}\\sum_m (\\hat G_m^*)'\\hat G_m^0\\right)^{-1}\\left( \\frac{1}{T}\\sum_m (\\hat G_m^*)'\\hat r_m^0 \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\hat \\xi^*$ denote the estimated residual evaluated at the new parameter estimates,\n",
    "\n",
    "$$\n",
    "\\hat \\xi_{mj}^* = \\hat \\xi_{mj}(\\hat \\theta^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "We may estimate the constant $\\sigma^2$ by\n",
    "\n",
    "$$\n",
    "\\hat \\sigma^2 = \\frac{1}{T}\\sum_{m}\\sum_{j = 1}^{J_m} \\left(\\hat \\xi_{mj}^*\\right)^2 \n",
    "$$\n",
    "\n",
    "The distribution of the estimator $\\hat \\theta^{\\text{IV}}$ is then\n",
    "\n",
    "$$\n",
    "\\hat \\theta^{\\text{IV}} \\sim \\mathcal{N}(\\theta_0, \\Sigma^{\\text{IV}})\n",
    "$$\n",
    "\n",
    "which can be consistently estimated by\n",
    "\n",
    "$$\n",
    "\\hat \\Sigma^{\\text{IV}} = \\hat \\sigma^2 \\left( \\sum_m (\\hat G_m^*)'\\hat G_m^0 \\right)^{-1}\n",
    "$$\n",
    "\n",
    "and the standard errors are then the square root of the diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_x(x, w, sample_share):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(w)\n",
    "    K = w[0].shape[1]\n",
    "\n",
    "    sWW = np.empty((T,K,K))\n",
    "    sWX = np.empty((T,K,K))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sWW[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], w[t])\n",
    "        sWX[t,:,:] = sample_share[t]*np.einsum('jk,jl->kl', w[t], x[t])\n",
    "\n",
    "    Pi_hat = la.solve(sWW.sum(axis=0), sWX.sum(axis=0))\n",
    "    X_hat = {t: np.einsum('jl,lk->jk', w[t], Pi_hat) for t in np.arange(T)}\n",
    "\n",
    "    return X_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BLP_se(Theta, y, x, psi_stack):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    S = T * np.array([x[t].shape[0] for t in np.arange(T)]).sum()\n",
    "\n",
    "    G = G_array(y, x, psi_stack)\n",
    "    d = G[0].shape[1]\n",
    "    r = {t: np.log(y[t]) for t in np.arange(T)}\n",
    "    \n",
    "    # We calculate \\sigma^2\n",
    "    xi = {t: np.einsum('jd,d->j', G[t], Theta) - r[t] for t in np.arange(T)}\n",
    "    sum_xij2 = np.empty((T,))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sum_xij2[t] = (xi[t]**2).sum()\n",
    "    \n",
    "    sigma2 = np.sum(sum_xij2) / S\n",
    "\n",
    "    # We calculate GG for each market t\n",
    "    GG = np.empty((T,d,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        GG[t,:,:] = np.einsum('jd,jp->dp', G[t], G[t])\n",
    "\n",
    "    # Finally we compute \\Sigma and the standard errors\n",
    "    Sigma = sigma2*la.inv(GG.sum(axis=0))\n",
    "    SE = np.sqrt(np.diag(Sigma))\n",
    "\n",
    "    return SE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "def OptimalBLP_estimator(Theta0, q_obs, w, x, sample_share, psi_stack):\n",
    "    ''' \n",
    "    '''\n",
    "    T = len(x)\n",
    "    K = x[0].shape[1]\n",
    "\n",
    "    beta0 = Theta0[:K]\n",
    "    lambda0 = Theta0[K:]\n",
    "    \n",
    "    X_hat = predict_x(x, w, sample_share)\n",
    "    q0 = IPDL_ccp(Theta0, X_hat, psi_stack)\n",
    "    G_star = G_array(q0, X_hat, psi_stack)\n",
    "    G0 = G_array(q_obs, x, psi_stack)\n",
    "    \n",
    "    r = {t: np.log(q_obs[t]) for t in np.arange(T)}\n",
    "\n",
    "    d = G0[0].shape[1]\n",
    "\n",
    "    sGG = np.empty((T,d,d))\n",
    "    sGr = np.empty((T,d))\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        sGG[t,:,:] = sample_share[t]*np.einsum('jd,jp->dp', G_star[t], G0[t])\n",
    "        sGr[t,:] = sample_share[t]*np.einsum('jd,j->d', G_star[t], r[t])\n",
    "\n",
    "    Theta_IV = la.solve(sGG.sum(axis=0), sGr.sum(axis=0))\n",
    "    SE_IV = BLP_se(Theta_IV, q_obs, x, psi_stack)\n",
    "\n",
    "    return Theta_IV, SE_IV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "ThetaOptBLP, SEOptBLP = OptimalBLP_estimator(FKN_theta, y, z_logit, x, np.ones((T,)), Psi)\n",
    "OptBLP_t, OptBLP_p = IPDL_t_p(SEOptBLP, ThetaOptBLP, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2252332005073343"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([p for p in ThetaOptBLP[K:]  if p > 0]).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variables</th>\n",
       "      <th>theta</th>\n",
       "      <th>se</th>\n",
       "      <th>t (theta == 0)</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-11.4319***</td>\n",
       "      <td>0.03410</td>\n",
       "      <td>335.214</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.7822***</td>\n",
       "      <td>0.01964</td>\n",
       "      <td>39.835</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-5.1791***</td>\n",
       "      <td>0.02399</td>\n",
       "      <td>215.868</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>0.0966***</td>\n",
       "      <td>0.01993</td>\n",
       "      <td>4.847</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-2.2447***</td>\n",
       "      <td>0.02282</td>\n",
       "      <td>98.360</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>5.7558***</td>\n",
       "      <td>0.03346</td>\n",
       "      <td>172.010</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>0.8248***</td>\n",
       "      <td>0.02717</td>\n",
       "      <td>30.356</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.8511***</td>\n",
       "      <td>0.01211</td>\n",
       "      <td>70.283</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>4.5807***</td>\n",
       "      <td>0.02574</td>\n",
       "      <td>177.983</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>1.2298***</td>\n",
       "      <td>0.01291</td>\n",
       "      <td>95.290</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>-0.1479***</td>\n",
       "      <td>0.00233</td>\n",
       "      <td>63.458</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_2</th>\n",
       "      <td>-0.9293***</td>\n",
       "      <td>0.03121</td>\n",
       "      <td>29.780</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_3</th>\n",
       "      <td>-0.1702***</td>\n",
       "      <td>0.00441</td>\n",
       "      <td>38.612</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_4</th>\n",
       "      <td>-0.9347***</td>\n",
       "      <td>0.00473</td>\n",
       "      <td>197.745</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_5</th>\n",
       "      <td>-0.4647***</td>\n",
       "      <td>0.00437</td>\n",
       "      <td>106.252</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_6</th>\n",
       "      <td>-0.3667***</td>\n",
       "      <td>0.00429</td>\n",
       "      <td>85.460</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_7</th>\n",
       "      <td>-1.007***</td>\n",
       "      <td>0.00749</td>\n",
       "      <td>134.531</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_8</th>\n",
       "      <td>-0.3604***</td>\n",
       "      <td>0.01085</td>\n",
       "      <td>33.226</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_9</th>\n",
       "      <td>-1.7434***</td>\n",
       "      <td>0.00708</td>\n",
       "      <td>246.204</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_10</th>\n",
       "      <td>-0.2666***</td>\n",
       "      <td>0.00422</td>\n",
       "      <td>63.106</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_11</th>\n",
       "      <td>-0.0659***</td>\n",
       "      <td>0.00432</td>\n",
       "      <td>15.260</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_12</th>\n",
       "      <td>-0.4907***</td>\n",
       "      <td>0.00517</td>\n",
       "      <td>94.861</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_13</th>\n",
       "      <td>-1.2808***</td>\n",
       "      <td>0.00649</td>\n",
       "      <td>197.398</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_14</th>\n",
       "      <td>-2.0711***</td>\n",
       "      <td>0.00904</td>\n",
       "      <td>229.029</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_15</th>\n",
       "      <td>-1.9547***</td>\n",
       "      <td>0.00861</td>\n",
       "      <td>226.919</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_16</th>\n",
       "      <td>-1.2525***</td>\n",
       "      <td>0.00420</td>\n",
       "      <td>298.329</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_17</th>\n",
       "      <td>-0.9843***</td>\n",
       "      <td>0.00499</td>\n",
       "      <td>197.379</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_18</th>\n",
       "      <td>0.4051***</td>\n",
       "      <td>0.00465</td>\n",
       "      <td>87.111</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_19</th>\n",
       "      <td>-1.2918***</td>\n",
       "      <td>0.00575</td>\n",
       "      <td>224.632</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_20</th>\n",
       "      <td>-0.3086***</td>\n",
       "      <td>0.00523</td>\n",
       "      <td>58.997</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_21</th>\n",
       "      <td>-0.2706***</td>\n",
       "      <td>0.00406</td>\n",
       "      <td>66.656</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_22</th>\n",
       "      <td>-0.2152***</td>\n",
       "      <td>0.00415</td>\n",
       "      <td>51.891</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_23</th>\n",
       "      <td>-0.1661***</td>\n",
       "      <td>0.00450</td>\n",
       "      <td>36.914</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_24</th>\n",
       "      <td>-0.7064***</td>\n",
       "      <td>0.00424</td>\n",
       "      <td>166.679</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_25</th>\n",
       "      <td>-0.815***</td>\n",
       "      <td>0.00503</td>\n",
       "      <td>161.986</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_26</th>\n",
       "      <td>-1.0517***</td>\n",
       "      <td>0.00501</td>\n",
       "      <td>210.110</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_27</th>\n",
       "      <td>-2.352***</td>\n",
       "      <td>0.05181</td>\n",
       "      <td>45.396</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_28</th>\n",
       "      <td>-0.9946***</td>\n",
       "      <td>0.00853</td>\n",
       "      <td>116.559</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_29</th>\n",
       "      <td>-1.9964***</td>\n",
       "      <td>0.01739</td>\n",
       "      <td>114.776</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_30</th>\n",
       "      <td>-1.2041***</td>\n",
       "      <td>0.00881</td>\n",
       "      <td>136.605</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_31</th>\n",
       "      <td>-1.5701***</td>\n",
       "      <td>0.01779</td>\n",
       "      <td>88.245</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_32</th>\n",
       "      <td>-0.3992***</td>\n",
       "      <td>0.00800</td>\n",
       "      <td>49.922</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_33</th>\n",
       "      <td>-1.6545***</td>\n",
       "      <td>0.03011</td>\n",
       "      <td>54.945</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_34</th>\n",
       "      <td>-0.8118***</td>\n",
       "      <td>0.00539</td>\n",
       "      <td>150.681</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_35</th>\n",
       "      <td>-0.5249***</td>\n",
       "      <td>0.00505</td>\n",
       "      <td>104.008</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_36</th>\n",
       "      <td>-0.1542***</td>\n",
       "      <td>0.00441</td>\n",
       "      <td>34.980</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brand_37</th>\n",
       "      <td>-1.8054***</td>\n",
       "      <td>0.01075</td>\n",
       "      <td>167.993</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>home_2</th>\n",
       "      <td>1.0396***</td>\n",
       "      <td>0.00190</td>\n",
       "      <td>546.009</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_2</th>\n",
       "      <td>0.0371***</td>\n",
       "      <td>0.00240</td>\n",
       "      <td>15.449</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_3</th>\n",
       "      <td>0.1443***</td>\n",
       "      <td>0.00323</td>\n",
       "      <td>44.711</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_4</th>\n",
       "      <td>0.1496***</td>\n",
       "      <td>0.00453</td>\n",
       "      <td>33.045</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cla_5</th>\n",
       "      <td>0.1759***</td>\n",
       "      <td>0.00576</td>\n",
       "      <td>30.545</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>0.8577***</td>\n",
       "      <td>0.00199</td>\n",
       "      <td>431.382</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>-0.0769***</td>\n",
       "      <td>0.00131</td>\n",
       "      <td>58.930</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>0.1603***</td>\n",
       "      <td>0.00177</td>\n",
       "      <td>90.396</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>0.004**</td>\n",
       "      <td>0.00155</td>\n",
       "      <td>2.400</td>\n",
       "      <td>0.016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>-0.035***</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>23.077</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>-0.0781***</td>\n",
       "      <td>0.00116</td>\n",
       "      <td>67.208</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>-0.0715***</td>\n",
       "      <td>0.00081</td>\n",
       "      <td>87.763</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>0.0255***</td>\n",
       "      <td>0.00091</td>\n",
       "      <td>28.192</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>-0.0378***</td>\n",
       "      <td>0.00151</td>\n",
       "      <td>24.989</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>-0.035***</td>\n",
       "      <td>0.00109</td>\n",
       "      <td>31.981</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>0.178***</td>\n",
       "      <td>0.00096</td>\n",
       "      <td>186.171</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>-0.3809***</td>\n",
       "      <td>0.00117</td>\n",
       "      <td>325.054</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>-0.1027***</td>\n",
       "      <td>0.00140</td>\n",
       "      <td>73.563</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "variables           theta       se  t (theta == 0)      p\n",
       "in_out        -11.4319***  0.03410         335.214  0.000\n",
       "cy             -0.7822***  0.01964          39.835  0.000\n",
       "hp             -5.1791***  0.02399         215.868  0.000\n",
       "we              0.0966***  0.01993           4.847  0.000\n",
       "le             -2.2447***  0.02282          98.360  0.000\n",
       "wi              5.7558***  0.03346         172.010  0.000\n",
       "he              0.8248***  0.02717          30.356  0.000\n",
       "li             -0.8511***  0.01211          70.283  0.000\n",
       "sp              4.5807***  0.02574         177.983  0.000\n",
       "ac              1.2298***  0.01291          95.290  0.000\n",
       "pr             -0.1479***  0.00233          63.458  0.000\n",
       "brand_2        -0.9293***  0.03121          29.780  0.000\n",
       "brand_3        -0.1702***  0.00441          38.612  0.000\n",
       "brand_4        -0.9347***  0.00473         197.745  0.000\n",
       "brand_5        -0.4647***  0.00437         106.252  0.000\n",
       "brand_6        -0.3667***  0.00429          85.460  0.000\n",
       "brand_7         -1.007***  0.00749         134.531  0.000\n",
       "brand_8        -0.3604***  0.01085          33.226  0.000\n",
       "brand_9        -1.7434***  0.00708         246.204  0.000\n",
       "brand_10       -0.2666***  0.00422          63.106  0.000\n",
       "brand_11       -0.0659***  0.00432          15.260  0.000\n",
       "brand_12       -0.4907***  0.00517          94.861  0.000\n",
       "brand_13       -1.2808***  0.00649         197.398  0.000\n",
       "brand_14       -2.0711***  0.00904         229.029  0.000\n",
       "brand_15       -1.9547***  0.00861         226.919  0.000\n",
       "brand_16       -1.2525***  0.00420         298.329  0.000\n",
       "brand_17       -0.9843***  0.00499         197.379  0.000\n",
       "brand_18        0.4051***  0.00465          87.111  0.000\n",
       "brand_19       -1.2918***  0.00575         224.632  0.000\n",
       "brand_20       -0.3086***  0.00523          58.997  0.000\n",
       "brand_21       -0.2706***  0.00406          66.656  0.000\n",
       "brand_22       -0.2152***  0.00415          51.891  0.000\n",
       "brand_23       -0.1661***  0.00450          36.914  0.000\n",
       "brand_24       -0.7064***  0.00424         166.679  0.000\n",
       "brand_25        -0.815***  0.00503         161.986  0.000\n",
       "brand_26       -1.0517***  0.00501         210.110  0.000\n",
       "brand_27        -2.352***  0.05181          45.396  0.000\n",
       "brand_28       -0.9946***  0.00853         116.559  0.000\n",
       "brand_29       -1.9964***  0.01739         114.776  0.000\n",
       "brand_30       -1.2041***  0.00881         136.605  0.000\n",
       "brand_31       -1.5701***  0.01779          88.245  0.000\n",
       "brand_32       -0.3992***  0.00800          49.922  0.000\n",
       "brand_33       -1.6545***  0.03011          54.945  0.000\n",
       "brand_34       -0.8118***  0.00539         150.681  0.000\n",
       "brand_35       -0.5249***  0.00505         104.008  0.000\n",
       "brand_36       -0.1542***  0.00441          34.980  0.000\n",
       "brand_37       -1.8054***  0.01075         167.993  0.000\n",
       "home_2          1.0396***  0.00190         546.009  0.000\n",
       "cla_2           0.0371***  0.00240          15.449  0.000\n",
       "cla_3           0.1443***  0.00323          44.711  0.000\n",
       "cla_4           0.1496***  0.00453          33.045  0.000\n",
       "cla_5           0.1759***  0.00576          30.545  0.000\n",
       "group_in_out    0.8577***  0.00199         431.382  0.000\n",
       "group_cy       -0.0769***  0.00131          58.930  0.000\n",
       "group_hp        0.1603***  0.00177          90.396  0.000\n",
       "group_we          0.004**  0.00155           2.400  0.016\n",
       "group_le        -0.035***  0.00151          23.077  0.000\n",
       "group_wi       -0.0781***  0.00116          67.208  0.000\n",
       "group_he       -0.0715***  0.00081          87.763  0.000\n",
       "group_li        0.0255***  0.00091          28.192  0.000\n",
       "group_sp       -0.0378***  0.00151          24.989  0.000\n",
       "group_ac        -0.035***  0.00109          31.981  0.000\n",
       "group_brand      0.178***  0.00096         186.171  0.000\n",
       "group_home     -0.3809***  0.00117         325.054  0.000\n",
       "group_cla      -0.1027***  0.00140          73.563  0.000"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_table(ThetaOptBLP, SEOptBLP, N, x_vars, nest_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.85770228, -0.0769479 ,  0.16025247,  0.00372009, -0.03495362,\n",
       "       -0.07813212, -0.0715241 ,  0.02554132, -0.0378455 , -0.03500794,\n",
       "        0.17801704, -0.38093785, -0.10268783])"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOptBLP[K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.17336341520624832"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogitBLP_beta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.1478762137180711"
      ]
     },
     "execution_count": 432,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ThetaOptBLP[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.22790639460890777"
      ]
     },
     "execution_count": 433,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FKN_theta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8471166690149027"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Logit_beta[pr_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "qOpt = IPDL_ccp(ThetaOptBLP, z_logit, Psi, tol = 1.0e-30)\n",
    "HessOpt = compute_pertubation_hessian(qOpt, z_logit, ThetaOptBLP, Psi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([qOpt[t].min() for t in np.arange(T)]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.array([la.eigvals(HessOpt[t]).min() for t in np.arange(T)]).min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For market $t=1$ the price elasticities are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Semi-elasticity wrt. product</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semi-elasticity of product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.001979</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000061</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>0.000067</td>\n",
       "      <td>0.000061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.144316</td>\n",
       "      <td>-0.246123</td>\n",
       "      <td>0.003592</td>\n",
       "      <td>0.009841</td>\n",
       "      <td>0.004461</td>\n",
       "      <td>-0.004246</td>\n",
       "      <td>0.005677</td>\n",
       "      <td>-0.027723</td>\n",
       "      <td>-0.002422</td>\n",
       "      <td>-0.005502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003966</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>0.004857</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.001766</td>\n",
       "      <td>-0.000215</td>\n",
       "      <td>0.004721</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>-0.011980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.145908</td>\n",
       "      <td>0.001760</td>\n",
       "      <td>-0.240558</td>\n",
       "      <td>-0.000314</td>\n",
       "      <td>0.003737</td>\n",
       "      <td>0.003791</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.009681</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>-0.003620</td>\n",
       "      <td>-0.001006</td>\n",
       "      <td>-0.000527</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.002802</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.005895</td>\n",
       "      <td>0.005980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.147739</td>\n",
       "      <td>0.002849</td>\n",
       "      <td>-0.000186</td>\n",
       "      <td>-0.248435</td>\n",
       "      <td>0.014689</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>-0.006868</td>\n",
       "      <td>0.023460</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000750</td>\n",
       "      <td>-0.004052</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>-0.000352</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>-0.001651</td>\n",
       "      <td>0.019442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.145260</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.003064</td>\n",
       "      <td>0.020379</td>\n",
       "      <td>-0.244481</td>\n",
       "      <td>0.004845</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.008788</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.002028</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002237</td>\n",
       "      <td>0.000887</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>-0.002453</td>\n",
       "      <td>0.002850</td>\n",
       "      <td>0.018763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.145252</td>\n",
       "      <td>-0.001429</td>\n",
       "      <td>0.002605</td>\n",
       "      <td>0.007306</td>\n",
       "      <td>0.004061</td>\n",
       "      <td>-0.218940</td>\n",
       "      <td>0.003768</td>\n",
       "      <td>-0.006449</td>\n",
       "      <td>-0.000892</td>\n",
       "      <td>-0.001054</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002263</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.003549</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>-0.004745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.146125</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>-0.013299</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.006275</td>\n",
       "      <td>-0.246834</td>\n",
       "      <td>0.053557</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>0.005577</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007328</td>\n",
       "      <td>0.003227</td>\n",
       "      <td>0.000284</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.000361</td>\n",
       "      <td>0.000087</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>-0.002954</td>\n",
       "      <td>0.010263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.146726</td>\n",
       "      <td>-0.005247</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.015339</td>\n",
       "      <td>0.004141</td>\n",
       "      <td>-0.003626</td>\n",
       "      <td>0.018082</td>\n",
       "      <td>-0.245662</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>...</td>\n",
       "      <td>0.011619</td>\n",
       "      <td>0.009894</td>\n",
       "      <td>0.006593</td>\n",
       "      <td>0.002128</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>-0.000166</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>-0.000284</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>-0.015202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.145076</td>\n",
       "      <td>-0.005477</td>\n",
       "      <td>0.003220</td>\n",
       "      <td>0.009259</td>\n",
       "      <td>0.004417</td>\n",
       "      <td>-0.005993</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>0.008605</td>\n",
       "      <td>-0.241839</td>\n",
       "      <td>-0.004817</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010990</td>\n",
       "      <td>0.009599</td>\n",
       "      <td>0.004265</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>0.004051</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.002310</td>\n",
       "      <td>-0.010082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.145270</td>\n",
       "      <td>-0.003000</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.010095</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>-0.001708</td>\n",
       "      <td>0.005426</td>\n",
       "      <td>-0.020565</td>\n",
       "      <td>-0.001162</td>\n",
       "      <td>-0.225734</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003310</td>\n",
       "      <td>0.003340</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001355</td>\n",
       "      <td>-0.000085</td>\n",
       "      <td>0.004207</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.000074</td>\n",
       "      <td>-0.009514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.145593</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>-0.009009</td>\n",
       "      <td>0.004104</td>\n",
       "      <td>0.003636</td>\n",
       "      <td>-0.004131</td>\n",
       "      <td>0.013605</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.029525</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>-0.001286</td>\n",
       "      <td>0.001299</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>-0.001805</td>\n",
       "      <td>0.009778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.145107</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>-0.003048</td>\n",
       "      <td>0.002862</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>-0.000930</td>\n",
       "      <td>-0.000134</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>-0.000654</td>\n",
       "      <td>0.023098</td>\n",
       "      <td>0.001986</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>-0.000011</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>-0.006761</td>\n",
       "      <td>0.002582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.145877</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>-0.000539</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.002948</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.000370</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001280</td>\n",
       "      <td>0.001980</td>\n",
       "      <td>0.017190</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>-0.000093</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.004312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.152510</td>\n",
       "      <td>0.002710</td>\n",
       "      <td>-0.000187</td>\n",
       "      <td>0.002691</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.005653</td>\n",
       "      <td>-0.000262</td>\n",
       "      <td>0.021146</td>\n",
       "      <td>0.000931</td>\n",
       "      <td>0.003907</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>-0.000572</td>\n",
       "      <td>-0.002514</td>\n",
       "      <td>0.021568</td>\n",
       "      <td>0.015639</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>-0.005173</td>\n",
       "      <td>0.002254</td>\n",
       "      <td>0.004862</td>\n",
       "      <td>0.005444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.145315</td>\n",
       "      <td>-0.001345</td>\n",
       "      <td>0.002566</td>\n",
       "      <td>0.007203</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.004078</td>\n",
       "      <td>-0.015061</td>\n",
       "      <td>-0.000922</td>\n",
       "      <td>-0.003413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.002325</td>\n",
       "      <td>0.004146</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>-0.000149</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>-0.000493</td>\n",
       "      <td>-0.005853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.151679</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>-0.002791</td>\n",
       "      <td>0.004413</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.004622</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.014634</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.002649</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000224</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>-0.000976</td>\n",
       "      <td>-0.000864</td>\n",
       "      <td>-0.001011</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.000152</td>\n",
       "      <td>0.009648</td>\n",
       "      <td>0.026062</td>\n",
       "      <td>0.002147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.145650</td>\n",
       "      <td>0.004652</td>\n",
       "      <td>-0.000081</td>\n",
       "      <td>-0.007451</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>-0.003877</td>\n",
       "      <td>0.009688</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.008126</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>-0.002123</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000211</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.004308</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>0.005675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.144985</td>\n",
       "      <td>0.009033</td>\n",
       "      <td>0.003422</td>\n",
       "      <td>-0.009550</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.003589</td>\n",
       "      <td>-0.004180</td>\n",
       "      <td>0.012906</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>-0.001134</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000197</td>\n",
       "      <td>0.001635</td>\n",
       "      <td>-0.000147</td>\n",
       "      <td>0.009112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.144166</td>\n",
       "      <td>0.008077</td>\n",
       "      <td>0.005312</td>\n",
       "      <td>0.007928</td>\n",
       "      <td>-0.004779</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.004293</td>\n",
       "      <td>-0.002646</td>\n",
       "      <td>-0.000031</td>\n",
       "      <td>-0.000640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>0.002875</td>\n",
       "      <td>0.005663</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>-0.000014</td>\n",
       "      <td>0.006015</td>\n",
       "      <td>-0.004506</td>\n",
       "      <td>-0.000376</td>\n",
       "      <td>0.003011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.148041</td>\n",
       "      <td>0.010522</td>\n",
       "      <td>-0.004498</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.005789</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.020151</td>\n",
       "      <td>0.000690</td>\n",
       "      <td>0.004415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000556</td>\n",
       "      <td>-0.001647</td>\n",
       "      <td>-0.005663</td>\n",
       "      <td>-0.002833</td>\n",
       "      <td>-0.001862</td>\n",
       "      <td>0.000156</td>\n",
       "      <td>-0.006478</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.008534</td>\n",
       "      <td>0.005411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.156210</td>\n",
       "      <td>0.011516</td>\n",
       "      <td>-0.002163</td>\n",
       "      <td>-0.000312</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.006449</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.023683</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.004846</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>-0.000944</td>\n",
       "      <td>-0.003596</td>\n",
       "      <td>-0.002610</td>\n",
       "      <td>-0.001445</td>\n",
       "      <td>0.000173</td>\n",
       "      <td>-0.005050</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.006805</td>\n",
       "      <td>0.006898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.144773</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.001542</td>\n",
       "      <td>0.004467</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>-0.000442</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>-0.018816</td>\n",
       "      <td>0.012131</td>\n",
       "      <td>-0.003249</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>-0.000348</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.002989</td>\n",
       "      <td>-0.010273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.145551</td>\n",
       "      <td>0.003782</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>-0.008436</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.010189</td>\n",
       "      <td>-0.004406</td>\n",
       "      <td>0.009319</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>-0.002390</td>\n",
       "      <td>0.002309</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>-0.000063</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.003493</td>\n",
       "      <td>-0.006278</td>\n",
       "      <td>0.005265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.145152</td>\n",
       "      <td>-0.000973</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.008398</td>\n",
       "      <td>-0.004684</td>\n",
       "      <td>0.005859</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>-0.003701</td>\n",
       "      <td>-0.000168</td>\n",
       "      <td>-0.002130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001618</td>\n",
       "      <td>0.002648</td>\n",
       "      <td>0.003929</td>\n",
       "      <td>0.001503</td>\n",
       "      <td>0.001129</td>\n",
       "      <td>-0.000142</td>\n",
       "      <td>0.003761</td>\n",
       "      <td>-0.002449</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>-0.000817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.145464</td>\n",
       "      <td>-0.002684</td>\n",
       "      <td>0.001041</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.007975</td>\n",
       "      <td>0.001847</td>\n",
       "      <td>-0.006397</td>\n",
       "      <td>-0.001128</td>\n",
       "      <td>-0.005236</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001609</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.002081</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.000392</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>0.002713</td>\n",
       "      <td>-0.002807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.145187</td>\n",
       "      <td>-0.002358</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.008525</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>-0.033454</td>\n",
       "      <td>0.005113</td>\n",
       "      <td>-0.006120</td>\n",
       "      <td>-0.017322</td>\n",
       "      <td>-0.000540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003740</td>\n",
       "      <td>0.003312</td>\n",
       "      <td>0.003951</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.000763</td>\n",
       "      <td>0.005413</td>\n",
       "      <td>0.003816</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.003119</td>\n",
       "      <td>-0.006150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.155977</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>-0.001902</td>\n",
       "      <td>0.004503</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.004446</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.017597</td>\n",
       "      <td>0.000796</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000223</td>\n",
       "      <td>0.000477</td>\n",
       "      <td>-0.002650</td>\n",
       "      <td>-0.001366</td>\n",
       "      <td>-0.000491</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>-0.005324</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.006331</td>\n",
       "      <td>0.004717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.144935</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.014987</td>\n",
       "      <td>0.001604</td>\n",
       "      <td>-0.004869</td>\n",
       "      <td>0.003653</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.005222</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002065</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.001054</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>-0.002268</td>\n",
       "      <td>0.003368</td>\n",
       "      <td>0.007367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.144885</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.015298</td>\n",
       "      <td>0.003013</td>\n",
       "      <td>-0.005256</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.005840</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002235</td>\n",
       "      <td>0.001580</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.001289</td>\n",
       "      <td>0.001051</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>0.003438</td>\n",
       "      <td>-0.000884</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.005305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.144924</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.014164</td>\n",
       "      <td>-0.006786</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.012707</td>\n",
       "      <td>-0.003555</td>\n",
       "      <td>0.004281</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.002670</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>-0.002157</td>\n",
       "      <td>0.004320</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.003883</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>-0.008480</td>\n",
       "      <td>0.002396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.153285</td>\n",
       "      <td>0.002390</td>\n",
       "      <td>0.010117</td>\n",
       "      <td>0.005192</td>\n",
       "      <td>0.003751</td>\n",
       "      <td>0.005247</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.021894</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.003798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000634</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>-0.003314</td>\n",
       "      <td>-0.001629</td>\n",
       "      <td>-0.000439</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-0.006596</td>\n",
       "      <td>0.002855</td>\n",
       "      <td>0.008400</td>\n",
       "      <td>0.005015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.145243</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.020770</td>\n",
       "      <td>0.005799</td>\n",
       "      <td>0.004873</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.008205</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003079</td>\n",
       "      <td>0.001584</td>\n",
       "      <td>0.001677</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.002822</td>\n",
       "      <td>-0.002451</td>\n",
       "      <td>0.002204</td>\n",
       "      <td>0.018368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.145152</td>\n",
       "      <td>-0.003937</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.028935</td>\n",
       "      <td>0.019174</td>\n",
       "      <td>-0.008462</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>-0.028431</td>\n",
       "      <td>-0.002183</td>\n",
       "      <td>-0.007437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>0.004942</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.004517</td>\n",
       "      <td>0.002186</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>-0.004305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.145730</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>-0.007172</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.005034</td>\n",
       "      <td>-0.002482</td>\n",
       "      <td>0.007931</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.007849</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001759</td>\n",
       "      <td>-0.003578</td>\n",
       "      <td>0.002639</td>\n",
       "      <td>-0.000421</td>\n",
       "      <td>-0.000729</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.003521</td>\n",
       "      <td>-0.003765</td>\n",
       "      <td>0.005441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.145702</td>\n",
       "      <td>0.004464</td>\n",
       "      <td>-0.000066</td>\n",
       "      <td>-0.008093</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.009835</td>\n",
       "      <td>-0.004209</td>\n",
       "      <td>0.011248</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.007134</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000662</td>\n",
       "      <td>-0.002225</td>\n",
       "      <td>0.000169</td>\n",
       "      <td>0.000183</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.003967</td>\n",
       "      <td>-0.002137</td>\n",
       "      <td>0.005954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.145396</td>\n",
       "      <td>0.002950</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>-0.001928</td>\n",
       "      <td>-0.004142</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>0.045659</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>0.004515</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245540</td>\n",
       "      <td>0.007418</td>\n",
       "      <td>-0.001155</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>-0.001850</td>\n",
       "      <td>0.005359</td>\n",
       "      <td>0.012133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.145588</td>\n",
       "      <td>0.003404</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>-0.012672</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>0.047329</td>\n",
       "      <td>0.003843</td>\n",
       "      <td>0.005544</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009030</td>\n",
       "      <td>-0.247433</td>\n",
       "      <td>0.001134</td>\n",
       "      <td>-0.000634</td>\n",
       "      <td>-0.000852</td>\n",
       "      <td>0.000108</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>-0.001784</td>\n",
       "      <td>0.010597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.147409</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>-0.003243</td>\n",
       "      <td>0.002515</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.000222</td>\n",
       "      <td>0.015286</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.003894</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.000681</td>\n",
       "      <td>0.000550</td>\n",
       "      <td>-0.238644</td>\n",
       "      <td>-0.001333</td>\n",
       "      <td>-0.000793</td>\n",
       "      <td>0.000102</td>\n",
       "      <td>-0.005362</td>\n",
       "      <td>0.002177</td>\n",
       "      <td>0.006956</td>\n",
       "      <td>0.005213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.146328</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>-0.002612</td>\n",
       "      <td>-0.000672</td>\n",
       "      <td>0.003135</td>\n",
       "      <td>0.005335</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.014302</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.003723</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>-0.000891</td>\n",
       "      <td>-0.003865</td>\n",
       "      <td>-0.230930</td>\n",
       "      <td>0.014014</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>-0.003692</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.006719</td>\n",
       "      <td>0.006319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.145922</td>\n",
       "      <td>0.003070</td>\n",
       "      <td>-0.001869</td>\n",
       "      <td>-0.002116</td>\n",
       "      <td>0.003139</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.008897</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>-0.001635</td>\n",
       "      <td>-0.003143</td>\n",
       "      <td>0.019147</td>\n",
       "      <td>-0.235891</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>-0.001495</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.004986</td>\n",
       "      <td>0.007454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.144337</td>\n",
       "      <td>-0.005087</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.007296</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.008647</td>\n",
       "      <td>0.003676</td>\n",
       "      <td>-0.020791</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>-0.003705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.002816</td>\n",
       "      <td>0.005514</td>\n",
       "      <td>0.025860</td>\n",
       "      <td>0.018866</td>\n",
       "      <td>-0.274655</td>\n",
       "      <td>0.005316</td>\n",
       "      <td>-0.000077</td>\n",
       "      <td>-0.000567</td>\n",
       "      <td>-0.008888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.147144</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>-0.002518</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.003436</td>\n",
       "      <td>0.004641</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.014112</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.003396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000235</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>-0.005378</td>\n",
       "      <td>-0.001277</td>\n",
       "      <td>-0.000379</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>-0.198041</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.005935</td>\n",
       "      <td>0.005342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.144404</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.003277</td>\n",
       "      <td>0.006685</td>\n",
       "      <td>-0.005593</td>\n",
       "      <td>0.005042</td>\n",
       "      <td>0.003365</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002278</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.004544</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>-0.000003</td>\n",
       "      <td>0.004124</td>\n",
       "      <td>-0.242762</td>\n",
       "      <td>0.018862</td>\n",
       "      <td>0.000209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.145017</td>\n",
       "      <td>0.000618</td>\n",
       "      <td>0.004708</td>\n",
       "      <td>-0.002232</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>0.003054</td>\n",
       "      <td>-0.002062</td>\n",
       "      <td>0.002539</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002819</td>\n",
       "      <td>-0.000771</td>\n",
       "      <td>0.006201</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>0.005275</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>-0.238206</td>\n",
       "      <td>0.001283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.145192</td>\n",
       "      <td>-0.005164</td>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.028952</td>\n",
       "      <td>0.020139</td>\n",
       "      <td>-0.006077</td>\n",
       "      <td>0.007892</td>\n",
       "      <td>-0.034625</td>\n",
       "      <td>-0.001922</td>\n",
       "      <td>-0.007521</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.005046</td>\n",
       "      <td>0.005121</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.001849</td>\n",
       "      <td>-0.000162</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>-0.254055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Semi-elasticity wrt. product        0         1         2         3   \\\n",
       "Semi-elasticity of product                                             \n",
       "0                            -0.001979  0.000026  0.000054  0.000093   \n",
       "1                             0.144316 -0.246123  0.003592  0.009841   \n",
       "2                             0.145908  0.001760 -0.240558 -0.000314   \n",
       "3                             0.147739  0.002849 -0.000186 -0.248435   \n",
       "4                             0.145260  0.001792  0.003064  0.020379   \n",
       "5                             0.145252 -0.001429  0.002605  0.007306   \n",
       "6                             0.146125  0.003182  0.000520 -0.013299   \n",
       "7                             0.146726 -0.005247  0.003741  0.015339   \n",
       "8                             0.145076 -0.005477  0.003220  0.009259   \n",
       "9                             0.145270 -0.003000  0.002881  0.010095   \n",
       "10                            0.145593  0.001545  0.002589 -0.009009   \n",
       "11                            0.145107  0.000603  0.004307 -0.003048   \n",
       "12                            0.145877  0.001076 -0.000539  0.005263   \n",
       "13                            0.152510  0.002710 -0.000187  0.002691   \n",
       "14                            0.145315 -0.001345  0.002566  0.007203   \n",
       "15                            0.151679  0.002114 -0.002791  0.004413   \n",
       "16                            0.145650  0.004652 -0.000081 -0.007451   \n",
       "17                            0.144985  0.009033  0.003422 -0.009550   \n",
       "18                            0.144166  0.008077  0.005312  0.007928   \n",
       "19                            0.148041  0.010522 -0.004498  0.002138   \n",
       "20                            0.156210  0.011516 -0.002163 -0.000312   \n",
       "21                            0.144773  0.002085  0.001542  0.004467   \n",
       "22                            0.145551  0.003782  0.000913 -0.008436   \n",
       "23                            0.145152 -0.000973  0.003542  0.008398   \n",
       "24                            0.145464 -0.002684  0.001041  0.003521   \n",
       "25                            0.145187 -0.002358  0.003056  0.008525   \n",
       "26                            0.155977  0.002384 -0.001902  0.004503   \n",
       "27                            0.144935  0.001643  0.014987  0.001604   \n",
       "28                            0.144885  0.003241  0.015298  0.003013   \n",
       "29                            0.144924  0.002258  0.014164 -0.006786   \n",
       "30                            0.153285  0.002390  0.010117  0.005192   \n",
       "31                            0.145243  0.001705  0.003601  0.020770   \n",
       "32                            0.145152 -0.003937  0.003373  0.028935   \n",
       "33                            0.145730  0.004184  0.001931 -0.007172   \n",
       "34                            0.145702  0.004464 -0.000066 -0.008093   \n",
       "35                            0.145396  0.002950  0.001846 -0.001928   \n",
       "36                            0.145588  0.003404  0.001891 -0.012672   \n",
       "37                            0.147409  0.002131 -0.003243  0.002515   \n",
       "38                            0.146328  0.002532 -0.002612 -0.000672   \n",
       "39                            0.145922  0.003070 -0.001869 -0.002116   \n",
       "40                            0.144337 -0.005087  0.003487  0.007296   \n",
       "41                            0.147144  0.002078 -0.002518  0.002146   \n",
       "42                            0.144404  0.002088  0.003277  0.006685   \n",
       "43                            0.145017  0.000618  0.004708 -0.002232   \n",
       "44                            0.145192 -0.005164  0.005263  0.028952   \n",
       "\n",
       "Semi-elasticity wrt. product        4         5         6         7   \\\n",
       "Semi-elasticity of product                                             \n",
       "0                             0.000066  0.000078  0.000047  0.000141   \n",
       "1                             0.004461 -0.004246  0.005677 -0.027723   \n",
       "2                             0.003737  0.003791  0.000454  0.009681   \n",
       "3                             0.014689  0.006283 -0.006868  0.023460   \n",
       "4                            -0.244481  0.004845  0.002169  0.008788   \n",
       "5                             0.004061 -0.218940  0.003768 -0.006449   \n",
       "6                             0.003027  0.006275 -0.246834  0.053557   \n",
       "7                             0.004141 -0.003626  0.018082 -0.245662   \n",
       "8                             0.004417 -0.005993  0.015221  0.008605   \n",
       "9                             0.002753 -0.001708  0.005426 -0.020565   \n",
       "10                            0.004104  0.003636 -0.004131  0.013605   \n",
       "11                            0.002862  0.003465 -0.000930 -0.000134   \n",
       "12                            0.001712  0.002277  0.002948  0.004619   \n",
       "13                            0.003334  0.005653 -0.000262  0.021146   \n",
       "14                            0.002200  0.002087  0.004078 -0.015061   \n",
       "15                            0.002439  0.004622  0.001052  0.014634   \n",
       "16                            0.001496  0.006102 -0.003877  0.009688   \n",
       "17                            0.005000  0.003589 -0.004180  0.012906   \n",
       "18                           -0.004779  0.002183  0.004293 -0.002646   \n",
       "19                            0.001709  0.005789  0.001068  0.020151   \n",
       "20                            0.003252  0.006449  0.001095  0.023683   \n",
       "21                            0.003521 -0.000442  0.001738 -0.018816   \n",
       "22                            0.002081  0.010189 -0.004406  0.009319   \n",
       "23                           -0.004684  0.005859  0.004060 -0.003701   \n",
       "24                            0.003248  0.007975  0.001847 -0.006397   \n",
       "25                            0.005102 -0.033454  0.005113 -0.006120   \n",
       "26                            0.002346  0.004446  0.001488  0.017597   \n",
       "27                           -0.004869  0.003653  0.001322  0.005222   \n",
       "28                           -0.005256  0.002961  0.001626  0.002069   \n",
       "29                            0.002287  0.012707 -0.003555  0.004281   \n",
       "30                            0.003751  0.005247  0.001499  0.021894   \n",
       "31                            0.005799  0.004873  0.001081  0.008205   \n",
       "32                            0.019174 -0.008462  0.007009 -0.028431   \n",
       "33                            0.001717  0.005034 -0.002482  0.007931   \n",
       "34                            0.000695  0.009835 -0.004209  0.011248   \n",
       "35                           -0.004142  0.005000  0.009722  0.045659   \n",
       "36                            0.001998  0.006697  0.005211  0.047329   \n",
       "37                            0.002259  0.004804  0.000222  0.015286   \n",
       "38                            0.003135  0.005335  0.000500  0.014302   \n",
       "39                            0.003139  0.005075  0.001120  0.008897   \n",
       "40                            0.000724  0.008647  0.003676 -0.020791   \n",
       "41                            0.003436  0.004641  0.000387  0.014112   \n",
       "42                           -0.005593  0.005042  0.003365 -0.001375   \n",
       "43                            0.002776  0.003054 -0.002062  0.002539   \n",
       "44                            0.020139 -0.006077  0.007892 -0.034625   \n",
       "\n",
       "Semi-elasticity wrt. product        8         9   ...        35        36  \\\n",
       "Semi-elasticity of product                        ...                       \n",
       "0                             0.000012  0.000048  ...  0.000035  0.000029   \n",
       "1                            -0.002422 -0.005502  ...  0.003966  0.003760   \n",
       "2                             0.000697  0.002588  ...  0.001216  0.001023   \n",
       "3                             0.001185  0.005359  ... -0.000750 -0.004052   \n",
       "4                             0.000784  0.002028  ... -0.002237  0.000887   \n",
       "5                            -0.000892 -0.001054  ...  0.002263  0.002490   \n",
       "6                             0.003773  0.005577  ...  0.007328  0.003227   \n",
       "7                             0.000720 -0.007137  ...  0.011619  0.009894   \n",
       "8                            -0.241839 -0.004817  ...  0.010990  0.009599   \n",
       "9                            -0.001162 -0.225734  ...  0.003310  0.003340   \n",
       "10                            0.000844  0.029525  ...  0.001311 -0.001286   \n",
       "11                            0.000417  0.000421  ...  0.003436 -0.000654   \n",
       "12                            0.000370  0.000561  ...  0.001280  0.001980   \n",
       "13                            0.000931  0.003907  ...  0.000758 -0.000572   \n",
       "14                           -0.000922 -0.003413  ...  0.002637  0.002325   \n",
       "15                            0.000693  0.002649  ... -0.000224  0.000494   \n",
       "16                            0.000783  0.008126  ...  0.000151 -0.002123   \n",
       "17                            0.000405  0.003741  ...  0.001237 -0.001134   \n",
       "18                           -0.000031 -0.000640  ... -0.001250  0.002875   \n",
       "19                            0.000690  0.004415  ... -0.000556 -0.001647   \n",
       "20                            0.000661  0.004846  ...  0.000717 -0.000944   \n",
       "21                            0.012131 -0.003249  ...  0.001254  0.001733   \n",
       "22                            0.001010  0.006687  ...  0.000896 -0.002390   \n",
       "23                           -0.000168 -0.002130  ... -0.001618  0.002648   \n",
       "24                           -0.001128 -0.005236  ...  0.001609  0.001373   \n",
       "25                           -0.017322 -0.000540  ...  0.003740  0.003312   \n",
       "26                            0.000796  0.003373  ...  0.000223  0.000477   \n",
       "27                            0.000660  0.002179  ... -0.002065  0.001286   \n",
       "28                            0.000617  0.005840  ... -0.002235  0.001580   \n",
       "29                            0.000473  0.002670  ...  0.001367 -0.002157   \n",
       "30                            0.000871  0.003798  ...  0.000634  0.000421   \n",
       "31                            0.000816  0.002815  ... -0.003079  0.001584   \n",
       "32                           -0.002183 -0.007437  ...  0.004632  0.003928   \n",
       "33                            0.000591  0.007849  ...  0.001759 -0.003578   \n",
       "34                            0.000825  0.007134  ... -0.000662 -0.002225   \n",
       "35                            0.003615  0.004515  ... -0.245540  0.007418   \n",
       "36                            0.003843  0.005544  ...  0.009030 -0.247433   \n",
       "37                            0.000828  0.003894  ... -0.000681  0.000550   \n",
       "38                            0.000894  0.003723  ...  0.000526 -0.000891   \n",
       "39                            0.000883  0.004319  ...  0.002069 -0.001635   \n",
       "40                            0.000994 -0.003705  ...  0.002232  0.002816   \n",
       "41                            0.000788  0.003396  ...  0.000235  0.000975   \n",
       "42                            0.000125  0.002908  ... -0.002278  0.002024   \n",
       "43                            0.000400  0.000053  ...  0.002819 -0.000771   \n",
       "44                           -0.001922 -0.007521  ...  0.007032  0.005046   \n",
       "\n",
       "Semi-elasticity wrt. product        37        38        39        40  \\\n",
       "Semi-elasticity of product                                             \n",
       "0                             0.000061  0.000021  0.000015  0.000001   \n",
       "1                             0.004857  0.001990  0.001766 -0.000215   \n",
       "2                            -0.003620 -0.001006 -0.000527  0.000072   \n",
       "3                             0.001659 -0.000153 -0.000352  0.000089   \n",
       "4                             0.002067  0.000990  0.000725  0.000012   \n",
       "5                             0.003685  0.001412  0.000983  0.000123   \n",
       "6                             0.000284  0.000220  0.000361  0.000087   \n",
       "7                             0.006593  0.002128  0.000969 -0.000166   \n",
       "8                             0.004265  0.001589  0.001149  0.000095   \n",
       "9                             0.004840  0.001596  0.001355 -0.000085   \n",
       "10                            0.001299  0.000865  0.000703  0.000051   \n",
       "11                            0.023098  0.001986  0.001189 -0.000011   \n",
       "12                            0.017190  0.000383 -0.000093  0.000024   \n",
       "13                           -0.002514  0.021568  0.015639  0.001503   \n",
       "14                            0.004146  0.001446  0.001180 -0.000149   \n",
       "15                           -0.000976 -0.000864 -0.001011  0.000099   \n",
       "16                            0.000907  0.000270  0.000211  0.000059   \n",
       "17                           -0.001318  0.000725  0.000743  0.000064   \n",
       "18                            0.005663  0.002434  0.001446 -0.000014   \n",
       "19                           -0.005663 -0.002833 -0.001862  0.000156   \n",
       "20                           -0.003596 -0.002610 -0.001445  0.000173   \n",
       "21                            0.001919  0.001029  0.001070 -0.000348   \n",
       "22                            0.002309  0.000638  0.000365 -0.000063   \n",
       "23                            0.003929  0.001503  0.001129 -0.000142   \n",
       "24                            0.002081  0.000840  0.000588 -0.000392   \n",
       "25                            0.003951  0.001037  0.000763  0.005413   \n",
       "26                           -0.002650 -0.001366 -0.000491  0.000125   \n",
       "27                            0.001094  0.001079  0.001054  0.000009   \n",
       "28                            0.002660  0.001289  0.001051  0.000013   \n",
       "29                            0.004320  0.001243  0.000528  0.000016   \n",
       "30                           -0.003314 -0.001629 -0.000439  0.000146   \n",
       "31                            0.001677  0.001070  0.000608  0.000028   \n",
       "32                            0.004942  0.001718  0.001415 -0.000162   \n",
       "33                            0.002639 -0.000421 -0.000729  0.000042   \n",
       "34                            0.000169  0.000183  0.000153  0.000064   \n",
       "35                           -0.001155  0.000308  0.000885  0.000070   \n",
       "36                            0.001134 -0.000634 -0.000852  0.000108   \n",
       "37                           -0.238644 -0.001333 -0.000793  0.000102   \n",
       "38                           -0.003865 -0.230930  0.014014  0.001389   \n",
       "39                           -0.003143  0.019147 -0.235891  0.001384   \n",
       "40                            0.005514  0.025860  0.018866 -0.274655   \n",
       "41                           -0.005378 -0.001277 -0.000379  0.000099   \n",
       "42                            0.004544  0.001466  0.001109 -0.000003   \n",
       "43                            0.006201  0.002066  0.001122 -0.000009   \n",
       "44                            0.005121  0.002141  0.001849 -0.000162   \n",
       "\n",
       "Semi-elasticity wrt. product        41        42        43        44  \n",
       "Semi-elasticity of product                                            \n",
       "0                             0.000061  0.000029  0.000067  0.000061  \n",
       "1                             0.004721  0.002280  0.001581 -0.011980  \n",
       "2                            -0.002802  0.001753  0.005895  0.005980  \n",
       "3                             0.001412  0.002113 -0.001651  0.019442  \n",
       "4                             0.003135 -0.002453  0.002850  0.018763  \n",
       "5                             0.003549  0.001853  0.002628 -0.004745  \n",
       "6                             0.000493  0.002060 -0.002954  0.010263  \n",
       "7                             0.006068 -0.000284  0.001228 -0.015202  \n",
       "8                             0.004051  0.000309  0.002310 -0.010082  \n",
       "9                             0.004207  0.001732  0.000074 -0.009514  \n",
       "10                            0.002408  0.001053 -0.001805  0.009778  \n",
       "11                            0.005396  0.000072 -0.006761  0.002582  \n",
       "12                            0.000564  0.000276  0.003850  0.004312  \n",
       "13                           -0.005173  0.002254  0.004862  0.005444  \n",
       "14                            0.003766  0.001486 -0.000493 -0.005853  \n",
       "15                           -0.000152  0.009648  0.026062  0.002147  \n",
       "16                            0.001468  0.004308 -0.003195  0.005675  \n",
       "17                            0.000197  0.001635 -0.000147  0.009112  \n",
       "18                            0.006015 -0.004506 -0.000376  0.003011  \n",
       "19                           -0.006478  0.002193  0.008534  0.005411  \n",
       "20                           -0.005050  0.002523  0.006805  0.006898  \n",
       "21                            0.001608  0.001412  0.002989 -0.010273  \n",
       "22                            0.002104  0.003493 -0.006278  0.005265  \n",
       "23                            0.003761 -0.002449  0.001182 -0.000817  \n",
       "24                            0.002430  0.001759  0.002713 -0.002807  \n",
       "25                            0.003816  0.001221  0.003119 -0.006150  \n",
       "26                           -0.005324  0.001824  0.006331  0.004717  \n",
       "27                            0.002789 -0.002268  0.003368  0.007367  \n",
       "28                            0.003438 -0.000884  0.000539  0.005305  \n",
       "29                            0.003883  0.002020 -0.008480  0.002396  \n",
       "30                           -0.006596  0.002855  0.008400  0.005015  \n",
       "31                            0.002822 -0.002451  0.002204  0.018368  \n",
       "32                            0.004517  0.002186  0.002865 -0.004305  \n",
       "33                            0.002440  0.003521 -0.003765  0.005441  \n",
       "34                            0.000869  0.003967 -0.002137  0.005954  \n",
       "35                            0.000397 -0.001850  0.005359  0.012133  \n",
       "36                            0.002005  0.002000 -0.001784  0.010597  \n",
       "37                           -0.005362  0.002177  0.006956  0.005213  \n",
       "38                           -0.003692  0.002037  0.006719  0.006319  \n",
       "39                           -0.001495  0.002105  0.004986  0.007454  \n",
       "40                            0.005316 -0.000077 -0.000567 -0.008888  \n",
       "41                           -0.198041  0.001982  0.005935  0.005342  \n",
       "42                            0.004124 -0.242762  0.018862  0.000209  \n",
       "43                            0.005275  0.008057 -0.238206  0.001283  \n",
       "44                            0.005232  0.000098  0.001414 -0.254055  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "E_hat = IPDL_elasticity(qOpt, z_logit, ThetaOptBLP, Psi, char_number = pr_index)\n",
    "pd.DataFrame(E_hat[0]).rename_axis(index = 'Semi-elasticity of product', columns = 'Semi-elasticity wrt. product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios for the IPDL model\n",
    "\n",
    "The diversion ratio to product j from product k is the fraction of consumers leaving product k and switching to product j following a one percent increase in the price of product k. Hence we have:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{jk}^i = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial x_{ik\\ell}}{\\partial P_k(u_i|\\lambda) / \\partial x_{ik\\ell}} = -100 \\cdot \\frac{\\partial P_j(u_i|\\lambda) / \\partial u_{ik}}{\\partial P_k(u_i|\\lambda) / \\partial u_{ik}}\n",
    "$$\n",
    "\n",
    "Where $\\mathcal{D}^i = \\left( \\mathcal{D}_{jk}^i \\right)_{j,k \\in \\{0,1,\\ldots ,5\\}}$ is the matrix of diversion ratios for individual i. This can be written more compactly as:\n",
    "\n",
    "$$\n",
    "\\mathcal{D}^i = -100 \\cdot  (\\nabla_u P(u|\\lambda) \\circ I_J)^{-1}\\nabla_u P(u|\\lambda)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IPDL_diversion_ratio(q, x, Theta, psi_stack):\n",
    "    '''\n",
    "    This function calculates diversion ratios from the IPDL model\n",
    "\n",
    "    Args.\n",
    "        q: a dictionary of T numpy arrays (J[t],) of choice probabilities for each market t\n",
    "        x: a dictionary of T numpy arrays (J[t],K) of covariates for each market t\n",
    "        Theta: a numpy array (K+G,) of parameters\n",
    "        psi_stack: a dictionary of T numpy arrays (J[t] + sum(C_g),J[t]) of the J[t] by J[t] identity stacked on top of the \\psi^g matrices for each market t as outputted by 'Create_nests'\n",
    "        nest_count: a dictionary of T numpy arrays (G,) containing the amount of nests in each category g in each market t\n",
    "\n",
    "    Returns\n",
    "        Diversion_ratio: a dictionary of T numpy arrays (J,J) of diversion ratios from product j to product k for each individual i\n",
    "    '''\n",
    "\n",
    "    T = len(q.keys())\n",
    "\n",
    "    Grad = ccp_gradient(q, x, Theta, psi_stack) # Find the derivatives of ccp's wrt. utilities\n",
    "    inv_diaggrad = {t: np.divide(1, np.diag(Grad[t]), out = np.zeros_like(np.diag(Grad[t])), where = (np.diag(Grad[t]) != 0)) for t in np.arange(T)}  # Compute the inverse of the 'own'-derivatives of ccp's\n",
    "    DR = {t: np.multiply(-100, np.einsum('j,jk->jk', inv_diaggrad[t], Grad[t])) for t in np.arange(T)} # Compute diversion ratios as a hadamard product.\n",
    "    \n",
    "    return DR "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculating the implied diversion ratios $\\mathcal{ D}^i$ from our estimates $\\hat \\theta^{\\text{IPDL}}$, we find for market $t=1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Diversion ratio wrt. product</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>35</th>\n",
       "      <th>36</th>\n",
       "      <th>37</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Diversion ratio of product</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.323381</td>\n",
       "      <td>2.731794</td>\n",
       "      <td>4.680272</td>\n",
       "      <td>3.316716</td>\n",
       "      <td>3.957332</td>\n",
       "      <td>2.390515</td>\n",
       "      <td>7.109412</td>\n",
       "      <td>0.588343</td>\n",
       "      <td>2.442890</td>\n",
       "      <td>...</td>\n",
       "      <td>1.792772</td>\n",
       "      <td>1.474740</td>\n",
       "      <td>3.080488</td>\n",
       "      <td>1.054796</td>\n",
       "      <td>0.769926</td>\n",
       "      <td>0.055885</td>\n",
       "      <td>3.065765</td>\n",
       "      <td>1.446066</td>\n",
       "      <td>3.399465</td>\n",
       "      <td>3.088710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>58.635744</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.459615</td>\n",
       "      <td>3.998358</td>\n",
       "      <td>1.812536</td>\n",
       "      <td>-1.724979</td>\n",
       "      <td>2.306451</td>\n",
       "      <td>-11.263936</td>\n",
       "      <td>-0.984062</td>\n",
       "      <td>-2.235557</td>\n",
       "      <td>...</td>\n",
       "      <td>1.611535</td>\n",
       "      <td>1.527815</td>\n",
       "      <td>1.973545</td>\n",
       "      <td>0.808561</td>\n",
       "      <td>0.717722</td>\n",
       "      <td>-0.087268</td>\n",
       "      <td>1.918023</td>\n",
       "      <td>0.926242</td>\n",
       "      <td>0.642374</td>\n",
       "      <td>-4.867375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60.654026</td>\n",
       "      <td>0.731429</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-0.130488</td>\n",
       "      <td>1.553330</td>\n",
       "      <td>1.575841</td>\n",
       "      <td>0.188814</td>\n",
       "      <td>4.024528</td>\n",
       "      <td>0.289897</td>\n",
       "      <td>1.075634</td>\n",
       "      <td>...</td>\n",
       "      <td>0.505512</td>\n",
       "      <td>0.425357</td>\n",
       "      <td>-1.504839</td>\n",
       "      <td>-0.418061</td>\n",
       "      <td>-0.218930</td>\n",
       "      <td>0.029980</td>\n",
       "      <td>-1.164879</td>\n",
       "      <td>0.728680</td>\n",
       "      <td>2.450398</td>\n",
       "      <td>2.486073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59.467739</td>\n",
       "      <td>1.146606</td>\n",
       "      <td>-0.074674</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>5.912413</td>\n",
       "      <td>2.529110</td>\n",
       "      <td>-2.764404</td>\n",
       "      <td>9.443279</td>\n",
       "      <td>0.477112</td>\n",
       "      <td>2.157002</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.302047</td>\n",
       "      <td>-1.631016</td>\n",
       "      <td>0.667693</td>\n",
       "      <td>-0.061546</td>\n",
       "      <td>-0.141859</td>\n",
       "      <td>0.035892</td>\n",
       "      <td>0.568213</td>\n",
       "      <td>0.850596</td>\n",
       "      <td>-0.664749</td>\n",
       "      <td>7.825647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59.415882</td>\n",
       "      <td>0.732830</td>\n",
       "      <td>1.253274</td>\n",
       "      <td>8.335825</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.981888</td>\n",
       "      <td>0.887179</td>\n",
       "      <td>3.594748</td>\n",
       "      <td>0.320877</td>\n",
       "      <td>0.829426</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.915001</td>\n",
       "      <td>0.362609</td>\n",
       "      <td>0.845562</td>\n",
       "      <td>0.404870</td>\n",
       "      <td>0.296667</td>\n",
       "      <td>0.005020</td>\n",
       "      <td>1.282455</td>\n",
       "      <td>-1.003393</td>\n",
       "      <td>1.165832</td>\n",
       "      <td>7.674722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>66.343329</td>\n",
       "      <td>-0.652681</td>\n",
       "      <td>1.189859</td>\n",
       "      <td>3.336969</td>\n",
       "      <td>1.854726</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.720893</td>\n",
       "      <td>-2.945748</td>\n",
       "      <td>-0.407480</td>\n",
       "      <td>-0.481558</td>\n",
       "      <td>...</td>\n",
       "      <td>1.033645</td>\n",
       "      <td>1.137359</td>\n",
       "      <td>1.683100</td>\n",
       "      <td>0.644750</td>\n",
       "      <td>0.448920</td>\n",
       "      <td>0.056131</td>\n",
       "      <td>1.621210</td>\n",
       "      <td>0.846447</td>\n",
       "      <td>1.200268</td>\n",
       "      <td>-2.167398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>59.199900</td>\n",
       "      <td>1.289128</td>\n",
       "      <td>0.210597</td>\n",
       "      <td>-5.387915</td>\n",
       "      <td>1.226440</td>\n",
       "      <td>2.542076</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>21.697620</td>\n",
       "      <td>1.528692</td>\n",
       "      <td>2.259545</td>\n",
       "      <td>...</td>\n",
       "      <td>2.968602</td>\n",
       "      <td>1.307242</td>\n",
       "      <td>0.114894</td>\n",
       "      <td>0.089297</td>\n",
       "      <td>0.146353</td>\n",
       "      <td>0.035246</td>\n",
       "      <td>0.199702</td>\n",
       "      <td>0.834568</td>\n",
       "      <td>-1.196819</td>\n",
       "      <td>4.157742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>59.727001</td>\n",
       "      <td>-2.135746</td>\n",
       "      <td>1.522791</td>\n",
       "      <td>6.243808</td>\n",
       "      <td>1.685821</td>\n",
       "      <td>-1.476175</td>\n",
       "      <td>7.360709</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.293184</td>\n",
       "      <td>-2.905293</td>\n",
       "      <td>...</td>\n",
       "      <td>4.729761</td>\n",
       "      <td>4.027683</td>\n",
       "      <td>2.683634</td>\n",
       "      <td>0.866091</td>\n",
       "      <td>0.394361</td>\n",
       "      <td>-0.067630</td>\n",
       "      <td>2.470121</td>\n",
       "      <td>-0.115704</td>\n",
       "      <td>0.499935</td>\n",
       "      <td>-6.188185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>59.988769</td>\n",
       "      <td>-2.264561</td>\n",
       "      <td>1.331286</td>\n",
       "      <td>3.828679</td>\n",
       "      <td>1.826347</td>\n",
       "      <td>-2.478280</td>\n",
       "      <td>6.294041</td>\n",
       "      <td>3.558304</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-1.991700</td>\n",
       "      <td>...</td>\n",
       "      <td>4.544349</td>\n",
       "      <td>3.969283</td>\n",
       "      <td>1.763562</td>\n",
       "      <td>0.656900</td>\n",
       "      <td>0.475265</td>\n",
       "      <td>0.039239</td>\n",
       "      <td>1.674894</td>\n",
       "      <td>0.127703</td>\n",
       "      <td>0.955321</td>\n",
       "      <td>-4.169045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>64.354626</td>\n",
       "      <td>-1.329180</td>\n",
       "      <td>1.276230</td>\n",
       "      <td>4.472148</td>\n",
       "      <td>1.219718</td>\n",
       "      <td>-0.756710</td>\n",
       "      <td>2.403629</td>\n",
       "      <td>-9.110217</td>\n",
       "      <td>-0.514589</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.466537</td>\n",
       "      <td>1.479437</td>\n",
       "      <td>2.143958</td>\n",
       "      <td>0.706934</td>\n",
       "      <td>0.600369</td>\n",
       "      <td>-0.037786</td>\n",
       "      <td>1.863715</td>\n",
       "      <td>0.767240</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>-4.214769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>65.684138</td>\n",
       "      <td>0.696823</td>\n",
       "      <td>1.167806</td>\n",
       "      <td>-4.064181</td>\n",
       "      <td>1.851658</td>\n",
       "      <td>1.640388</td>\n",
       "      <td>-1.863654</td>\n",
       "      <td>6.137791</td>\n",
       "      <td>0.380987</td>\n",
       "      <td>13.320201</td>\n",
       "      <td>...</td>\n",
       "      <td>0.591262</td>\n",
       "      <td>-0.580230</td>\n",
       "      <td>0.586168</td>\n",
       "      <td>0.390451</td>\n",
       "      <td>0.317014</td>\n",
       "      <td>0.022877</td>\n",
       "      <td>1.086371</td>\n",
       "      <td>0.475087</td>\n",
       "      <td>-0.814534</td>\n",
       "      <td>4.411534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>61.787759</td>\n",
       "      <td>0.256922</td>\n",
       "      <td>1.833986</td>\n",
       "      <td>-1.298000</td>\n",
       "      <td>1.218802</td>\n",
       "      <td>1.475291</td>\n",
       "      <td>-0.395842</td>\n",
       "      <td>-0.057005</td>\n",
       "      <td>0.177376</td>\n",
       "      <td>0.179258</td>\n",
       "      <td>...</td>\n",
       "      <td>1.463031</td>\n",
       "      <td>-0.278312</td>\n",
       "      <td>9.835417</td>\n",
       "      <td>0.845449</td>\n",
       "      <td>0.506314</td>\n",
       "      <td>-0.004508</td>\n",
       "      <td>2.297745</td>\n",
       "      <td>0.030518</td>\n",
       "      <td>-2.879016</td>\n",
       "      <td>1.099427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>62.449989</td>\n",
       "      <td>0.460513</td>\n",
       "      <td>-0.230878</td>\n",
       "      <td>2.252943</td>\n",
       "      <td>0.733061</td>\n",
       "      <td>0.974846</td>\n",
       "      <td>1.262019</td>\n",
       "      <td>1.977223</td>\n",
       "      <td>0.158477</td>\n",
       "      <td>0.240375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.548134</td>\n",
       "      <td>0.847698</td>\n",
       "      <td>7.359106</td>\n",
       "      <td>0.164122</td>\n",
       "      <td>-0.039755</td>\n",
       "      <td>0.010168</td>\n",
       "      <td>0.241603</td>\n",
       "      <td>0.118151</td>\n",
       "      <td>1.648238</td>\n",
       "      <td>1.845910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>60.983319</td>\n",
       "      <td>1.083794</td>\n",
       "      <td>-0.074944</td>\n",
       "      <td>1.076219</td>\n",
       "      <td>1.333148</td>\n",
       "      <td>2.260498</td>\n",
       "      <td>-0.104902</td>\n",
       "      <td>8.455743</td>\n",
       "      <td>0.372459</td>\n",
       "      <td>1.562245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303102</td>\n",
       "      <td>-0.228600</td>\n",
       "      <td>-1.005066</td>\n",
       "      <td>8.624110</td>\n",
       "      <td>6.253666</td>\n",
       "      <td>0.601172</td>\n",
       "      <td>-2.068338</td>\n",
       "      <td>0.901179</td>\n",
       "      <td>1.943991</td>\n",
       "      <td>2.177017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>75.263315</td>\n",
       "      <td>-0.696706</td>\n",
       "      <td>1.328814</td>\n",
       "      <td>3.730901</td>\n",
       "      <td>1.139576</td>\n",
       "      <td>1.080795</td>\n",
       "      <td>2.112275</td>\n",
       "      <td>-7.800396</td>\n",
       "      <td>-0.477308</td>\n",
       "      <td>-1.767775</td>\n",
       "      <td>...</td>\n",
       "      <td>1.366005</td>\n",
       "      <td>1.203943</td>\n",
       "      <td>2.147420</td>\n",
       "      <td>0.749025</td>\n",
       "      <td>0.611034</td>\n",
       "      <td>-0.076988</td>\n",
       "      <td>1.950338</td>\n",
       "      <td>0.769770</td>\n",
       "      <td>-0.255190</td>\n",
       "      <td>-3.031406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>63.757361</td>\n",
       "      <td>0.888664</td>\n",
       "      <td>-1.172987</td>\n",
       "      <td>1.855003</td>\n",
       "      <td>1.025395</td>\n",
       "      <td>1.942805</td>\n",
       "      <td>0.442317</td>\n",
       "      <td>6.151315</td>\n",
       "      <td>0.291365</td>\n",
       "      <td>1.113676</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.094108</td>\n",
       "      <td>0.207841</td>\n",
       "      <td>-0.410130</td>\n",
       "      <td>-0.363001</td>\n",
       "      <td>-0.424817</td>\n",
       "      <td>0.041670</td>\n",
       "      <td>-0.063925</td>\n",
       "      <td>4.055439</td>\n",
       "      <td>10.955208</td>\n",
       "      <td>0.902436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>75.971924</td>\n",
       "      <td>2.426622</td>\n",
       "      <td>-0.042448</td>\n",
       "      <td>-3.886240</td>\n",
       "      <td>0.780535</td>\n",
       "      <td>3.182838</td>\n",
       "      <td>-2.022155</td>\n",
       "      <td>5.053575</td>\n",
       "      <td>0.408543</td>\n",
       "      <td>4.238372</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078788</td>\n",
       "      <td>-1.107352</td>\n",
       "      <td>0.473335</td>\n",
       "      <td>0.140666</td>\n",
       "      <td>0.109904</td>\n",
       "      <td>0.030935</td>\n",
       "      <td>0.765518</td>\n",
       "      <td>2.247310</td>\n",
       "      <td>-1.666379</td>\n",
       "      <td>2.960162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>58.995176</td>\n",
       "      <td>3.675547</td>\n",
       "      <td>1.392323</td>\n",
       "      <td>-3.886004</td>\n",
       "      <td>2.034396</td>\n",
       "      <td>1.460416</td>\n",
       "      <td>-1.700667</td>\n",
       "      <td>5.251639</td>\n",
       "      <td>0.164688</td>\n",
       "      <td>1.522152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.503183</td>\n",
       "      <td>-0.461456</td>\n",
       "      <td>-0.536435</td>\n",
       "      <td>0.294992</td>\n",
       "      <td>0.302373</td>\n",
       "      <td>0.026160</td>\n",
       "      <td>0.080085</td>\n",
       "      <td>0.665466</td>\n",
       "      <td>-0.059747</td>\n",
       "      <td>3.707548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>59.016241</td>\n",
       "      <td>3.306305</td>\n",
       "      <td>2.174736</td>\n",
       "      <td>3.245387</td>\n",
       "      <td>-1.956203</td>\n",
       "      <td>0.893713</td>\n",
       "      <td>1.757436</td>\n",
       "      <td>-1.083359</td>\n",
       "      <td>-0.012798</td>\n",
       "      <td>-0.261789</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.511635</td>\n",
       "      <td>1.177084</td>\n",
       "      <td>2.318129</td>\n",
       "      <td>0.996554</td>\n",
       "      <td>0.591743</td>\n",
       "      <td>-0.005933</td>\n",
       "      <td>2.462133</td>\n",
       "      <td>-1.844449</td>\n",
       "      <td>-0.153968</td>\n",
       "      <td>1.232440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>60.120118</td>\n",
       "      <td>4.273184</td>\n",
       "      <td>-1.826741</td>\n",
       "      <td>0.868362</td>\n",
       "      <td>0.693994</td>\n",
       "      <td>2.351071</td>\n",
       "      <td>0.433536</td>\n",
       "      <td>8.183492</td>\n",
       "      <td>0.280270</td>\n",
       "      <td>1.792799</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225596</td>\n",
       "      <td>-0.668756</td>\n",
       "      <td>-2.299884</td>\n",
       "      <td>-1.150686</td>\n",
       "      <td>-0.756035</td>\n",
       "      <td>0.063309</td>\n",
       "      <td>-2.630913</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>3.465599</td>\n",
       "      <td>2.197591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>58.707925</td>\n",
       "      <td>4.327924</td>\n",
       "      <td>-0.812739</td>\n",
       "      <td>-0.117403</td>\n",
       "      <td>1.222155</td>\n",
       "      <td>2.423891</td>\n",
       "      <td>0.411516</td>\n",
       "      <td>8.900889</td>\n",
       "      <td>0.248294</td>\n",
       "      <td>1.821336</td>\n",
       "      <td>...</td>\n",
       "      <td>0.269629</td>\n",
       "      <td>-0.354834</td>\n",
       "      <td>-1.351526</td>\n",
       "      <td>-0.980896</td>\n",
       "      <td>-0.543213</td>\n",
       "      <td>0.064894</td>\n",
       "      <td>-1.898000</td>\n",
       "      <td>0.948072</td>\n",
       "      <td>2.557573</td>\n",
       "      <td>2.592399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>65.302722</td>\n",
       "      <td>0.940385</td>\n",
       "      <td>0.695409</td>\n",
       "      <td>2.014773</td>\n",
       "      <td>1.588337</td>\n",
       "      <td>-0.199296</td>\n",
       "      <td>0.783751</td>\n",
       "      <td>-8.487396</td>\n",
       "      <td>5.471788</td>\n",
       "      <td>-1.465632</td>\n",
       "      <td>...</td>\n",
       "      <td>0.565626</td>\n",
       "      <td>0.781489</td>\n",
       "      <td>0.865714</td>\n",
       "      <td>0.463965</td>\n",
       "      <td>0.482818</td>\n",
       "      <td>-0.156970</td>\n",
       "      <td>0.725169</td>\n",
       "      <td>0.636794</td>\n",
       "      <td>1.348235</td>\n",
       "      <td>-4.633665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>62.465287</td>\n",
       "      <td>1.623289</td>\n",
       "      <td>0.391644</td>\n",
       "      <td>-3.620365</td>\n",
       "      <td>0.892993</td>\n",
       "      <td>4.372630</td>\n",
       "      <td>-1.890802</td>\n",
       "      <td>3.999432</td>\n",
       "      <td>0.433400</td>\n",
       "      <td>2.869911</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384371</td>\n",
       "      <td>-1.025716</td>\n",
       "      <td>0.991049</td>\n",
       "      <td>0.273701</td>\n",
       "      <td>0.156740</td>\n",
       "      <td>-0.027179</td>\n",
       "      <td>0.902969</td>\n",
       "      <td>1.498962</td>\n",
       "      <td>-2.694143</td>\n",
       "      <td>2.259362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>62.485767</td>\n",
       "      <td>-0.418686</td>\n",
       "      <td>1.524668</td>\n",
       "      <td>3.615373</td>\n",
       "      <td>-2.016571</td>\n",
       "      <td>2.522107</td>\n",
       "      <td>1.747979</td>\n",
       "      <td>-1.593039</td>\n",
       "      <td>-0.072384</td>\n",
       "      <td>-0.916767</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.696687</td>\n",
       "      <td>1.140045</td>\n",
       "      <td>1.691454</td>\n",
       "      <td>0.647062</td>\n",
       "      <td>0.486007</td>\n",
       "      <td>-0.061063</td>\n",
       "      <td>1.619127</td>\n",
       "      <td>-1.054408</td>\n",
       "      <td>0.508771</td>\n",
       "      <td>-0.351733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>60.529126</td>\n",
       "      <td>-1.116869</td>\n",
       "      <td>0.433134</td>\n",
       "      <td>1.464952</td>\n",
       "      <td>1.351596</td>\n",
       "      <td>3.318495</td>\n",
       "      <td>0.768361</td>\n",
       "      <td>-2.661857</td>\n",
       "      <td>-0.469183</td>\n",
       "      <td>-2.178876</td>\n",
       "      <td>...</td>\n",
       "      <td>0.669609</td>\n",
       "      <td>0.571249</td>\n",
       "      <td>0.865845</td>\n",
       "      <td>0.349466</td>\n",
       "      <td>0.244544</td>\n",
       "      <td>-0.163026</td>\n",
       "      <td>1.011076</td>\n",
       "      <td>0.732066</td>\n",
       "      <td>1.128784</td>\n",
       "      <td>-1.167876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>66.191076</td>\n",
       "      <td>-1.074994</td>\n",
       "      <td>1.393352</td>\n",
       "      <td>3.886740</td>\n",
       "      <td>2.325925</td>\n",
       "      <td>-15.251920</td>\n",
       "      <td>2.331191</td>\n",
       "      <td>-2.790071</td>\n",
       "      <td>-7.897045</td>\n",
       "      <td>-0.246226</td>\n",
       "      <td>...</td>\n",
       "      <td>1.704886</td>\n",
       "      <td>1.510063</td>\n",
       "      <td>1.801103</td>\n",
       "      <td>0.472776</td>\n",
       "      <td>0.347940</td>\n",
       "      <td>2.467974</td>\n",
       "      <td>1.739735</td>\n",
       "      <td>0.556793</td>\n",
       "      <td>1.421771</td>\n",
       "      <td>-2.803754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>77.736269</td>\n",
       "      <td>1.188106</td>\n",
       "      <td>-0.948052</td>\n",
       "      <td>2.244432</td>\n",
       "      <td>1.169371</td>\n",
       "      <td>2.216052</td>\n",
       "      <td>0.741378</td>\n",
       "      <td>8.770042</td>\n",
       "      <td>0.396577</td>\n",
       "      <td>1.681148</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111206</td>\n",
       "      <td>0.237888</td>\n",
       "      <td>-1.320772</td>\n",
       "      <td>-0.680712</td>\n",
       "      <td>-0.244883</td>\n",
       "      <td>0.062111</td>\n",
       "      <td>-2.653462</td>\n",
       "      <td>0.908855</td>\n",
       "      <td>3.155390</td>\n",
       "      <td>2.350971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>60.575020</td>\n",
       "      <td>0.686696</td>\n",
       "      <td>6.263833</td>\n",
       "      <td>0.670539</td>\n",
       "      <td>-2.035187</td>\n",
       "      <td>1.526626</td>\n",
       "      <td>0.552569</td>\n",
       "      <td>2.182462</td>\n",
       "      <td>0.275644</td>\n",
       "      <td>0.910628</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.862911</td>\n",
       "      <td>0.537619</td>\n",
       "      <td>0.457063</td>\n",
       "      <td>0.450845</td>\n",
       "      <td>0.440616</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>1.165598</td>\n",
       "      <td>-0.948088</td>\n",
       "      <td>1.407442</td>\n",
       "      <td>3.079063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>60.431618</td>\n",
       "      <td>1.351722</td>\n",
       "      <td>6.380857</td>\n",
       "      <td>1.256663</td>\n",
       "      <td>-2.192106</td>\n",
       "      <td>1.235234</td>\n",
       "      <td>0.678247</td>\n",
       "      <td>0.863187</td>\n",
       "      <td>0.257413</td>\n",
       "      <td>2.436014</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.932118</td>\n",
       "      <td>0.659141</td>\n",
       "      <td>1.109666</td>\n",
       "      <td>0.537767</td>\n",
       "      <td>0.438383</td>\n",
       "      <td>0.005452</td>\n",
       "      <td>1.434081</td>\n",
       "      <td>-0.368834</td>\n",
       "      <td>0.225024</td>\n",
       "      <td>2.212748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>59.142566</td>\n",
       "      <td>0.921605</td>\n",
       "      <td>5.780423</td>\n",
       "      <td>-2.769322</td>\n",
       "      <td>0.933183</td>\n",
       "      <td>5.185651</td>\n",
       "      <td>-1.450933</td>\n",
       "      <td>1.747040</td>\n",
       "      <td>0.193148</td>\n",
       "      <td>1.089797</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557829</td>\n",
       "      <td>-0.880178</td>\n",
       "      <td>1.763146</td>\n",
       "      <td>0.507425</td>\n",
       "      <td>0.215367</td>\n",
       "      <td>0.006629</td>\n",
       "      <td>1.584650</td>\n",
       "      <td>0.824468</td>\n",
       "      <td>-3.460632</td>\n",
       "      <td>0.977622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>59.162225</td>\n",
       "      <td>0.922537</td>\n",
       "      <td>3.904705</td>\n",
       "      <td>2.003876</td>\n",
       "      <td>1.447847</td>\n",
       "      <td>2.025012</td>\n",
       "      <td>0.578426</td>\n",
       "      <td>8.450191</td>\n",
       "      <td>0.336066</td>\n",
       "      <td>1.465835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.244522</td>\n",
       "      <td>0.162575</td>\n",
       "      <td>-1.278971</td>\n",
       "      <td>-0.628567</td>\n",
       "      <td>-0.169387</td>\n",
       "      <td>0.056165</td>\n",
       "      <td>-2.545708</td>\n",
       "      <td>1.101920</td>\n",
       "      <td>3.242223</td>\n",
       "      <td>1.935776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>59.340935</td>\n",
       "      <td>0.696513</td>\n",
       "      <td>1.471301</td>\n",
       "      <td>8.485854</td>\n",
       "      <td>2.369107</td>\n",
       "      <td>1.990907</td>\n",
       "      <td>0.441524</td>\n",
       "      <td>3.352109</td>\n",
       "      <td>0.333206</td>\n",
       "      <td>1.149920</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.257837</td>\n",
       "      <td>0.647065</td>\n",
       "      <td>0.685282</td>\n",
       "      <td>0.437203</td>\n",
       "      <td>0.248250</td>\n",
       "      <td>0.011611</td>\n",
       "      <td>1.152886</td>\n",
       "      <td>-1.001211</td>\n",
       "      <td>0.900492</td>\n",
       "      <td>7.504629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>57.373387</td>\n",
       "      <td>-1.556227</td>\n",
       "      <td>1.333038</td>\n",
       "      <td>11.437049</td>\n",
       "      <td>7.578746</td>\n",
       "      <td>-3.344926</td>\n",
       "      <td>2.770227</td>\n",
       "      <td>-11.237890</td>\n",
       "      <td>-0.862938</td>\n",
       "      <td>-2.939605</td>\n",
       "      <td>...</td>\n",
       "      <td>1.830785</td>\n",
       "      <td>1.552788</td>\n",
       "      <td>1.953419</td>\n",
       "      <td>0.678953</td>\n",
       "      <td>0.559447</td>\n",
       "      <td>-0.064177</td>\n",
       "      <td>1.785580</td>\n",
       "      <td>0.864122</td>\n",
       "      <td>1.132313</td>\n",
       "      <td>-1.701809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>66.490391</td>\n",
       "      <td>1.908963</td>\n",
       "      <td>0.881259</td>\n",
       "      <td>-3.272264</td>\n",
       "      <td>0.783507</td>\n",
       "      <td>2.296892</td>\n",
       "      <td>-1.132646</td>\n",
       "      <td>3.618542</td>\n",
       "      <td>0.269797</td>\n",
       "      <td>3.581325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.802711</td>\n",
       "      <td>-1.632430</td>\n",
       "      <td>1.203999</td>\n",
       "      <td>-0.192168</td>\n",
       "      <td>-0.332747</td>\n",
       "      <td>0.019084</td>\n",
       "      <td>1.113071</td>\n",
       "      <td>1.606441</td>\n",
       "      <td>-1.717882</td>\n",
       "      <td>2.482621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>65.310558</td>\n",
       "      <td>2.001105</td>\n",
       "      <td>-0.029449</td>\n",
       "      <td>-3.627613</td>\n",
       "      <td>0.311620</td>\n",
       "      <td>4.408405</td>\n",
       "      <td>-1.886658</td>\n",
       "      <td>5.041753</td>\n",
       "      <td>0.369832</td>\n",
       "      <td>3.197788</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.296715</td>\n",
       "      <td>-0.997336</td>\n",
       "      <td>0.075784</td>\n",
       "      <td>0.081829</td>\n",
       "      <td>0.068565</td>\n",
       "      <td>0.028623</td>\n",
       "      <td>0.389694</td>\n",
       "      <td>1.778271</td>\n",
       "      <td>-0.958003</td>\n",
       "      <td>2.668787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>59.214931</td>\n",
       "      <td>1.201347</td>\n",
       "      <td>0.752014</td>\n",
       "      <td>-0.785183</td>\n",
       "      <td>-1.687073</td>\n",
       "      <td>2.036492</td>\n",
       "      <td>3.959394</td>\n",
       "      <td>18.595520</td>\n",
       "      <td>1.472105</td>\n",
       "      <td>1.838753</td>\n",
       "      <td>...</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>3.021182</td>\n",
       "      <td>-0.470211</td>\n",
       "      <td>0.125302</td>\n",
       "      <td>0.360631</td>\n",
       "      <td>0.028548</td>\n",
       "      <td>0.161710</td>\n",
       "      <td>-0.753407</td>\n",
       "      <td>2.182649</td>\n",
       "      <td>4.941219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>58.839195</td>\n",
       "      <td>1.375766</td>\n",
       "      <td>0.764352</td>\n",
       "      <td>-5.121524</td>\n",
       "      <td>0.807599</td>\n",
       "      <td>2.706786</td>\n",
       "      <td>2.106094</td>\n",
       "      <td>19.127999</td>\n",
       "      <td>1.553189</td>\n",
       "      <td>2.240639</td>\n",
       "      <td>...</td>\n",
       "      <td>3.649405</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.458330</td>\n",
       "      <td>-0.256382</td>\n",
       "      <td>-0.344291</td>\n",
       "      <td>0.043508</td>\n",
       "      <td>0.810438</td>\n",
       "      <td>0.808475</td>\n",
       "      <td>-0.721067</td>\n",
       "      <td>4.282970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>61.769259</td>\n",
       "      <td>0.893146</td>\n",
       "      <td>-1.359037</td>\n",
       "      <td>1.053706</td>\n",
       "      <td>0.946464</td>\n",
       "      <td>2.013112</td>\n",
       "      <td>0.093029</td>\n",
       "      <td>6.405294</td>\n",
       "      <td>0.346820</td>\n",
       "      <td>1.631899</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.285456</td>\n",
       "      <td>0.230346</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>-0.558588</td>\n",
       "      <td>-0.332500</td>\n",
       "      <td>0.042808</td>\n",
       "      <td>-2.247008</td>\n",
       "      <td>0.912349</td>\n",
       "      <td>2.914872</td>\n",
       "      <td>2.184385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>63.364585</td>\n",
       "      <td>1.096259</td>\n",
       "      <td>-1.131112</td>\n",
       "      <td>-0.290983</td>\n",
       "      <td>1.357688</td>\n",
       "      <td>2.310334</td>\n",
       "      <td>0.216614</td>\n",
       "      <td>6.193055</td>\n",
       "      <td>0.387024</td>\n",
       "      <td>1.612061</td>\n",
       "      <td>...</td>\n",
       "      <td>0.227892</td>\n",
       "      <td>-0.386024</td>\n",
       "      <td>-1.673467</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>6.068719</td>\n",
       "      <td>0.601479</td>\n",
       "      <td>-1.598682</td>\n",
       "      <td>0.882039</td>\n",
       "      <td>2.909371</td>\n",
       "      <td>2.736496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>61.859755</td>\n",
       "      <td>1.301482</td>\n",
       "      <td>-0.792234</td>\n",
       "      <td>-0.897026</td>\n",
       "      <td>1.330559</td>\n",
       "      <td>2.151460</td>\n",
       "      <td>0.474823</td>\n",
       "      <td>3.771514</td>\n",
       "      <td>0.374504</td>\n",
       "      <td>1.831057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.877237</td>\n",
       "      <td>-0.693320</td>\n",
       "      <td>-1.332290</td>\n",
       "      <td>8.116676</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.586901</td>\n",
       "      <td>-0.633832</td>\n",
       "      <td>0.892516</td>\n",
       "      <td>2.113496</td>\n",
       "      <td>3.159724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>52.552082</td>\n",
       "      <td>-1.852140</td>\n",
       "      <td>1.269738</td>\n",
       "      <td>2.656303</td>\n",
       "      <td>0.263504</td>\n",
       "      <td>3.148468</td>\n",
       "      <td>1.338374</td>\n",
       "      <td>-7.569995</td>\n",
       "      <td>0.361885</td>\n",
       "      <td>-1.348814</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812751</td>\n",
       "      <td>1.025445</td>\n",
       "      <td>2.007541</td>\n",
       "      <td>9.415363</td>\n",
       "      <td>6.869112</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.935488</td>\n",
       "      <td>-0.028018</td>\n",
       "      <td>-0.206269</td>\n",
       "      <td>-3.236072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>74.299709</td>\n",
       "      <td>1.049118</td>\n",
       "      <td>-1.271503</td>\n",
       "      <td>1.083799</td>\n",
       "      <td>1.734987</td>\n",
       "      <td>2.343650</td>\n",
       "      <td>0.195435</td>\n",
       "      <td>7.125730</td>\n",
       "      <td>0.398104</td>\n",
       "      <td>1.714557</td>\n",
       "      <td>...</td>\n",
       "      <td>0.118653</td>\n",
       "      <td>0.492285</td>\n",
       "      <td>-2.715814</td>\n",
       "      <td>-0.644959</td>\n",
       "      <td>-0.191189</td>\n",
       "      <td>0.049882</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>1.000926</td>\n",
       "      <td>2.997042</td>\n",
       "      <td>2.697419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>59.483984</td>\n",
       "      <td>0.859922</td>\n",
       "      <td>1.350011</td>\n",
       "      <td>2.753751</td>\n",
       "      <td>-2.304034</td>\n",
       "      <td>2.076907</td>\n",
       "      <td>1.386258</td>\n",
       "      <td>-0.566531</td>\n",
       "      <td>0.051520</td>\n",
       "      <td>1.198029</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.938286</td>\n",
       "      <td>0.833541</td>\n",
       "      <td>1.871631</td>\n",
       "      <td>0.603978</td>\n",
       "      <td>0.456950</td>\n",
       "      <td>-0.001226</td>\n",
       "      <td>1.698892</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>7.769707</td>\n",
       "      <td>0.085989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>60.878528</td>\n",
       "      <td>0.259635</td>\n",
       "      <td>1.976416</td>\n",
       "      <td>-0.936917</td>\n",
       "      <td>1.165454</td>\n",
       "      <td>1.282144</td>\n",
       "      <td>-0.865471</td>\n",
       "      <td>1.065689</td>\n",
       "      <td>0.167789</td>\n",
       "      <td>0.022373</td>\n",
       "      <td>...</td>\n",
       "      <td>1.183399</td>\n",
       "      <td>-0.323652</td>\n",
       "      <td>2.603276</td>\n",
       "      <td>0.867310</td>\n",
       "      <td>0.471081</td>\n",
       "      <td>-0.003928</td>\n",
       "      <td>2.214615</td>\n",
       "      <td>3.382565</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.538609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>57.149985</td>\n",
       "      <td>-2.032620</td>\n",
       "      <td>2.071768</td>\n",
       "      <td>11.395906</td>\n",
       "      <td>7.926974</td>\n",
       "      <td>-2.392119</td>\n",
       "      <td>3.106469</td>\n",
       "      <td>-13.629039</td>\n",
       "      <td>-0.756548</td>\n",
       "      <td>-2.960308</td>\n",
       "      <td>...</td>\n",
       "      <td>2.768005</td>\n",
       "      <td>1.986245</td>\n",
       "      <td>2.015651</td>\n",
       "      <td>0.842860</td>\n",
       "      <td>0.727660</td>\n",
       "      <td>-0.063674</td>\n",
       "      <td>2.059393</td>\n",
       "      <td>0.038678</td>\n",
       "      <td>0.556492</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>45 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Diversion ratio wrt. product          0           1           2           3   \\\n",
       "Diversion ratio of product                                                     \n",
       "0                            -100.000000    1.323381    2.731794    4.680272   \n",
       "1                              58.635744 -100.000000    1.459615    3.998358   \n",
       "2                              60.654026    0.731429 -100.000000   -0.130488   \n",
       "3                              59.467739    1.146606   -0.074674 -100.000000   \n",
       "4                              59.415882    0.732830    1.253274    8.335825   \n",
       "5                              66.343329   -0.652681    1.189859    3.336969   \n",
       "6                              59.199900    1.289128    0.210597   -5.387915   \n",
       "7                              59.727001   -2.135746    1.522791    6.243808   \n",
       "8                              59.988769   -2.264561    1.331286    3.828679   \n",
       "9                              64.354626   -1.329180    1.276230    4.472148   \n",
       "10                             65.684138    0.696823    1.167806   -4.064181   \n",
       "11                             61.787759    0.256922    1.833986   -1.298000   \n",
       "12                             62.449989    0.460513   -0.230878    2.252943   \n",
       "13                             60.983319    1.083794   -0.074944    1.076219   \n",
       "14                             75.263315   -0.696706    1.328814    3.730901   \n",
       "15                             63.757361    0.888664   -1.172987    1.855003   \n",
       "16                             75.971924    2.426622   -0.042448   -3.886240   \n",
       "17                             58.995176    3.675547    1.392323   -3.886004   \n",
       "18                             59.016241    3.306305    2.174736    3.245387   \n",
       "19                             60.120118    4.273184   -1.826741    0.868362   \n",
       "20                             58.707925    4.327924   -0.812739   -0.117403   \n",
       "21                             65.302722    0.940385    0.695409    2.014773   \n",
       "22                             62.465287    1.623289    0.391644   -3.620365   \n",
       "23                             62.485767   -0.418686    1.524668    3.615373   \n",
       "24                             60.529126   -1.116869    0.433134    1.464952   \n",
       "25                             66.191076   -1.074994    1.393352    3.886740   \n",
       "26                             77.736269    1.188106   -0.948052    2.244432   \n",
       "27                             60.575020    0.686696    6.263833    0.670539   \n",
       "28                             60.431618    1.351722    6.380857    1.256663   \n",
       "29                             59.142566    0.921605    5.780423   -2.769322   \n",
       "30                             59.162225    0.922537    3.904705    2.003876   \n",
       "31                             59.340935    0.696513    1.471301    8.485854   \n",
       "32                             57.373387   -1.556227    1.333038   11.437049   \n",
       "33                             66.490391    1.908963    0.881259   -3.272264   \n",
       "34                             65.310558    2.001105   -0.029449   -3.627613   \n",
       "35                             59.214931    1.201347    0.752014   -0.785183   \n",
       "36                             58.839195    1.375766    0.764352   -5.121524   \n",
       "37                             61.769259    0.893146   -1.359037    1.053706   \n",
       "38                             63.364585    1.096259   -1.131112   -0.290983   \n",
       "39                             61.859755    1.301482   -0.792234   -0.897026   \n",
       "40                             52.552082   -1.852140    1.269738    2.656303   \n",
       "41                             74.299709    1.049118   -1.271503    1.083799   \n",
       "42                             59.483984    0.859922    1.350011    2.753751   \n",
       "43                             60.878528    0.259635    1.976416   -0.936917   \n",
       "44                             57.149985   -2.032620    2.071768   11.395906   \n",
       "\n",
       "Diversion ratio wrt. product          4           5           6           7   \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               3.316716    3.957332    2.390515    7.109412   \n",
       "1                               1.812536   -1.724979    2.306451  -11.263936   \n",
       "2                               1.553330    1.575841    0.188814    4.024528   \n",
       "3                               5.912413    2.529110   -2.764404    9.443279   \n",
       "4                            -100.000000    1.981888    0.887179    3.594748   \n",
       "5                               1.854726 -100.000000    1.720893   -2.945748   \n",
       "6                               1.226440    2.542076 -100.000000   21.697620   \n",
       "7                               1.685821   -1.476175    7.360709 -100.000000   \n",
       "8                               1.826347   -2.478280    6.294041    3.558304   \n",
       "9                               1.219718   -0.756710    2.403629   -9.110217   \n",
       "10                              1.851658    1.640388   -1.863654    6.137791   \n",
       "11                              1.218802    1.475291   -0.395842   -0.057005   \n",
       "12                              0.733061    0.974846    1.262019    1.977223   \n",
       "13                              1.333148    2.260498   -0.104902    8.455743   \n",
       "14                              1.139576    1.080795    2.112275   -7.800396   \n",
       "15                              1.025395    1.942805    0.442317    6.151315   \n",
       "16                              0.780535    3.182838   -2.022155    5.053575   \n",
       "17                              2.034396    1.460416   -1.700667    5.251639   \n",
       "18                             -1.956203    0.893713    1.757436   -1.083359   \n",
       "19                              0.693994    2.351071    0.433536    8.183492   \n",
       "20                              1.222155    2.423891    0.411516    8.900889   \n",
       "21                              1.588337   -0.199296    0.783751   -8.487396   \n",
       "22                              0.892993    4.372630   -1.890802    3.999432   \n",
       "23                             -2.016571    2.522107    1.747979   -1.593039   \n",
       "24                              1.351596    3.318495    0.768361   -2.661857   \n",
       "25                              2.325925  -15.251920    2.331191   -2.790071   \n",
       "26                              1.169371    2.216052    0.741378    8.770042   \n",
       "27                             -2.035187    1.526626    0.552569    2.182462   \n",
       "28                             -2.192106    1.235234    0.678247    0.863187   \n",
       "29                              0.933183    5.185651   -1.450933    1.747040   \n",
       "30                              1.447847    2.025012    0.578426    8.450191   \n",
       "31                              2.369107    1.990907    0.441524    3.352109   \n",
       "32                              7.578746   -3.344926    2.770227  -11.237890   \n",
       "33                              0.783507    2.296892   -1.132646    3.618542   \n",
       "34                              0.311620    4.408405   -1.886658    5.041753   \n",
       "35                             -1.687073    2.036492    3.959394   18.595520   \n",
       "36                              0.807599    2.706786    2.106094   19.127999   \n",
       "37                              0.946464    2.013112    0.093029    6.405294   \n",
       "38                              1.357688    2.310334    0.216614    6.193055   \n",
       "39                              1.330559    2.151460    0.474823    3.771514   \n",
       "40                              0.263504    3.148468    1.338374   -7.569995   \n",
       "41                              1.734987    2.343650    0.195435    7.125730   \n",
       "42                             -2.304034    2.076907    1.386258   -0.566531   \n",
       "43                              1.165454    1.282144   -0.865471    1.065689   \n",
       "44                              7.926974   -2.392119    3.106469  -13.629039   \n",
       "\n",
       "Diversion ratio wrt. product          8           9   ...          35  \\\n",
       "Diversion ratio of product                            ...               \n",
       "0                               0.588343    2.442890  ...    1.792772   \n",
       "1                              -0.984062   -2.235557  ...    1.611535   \n",
       "2                               0.289897    1.075634  ...    0.505512   \n",
       "3                               0.477112    2.157002  ...   -0.302047   \n",
       "4                               0.320877    0.829426  ...   -0.915001   \n",
       "5                              -0.407480   -0.481558  ...    1.033645   \n",
       "6                               1.528692    2.259545  ...    2.968602   \n",
       "7                               0.293184   -2.905293  ...    4.729761   \n",
       "8                            -100.000000   -1.991700  ...    4.544349   \n",
       "9                              -0.514589 -100.000000  ...    1.466537   \n",
       "10                              0.380987   13.320201  ...    0.591262   \n",
       "11                              0.177376    0.179258  ...    1.463031   \n",
       "12                              0.158477    0.240375  ...    0.548134   \n",
       "13                              0.372459    1.562245  ...    0.303102   \n",
       "14                             -0.477308   -1.767775  ...    1.366005   \n",
       "15                              0.291365    1.113676  ...   -0.094108   \n",
       "16                              0.408543    4.238372  ...    0.078788   \n",
       "17                              0.164688    1.522152  ...    0.503183   \n",
       "18                             -0.012798   -0.261789  ...   -0.511635   \n",
       "19                              0.280270    1.792799  ...   -0.225596   \n",
       "20                              0.248294    1.821336  ...    0.269629   \n",
       "21                              5.471788   -1.465632  ...    0.565626   \n",
       "22                              0.433400    2.869911  ...    0.384371   \n",
       "23                             -0.072384   -0.916767  ...   -0.696687   \n",
       "24                             -0.469183   -2.178876  ...    0.669609   \n",
       "25                             -7.897045   -0.246226  ...    1.704886   \n",
       "26                              0.396577    1.681148  ...    0.111206   \n",
       "27                              0.275644    0.910628  ...   -0.862911   \n",
       "28                              0.257413    2.436014  ...   -0.932118   \n",
       "29                              0.193148    1.089797  ...    0.557829   \n",
       "30                              0.336066    1.465835  ...    0.244522   \n",
       "31                              0.333206    1.149920  ...   -1.257837   \n",
       "32                             -0.862938   -2.939605  ...    1.830785   \n",
       "33                              0.269797    3.581325  ...    0.802711   \n",
       "34                              0.369832    3.197788  ...   -0.296715   \n",
       "35                              1.472105    1.838753  ... -100.000000   \n",
       "36                              1.553189    2.240639  ...    3.649405   \n",
       "37                              0.346820    1.631899  ...   -0.285456   \n",
       "38                              0.387024    1.612061  ...    0.227892   \n",
       "39                              0.374504    1.831057  ...    0.877237   \n",
       "40                              0.361885   -1.348814  ...    0.812751   \n",
       "41                              0.398104    1.714557  ...    0.118653   \n",
       "42                              0.051520    1.198029  ...   -0.938286   \n",
       "43                              0.167789    0.022373  ...    1.183399   \n",
       "44                             -0.756548   -2.960308  ...    2.768005   \n",
       "\n",
       "Diversion ratio wrt. product          36          37          38          39  \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               1.474740    3.080488    1.054796    0.769926   \n",
       "1                               1.527815    1.973545    0.808561    0.717722   \n",
       "2                               0.425357   -1.504839   -0.418061   -0.218930   \n",
       "3                              -1.631016    0.667693   -0.061546   -0.141859   \n",
       "4                               0.362609    0.845562    0.404870    0.296667   \n",
       "5                               1.137359    1.683100    0.644750    0.448920   \n",
       "6                               1.307242    0.114894    0.089297    0.146353   \n",
       "7                               4.027683    2.683634    0.866091    0.394361   \n",
       "8                               3.969283    1.763562    0.656900    0.475265   \n",
       "9                               1.479437    2.143958    0.706934    0.600369   \n",
       "10                             -0.580230    0.586168    0.390451    0.317014   \n",
       "11                             -0.278312    9.835417    0.845449    0.506314   \n",
       "12                              0.847698    7.359106    0.164122   -0.039755   \n",
       "13                             -0.228600   -1.005066    8.624110    6.253666   \n",
       "14                              1.203943    2.147420    0.749025    0.611034   \n",
       "15                              0.207841   -0.410130   -0.363001   -0.424817   \n",
       "16                             -1.107352    0.473335    0.140666    0.109904   \n",
       "17                             -0.461456   -0.536435    0.294992    0.302373   \n",
       "18                              1.177084    2.318129    0.996554    0.591743   \n",
       "19                             -0.668756   -2.299884   -1.150686   -0.756035   \n",
       "20                             -0.354834   -1.351526   -0.980896   -0.543213   \n",
       "21                              0.781489    0.865714    0.463965    0.482818   \n",
       "22                             -1.025716    0.991049    0.273701    0.156740   \n",
       "23                              1.140045    1.691454    0.647062    0.486007   \n",
       "24                              0.571249    0.865845    0.349466    0.244544   \n",
       "25                              1.510063    1.801103    0.472776    0.347940   \n",
       "26                              0.237888   -1.320772   -0.680712   -0.244883   \n",
       "27                              0.537619    0.457063    0.450845    0.440616   \n",
       "28                              0.659141    1.109666    0.537767    0.438383   \n",
       "29                             -0.880178    1.763146    0.507425    0.215367   \n",
       "30                              0.162575   -1.278971   -0.628567   -0.169387   \n",
       "31                              0.647065    0.685282    0.437203    0.248250   \n",
       "32                              1.552788    1.953419    0.678953    0.559447   \n",
       "33                             -1.632430    1.203999   -0.192168   -0.332747   \n",
       "34                             -0.997336    0.075784    0.081829    0.068565   \n",
       "35                              3.021182   -0.470211    0.125302    0.360631   \n",
       "36                           -100.000000    0.458330   -0.256382   -0.344291   \n",
       "37                              0.230346 -100.000000   -0.558588   -0.332500   \n",
       "38                             -0.386024   -1.673467 -100.000000    6.068719   \n",
       "39                             -0.693320   -1.332290    8.116676 -100.000000   \n",
       "40                              1.025445    2.007541    9.415363    6.869112   \n",
       "41                              0.492285   -2.715814   -0.644959   -0.191189   \n",
       "42                              0.833541    1.871631    0.603978    0.456950   \n",
       "43                             -0.323652    2.603276    0.867310    0.471081   \n",
       "44                              1.986245    2.015651    0.842860    0.727660   \n",
       "\n",
       "Diversion ratio wrt. product          40          41          42          43  \\\n",
       "Diversion ratio of product                                                     \n",
       "0                               0.055885    3.065765    1.446066    3.399465   \n",
       "1                              -0.087268    1.918023    0.926242    0.642374   \n",
       "2                               0.029980   -1.164879    0.728680    2.450398   \n",
       "3                               0.035892    0.568213    0.850596   -0.664749   \n",
       "4                               0.005020    1.282455   -1.003393    1.165832   \n",
       "5                               0.056131    1.621210    0.846447    1.200268   \n",
       "6                               0.035246    0.199702    0.834568   -1.196819   \n",
       "7                              -0.067630    2.470121   -0.115704    0.499935   \n",
       "8                               0.039239    1.674894    0.127703    0.955321   \n",
       "9                              -0.037786    1.863715    0.767240    0.032912   \n",
       "10                              0.022877    1.086371    0.475087   -0.814534   \n",
       "11                             -0.004508    2.297745    0.030518   -2.879016   \n",
       "12                              0.010168    0.241603    0.118151    1.648238   \n",
       "13                              0.601172   -2.068338    0.901179    1.943991   \n",
       "14                             -0.076988    1.950338    0.769770   -0.255190   \n",
       "15                              0.041670   -0.063925    4.055439   10.955208   \n",
       "16                              0.030935    0.765518    2.247310   -1.666379   \n",
       "17                              0.026160    0.080085    0.665466   -0.059747   \n",
       "18                             -0.005933    2.462133   -1.844449   -0.153968   \n",
       "19                              0.063309   -2.630913    0.890400    3.465599   \n",
       "20                              0.064894   -1.898000    0.948072    2.557573   \n",
       "21                             -0.156970    0.725169    0.636794    1.348235   \n",
       "22                             -0.027179    0.902969    1.498962   -2.694143   \n",
       "23                             -0.061063    1.619127   -1.054408    0.508771   \n",
       "24                             -0.163026    1.011076    0.732066    1.128784   \n",
       "25                              2.467974    1.739735    0.556793    1.421771   \n",
       "26                              0.062111   -2.653462    0.908855    3.155390   \n",
       "27                              0.003868    1.165598   -0.948088    1.407442   \n",
       "28                              0.005452    1.434081   -0.368834    0.225024   \n",
       "29                              0.006629    1.584650    0.824468   -3.460632   \n",
       "30                              0.056165   -2.545708    1.101920    3.242223   \n",
       "31                              0.011611    1.152886   -1.001211    0.900492   \n",
       "32                             -0.064177    1.785580    0.864122    1.132313   \n",
       "33                              0.019084    1.113071    1.606441   -1.717882   \n",
       "34                              0.028623    0.389694    1.778271   -0.958003   \n",
       "35                              0.028548    0.161710   -0.753407    2.182649   \n",
       "36                              0.043508    0.810438    0.808475   -0.721067   \n",
       "37                              0.042808   -2.247008    0.912349    2.914872   \n",
       "38                              0.601479   -1.598682    0.882039    2.909371   \n",
       "39                              0.586901   -0.633832    0.892516    2.113496   \n",
       "40                           -100.000000    1.935488   -0.028018   -0.206269   \n",
       "41                              0.049882 -100.000000    1.000926    2.997042   \n",
       "42                             -0.001226    1.698892 -100.000000    7.769707   \n",
       "43                             -0.003928    2.214615    3.382565 -100.000000   \n",
       "44                             -0.063674    2.059393    0.038678    0.556492   \n",
       "\n",
       "Diversion ratio wrt. product          44  \n",
       "Diversion ratio of product                \n",
       "0                               3.088710  \n",
       "1                              -4.867375  \n",
       "2                               2.486073  \n",
       "3                               7.825647  \n",
       "4                               7.674722  \n",
       "5                              -2.167398  \n",
       "6                               4.157742  \n",
       "7                              -6.188185  \n",
       "8                              -4.169045  \n",
       "9                              -4.214769  \n",
       "10                              4.411534  \n",
       "11                              1.099427  \n",
       "12                              1.845910  \n",
       "13                              2.177017  \n",
       "14                             -3.031406  \n",
       "15                              0.902436  \n",
       "16                              2.960162  \n",
       "17                              3.707548  \n",
       "18                              1.232440  \n",
       "19                              2.197591  \n",
       "20                              2.592399  \n",
       "21                             -4.633665  \n",
       "22                              2.259362  \n",
       "23                             -0.351733  \n",
       "24                             -1.167876  \n",
       "25                             -2.803754  \n",
       "26                              2.350971  \n",
       "27                              3.079063  \n",
       "28                              2.212748  \n",
       "29                              0.977622  \n",
       "30                              1.935776  \n",
       "31                              7.504629  \n",
       "32                             -1.701809  \n",
       "33                              2.482621  \n",
       "34                              2.668787  \n",
       "35                              4.941219  \n",
       "36                              4.282970  \n",
       "37                              2.184385  \n",
       "38                              2.736496  \n",
       "39                              3.159724  \n",
       "40                             -3.236072  \n",
       "41                              2.697419  \n",
       "42                              0.085989  \n",
       "43                              0.538609  \n",
       "44                           -100.000000  \n",
       "\n",
       "[45 rows x 45 columns]"
      ]
     },
     "execution_count": 438,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DR_hat = IPDL_diversion_ratio(qOpt, z_logit, ThetaOptBLP, Psi)\n",
    "pd.DataFrame(DR_hat[0]).rename_axis(index = 'Diversion ratio of product', columns = 'Diversion ratio wrt. product')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elasticities and Diversion Ratios visualization\n",
    "\n",
    "Finally we advance our exposition towards visualing the IPDL semi-elasticities $\\mathcal{E}$ and diversion ratios $\\mathcal{D}$ compared to those implied by a multinomial Logit model. Since the number of products varies across markets $t$ we aggregate our results according to the categorical variable `cla` describing the class or segment of each vehicle $j$. This variable takes values 'subcompact', 'compact', 'intermediate', 'standard', and 'luxury' encoded as the integers $0,1,\\ldots, 5$ in our dataset. To this end we consider the 'pooled' elasticities and diversion ratios calculated using the directional derivative $\\frac{\\partial q_c}{\\partial u_{\\ell}} = \\sum_{j: x_{j,\\text{cla}} = c} \\sum_{k: x_{k,\\text{cla}} = \\ell} \\frac{\\partial q_j}{\\partial u_k}$ of class $c$ wrt. the utility of class $\\ell$, where $q_c = \\sum_{j: x_{j,\\text{cla}} = c} q_j$ denotes the within-group choice proabbilitity of choosing a car of class $c$. \n",
    "\n",
    "The pooled choice probability semi-elasticity $\\mathcal{E}_{c\\ell}$ of class $c$ wrt. the prices of cars of class $\\ell$ can then be computed as $\\mathcal{E}_{c\\ell} = \\frac{\\partial q_c}{\\partial u_{\\ell}}\\frac{1}{q_c}\\theta^{\\text{price}}$. \n",
    "\n",
    "Similarly, we may compute the pooled diversion ratio $\\mathcal{D}_{c\\ell}$ following a unit increase in the price of cars of class $\\ell$ from class $\\ell$ to cars of class $c$ as $\\mathcal{D}_{c\\ell} = -100\\cdot\\frac{\\partial q_c / \\partial u_{\\ell}}{\\partial q_{\\ell} / \\partial u_{\\ell}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccp_directionalgrad(data, Theta, q, x, psi, direction_var, market_id = 'market', product_id = 'co', model = 'IPDL', outside_option = True):\n",
    "    '''\n",
    "    '''\n",
    "    T = len(q)\n",
    "    J = np.array([q[t].shape[0] for t in np.arange(T)])\n",
    "    data = data.sort_values([market_id, product_id])\n",
    "\n",
    "    vec = {}\n",
    "    q_agg = {}\n",
    "    dq_du_agg = {}\n",
    "\n",
    "    if model == 'IPDL':\n",
    "        Grad = ccp_gradient(q, x, Theta, psi)\n",
    "    else:\n",
    "        Grad = {t: (np.diag(q[t]) - q[t][:,None]*q[t][None,:]) for t in np.arange(T)} \n",
    "    \n",
    "    for t in np.arange(T):\n",
    "        G_t = data[data[market_id] == t][direction_var].nunique()\n",
    "        vec[t] = pd.get_dummies(data[data[market_id] == t][direction_var], columns = direction_var).values.reshape((J[t], G_t)).transpose()\n",
    "        \n",
    "        # Calculate the sum of within-group probabilities\n",
    "        q_agg[t] = vec[t]@q[t]\n",
    "\n",
    "        # Calculate directional derivatives\n",
    "        dq_du_agg[t] = np.einsum('cj,jk,lk->cl', vec[t], Grad[t], vec[t])\n",
    "    \n",
    "    return q_agg, dq_du_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Elasticity_agg(data, Theta, q, x, psi, direction_var, market_id = 'market', product_id = 'co', char_number = K-1, model = 'IPDL', outside_option = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    q_agg, dq_du_agg = ccp_directionalgrad(data, Theta, q, x, psi, direction_var, market_id, product_id, model, outside_option)\n",
    "    E_agg = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        E_agg[t] = Theta[char_number]*np.einsum('cl,c->cl', dq_du_agg[t], 1./q_agg[t])\n",
    "\n",
    "    return E_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_IPDLagg = Elasticity_agg(dat, ThetaOptBLP, qOpt, z_logit, Psi, 'cla', char_number = pr_index)\n",
    "E_Logitagg = Elasticity_agg(dat, LogitBLP_beta, logit.logit_ccp(LogitBLP_beta, z_logit), z_logit, Psi, 'cla', char_number = pr_index, model = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_agg = E_IPDLagg[0].shape[0]\n",
    "\n",
    "E0, E1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    E0[t,:,:] = E_Logitagg[t]\n",
    "    E1[t,:,:] = E_IPDLagg[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we plot histograms of our results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E0p = {j : (E0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E0p[j], num_bins, range = (np.quantile(E0p[j], 0.10), np.quantile(E0p[j], 0.90)), color = 'r', alpha = 1) # Logit is blue\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "E1p = {j : (E1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(E1p[j], num_bins, range = (np.quantile(E1p[j], 0.10), np.quantile(E1p[j], 0.90)), color = 'b', alpha = 1) # IPDL is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled IPDL price semi-elasticities by class')\n",
    "fig.supxlabel('Semi-elasticity wrt. class')\n",
    "fig.supylabel('Semi-elasticity of class')\n",
    "\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The mean elasticities for the logit model are given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean elasticity wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean elasticity of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003261</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>-0.172311</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>-0.172365</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>-0.172652</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>-0.173053</td>\n",
       "      <td>0.000189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.170102</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.000311</td>\n",
       "      <td>-0.173175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean elasticity wrt. class         0         1         2         3         4  \\\n",
       "Mean elasticity of class                                                       \n",
       "0                          -0.003261  0.001052  0.000998  0.000711  0.000311   \n",
       "1                           0.170102 -0.172311  0.000998  0.000711  0.000311   \n",
       "2                           0.170102  0.001052 -0.172365  0.000711  0.000311   \n",
       "3                           0.170102  0.001052  0.000998 -0.172652  0.000311   \n",
       "4                           0.170102  0.001052  0.000998  0.000711 -0.173053   \n",
       "5                           0.170102  0.001052  0.000998  0.000711  0.000311   \n",
       "\n",
       "Mean elasticity wrt. class         5  \n",
       "Mean elasticity of class              \n",
       "0                           0.000189  \n",
       "1                           0.000189  \n",
       "2                           0.000189  \n",
       "3                           0.000189  \n",
       "4                           0.000189  \n",
       "5                          -0.173175  "
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(E0.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. class', index = 'Mean elasticity of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For IPDL the mean elasticities are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean elasticity wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean elasticity of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.003272</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>0.000197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.147491</td>\n",
       "      <td>-0.270553</td>\n",
       "      <td>0.057264</td>\n",
       "      <td>0.040363</td>\n",
       "      <td>0.017425</td>\n",
       "      <td>0.008009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.144136</td>\n",
       "      <td>0.062392</td>\n",
       "      <td>-0.255964</td>\n",
       "      <td>0.029663</td>\n",
       "      <td>0.014886</td>\n",
       "      <td>0.004887</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.143833</td>\n",
       "      <td>0.064125</td>\n",
       "      <td>0.040621</td>\n",
       "      <td>-0.263520</td>\n",
       "      <td>0.012624</td>\n",
       "      <td>0.002318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.143767</td>\n",
       "      <td>0.063279</td>\n",
       "      <td>0.042897</td>\n",
       "      <td>0.026371</td>\n",
       "      <td>-0.282027</td>\n",
       "      <td>0.005714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.143987</td>\n",
       "      <td>0.048335</td>\n",
       "      <td>0.029776</td>\n",
       "      <td>0.010685</td>\n",
       "      <td>0.008115</td>\n",
       "      <td>-0.240898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean elasticity wrt. class         0         1         2         3         4  \\\n",
       "Mean elasticity of class                                                       \n",
       "0                          -0.003272  0.001090  0.000970  0.000701  0.000313   \n",
       "1                           0.147491 -0.270553  0.057264  0.040363  0.017425   \n",
       "2                           0.144136  0.062392 -0.255964  0.029663  0.014886   \n",
       "3                           0.143833  0.064125  0.040621 -0.263520  0.012624   \n",
       "4                           0.143767  0.063279  0.042897  0.026371 -0.282027   \n",
       "5                           0.143987  0.048335  0.029776  0.010685  0.008115   \n",
       "\n",
       "Mean elasticity wrt. class         5  \n",
       "Mean elasticity of class              \n",
       "0                           0.000197  \n",
       "1                           0.008009  \n",
       "2                           0.004887  \n",
       "3                           0.002318  \n",
       "4                           0.005714  \n",
       "5                          -0.240898  "
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(E1.mean(axis = 0)).rename_axis(columns = 'Mean elasticity wrt. class', index = 'Mean elasticity of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diversion ratios\n",
    "\n",
    "We now visualize the implied diversion ratios $\\mathcal{D}$. If $\\bar D_{c\\ell}$ denotes the sum of choice probability weigthed diversion ratios, then we have as above that $\\bar D_{c\\ell} = \\sum_{j}\\sum_{k} \\mathrm{1}_{\\{j\\in c\\}} \\mathrm{1}_{\\{k\\in \\ell\\}} q_j q_k \\mathcal{D}_{jk}$ i.e. more generally $\\bar D = (\\psi^{\\text{class}} \\circ q) \\mathcal{D} (\\psi^{\\text{class}} \\circ q).'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DiversionRatio_agg(data, Theta, q, x, psi, direction_var, market_id = 'market', product_id = 'co', char_number = K-1, model = 'IPDL', outside_option = True):\n",
    "    ''' \n",
    "    '''\n",
    "\n",
    "    dq_du_agg = ccp_directionalgrad(data, Theta, q, x, psi, direction_var, market_id, product_id, model, outside_option)[1]\n",
    "    D_agg = {}\n",
    "\n",
    "    for t in np.arange(T):\n",
    "        D_agg[t] = -100*np.einsum('cl,c->cl', dq_du_agg[t], 1./np.diag(dq_du_agg[t]))\n",
    "\n",
    "    return D_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_IPDLagg = DiversionRatio_agg(dat, ThetaOptBLP, qOpt, z_logit, Psi, 'cla', char_number = pr_index)\n",
    "D_Logitagg = DiversionRatio_agg(dat, LogitBLP_beta, logit.logit_ccp(LogitBLP_beta, z_logit), z_logit, Psi, 'cla', char_number = pr_index, model = 'Logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "D0, D1 = np.empty((T, T_agg, T_agg)), np.empty((T, T_agg, T_agg))\n",
    "for t in np.arange(T):\n",
    "    D0[t,:,:] = D_Logitagg[t]\n",
    "    D1[t,:,:] = D_IPDLagg[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D0p = {j : (D0.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)} # Finds j'th entry in each of the elasticity matrices of individuals i.\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D0p[j], num_bins, range = (np.quantile(D0p[j], 0.10), np.quantile(D0p[j], 0.90)), color = 'r', alpha = 1) # Logit is red\n",
    "    axes[p].vlines(0, 0, 25, 'g', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled Logit diversion ratios by class')\n",
    "fig.supxlabel('Diversion ratio from class')\n",
    "fig.supylabel('Diversion ratio to class')\n",
    "\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 36 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "D1p = {j : (D1.reshape((T, T_agg**2))[:,j]).flatten() for j in np.arange(T_agg**2)}\n",
    "\n",
    "j_pairs = iter.product(np.arange(T_agg), np.arange(T_agg))\n",
    "num_bins = 25\n",
    "\n",
    "fig, axes = plt.subplots(T_agg, T_agg, sharex=False, sharey=False)\n",
    "\n",
    "for p, j in zip(j_pairs, np.arange(T_agg**2)):\n",
    "    axes[p].hist(D1p[j], num_bins, range = (np.quantile(D1p[j], 0.10), np.quantile(D1p[j], 0.90)), color = 'b', alpha = 1) # IPDL is blue\n",
    "    axes[p].vlines(0, 0, 25, 'red', 'dotted')\n",
    "    axes[p].get_xaxis().set_visible(False)\n",
    "    axes[p].get_yaxis().set_visible(False)\n",
    "\n",
    "fig.suptitle('Histograms of pooled IPDL diversion ratios by class')\n",
    "fig.supxlabel('Diversion ratio from class')\n",
    "fig.supylabel('Diversion ratio to class')\n",
    "if OO:\n",
    "    fig.text(0.11, 0.82, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.68, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.56, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.42, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.3, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.18, 0.9, '0', ha = 'center', va = 'center')\n",
    "    fig.text(0.32, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.46, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.58, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.72, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "else:\n",
    "    fig.text(0.11, 0.8, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.64, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.48, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.32, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.11, 0.16, '5', ha = 'center', va = 'center')\n",
    "    fig.text(0.2, 0.9, '1', ha = 'center', va = 'center')\n",
    "    fig.text(0.36, 0.9, '2', ha = 'center', va = 'center')\n",
    "    fig.text(0.52, 0.9, '3', ha = 'center', va = 'center')\n",
    "    fig.text(0.68, 0.9, '4', ha = 'center', va = 'center')\n",
    "    fig.text(0.84, 0.9, '5', ha = 'center', va = 'center')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We also calculate the mean diversion ratios within each class. For the Logit model these are given as..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean diversion ratio wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean diversion ratio of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>32.977441</td>\n",
       "      <td>30.037606</td>\n",
       "      <td>21.805955</td>\n",
       "      <td>9.610813</td>\n",
       "      <td>5.568185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>98.716873</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.579788</td>\n",
       "      <td>0.413296</td>\n",
       "      <td>0.180329</td>\n",
       "      <td>0.109713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.685594</td>\n",
       "      <td>0.611145</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.413182</td>\n",
       "      <td>0.180340</td>\n",
       "      <td>0.109739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>98.522081</td>\n",
       "      <td>0.609950</td>\n",
       "      <td>0.578479</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.179993</td>\n",
       "      <td>0.109496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>98.294337</td>\n",
       "      <td>0.608266</td>\n",
       "      <td>0.576919</td>\n",
       "      <td>0.411271</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.109208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>98.225371</td>\n",
       "      <td>0.607820</td>\n",
       "      <td>0.576488</td>\n",
       "      <td>0.410944</td>\n",
       "      <td>0.179377</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean diversion ratio wrt. class           0           1           2  \\\n",
       "Mean diversion ratio of class                                         \n",
       "0                               -100.000000   32.977441   30.037606   \n",
       "1                                 98.716873 -100.000000    0.579788   \n",
       "2                                 98.685594    0.611145 -100.000000   \n",
       "3                                 98.522081    0.609950    0.578479   \n",
       "4                                 98.294337    0.608266    0.576919   \n",
       "5                                 98.225371    0.607820    0.576488   \n",
       "\n",
       "Mean diversion ratio wrt. class           3           4           5  \n",
       "Mean diversion ratio of class                                        \n",
       "0                                 21.805955    9.610813    5.568185  \n",
       "1                                  0.413296    0.180329    0.109713  \n",
       "2                                  0.413182    0.180340    0.109739  \n",
       "3                               -100.000000    0.179993    0.109496  \n",
       "4                                  0.411271 -100.000000    0.109208  \n",
       "5                                  0.410944    0.179377 -100.000000  "
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D0.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. class', index = 'Mean diversion ratio of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For the IPDL model the mean diversion ratios are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Mean diversion ratio wrt. class</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mean diversion ratio of class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-100.000000</td>\n",
       "      <td>34.330387</td>\n",
       "      <td>29.323832</td>\n",
       "      <td>21.240174</td>\n",
       "      <td>9.522362</td>\n",
       "      <td>5.583245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55.261252</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>20.668214</td>\n",
       "      <td>14.863655</td>\n",
       "      <td>6.323202</td>\n",
       "      <td>2.883676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>56.457882</td>\n",
       "      <td>24.247124</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>11.573589</td>\n",
       "      <td>5.796664</td>\n",
       "      <td>1.924741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54.696419</td>\n",
       "      <td>24.143564</td>\n",
       "      <td>15.447487</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>4.817607</td>\n",
       "      <td>0.894924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51.145459</td>\n",
       "      <td>22.101523</td>\n",
       "      <td>15.187311</td>\n",
       "      <td>9.388808</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>2.176899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>59.991331</td>\n",
       "      <td>20.059291</td>\n",
       "      <td>12.126499</td>\n",
       "      <td>4.403225</td>\n",
       "      <td>3.419654</td>\n",
       "      <td>-100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Mean diversion ratio wrt. class           0           1           2  \\\n",
       "Mean diversion ratio of class                                         \n",
       "0                               -100.000000   34.330387   29.323832   \n",
       "1                                 55.261252 -100.000000   20.668214   \n",
       "2                                 56.457882   24.247124 -100.000000   \n",
       "3                                 54.696419   24.143564   15.447487   \n",
       "4                                 51.145459   22.101523   15.187311   \n",
       "5                                 59.991331   20.059291   12.126499   \n",
       "\n",
       "Mean diversion ratio wrt. class           3           4           5  \n",
       "Mean diversion ratio of class                                        \n",
       "0                                 21.240174    9.522362    5.583245  \n",
       "1                                 14.863655    6.323202    2.883676  \n",
       "2                                 11.573589    5.796664    1.924741  \n",
       "3                               -100.000000    4.817607    0.894924  \n",
       "4                                  9.388808 -100.000000    2.176899  \n",
       "5                                  4.403225    3.419654 -100.000000  "
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(D1.mean(axis = 0)).rename_axis(columns = 'Mean diversion ratio wrt. class', index = 'Mean diversion ratio of class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "To conclude we compare parameter estimates from the above models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logit_MLE</th>\n",
       "      <th>IPDL_MLE</th>\n",
       "      <th>FKN</th>\n",
       "      <th>IPDL_BLP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>in_out</th>\n",
       "      <td>-2.468*** (4e-05)</td>\n",
       "      <td>-2.545 (5.43585)</td>\n",
       "      <td>-10.224*** (0.00334)</td>\n",
       "      <td>-11.432*** (0.0341)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cy</th>\n",
       "      <td>-0.318*** (2e-05)</td>\n",
       "      <td>-0.318 (2.79781)</td>\n",
       "      <td>-0.459*** (0.00169)</td>\n",
       "      <td>-0.782*** (0.01964)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hp</th>\n",
       "      <td>-0.457*** (2e-05)</td>\n",
       "      <td>-0.457 (3.15701)</td>\n",
       "      <td>-3.322*** (0.0024)</td>\n",
       "      <td>-5.179*** (0.02399)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>we</th>\n",
       "      <td>-0.948*** (2e-05)</td>\n",
       "      <td>-0.945 (2.89147)</td>\n",
       "      <td>-0.027*** (0.00157)</td>\n",
       "      <td>0.097*** (0.01993)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>le</th>\n",
       "      <td>-1.928*** (3e-05)</td>\n",
       "      <td>-1.953 (4.01604)</td>\n",
       "      <td>-2.092*** (0.00168)</td>\n",
       "      <td>-2.245*** (0.02282)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wi</th>\n",
       "      <td>-2.056*** (3e-05)</td>\n",
       "      <td>-2.093 (5.26651)</td>\n",
       "      <td>5.44*** (0.00315)</td>\n",
       "      <td>5.756*** (0.03346)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>he</th>\n",
       "      <td>-2.021*** (3e-05)</td>\n",
       "      <td>-2.081 (3.92949)</td>\n",
       "      <td>0.275*** (0.00196)</td>\n",
       "      <td>0.825*** (0.02717)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>li</th>\n",
       "      <td>-0.715*** (1e-05)</td>\n",
       "      <td>-0.742 (1.60313)</td>\n",
       "      <td>-1.025*** (0.00111)</td>\n",
       "      <td>-0.851*** (0.01211)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sp</th>\n",
       "      <td>-1.373*** (3e-05)</td>\n",
       "      <td>-1.39 (3.13216)</td>\n",
       "      <td>2.985*** (0.00224)</td>\n",
       "      <td>4.581*** (0.02574)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ac</th>\n",
       "      <td>0.01*** (1e-05)</td>\n",
       "      <td>-0.054 (1.33363)</td>\n",
       "      <td>0.786*** (0.00081)</td>\n",
       "      <td>1.23*** (0.01291)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pr</th>\n",
       "      <td>0.847*** (1e-05)</td>\n",
       "      <td>0.844 (1.09137)</td>\n",
       "      <td>-0.228*** (0.00104)</td>\n",
       "      <td>-0.148*** (0.00233)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_in_out</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.109 (0.47982)</td>\n",
       "      <td>0.736*** (0.00025)</td>\n",
       "      <td>0.858*** (0.00199)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cy</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.009 (0.27073)</td>\n",
       "      <td>-0.106*** (0.00015)</td>\n",
       "      <td>-0.077*** (0.00131)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_hp</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.068 (0.34245)</td>\n",
       "      <td>0.065*** (0.0002)</td>\n",
       "      <td>0.16*** (0.00177)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_we</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.041 (0.30303)</td>\n",
       "      <td>0.035*** (0.00016)</td>\n",
       "      <td>0.004** (0.00155)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_le</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.082 (0.38956)</td>\n",
       "      <td>-0.085*** (0.00015)</td>\n",
       "      <td>-0.035*** (0.00151)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_wi</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.131 (0.28188)</td>\n",
       "      <td>-0.055*** (0.00012)</td>\n",
       "      <td>-0.078*** (0.00116)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_he</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.025 (0.20183)</td>\n",
       "      <td>-0.076*** (9e-05)</td>\n",
       "      <td>-0.072*** (0.00081)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_li</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.003 (0.15439)</td>\n",
       "      <td>0.045*** (0.0001)</td>\n",
       "      <td>0.026*** (0.00091)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_sp</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.073 (0.25448)</td>\n",
       "      <td>-0.024*** (0.00015)</td>\n",
       "      <td>-0.038*** (0.00151)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_ac</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>-0.198 (0.16537)</td>\n",
       "      <td>-0.033*** (0.00011)</td>\n",
       "      <td>-0.035*** (0.00109)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_brand</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.058 (0.22784)</td>\n",
       "      <td>0.258*** (0.00023)</td>\n",
       "      <td>0.178*** (0.00096)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_home</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.03 (0.22976)</td>\n",
       "      <td>-0.22*** (0.00022)</td>\n",
       "      <td>-0.381*** (0.00117)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group_cla</th>\n",
       "      <td>- (-)</td>\n",
       "      <td>0.157 (0.40932)</td>\n",
       "      <td>-0.067*** (0.00013)</td>\n",
       "      <td>-0.103*** (0.0014)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Logit_MLE          IPDL_MLE                   FKN  \\\n",
       "in_out        -2.468*** (4e-05)  -2.545 (5.43585)  -10.224*** (0.00334)   \n",
       "cy            -0.318*** (2e-05)  -0.318 (2.79781)   -0.459*** (0.00169)   \n",
       "hp            -0.457*** (2e-05)  -0.457 (3.15701)    -3.322*** (0.0024)   \n",
       "we            -0.948*** (2e-05)  -0.945 (2.89147)   -0.027*** (0.00157)   \n",
       "le            -1.928*** (3e-05)  -1.953 (4.01604)   -2.092*** (0.00168)   \n",
       "wi            -2.056*** (3e-05)  -2.093 (5.26651)     5.44*** (0.00315)   \n",
       "he            -2.021*** (3e-05)  -2.081 (3.92949)    0.275*** (0.00196)   \n",
       "li            -0.715*** (1e-05)  -0.742 (1.60313)   -1.025*** (0.00111)   \n",
       "sp            -1.373*** (3e-05)   -1.39 (3.13216)    2.985*** (0.00224)   \n",
       "ac              0.01*** (1e-05)  -0.054 (1.33363)    0.786*** (0.00081)   \n",
       "pr             0.847*** (1e-05)   0.844 (1.09137)   -0.228*** (0.00104)   \n",
       "group_in_out              - (-)   0.109 (0.47982)    0.736*** (0.00025)   \n",
       "group_cy                  - (-)  -0.009 (0.27073)   -0.106*** (0.00015)   \n",
       "group_hp                  - (-)   0.068 (0.34245)     0.065*** (0.0002)   \n",
       "group_we                  - (-)   0.041 (0.30303)    0.035*** (0.00016)   \n",
       "group_le                  - (-)  -0.082 (0.38956)   -0.085*** (0.00015)   \n",
       "group_wi                  - (-)  -0.131 (0.28188)   -0.055*** (0.00012)   \n",
       "group_he                  - (-)  -0.025 (0.20183)     -0.076*** (9e-05)   \n",
       "group_li                  - (-)  -0.003 (0.15439)     0.045*** (0.0001)   \n",
       "group_sp                  - (-)  -0.073 (0.25448)   -0.024*** (0.00015)   \n",
       "group_ac                  - (-)  -0.198 (0.16537)   -0.033*** (0.00011)   \n",
       "group_brand               - (-)   0.058 (0.22784)    0.258*** (0.00023)   \n",
       "group_home                - (-)    0.03 (0.22976)    -0.22*** (0.00022)   \n",
       "group_cla                 - (-)   0.157 (0.40932)   -0.067*** (0.00013)   \n",
       "\n",
       "                         IPDL_BLP  \n",
       "in_out        -11.432*** (0.0341)  \n",
       "cy            -0.782*** (0.01964)  \n",
       "hp            -5.179*** (0.02399)  \n",
       "we             0.097*** (0.01993)  \n",
       "le            -2.245*** (0.02282)  \n",
       "wi             5.756*** (0.03346)  \n",
       "he             0.825*** (0.02717)  \n",
       "li            -0.851*** (0.01211)  \n",
       "sp             4.581*** (0.02574)  \n",
       "ac              1.23*** (0.01291)  \n",
       "pr            -0.148*** (0.00233)  \n",
       "group_in_out   0.858*** (0.00199)  \n",
       "group_cy      -0.077*** (0.00131)  \n",
       "group_hp        0.16*** (0.00177)  \n",
       "group_we        0.004** (0.00155)  \n",
       "group_le      -0.035*** (0.00151)  \n",
       "group_wi      -0.078*** (0.00116)  \n",
       "group_he      -0.072*** (0.00081)  \n",
       "group_li       0.026*** (0.00091)  \n",
       "group_sp      -0.038*** (0.00151)  \n",
       "group_ac      -0.035*** (0.00109)  \n",
       "group_brand    0.178*** (0.00096)  \n",
       "group_home    -0.381*** (0.00117)  \n",
       "group_cla      -0.103*** (0.0014)  "
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_index = pr_index + 1\n",
    "Logit_nest = ['-' for i in np.arange(G)]\n",
    "Logitbeta_show = [*(np.round(Logit_beta[:beta_index], decimals = 3).astype('str')), *Logit_nest]\n",
    "Logitse_show = [*(np.round(Logit_SE[:beta_index], decimals=5).astype('str')), *Logit_nest]\n",
    "Logitp_show = [*Logit_p[:beta_index], *[1 for i in np.arange(G)]]\n",
    "\n",
    "pd.DataFrame({\n",
    "    'Logit_MLE' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip(Logitp_show, Logitbeta_show, Logitse_show)],\n",
    "    'IPDL_MLE': [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*Similarity_p[:beta_index], *Similarity_p[K:]], [*(np.round(Similarity_theta[:beta_index], decimals = 3).astype('str')), *(np.round(Similarity_theta[K:], decimals = 3).astype('str'))], [*(np.round(Similarity_SE[:beta_index], decimals=5).astype('str')), *(np.round(Similarity_SE[K:], decimals=5).astype('str'))])],\n",
    "    'FKN' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*FKN_p[:beta_index], *FKN_p[K:]], [*(np.round(FKN_theta[:beta_index], decimals = 3).astype('str')), *(np.round(FKN_theta[K:], decimals = 3).astype('str'))], [*(np.round(FKN_SE[:beta_index], decimals=5).astype('str')), *(np.round(FKN_SE[K:], decimals=5).astype('str'))])],\n",
    "    'IPDL_BLP' : [par + '***' + ' (' + se + ')' if p < 0.01 else par + '**' + ' (' + se + ')' if p < 0.05 else par + '*' + ' (' + se + ')' if p < 0.1 else par + ' (' + se + ')' for p,par,se in zip([*OptBLP_p[:beta_index], *OptBLP_p[K:]], [*(np.round(ThetaOptBLP[:beta_index], decimals = 3).astype('str')), *(np.round(ThetaOptBLP[K:], decimals = 3).astype('str'))], [*(np.round(SEOptBLP[:beta_index], decimals=5).astype('str')), *(np.round(SEOptBLP[K:], decimals=5).astype('str'))])]\n",
    "}, \n",
    "index = [*x_vars[:beta_index], *['group_' + par for par in nest_vars]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
